[
  {
    "objectID": "index.html#useful-tutorials",
    "href": "index.html#useful-tutorials",
    "title": "Bioinformatics guidance page",
    "section": "Useful tutorials",
    "text": "Useful tutorials\n\nGetting started\n\nGetting started on the Crunchomics server\nGetting started with the command line\nVersion control with git\nA tutorial on using AWK, an excellent command line tool for filtering tables, extracting patterns, etc… If you want to follow this tutorial then you can download the required input files from here\n\n\n\nUsing R\n\nAn R cookbook including some example files if you want to code along\nTutorial on data manipulation with dplyr\nTutorial on data visualization with ggplot2\n\n\n\nBioinformatic workflows\n\nFrom sequence file to OTU table with Qiime\nAssembling a metagenome\nMetagenomic binning\nAnnotating microbial genomes\nHow to do generate a species tree"
  },
  {
    "objectID": "index.html#bioinformatic-tools-a-z",
    "href": "index.html#bioinformatic-tools-a-z",
    "title": "Bioinformatics guidance page",
    "section": "Bioinformatic tools A-Z",
    "text": "Bioinformatic tools A-Z\n\nChopper\nFAMA\nFastQC\nInterproscan\nITSx\nKraken2\nMETABOLIC\nMinimap2\nNanoClass\nNanoITS\nNanoPlot\nNanoQC\nPorechop"
  },
  {
    "objectID": "source/snakemake/tutorial.html",
    "href": "source/snakemake/tutorial.html",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "Notes following the snakemake tutorial outlined here. Also, check out this snakemake presentation for more information.\nformat: pdf: toc: true number-sections: true colorlinks: true\n\n\n\nPython-based workflow system\nCommand-line interface\nScheduling algorithm suitable to work on clusters and batch systems allowing scaling of workflows\nReproducibility:\n\nInstall required software and all dependencies in exact versions\nSoftware install from different sources\nNormalize installation via build script\nNo admin rights needed\nIsolated environments\n\nIntegrates with Conda package manager and the Singularity container engine\nGeneration of self-contained HTML reports\nWorkflows are defined in terms of rules that define how to create output files from input files. The are executed in three phases:\n\nInitialization (parsing)\nDAG phase (DAG is build)\nScheduling phase (execution of DAG)\n\nDependencies\n\nbetween the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be automatically parallelized\nDependencies are determined top-down, i.e. the rule on top which defines the input gets passed to a rule further down were files are for example sorted\n\nJob execution only if:\n\nOutput file is target and does notexist\nInput file newer than output file\nRule has been modified\nExecution is enforced\n\nEasy distribution of workflows via git repositories with standardized folder structure\n\n\n\n\n\nIt is best practice to have subsequent steps of a workflow in separate, unique, output folders. This keeps the working directory structured. Further, such unique prefixes allow Snakemake to quickly discard most rules in its search for rules that can provide the requested input. This accelerates the resolution of the rule dependencies in a workflow.\nSnakemake (&gt;=5.11) comes with a code quality checker (a so called linter), that analyzes your workflow and highlights issues that should be solved in order to follow best practices, achieve maximum readability, and reproducibility. The linter can be invoked with snakemake --lint\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nFurther, scripts should be stored in a subfolder workflow/scripts and notebooks in a subfolder workflow/notebooks\nConda environments (see Integrated Package Management) should be stored in a subfolder workflow/envs\nWhen publishing your workflow in a Github repository, it is a good idea to add some minimal test data and configure Github Actions for continuously testing the workflow on each new commit. For this purpose, we provide predefined Github actions for both running tests and linting here, as well as formatting here.\nConfiguration of a workflow should be handled via config files and, if needed, tabular configuration like sample sheets (either via Pandas or PEPs). Use such configuration for metadata and experiement information, not for runtime specific configuration like threads, resources and output folders. For those, just rely on Snakemake’s CLI arguments like –set-threads, –set-resources, –set-default-resources, and –directory. This makes workflows more readable, scalable, and portable.\nTry to keep filenames short (thus easier on the eye), but informative. Avoid mixing of too many special characters (e.g. decide whether to use _ or - as a separator and do that consistently throughout the workflow).\nTry to keep Python code like helper functions separate from rules (e.g. in a workflow/rules/common.smk file). This way, you help non-experts to read the workflow without needing to parse internals that are irrelevant for them. The helper function names should be chosen in a way that makes them sufficiently informative without looking at their content. Also avoid lambda expressions inside of rules.\nMake use of Snakemake wrappers whenever possible. Consider contributing to the wrapper repo whenever you have a rule that reoccurs in at least two of your workflows. More about wrappers can be found here.\n\nFollow this link to view the recommended structure to store a workflow in a git repository. A snakemake workflow template can be found here. Conda determines the priority of channels depending on their order of appearance in the channels list. For instance, the channel that comes first in the list gets the highest priority.\n├── .gitignore\n├── README.md\n├── LICENSE.md\n├── workflow\n│   ├── rules\n|   │   ├── module1.smk\n|   │   └── module2.smk\n│   ├── envs\n|   │   ├── tool1.yaml\n|   │   └── tool2.yaml\n│   ├── scripts\n|   │   ├── script1.py\n|   │   └── script2.R\n│   ├── notebooks\n|   │   ├── notebook1.py.ipynb\n|   │   └── notebook2.r.ipynb\n│   ├── report\n|   │   ├── plot1.rst\n|   │   └── plot2.rst\n|   └── Snakefile\n├── config\n│   ├── config.yaml\n│   └── some-sheet.tsv\n├── results\n└── resources\n\n\n\n\n-n --dry-run: Snakemake will only show the execution plan instead of actually performing the steps\n-p: print the resulting shell command for illustration.\n--cores {INTEGER}: The numbers of course to use. Must be part of every executed workflow\n--forcerun allows to repeat a run even if the output files already exist from a given rule\n--foreall enforces a complete re-execution of the workflow.\n\n\n\n\nA Snakemake workflow is defined by specifying rules in a Snakefile. Rules decompose the workflow into small steps (for example, the application of a single tool) by specifying how to create sets of output files from sets of input files. Snakemake automatically determines the dependencies between the rules by matching file names.\nAll added syntactic structures begin with a keyword followed by a code block that is either in the same line or indented and consisting of multiple lines. The resulting syntax resembles that of original Python constructs.\nIn this workflow we perform a read mapping workflow.\n\n\n\n\n\nNotice:\n\nSetup done in WSL\nWe will use Conda to create an isolated environment with all the required software for this tutorial.\nMambaforge already installed, if instructions are needed go here\n\n\n\n\n\n#define wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/Snakemake\"\ncd $wdir\n\n#activate conda enviornment (for this to work, see code cell below)\nconda activate snakemake-tutorial\n\n\n#download the example data\ncurl -L https://api.github.com/repos/snakemake/snakemake-tutorial-data/tarball -o snakemake-tutorial-data.tar.gz\n\n#extract the data\ntar --wildcards -xf snakemake-tutorial-data.tar.gz --strip 1 \"*/data\" \"*/environment.yaml\"\n\n#create conda env (/home/ndombrowski/mambaforge/envs/snakemake-tutorial)\nmamba env create --name snakemake-tutorial --file environment.yaml\n\n\n\n\n\n\n\n\n\nTo write our first rule, we create a new document and use it to map a sample to a reference genome using bwa mem.\nA snakemake rule:\n\nHas a name, i.e. bwa_map\nHas a number of directives (i.e. input, output, shell)\ninput can provide a list of files that are expected to be used. Important: have a comma between input/output items.\nHas a shell command, i.e. the shell command to be executed\nIf a rule has multiple input files, they will be concatenated and separated by a whitespace (i.e. we get data/genome.fa data/samples/A.fastq)\n\nBasics: - Snakemake applies the rules given in the Snakefile in a top-down way - The application of a rule to generate a set of output files is called job - Note that Snakemake automatically creates missing directories before jobs are executed. - Snakemake works backwards from requested output, and not from available input. Thus, it does not automatically infer all possible output from, for example, the fastq files in the data folder.\n\nnano Snakefile\n\nContent:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/A.fastq\"\n    output:\n        \"mapped_reads/A.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#perform dry-run workflow\nsnakemake -np mapped_reads/A.bam\n\n#execute workflow \nsnakemake --cores 1 mapped_reads/A.bam\n\nSince mapped_reads/A.bam now exists snakemake WILL NOT try to create this file again. Snakemake only re-runs jobs if one of the input files is newer than one of the output files or one of the input files will be updated by another job.\n\n\n\n\nIf we want to work on more than one sample not only data/samples/A.fastq then we can generalize rules using named wildcards. Below, we for example replace the A with {sample}\nNote that you can have multiple wildcards in your file paths, however, to avoid conflicts with other jobs of the same rule, all output files of a rule have to contain exactly the same wildcards.\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#execute command (dry-run)\nsnakemake -np mapped_reads/B.bam\n\nIf we run this, we will see that bwa will use an input name that is based on the B.bam name, i.e. data/samples/B.fastq. You can see how the character B is propagaged through the input file names and the names in the shell command.\n\n#specify more than one target\nsnakemake -np mapped_reads/A.bam mapped_reads/B.bam\n\n#same command but more condensed using bash magic \nsnakemake -np mapped_reads/{A,B}.bam\n\nIn both cases, you will see that Snakemake only proposes to create the output file mapped_reads/B.bam. This is because you already executed the workflow before (see the previous step) and no input file is newer than the output file mapped_reads/A.bam.\n\n\n\nAdd a new rule:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam\"\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nComments:\n\nIn the shell command above we split the string into two lines, which are however automatically concatenated into one by Python. This is a handy pattern to avoid too long shell command lines. When using this, make sure to have a trailing whitespace in each line but the last, in order to avoid arguments to become not properly separated.\nWith sorted_reads/{wildcards.sample} we extract the value of the sample wildcard, i.e. we get sorted_reads/B\n\n\n#dry run\nsnakemake -np sorted_reads/B.bam\n\n\n\n\nAdd a new rule:\n\nrule samtools_index:\n    input:\n        \"sorted_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam.bai\"\n    shell:\n        \"samtools index {input}\"\n\n\n#dry run \nsnakemake -np sorted_reads/B.bam.bai\n\n#create a DAG\nsnakemake --dag sorted_reads/{A,B}.bam.bai | dot -Tsvg &gt; dag.svg\n\nThe last command will create a visualization of the DAG using the dot command provided by Graphviz. For the given target files, Snakemake specifies the DAG in the dot language and pipes it into the dot command, which renders the definition into SVG format. The rendered DAG is piped into the file dag.svg.\nThe DAG contains a node for each job with the edges connecting them representing the dependencies. The frames of jobs that don’t need to be run (because their output is up-to-date) are dashed. For rules with wildcards, the value of the wildcard for the particular job is displayed in the job node.\n\n\n\nNext, we will aggregate the mapped reads from all samples and jointly call genomic variants on them. Therefore, we will combine the two utilities samtools and bcftools. Snakemake provides a helper function for collecting input files that helps us to describe the aggregation in this step. With\nexpand(\"sorted_reads/{sample}.bam\", sample=SAMPLES)\nwe obtain a list of files where the given pattern “sorted_reads/{sample}.bam” was formatted with the values in a given list of samples SAMPLES, i.e.\n[\"sorted_reads/A.bam\", \"sorted_reads/B.bam\"]\nThis can also be used if multiple wildcards are used i.e.\nexpand(\"sorted_reads/{sample}.{replicate}.bam\", sample=SAMPLES, replicate=[0, 1])\nwould create\n[\"sorted_reads/A.0.bam\", \"sorted_reads/A.1.bam\", \"sorted_reads/B.0.bam\", \"sorted_reads/B.1.bam\"]\nLet’s start with defining the list of samples ad-hoc in plain Python at the top of the Snakefile and add a rule at the end of our Snakefile:\n\nSAMPLES = [\"A\", \"B\"]\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=SAMPLES),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=SAMPLES)\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\nWith multiple input or output files, it is sometimes handy to refer to them separately in the shell command. This can be done by specifying names for input or output files, for example with fa=.... The files can then be referred to in the shell command by name, for example with {input.fa}.\nFor long shell commands like this one, it is advisable to split the string over multiple indented lines. Python will automatically merge it into one. Further, you will notice that the input or output file lists can contain arbitrary Python statements, as long as it returns a string, or a list of strings. Here, we invoke our expand function to aggregate over the aligned reads of all samples.\n\n#create a DAG\nsnakemake --dag calls/all.vcf | dot -Tsvg &gt; dag.svg\n\n\n\n\nUsually, a workflow not only consists of invoking various tools, but also contains custom code to for example calculate summary statistics or create plots. While Snakemake also allows you to directly write Python code inside a rule, it is usually reasonable to move such logic into separate scripts. For this purpose, Snakemake offers the script directive. Add the following rule to your Snakefile:\nWith this rule, we will eventually generate a histogram of the quality scores that have been assigned to the variant calls in the file calls/all.vcf. The actual Python code to generate the plot is hidden in the script scripts/plot-quals.py.\nScript paths are always relative to the referring Snakefile. In the script, all properties of the rule like input, output, wildcards, etc. are available as attributes of a global snakemake object.\n\nrule plot_quals:\n    input:\n        \"calls/all.vcf\"\n    output:\n        \"plots/quals.svg\"\n    script:\n        \"scripts/plot-quals.py\"\n\nCreate the file scripts/plot-quals.py, with the following content:\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom pysam import VariantFile\n\nquals = [record.qual for record in VariantFile(snakemake.input[0])]\nplt.hist(quals)\n\nplt.savefig(snakemake.output[0])\n\nAlthough there are other strategies to invoke separate scripts from your workflow (for example, invoking them via shell commands), the benefit of this is obvious: the script logic is separated from the workflow logic (and can even be shared between workflows), but boilerplate code like the parsing of command line arguments is unnecessary.\nApart from Python scripts, it is also possible to use R scripts. In R scripts, an S4 object named snakemake analogous to the Python case above is available and allows access to input and output files and other parameters. Here, the syntax follows that of S4 classes with attributes that are R lists, for example we can access the first input file with snakemake@input[[1]] (note that the first file does not have index 0 here, because R starts counting from 1). Named input and output files can be accessed in the same way, by just providing the name instead of an index, for example snakemake@input[[“myfile”]].\nFor details and examples, see the External scripts section in the Documentation.\n\n\n\nSo far, we always executed the workflow by specifying a target file at the command line. Apart from filenames, Snakemake also accepts rule names as targets if the requested rule does not have wildcards.\nIt is possible to write target rules collecting particular subsets of the desired results or all results. Moreover, if no target is given at the command line, Snakemake will define the first rule of the Snakefile as the target. Hence, it is best practice to have a rule all at the top of the workflow which has all typically desired target files as input files.\nTo do this add the following to the top of the workflow:\n\nrule all:\n    input:\n        \"plots/quals.svg\"\n\nAnd execute snakemake with:\n\nsnakemake -n\n\nIf we run this the execution plan for creating the file plots/quals.svg, which contains and summarizes all our results, will be shown.\nNote that, apart from Snakemake considering the first rule of the workflow as the default target, the order of rules in the Snakefile is arbitrary and does not influence the DAG of jobs.\n\n\n\n\n#execution \nsnakemake --cores 1\n\n#view output\nwslview plots/quals.svg\n\n#force to re-execute samtools_sort\nsnakemake --cores 1 --forcerun samtools_sort\n\n\n\n\n\n\n\nSnakemake can be made aware of the threads a rule needs with the threads directive. Ie change the bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    threads: 2\n    shell:\n        \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\nThe number of threads can be propagated to the shell command with the familiar braces notation (i.e. {threads}). If no threads directive is given, a rule is assumed to need 1 thread.\nWhen a workflow is executed, the number of threads the jobs need is considered by the Snakemake scheduler. In particular, the scheduler ensures that the sum of the threads of all jobs running at the same time does not exceed a given number of available CPU cores. This number is given with the –cores command line argument, which is mandatory for snakemake calls that actually run the workflow.\nFor example snakemake --cores 10 would execute the workflow with 10 cores. Since the rule bwa_map needs 8 threads, only one job of the rule can run at a time, and the Snakemake scheduler will try to saturate the remaining cores with other jobs like, e.g., samtools_sort.\nThe threads directive in a rule is interpreted as a maximum: when less cores than threads are provided, the number of threads a rule uses will be reduced to the number of given cores.\nIf --cores is given without a number, all available cores are used.\n\nsnakemake --forceall --cores 2\nsnakemake --forceall --cores 4\n\n\n\n\nWe can use a configuration file if we want your workflow to be customizable:\n\nCheck out the full readme here\nCan be provided as JSON or YAML\nThey are used with the configfile direction\nvariable names in the rule and the key name in the configuration file do not have to be identical. Snakemake matches variables in the rule based on their position within the curly braces {}.\n\nA config file is specified by adding this to the top of our Snakemake workflow:\n\nconfigfile: \"config.yaml\"\n\nSnakemake will load the config file and store its contents into a globally available dictionary named config. In our case, it makes sense to specify the samples in config.yaml as:\nsamples:\n    A: data/samples/A.fastq\n    B: data/samples/B.fastq\nAnd remove the statement defining the SAMPLES in the Snakemake file. And change the rule of bcftools_call to:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\n\n\n\nSince we have stored the path to the FASTQ files in the config file, we can also generalize the rule bwa_map to use these paths. This case is different to the rule bcftools_call we modified above. To understand this, it is important to know that Snakemake workflows are executed in three phases.\n\nIn the initialization phase, the files defining the workflow are parsed and all rules are instantiated.\nIn the DAG phase, the directed acyclic dependency graph of all jobs is built by filling wildcards and matching input files to output files.\nIn the scheduling phase, the DAG of jobs is executed, with jobs started according to the available resources.\n\nThe expand functions in the list of input files of the rule bcftools_call are executed during the initialization phase.\nIn this phase, we don’t know about jobs, wildcard values and rule dependencies. Hence, we cannot determine the FASTQ paths for rule bwa_map from the config file in this phase, because we don’t even know which jobs will be generated from that rule.\nInstead, we need to defer the determination of input files to the DAG phase. This can be achieved by specifying an input function instead of a string as inside of the input directive. For the rule bwa_map this works as follows:\n::: {.cell}\ndef get_bwa_map_input_fastqs(wildcards):\n   return config[\"samples\"][wildcards.sample]\n\nrule bwa_map:\n   input:\n       \"data/genome.fa\",\n       get_bwa_map_input_fastqs\n   output:\n       \"mapped_reads/{sample}.bam\"\n   threads: 8\n   shell:\n       \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n:::\nHow wild cards work here:\n\nThe wildcards object is a special Snakemake variable that represents placeholders in rule input and output file names. In your case, you are using {sample} as a placeholder in the rule’s input section, and Snakemake automatically parses this placeholder into a wildcards object.\nWhen you define the function get_bwa_map_input_fastqs(wildcards), you are essentially telling Snakemake that this function accepts a wildcards object as an argument. This allows the function to access and use the wildcards to determine which sample’s input files to fetch.\nWhen Snakemake processes the rule bwa_map, it looks at the rule’s input section and identifies that it contains the {sample} placeholder.\nSnakemake then matches the placeholder to the wildcards object. If you have multiple samples defined in your config.yaml, Snakemake will create separate jobs for each sample, each with a different wildcards.sample value.\nFor each job, the wildcards.sample attribute will be replaced with the actual sample name being processed. For example, if Snakemake is processing the sample “A,” then wildcards.sample will be “A.”\nThe function get_bwa_map_input_fastqs uses config[“samples”][wildcards.sample] to fetch the corresponding input file path for the sample being processed. So, for sample “A,” it would retrieve “data/samples/A.fastq” as the input file path.\n\nBenefits of using python function:\n\nModularity and Reusability: By defining a separate Python function (get_bwa_map_input_fastqs) to retrieve the input files, you create a modular and reusable piece of code. This can be particularly useful if you have multiple rules that need to access the same input files, as you can reuse the function across those rules.\nAbstraction: The function abstracts the details of where the input files come from. In your first version, you had to specify the exact path to the input files in the rule itself. With the function, you only need to specify how to obtain the input files for a given sample, making your rule more abstract and easier to maintain.\nFlexibility: If you need to change the way you obtain input files in the future (for example, if you decide to store them in a different directory or use a different naming convention), you can update the function in one place, and all the rules that use it will automatically adapt.\nReduced Redundancy: Your original version of the rule had repetitive code for specifying input and output paths for each sample. With the function, you reduce redundancy and the potential for errors, as the input file retrieval logic is centralized.\nEasier Testing: You can more easily write unit tests for the input file retrieval function to ensure it works correctly, which can be challenging when input paths are hard-coded in multiple rules.\n\n\n\nIn the data/samples folder, there is an additional sample C.fastq. Add that sample to the config file and see how Snakemake wants to recompute the part of the workflow belonging to the new sample, when invoking with snakemake -n --forcerun bcftools_call\nNote: Snakemake does not automatically rerun jobs when new input files are added as in the excercise below. However, you can get a list of output files that are affected by such changes with snakemake –list-input-changes. To trigger a rerun, this bit of bash magic helps:\n\nsnakemake -n --forcerun $(snakemake --list-input-changes)\n\n\n\n\n\nSometimes shell commands are not only composed of input and output files but require additional parameters.\nWe can set additional parameters need to be set depending on the wildcard values of the job. For this, Snakemake allows to define arbitrary parameters for rules with the params directive. I.e. edit bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    threads: 8\n    shell:\n        \"bwa mem -R '{params.rg}' -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\n\nsnakemake -np --forceall\n\nNext, consider another important parameter, the prior mutation rate (1e-3 per default). It is set via the flag -P of the bcftools call command. Consider making this flag configurable via adding a new key to the config file and using the params directive in the rule bcftools_call to propagate it to the shell command.\nAdd this in the config file:\nbcftools:\n    evalue: 0.001\nChange this in the Snakemakefile:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}\"\n\n\nsnakemake -p --cores 1 --forceall\n\n\n\n\nWhen executing a large workflow, it is usually desirable to store the logging output of each job into a separate file, instead of just printing all logging output to the terminal—when multiple jobs are run in parallel, this would result in chaotic output. For this purpose, Snakemake allows to specify log files for rules. Log files are defined via the log directive and handled similarly to output files, but they are not subject of rule matching and are not cleaned up when a job fails. We modify our rule bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThe shell command is modified to collect STDERR output of both bwa and samtools and pipe it into the file referred to by {log}. Log files must contain exactly the same wildcards as the output files to avoid file name clashes between different jobs of the same rule.\nLet’s add a log to the to the bcftools_call rule as well.\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    log:\n        \"logs/bcftools/all.log\"\n    shell:\n        \"(bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}) 2&gt; {log}\"\n\n\nsnakemake -p --cores 1 --forceall\n\nThe ability to track the provenance of each generated result is an important step towards reproducible analyses. Apart from the report functionality discussed before, Snakemake can summarize various provenance information for all output files of the workflow. The flag –summary prints a table associating each output file with the rule used to generate it, the creation date and optionally the version of the tool used for creation is provided. Further, the table informs about updated input files and changes to the source code of the rule after creation of the output file.\n\nsnakemake -p --cores 1 --forceall --summary\n\n\n\n\nIn our workflow, we create two BAM files for each sample, namely the output of the rules bwa_map and samtools_sort. When not dealing with examples, the underlying data is usually huge. Hence, the resulting BAM files need a lot of disk space and their creation takes some time. To save disk space, you can mark output files as temporary. Snakemake will delete the marked files for you, once all the consuming jobs (that need it as input) have been executed. We use this mechanism for the output file of the rule bwa_map:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThis results in the deletion of the BAM file once the corresponding samtools_sort job has been executed. Since the creation of BAM files via read mapping and sorting is computationally expensive, it is reasonable to protect the final BAM file from accidental deletion or modification. We modify the rule samtools_sort to mark its output file as protected:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        protected(\"sorted_reads/{sample}.bam\")\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nRun Snakemake with the target mapped_reads/A.bam. Although the file is marked as temporary, you will see that Snakemake does not delete it because it is specified as a target file.\n\nsnakemake --forceall --cores 1 mapped_reads/A.bam \n\n\n\n\n\nFor more, check out this documentation.\n\n\nIn order to re-use building blocks or simply to structure large workflows, it is sometimes reasonable to split a workflow into modules. For this, Snakemake provides the include directive to include another Snakefile into the current one, e.g.:\ninclude: \"path/to/other.snakefile\"\nAlternatively, Snakemake allows to define sub-workflows. A sub-workflow refers to a working directory with a complete Snakemake workflow. Output files of that sub-workflow can be used in the current Snakefile. When executing, Snakemake ensures that the output files of the sub-workflow are up-to-date before executing the current workflow. This mechanism is particularly useful when you want to extend a previous analysis without modifying it. For details about sub-workflows, see the documentation.\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nLets put the read mapping into a separate snakefile and use include to make it available in our workflow:\nIn rules add a new file bwa_mem.smk with the following content:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 2\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nRemove the bwa_mem rule from the workflow and add include: \"rules/bwa_mem.smk and run the workflow with :\n\nsnakemake --forceall -p --cores 1\n\n\n\n\nIn order to get a fully reproducible data analysis, it is not sufficient to be able to execute each step and document all used parameters. The used software tools and libraries have to be documented as well. In this tutorial, you have already seen how Conda can be used to specify an isolated software environment for a whole workflow. With Snakemake, you can go one step further and specify Conda environments per rule. This way, you can even make use of conflicting software versions (e.g. combine Python 2 with Python 3).\nIn our example, instead of using an external environment we can specify environments per rule, e.g.:\n\nrule samtools_index:\n  input:\n      \"sorted_reads/{sample}.bam\"\n  output:\n      \"sorted_reads/{sample}.bam.bai\"\n  conda:\n      \"envs/samtools.yaml\"\n  shell:\n      \"samtools index {input}\"\n\nwith envs/samtools.yaml defined as:\n\nchannels:\n  - bioconda\n  - conda-forge\ndependencies:\n  - samtools =1.9\n\nWhen executing the workflow with snakemake --use-conda --cores 1 it will automatically create required environments and activate them before a job is executed. It is best practice to specify at least the major and minor version of any packages in the environment definition.\nSpecifying environments per rule in this way has two advantages: - First, the workflow definition also documents all used software versions. - Second, a workflow can be re-executed (without admin rights) on a vanilla system, without installing any prerequisites apart from Snakemake and Miniconda.\n\n\n\nIn order to simplify the utilization of popular tools, Snakemake provides a repository of so-called wrappers (the Snakemake wrapper repository). A wrapper is a short script that wraps (typically) a command line application and makes it directly addressable from within Snakemake. For this, Snakemake provides the wrapper directive that can be used instead of shell, script, or run. For example, the rule bwa_map could alternatively look like this:\n\nrule bwa_mem:\n  input:\n      ref=\"data/genome.fa\",\n      sample=lambda wildcards: config[\"samples\"][wildcards.sample]\n  output:\n      temp(\"mapped_reads/{sample}.bam\")\n  log:\n      \"logs/bwa_mem/{sample}.log\"\n  params:\n      \"-R '@RG\\tID:{sample}\\tSM:{sample}'\"\n  threads: 8\n  wrapper:\n      \"0.15.3/bio/bwa/mem\"\n\n\n\n\nBy default, Snakemake executes jobs on the local machine it is invoked on. Alternatively, it can execute jobs in distributed environments, e.g., compute clusters or batch systems. If the nodes share a common file system, Snakemake supports three alternative execution modes.\nIn cluster environments, compute jobs are usually submitted as shell scripts via commands like qsub. Snakemake provides a generic mode to execute on such clusters. By invoking Snakemake with snakemake --cluster qsub --jobs 100 each job will be compiled into a shell script that is submitted with the given command (here qsub). The –jobs flag limits the number of concurrently submitted jobs to 100.\nFind out more for SLURM submissions.\n\n\n\nSometimes it is useful to constrain the values a wildcard can have. This can be achieved by adding a regular expression that describes the set of allowed wildcard values. For example, the wildcard sample in the output file \"sorted_reads/{sample}.bam\" can be constrained to only allow alphanumeric sample names as \"sorted_reads/{sample,[A-Za-z0-9]+}.bam\".\nConstraints may be defined per rule or globally using the wildcard_constraints keyword, as demonstrated in Wildcards. This mechanism helps to solve two kinds of ambiguity.\n\nIt can help to avoid ambiguous rules, i.e. two or more rules that can be applied to generate the same output file. Other ways of handling ambiguous rules are described in the Section Handling Ambiguous Rules.\nIt can help to guide the regular expression based matching so that wildcards are assigned to the right parts of a file name. Consider the output file {sample}.{group}.txt and assume that the target file is A.1.normal.txt. It is not clear whether dataset=“A.1” and group=“normal” or dataset=“A” and group=“1.normal” is the right assignment. Here, constraining the dataset wildcard by {sample,[A-Z]+}.{group}solves the problem.\n\nWhen dealing with ambiguous rules, it is best practice to first try to solve the ambiguity by using a proper file structure, for example, by separating the output files of different steps in different directories.\n\n\n\nA basic report contains runtime statistics, a visualization of the workflow topology, used software and data provenance information.\n\nsnakemake --report report.html --rerun-incomplete\n\nIn addition, you can mark any output file generated in your workflow for inclusion into the report. It will be encoded directly into the report, such that it can be, e.g., emailed as a self-contained document. The reader (e.g., a collaborator of yours) can at any time download the enclosed results from the report for further use, e.g., in a manuscript you write together.\nLet’s mark the output file “results/plots/quals.svg” for inclusion by replacing it with report(“results/plots/quals.svg”, caption=“report/calling.rst”) and adding a file report/calling.rst, containing some description of the output file. This description will be presented as caption in the resulting report."
  },
  {
    "objectID": "source/snakemake/tutorial.html#general",
    "href": "source/snakemake/tutorial.html#general",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "Python-based workflow system\nCommand-line interface\nScheduling algorithm suitable to work on clusters and batch systems allowing scaling of workflows\nReproducibility:\n\nInstall required software and all dependencies in exact versions\nSoftware install from different sources\nNormalize installation via build script\nNo admin rights needed\nIsolated environments\n\nIntegrates with Conda package manager and the Singularity container engine\nGeneration of self-contained HTML reports\nWorkflows are defined in terms of rules that define how to create output files from input files. The are executed in three phases:\n\nInitialization (parsing)\nDAG phase (DAG is build)\nScheduling phase (execution of DAG)\n\nDependencies\n\nbetween the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be automatically parallelized\nDependencies are determined top-down, i.e. the rule on top which defines the input gets passed to a rule further down were files are for example sorted\n\nJob execution only if:\n\nOutput file is target and does notexist\nInput file newer than output file\nRule has been modified\nExecution is enforced\n\nEasy distribution of workflows via git repositories with standardized folder structure"
  },
  {
    "objectID": "source/snakemake/tutorial.html#best-practices",
    "href": "source/snakemake/tutorial.html#best-practices",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "It is best practice to have subsequent steps of a workflow in separate, unique, output folders. This keeps the working directory structured. Further, such unique prefixes allow Snakemake to quickly discard most rules in its search for rules that can provide the requested input. This accelerates the resolution of the rule dependencies in a workflow.\nSnakemake (&gt;=5.11) comes with a code quality checker (a so called linter), that analyzes your workflow and highlights issues that should be solved in order to follow best practices, achieve maximum readability, and reproducibility. The linter can be invoked with snakemake --lint\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nFurther, scripts should be stored in a subfolder workflow/scripts and notebooks in a subfolder workflow/notebooks\nConda environments (see Integrated Package Management) should be stored in a subfolder workflow/envs\nWhen publishing your workflow in a Github repository, it is a good idea to add some minimal test data and configure Github Actions for continuously testing the workflow on each new commit. For this purpose, we provide predefined Github actions for both running tests and linting here, as well as formatting here.\nConfiguration of a workflow should be handled via config files and, if needed, tabular configuration like sample sheets (either via Pandas or PEPs). Use such configuration for metadata and experiement information, not for runtime specific configuration like threads, resources and output folders. For those, just rely on Snakemake’s CLI arguments like –set-threads, –set-resources, –set-default-resources, and –directory. This makes workflows more readable, scalable, and portable.\nTry to keep filenames short (thus easier on the eye), but informative. Avoid mixing of too many special characters (e.g. decide whether to use _ or - as a separator and do that consistently throughout the workflow).\nTry to keep Python code like helper functions separate from rules (e.g. in a workflow/rules/common.smk file). This way, you help non-experts to read the workflow without needing to parse internals that are irrelevant for them. The helper function names should be chosen in a way that makes them sufficiently informative without looking at their content. Also avoid lambda expressions inside of rules.\nMake use of Snakemake wrappers whenever possible. Consider contributing to the wrapper repo whenever you have a rule that reoccurs in at least two of your workflows. More about wrappers can be found here.\n\nFollow this link to view the recommended structure to store a workflow in a git repository. A snakemake workflow template can be found here. Conda determines the priority of channels depending on their order of appearance in the channels list. For instance, the channel that comes first in the list gets the highest priority.\n├── .gitignore\n├── README.md\n├── LICENSE.md\n├── workflow\n│   ├── rules\n|   │   ├── module1.smk\n|   │   └── module2.smk\n│   ├── envs\n|   │   ├── tool1.yaml\n|   │   └── tool2.yaml\n│   ├── scripts\n|   │   ├── script1.py\n|   │   └── script2.R\n│   ├── notebooks\n|   │   ├── notebook1.py.ipynb\n|   │   └── notebook2.r.ipynb\n│   ├── report\n|   │   ├── plot1.rst\n|   │   └── plot2.rst\n|   └── Snakefile\n├── config\n│   ├── config.yaml\n│   └── some-sheet.tsv\n├── results\n└── resources"
  },
  {
    "objectID": "source/snakemake/tutorial.html#useful-options",
    "href": "source/snakemake/tutorial.html#useful-options",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "-n --dry-run: Snakemake will only show the execution plan instead of actually performing the steps\n-p: print the resulting shell command for illustration.\n--cores {INTEGER}: The numbers of course to use. Must be part of every executed workflow\n--forcerun allows to repeat a run even if the output files already exist from a given rule\n--foreall enforces a complete re-execution of the workflow."
  },
  {
    "objectID": "source/snakemake/tutorial.html#description-of-example-workflow",
    "href": "source/snakemake/tutorial.html#description-of-example-workflow",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "A Snakemake workflow is defined by specifying rules in a Snakefile. Rules decompose the workflow into small steps (for example, the application of a single tool) by specifying how to create sets of output files from sets of input files. Snakemake automatically determines the dependencies between the rules by matching file names.\nAll added syntactic structures begin with a keyword followed by a code block that is either in the same line or indented and consisting of multiple lines. The resulting syntax resembles that of original Python constructs.\nIn this workflow we perform a read mapping workflow."
  },
  {
    "objectID": "source/snakemake/tutorial.html#setup",
    "href": "source/snakemake/tutorial.html#setup",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "Notice:\n\nSetup done in WSL\nWe will use Conda to create an isolated environment with all the required software for this tutorial.\nMambaforge already installed, if instructions are needed go here\n\n\n\n\n\n#define wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/Snakemake\"\ncd $wdir\n\n#activate conda enviornment (for this to work, see code cell below)\nconda activate snakemake-tutorial\n\n\n#download the example data\ncurl -L https://api.github.com/repos/snakemake/snakemake-tutorial-data/tarball -o snakemake-tutorial-data.tar.gz\n\n#extract the data\ntar --wildcards -xf snakemake-tutorial-data.tar.gz --strip 1 \"*/data\" \"*/environment.yaml\"\n\n#create conda env (/home/ndombrowski/mambaforge/envs/snakemake-tutorial)\nmamba env create --name snakemake-tutorial --file environment.yaml"
  },
  {
    "objectID": "source/snakemake/tutorial.html#basic-workflow",
    "href": "source/snakemake/tutorial.html#basic-workflow",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "To write our first rule, we create a new document and use it to map a sample to a reference genome using bwa mem.\nA snakemake rule:\n\nHas a name, i.e. bwa_map\nHas a number of directives (i.e. input, output, shell)\ninput can provide a list of files that are expected to be used. Important: have a comma between input/output items.\nHas a shell command, i.e. the shell command to be executed\nIf a rule has multiple input files, they will be concatenated and separated by a whitespace (i.e. we get data/genome.fa data/samples/A.fastq)\n\nBasics: - Snakemake applies the rules given in the Snakefile in a top-down way - The application of a rule to generate a set of output files is called job - Note that Snakemake automatically creates missing directories before jobs are executed. - Snakemake works backwards from requested output, and not from available input. Thus, it does not automatically infer all possible output from, for example, the fastq files in the data folder.\n\nnano Snakefile\n\nContent:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/A.fastq\"\n    output:\n        \"mapped_reads/A.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#perform dry-run workflow\nsnakemake -np mapped_reads/A.bam\n\n#execute workflow \nsnakemake --cores 1 mapped_reads/A.bam\n\nSince mapped_reads/A.bam now exists snakemake WILL NOT try to create this file again. Snakemake only re-runs jobs if one of the input files is newer than one of the output files or one of the input files will be updated by another job.\n\n\n\n\nIf we want to work on more than one sample not only data/samples/A.fastq then we can generalize rules using named wildcards. Below, we for example replace the A with {sample}\nNote that you can have multiple wildcards in your file paths, however, to avoid conflicts with other jobs of the same rule, all output files of a rule have to contain exactly the same wildcards.\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#execute command (dry-run)\nsnakemake -np mapped_reads/B.bam\n\nIf we run this, we will see that bwa will use an input name that is based on the B.bam name, i.e. data/samples/B.fastq. You can see how the character B is propagaged through the input file names and the names in the shell command.\n\n#specify more than one target\nsnakemake -np mapped_reads/A.bam mapped_reads/B.bam\n\n#same command but more condensed using bash magic \nsnakemake -np mapped_reads/{A,B}.bam\n\nIn both cases, you will see that Snakemake only proposes to create the output file mapped_reads/B.bam. This is because you already executed the workflow before (see the previous step) and no input file is newer than the output file mapped_reads/A.bam.\n\n\n\nAdd a new rule:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam\"\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nComments:\n\nIn the shell command above we split the string into two lines, which are however automatically concatenated into one by Python. This is a handy pattern to avoid too long shell command lines. When using this, make sure to have a trailing whitespace in each line but the last, in order to avoid arguments to become not properly separated.\nWith sorted_reads/{wildcards.sample} we extract the value of the sample wildcard, i.e. we get sorted_reads/B\n\n\n#dry run\nsnakemake -np sorted_reads/B.bam\n\n\n\n\nAdd a new rule:\n\nrule samtools_index:\n    input:\n        \"sorted_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam.bai\"\n    shell:\n        \"samtools index {input}\"\n\n\n#dry run \nsnakemake -np sorted_reads/B.bam.bai\n\n#create a DAG\nsnakemake --dag sorted_reads/{A,B}.bam.bai | dot -Tsvg &gt; dag.svg\n\nThe last command will create a visualization of the DAG using the dot command provided by Graphviz. For the given target files, Snakemake specifies the DAG in the dot language and pipes it into the dot command, which renders the definition into SVG format. The rendered DAG is piped into the file dag.svg.\nThe DAG contains a node for each job with the edges connecting them representing the dependencies. The frames of jobs that don’t need to be run (because their output is up-to-date) are dashed. For rules with wildcards, the value of the wildcard for the particular job is displayed in the job node.\n\n\n\nNext, we will aggregate the mapped reads from all samples and jointly call genomic variants on them. Therefore, we will combine the two utilities samtools and bcftools. Snakemake provides a helper function for collecting input files that helps us to describe the aggregation in this step. With\nexpand(\"sorted_reads/{sample}.bam\", sample=SAMPLES)\nwe obtain a list of files where the given pattern “sorted_reads/{sample}.bam” was formatted with the values in a given list of samples SAMPLES, i.e.\n[\"sorted_reads/A.bam\", \"sorted_reads/B.bam\"]\nThis can also be used if multiple wildcards are used i.e.\nexpand(\"sorted_reads/{sample}.{replicate}.bam\", sample=SAMPLES, replicate=[0, 1])\nwould create\n[\"sorted_reads/A.0.bam\", \"sorted_reads/A.1.bam\", \"sorted_reads/B.0.bam\", \"sorted_reads/B.1.bam\"]\nLet’s start with defining the list of samples ad-hoc in plain Python at the top of the Snakefile and add a rule at the end of our Snakefile:\n\nSAMPLES = [\"A\", \"B\"]\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=SAMPLES),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=SAMPLES)\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\nWith multiple input or output files, it is sometimes handy to refer to them separately in the shell command. This can be done by specifying names for input or output files, for example with fa=.... The files can then be referred to in the shell command by name, for example with {input.fa}.\nFor long shell commands like this one, it is advisable to split the string over multiple indented lines. Python will automatically merge it into one. Further, you will notice that the input or output file lists can contain arbitrary Python statements, as long as it returns a string, or a list of strings. Here, we invoke our expand function to aggregate over the aligned reads of all samples.\n\n#create a DAG\nsnakemake --dag calls/all.vcf | dot -Tsvg &gt; dag.svg\n\n\n\n\nUsually, a workflow not only consists of invoking various tools, but also contains custom code to for example calculate summary statistics or create plots. While Snakemake also allows you to directly write Python code inside a rule, it is usually reasonable to move such logic into separate scripts. For this purpose, Snakemake offers the script directive. Add the following rule to your Snakefile:\nWith this rule, we will eventually generate a histogram of the quality scores that have been assigned to the variant calls in the file calls/all.vcf. The actual Python code to generate the plot is hidden in the script scripts/plot-quals.py.\nScript paths are always relative to the referring Snakefile. In the script, all properties of the rule like input, output, wildcards, etc. are available as attributes of a global snakemake object.\n\nrule plot_quals:\n    input:\n        \"calls/all.vcf\"\n    output:\n        \"plots/quals.svg\"\n    script:\n        \"scripts/plot-quals.py\"\n\nCreate the file scripts/plot-quals.py, with the following content:\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom pysam import VariantFile\n\nquals = [record.qual for record in VariantFile(snakemake.input[0])]\nplt.hist(quals)\n\nplt.savefig(snakemake.output[0])\n\nAlthough there are other strategies to invoke separate scripts from your workflow (for example, invoking them via shell commands), the benefit of this is obvious: the script logic is separated from the workflow logic (and can even be shared between workflows), but boilerplate code like the parsing of command line arguments is unnecessary.\nApart from Python scripts, it is also possible to use R scripts. In R scripts, an S4 object named snakemake analogous to the Python case above is available and allows access to input and output files and other parameters. Here, the syntax follows that of S4 classes with attributes that are R lists, for example we can access the first input file with snakemake@input[[1]] (note that the first file does not have index 0 here, because R starts counting from 1). Named input and output files can be accessed in the same way, by just providing the name instead of an index, for example snakemake@input[[“myfile”]].\nFor details and examples, see the External scripts section in the Documentation.\n\n\n\nSo far, we always executed the workflow by specifying a target file at the command line. Apart from filenames, Snakemake also accepts rule names as targets if the requested rule does not have wildcards.\nIt is possible to write target rules collecting particular subsets of the desired results or all results. Moreover, if no target is given at the command line, Snakemake will define the first rule of the Snakefile as the target. Hence, it is best practice to have a rule all at the top of the workflow which has all typically desired target files as input files.\nTo do this add the following to the top of the workflow:\n\nrule all:\n    input:\n        \"plots/quals.svg\"\n\nAnd execute snakemake with:\n\nsnakemake -n\n\nIf we run this the execution plan for creating the file plots/quals.svg, which contains and summarizes all our results, will be shown.\nNote that, apart from Snakemake considering the first rule of the workflow as the default target, the order of rules in the Snakefile is arbitrary and does not influence the DAG of jobs.\n\n\n\n\n#execution \nsnakemake --cores 1\n\n#view output\nwslview plots/quals.svg\n\n#force to re-execute samtools_sort\nsnakemake --cores 1 --forcerun samtools_sort"
  },
  {
    "objectID": "source/snakemake/tutorial.html#advanced-options",
    "href": "source/snakemake/tutorial.html#advanced-options",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "Snakemake can be made aware of the threads a rule needs with the threads directive. Ie change the bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    threads: 2\n    shell:\n        \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\nThe number of threads can be propagated to the shell command with the familiar braces notation (i.e. {threads}). If no threads directive is given, a rule is assumed to need 1 thread.\nWhen a workflow is executed, the number of threads the jobs need is considered by the Snakemake scheduler. In particular, the scheduler ensures that the sum of the threads of all jobs running at the same time does not exceed a given number of available CPU cores. This number is given with the –cores command line argument, which is mandatory for snakemake calls that actually run the workflow.\nFor example snakemake --cores 10 would execute the workflow with 10 cores. Since the rule bwa_map needs 8 threads, only one job of the rule can run at a time, and the Snakemake scheduler will try to saturate the remaining cores with other jobs like, e.g., samtools_sort.\nThe threads directive in a rule is interpreted as a maximum: when less cores than threads are provided, the number of threads a rule uses will be reduced to the number of given cores.\nIf --cores is given without a number, all available cores are used.\n\nsnakemake --forceall --cores 2\nsnakemake --forceall --cores 4\n\n\n\n\nWe can use a configuration file if we want your workflow to be customizable:\n\nCheck out the full readme here\nCan be provided as JSON or YAML\nThey are used with the configfile direction\nvariable names in the rule and the key name in the configuration file do not have to be identical. Snakemake matches variables in the rule based on their position within the curly braces {}.\n\nA config file is specified by adding this to the top of our Snakemake workflow:\n\nconfigfile: \"config.yaml\"\n\nSnakemake will load the config file and store its contents into a globally available dictionary named config. In our case, it makes sense to specify the samples in config.yaml as:\nsamples:\n    A: data/samples/A.fastq\n    B: data/samples/B.fastq\nAnd remove the statement defining the SAMPLES in the Snakemake file. And change the rule of bcftools_call to:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\n\n\n\nSince we have stored the path to the FASTQ files in the config file, we can also generalize the rule bwa_map to use these paths. This case is different to the rule bcftools_call we modified above. To understand this, it is important to know that Snakemake workflows are executed in three phases.\n\nIn the initialization phase, the files defining the workflow are parsed and all rules are instantiated.\nIn the DAG phase, the directed acyclic dependency graph of all jobs is built by filling wildcards and matching input files to output files.\nIn the scheduling phase, the DAG of jobs is executed, with jobs started according to the available resources.\n\nThe expand functions in the list of input files of the rule bcftools_call are executed during the initialization phase.\nIn this phase, we don’t know about jobs, wildcard values and rule dependencies. Hence, we cannot determine the FASTQ paths for rule bwa_map from the config file in this phase, because we don’t even know which jobs will be generated from that rule.\nInstead, we need to defer the determination of input files to the DAG phase. This can be achieved by specifying an input function instead of a string as inside of the input directive. For the rule bwa_map this works as follows:\n::: {.cell}\ndef get_bwa_map_input_fastqs(wildcards):\n   return config[\"samples\"][wildcards.sample]\n\nrule bwa_map:\n   input:\n       \"data/genome.fa\",\n       get_bwa_map_input_fastqs\n   output:\n       \"mapped_reads/{sample}.bam\"\n   threads: 8\n   shell:\n       \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n:::\nHow wild cards work here:\n\nThe wildcards object is a special Snakemake variable that represents placeholders in rule input and output file names. In your case, you are using {sample} as a placeholder in the rule’s input section, and Snakemake automatically parses this placeholder into a wildcards object.\nWhen you define the function get_bwa_map_input_fastqs(wildcards), you are essentially telling Snakemake that this function accepts a wildcards object as an argument. This allows the function to access and use the wildcards to determine which sample’s input files to fetch.\nWhen Snakemake processes the rule bwa_map, it looks at the rule’s input section and identifies that it contains the {sample} placeholder.\nSnakemake then matches the placeholder to the wildcards object. If you have multiple samples defined in your config.yaml, Snakemake will create separate jobs for each sample, each with a different wildcards.sample value.\nFor each job, the wildcards.sample attribute will be replaced with the actual sample name being processed. For example, if Snakemake is processing the sample “A,” then wildcards.sample will be “A.”\nThe function get_bwa_map_input_fastqs uses config[“samples”][wildcards.sample] to fetch the corresponding input file path for the sample being processed. So, for sample “A,” it would retrieve “data/samples/A.fastq” as the input file path.\n\nBenefits of using python function:\n\nModularity and Reusability: By defining a separate Python function (get_bwa_map_input_fastqs) to retrieve the input files, you create a modular and reusable piece of code. This can be particularly useful if you have multiple rules that need to access the same input files, as you can reuse the function across those rules.\nAbstraction: The function abstracts the details of where the input files come from. In your first version, you had to specify the exact path to the input files in the rule itself. With the function, you only need to specify how to obtain the input files for a given sample, making your rule more abstract and easier to maintain.\nFlexibility: If you need to change the way you obtain input files in the future (for example, if you decide to store them in a different directory or use a different naming convention), you can update the function in one place, and all the rules that use it will automatically adapt.\nReduced Redundancy: Your original version of the rule had repetitive code for specifying input and output paths for each sample. With the function, you reduce redundancy and the potential for errors, as the input file retrieval logic is centralized.\nEasier Testing: You can more easily write unit tests for the input file retrieval function to ensure it works correctly, which can be challenging when input paths are hard-coded in multiple rules.\n\n\n\nIn the data/samples folder, there is an additional sample C.fastq. Add that sample to the config file and see how Snakemake wants to recompute the part of the workflow belonging to the new sample, when invoking with snakemake -n --forcerun bcftools_call\nNote: Snakemake does not automatically rerun jobs when new input files are added as in the excercise below. However, you can get a list of output files that are affected by such changes with snakemake –list-input-changes. To trigger a rerun, this bit of bash magic helps:\n\nsnakemake -n --forcerun $(snakemake --list-input-changes)\n\n\n\n\n\nSometimes shell commands are not only composed of input and output files but require additional parameters.\nWe can set additional parameters need to be set depending on the wildcard values of the job. For this, Snakemake allows to define arbitrary parameters for rules with the params directive. I.e. edit bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    threads: 8\n    shell:\n        \"bwa mem -R '{params.rg}' -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\n\nsnakemake -np --forceall\n\nNext, consider another important parameter, the prior mutation rate (1e-3 per default). It is set via the flag -P of the bcftools call command. Consider making this flag configurable via adding a new key to the config file and using the params directive in the rule bcftools_call to propagate it to the shell command.\nAdd this in the config file:\nbcftools:\n    evalue: 0.001\nChange this in the Snakemakefile:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}\"\n\n\nsnakemake -p --cores 1 --forceall\n\n\n\n\nWhen executing a large workflow, it is usually desirable to store the logging output of each job into a separate file, instead of just printing all logging output to the terminal—when multiple jobs are run in parallel, this would result in chaotic output. For this purpose, Snakemake allows to specify log files for rules. Log files are defined via the log directive and handled similarly to output files, but they are not subject of rule matching and are not cleaned up when a job fails. We modify our rule bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThe shell command is modified to collect STDERR output of both bwa and samtools and pipe it into the file referred to by {log}. Log files must contain exactly the same wildcards as the output files to avoid file name clashes between different jobs of the same rule.\nLet’s add a log to the to the bcftools_call rule as well.\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    log:\n        \"logs/bcftools/all.log\"\n    shell:\n        \"(bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}) 2&gt; {log}\"\n\n\nsnakemake -p --cores 1 --forceall\n\nThe ability to track the provenance of each generated result is an important step towards reproducible analyses. Apart from the report functionality discussed before, Snakemake can summarize various provenance information for all output files of the workflow. The flag –summary prints a table associating each output file with the rule used to generate it, the creation date and optionally the version of the tool used for creation is provided. Further, the table informs about updated input files and changes to the source code of the rule after creation of the output file.\n\nsnakemake -p --cores 1 --forceall --summary\n\n\n\n\nIn our workflow, we create two BAM files for each sample, namely the output of the rules bwa_map and samtools_sort. When not dealing with examples, the underlying data is usually huge. Hence, the resulting BAM files need a lot of disk space and their creation takes some time. To save disk space, you can mark output files as temporary. Snakemake will delete the marked files for you, once all the consuming jobs (that need it as input) have been executed. We use this mechanism for the output file of the rule bwa_map:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThis results in the deletion of the BAM file once the corresponding samtools_sort job has been executed. Since the creation of BAM files via read mapping and sorting is computationally expensive, it is reasonable to protect the final BAM file from accidental deletion or modification. We modify the rule samtools_sort to mark its output file as protected:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        protected(\"sorted_reads/{sample}.bam\")\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nRun Snakemake with the target mapped_reads/A.bam. Although the file is marked as temporary, you will see that Snakemake does not delete it because it is specified as a target file.\n\nsnakemake --forceall --cores 1 mapped_reads/A.bam"
  },
  {
    "objectID": "source/snakemake/tutorial.html#additional-features",
    "href": "source/snakemake/tutorial.html#additional-features",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "For more, check out this documentation.\n\n\nIn order to re-use building blocks or simply to structure large workflows, it is sometimes reasonable to split a workflow into modules. For this, Snakemake provides the include directive to include another Snakefile into the current one, e.g.:\ninclude: \"path/to/other.snakefile\"\nAlternatively, Snakemake allows to define sub-workflows. A sub-workflow refers to a working directory with a complete Snakemake workflow. Output files of that sub-workflow can be used in the current Snakefile. When executing, Snakemake ensures that the output files of the sub-workflow are up-to-date before executing the current workflow. This mechanism is particularly useful when you want to extend a previous analysis without modifying it. For details about sub-workflows, see the documentation.\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nLets put the read mapping into a separate snakefile and use include to make it available in our workflow:\nIn rules add a new file bwa_mem.smk with the following content:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 2\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nRemove the bwa_mem rule from the workflow and add include: \"rules/bwa_mem.smk and run the workflow with :\n\nsnakemake --forceall -p --cores 1\n\n\n\n\nIn order to get a fully reproducible data analysis, it is not sufficient to be able to execute each step and document all used parameters. The used software tools and libraries have to be documented as well. In this tutorial, you have already seen how Conda can be used to specify an isolated software environment for a whole workflow. With Snakemake, you can go one step further and specify Conda environments per rule. This way, you can even make use of conflicting software versions (e.g. combine Python 2 with Python 3).\nIn our example, instead of using an external environment we can specify environments per rule, e.g.:\n\nrule samtools_index:\n  input:\n      \"sorted_reads/{sample}.bam\"\n  output:\n      \"sorted_reads/{sample}.bam.bai\"\n  conda:\n      \"envs/samtools.yaml\"\n  shell:\n      \"samtools index {input}\"\n\nwith envs/samtools.yaml defined as:\n\nchannels:\n  - bioconda\n  - conda-forge\ndependencies:\n  - samtools =1.9\n\nWhen executing the workflow with snakemake --use-conda --cores 1 it will automatically create required environments and activate them before a job is executed. It is best practice to specify at least the major and minor version of any packages in the environment definition.\nSpecifying environments per rule in this way has two advantages: - First, the workflow definition also documents all used software versions. - Second, a workflow can be re-executed (without admin rights) on a vanilla system, without installing any prerequisites apart from Snakemake and Miniconda.\n\n\n\nIn order to simplify the utilization of popular tools, Snakemake provides a repository of so-called wrappers (the Snakemake wrapper repository). A wrapper is a short script that wraps (typically) a command line application and makes it directly addressable from within Snakemake. For this, Snakemake provides the wrapper directive that can be used instead of shell, script, or run. For example, the rule bwa_map could alternatively look like this:\n\nrule bwa_mem:\n  input:\n      ref=\"data/genome.fa\",\n      sample=lambda wildcards: config[\"samples\"][wildcards.sample]\n  output:\n      temp(\"mapped_reads/{sample}.bam\")\n  log:\n      \"logs/bwa_mem/{sample}.log\"\n  params:\n      \"-R '@RG\\tID:{sample}\\tSM:{sample}'\"\n  threads: 8\n  wrapper:\n      \"0.15.3/bio/bwa/mem\"\n\n\n\n\nBy default, Snakemake executes jobs on the local machine it is invoked on. Alternatively, it can execute jobs in distributed environments, e.g., compute clusters or batch systems. If the nodes share a common file system, Snakemake supports three alternative execution modes.\nIn cluster environments, compute jobs are usually submitted as shell scripts via commands like qsub. Snakemake provides a generic mode to execute on such clusters. By invoking Snakemake with snakemake --cluster qsub --jobs 100 each job will be compiled into a shell script that is submitted with the given command (here qsub). The –jobs flag limits the number of concurrently submitted jobs to 100.\nFind out more for SLURM submissions.\n\n\n\nSometimes it is useful to constrain the values a wildcard can have. This can be achieved by adding a regular expression that describes the set of allowed wildcard values. For example, the wildcard sample in the output file \"sorted_reads/{sample}.bam\" can be constrained to only allow alphanumeric sample names as \"sorted_reads/{sample,[A-Za-z0-9]+}.bam\".\nConstraints may be defined per rule or globally using the wildcard_constraints keyword, as demonstrated in Wildcards. This mechanism helps to solve two kinds of ambiguity.\n\nIt can help to avoid ambiguous rules, i.e. two or more rules that can be applied to generate the same output file. Other ways of handling ambiguous rules are described in the Section Handling Ambiguous Rules.\nIt can help to guide the regular expression based matching so that wildcards are assigned to the right parts of a file name. Consider the output file {sample}.{group}.txt and assume that the target file is A.1.normal.txt. It is not clear whether dataset=“A.1” and group=“normal” or dataset=“A” and group=“1.normal” is the right assignment. Here, constraining the dataset wildcard by {sample,[A-Z]+}.{group}solves the problem.\n\nWhen dealing with ambiguous rules, it is best practice to first try to solve the ambiguity by using a proper file structure, for example, by separating the output files of different steps in different directories.\n\n\n\nA basic report contains runtime statistics, a visualization of the workflow topology, used software and data provenance information.\n\nsnakemake --report report.html --rerun-incomplete\n\nIn addition, you can mark any output file generated in your workflow for inclusion into the report. It will be encoded directly into the report, such that it can be, e.g., emailed as a self-contained document. The reader (e.g., a collaborator of yours) can at any time download the enclosed results from the report for further use, e.g., in a manuscript you write together.\nLet’s mark the output file “results/plots/quals.svg” for inclusion by replacing it with report(“results/plots/quals.svg”, caption=“report/calling.rst”) and adding a file report/calling.rst, containing some description of the output file. This description will be presented as caption in the resulting report."
  },
  {
    "objectID": "source/nanopore/porechop_readme.html",
    "href": "source/nanopore/porechop_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Porechop is a tool for finding and removing adapters from Oxford Nanopore reads. Adapters on the ends of reads are trimmed off, and when a read has an adapter in its middle, it is treated as chimeric and chopped into separate reads. Porechop performs thorough alignments to effectively find adapters, even at low sequence identity.\nNotice: From 2018 on, porechop is not actively maintained anymore. It runs perfectly fine, but that is something to keep in mind when running into bugs.\nAvailable on Crunchomics: Not by default\n\n\n\nIt is easiest to install porechop with conda:\n\nmamba create --name porechop -c conda-forge -c bioconda porechop=0.2.4\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAdaptor trimming very much depends on how the sequencing library was generated. Therefore, I recommend to carefully read through the How it works section of the softwares manual to know what to expect and look out for.\nSimilarly, porechop works with both demultiplexed and non-demultiplexed sequences. Also here, the manual explains in more detail how to perform barcode demultiplexing.\n\n\nRequired input:\n\nFASTA/FASTQ of input reads or a directory which will be recursively searched for FASTQ files (required and can be fasta,fastq,fasta.gz,fastq.gz)\n\nOutput:\n\nFASTA or FASTQ of trimmed reads\n\nExample code:\n\nporechop --input myfile.fastq.gz \\\n  --output outputfolder/myfile_filtered.fastq.gz \\\n  --threads 1 \\\n  --discard_middle\n\nUseful arguments (for the full version, check the manual):\n\n-b {BARCODE_DIR}, --barcode_dir {BARCODE_DIR}: Reads will be binned based on their barcode and saved to separate files in this directory (incompatible with –output)\n--barcode_threshold {BARCODE_THRESHOLD} A read must have at least this percent identity to a barcode to be binned (default: 75.0)\n--barcode_diff {BARCODE_DIFF} If the difference between a read’s best barcode identity and its second-best barcode identity is less than this value, it will not be put in a barcode bin (to exclude cases which are too close to call) (default: 5.0)\n--adapter_threshold {ADAPTER_THRESHOLD} An adapter set has to have at least this percent identity to be labelled as present and trimmed off (0 to 100) (default: 90.0)\n--check_reads{CHECK_READS} This many reads will be aligned to all possible adapters to determine which adapter sets are present (default: 10000)\n--no_split Skip splitting reads based on middle adapters (default: split reads when an adapter is found in the middle)\n--discard_middle Reads with middle adapters will be discarded (default: reads with middle adapters are split) (required for reads to be used with Nanopolish, this option is on by default when outputting reads into barcode bins)"
  },
  {
    "objectID": "source/nanopore/porechop_readme.html#porechop",
    "href": "source/nanopore/porechop_readme.html#porechop",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Porechop is a tool for finding and removing adapters from Oxford Nanopore reads. Adapters on the ends of reads are trimmed off, and when a read has an adapter in its middle, it is treated as chimeric and chopped into separate reads. Porechop performs thorough alignments to effectively find adapters, even at low sequence identity.\nNotice: From 2018 on, porechop is not actively maintained anymore. It runs perfectly fine, but that is something to keep in mind when running into bugs.\nAvailable on Crunchomics: Not by default\n\n\n\nIt is easiest to install porechop with conda:\n\nmamba create --name porechop -c conda-forge -c bioconda porechop=0.2.4\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAdaptor trimming very much depends on how the sequencing library was generated. Therefore, I recommend to carefully read through the How it works section of the softwares manual to know what to expect and look out for.\nSimilarly, porechop works with both demultiplexed and non-demultiplexed sequences. Also here, the manual explains in more detail how to perform barcode demultiplexing.\n\n\nRequired input:\n\nFASTA/FASTQ of input reads or a directory which will be recursively searched for FASTQ files (required and can be fasta,fastq,fasta.gz,fastq.gz)\n\nOutput:\n\nFASTA or FASTQ of trimmed reads\n\nExample code:\n\nporechop --input myfile.fastq.gz \\\n  --output outputfolder/myfile_filtered.fastq.gz \\\n  --threads 1 \\\n  --discard_middle\n\nUseful arguments (for the full version, check the manual):\n\n-b {BARCODE_DIR}, --barcode_dir {BARCODE_DIR}: Reads will be binned based on their barcode and saved to separate files in this directory (incompatible with –output)\n--barcode_threshold {BARCODE_THRESHOLD} A read must have at least this percent identity to a barcode to be binned (default: 75.0)\n--barcode_diff {BARCODE_DIFF} If the difference between a read’s best barcode identity and its second-best barcode identity is less than this value, it will not be put in a barcode bin (to exclude cases which are too close to call) (default: 5.0)\n--adapter_threshold {ADAPTER_THRESHOLD} An adapter set has to have at least this percent identity to be labelled as present and trimmed off (0 to 100) (default: 90.0)\n--check_reads{CHECK_READS} This many reads will be aligned to all possible adapters to determine which adapter sets are present (default: 10000)\n--no_split Skip splitting reads based on middle adapters (default: split reads when an adapter is found in the middle)\n--discard_middle Reads with middle adapters will be discarded (default: reads with middle adapters are split) (required for reads to be used with Nanopolish, this option is on by default when outputting reads into barcode bins)"
  },
  {
    "objectID": "source/nanopore/nanoplot_readme.html",
    "href": "source/nanopore/nanoplot_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoPlot is a olotting tool for long read sequencing data and alignment (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\n\nNanoPlot is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools \n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\n\nPossible input formats :\n\nfastq files (can be bgzip, bzip2 or gzip compressed)\nfastq files generated by albacore, guppy or MinKNOW containing additional information (can be bgzip, bzip2 or gzip compressed)\n\nsorted bam files\n\nsequencing_summary.txt output table generated by albacore, guppy or MinKnow basecalling (can be gzip, bz2, zip and xz compressed)\nfasta files (can be bgzip, bzip2 or gzip compressed); Multiple files of the same type can be offered simultaneously\n\nOutput:\n\na statistical summary\na number of plots\na html summary file\n\nExample code:\n\n#start environment\nconda activate nanopack\n\n#run on a single file (if you are using other inputs, check the readme for the appropriate flag)\nNanoPlot --fastq myfile.fastq.gz -o outputfolder --threads 1\n\nUseful arguments (for the full version, check the manual):\n\n--tsv_stats Output the stats file as a properly formatted TSV.\n--info_in_report Add NanoPlot run info in the report.\n--barcoded Use if you want to split the summary file by barcode\n-f, --format {[{png,jpg,jpeg,webp,svg,pdf,eps,json} …] } Specify the output format of the plots, which are in addition to the html files"
  },
  {
    "objectID": "source/nanopore/nanoplot_readme.html#nanoplot",
    "href": "source/nanopore/nanoplot_readme.html#nanoplot",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoPlot is a olotting tool for long read sequencing data and alignment (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\n\nNanoPlot is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools \n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\n\nPossible input formats :\n\nfastq files (can be bgzip, bzip2 or gzip compressed)\nfastq files generated by albacore, guppy or MinKNOW containing additional information (can be bgzip, bzip2 or gzip compressed)\n\nsorted bam files\n\nsequencing_summary.txt output table generated by albacore, guppy or MinKnow basecalling (can be gzip, bz2, zip and xz compressed)\nfasta files (can be bgzip, bzip2 or gzip compressed); Multiple files of the same type can be offered simultaneously\n\nOutput:\n\na statistical summary\na number of plots\na html summary file\n\nExample code:\n\n#start environment\nconda activate nanopack\n\n#run on a single file (if you are using other inputs, check the readme for the appropriate flag)\nNanoPlot --fastq myfile.fastq.gz -o outputfolder --threads 1\n\nUseful arguments (for the full version, check the manual):\n\n--tsv_stats Output the stats file as a properly formatted TSV.\n--info_in_report Add NanoPlot run info in the report.\n--barcoded Use if you want to split the summary file by barcode\n-f, --format {[{png,jpg,jpeg,webp,svg,pdf,eps,json} …] } Specify the output format of the plots, which are in addition to the html files"
  },
  {
    "objectID": "source/nanopore/nanoITS.html",
    "href": "source/nanopore/nanoITS.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoITS is a classifier for long-read Oxford Nanopore data of the eukaryotic 18S/SSU-ITS1 operon.\nWhen giving the tool some nanopore long-read data it will:\n\nProvide a quality report of the raw reads\nCheck the reads for adaptors and barcodes and if present trim the reads\nRemove low-quality and short reads\nProvide a quality report for the cleaned reads\nIdentify and separate both the ITS1 and 18S rRNA gene\nClassify the SSU and/or ITS1 gene using kraken2 and/or minimap2\nGenerate taxonomic barplots and OTU tables\n\nFor a more detailed explanation, check out the manual.\nBelow you can find the full workflow:\n\n\n\n\nTo run NanoITs, install conda and snakemake and clone the directory from github via:\n\ngit clone https://github.com/ndombrowski/NanoITS.git\n\nProvide your sample names and path to the samples as a comma-separated file, for example, a file looking similar as the one provided in example_files/mapping.csv. Sample names should be unique and consist of letter, numbers and - only. The barcode column can be left empty as it is not yet implemented. The path should contain the path to your demultiplexed, compressed fastq file.\nAdjust config/config.yaml to configure the location of your mapping file as well as specificy the parameters used by NanoITs.\nNanoITs can then be run with:\n\nsnakemake --use-conda --cores &lt;nr_cores&gt; \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile \\\n  --configfile config/config.yaml \\\n  --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  \\\n  --rerun-incomplete --nolock -np \n\nFor a more detailed explanation, check out the manual."
  },
  {
    "objectID": "source/nanopore/nanoITS.html#nanoits",
    "href": "source/nanopore/nanoITS.html#nanoits",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoITS is a classifier for long-read Oxford Nanopore data of the eukaryotic 18S/SSU-ITS1 operon.\nWhen giving the tool some nanopore long-read data it will:\n\nProvide a quality report of the raw reads\nCheck the reads for adaptors and barcodes and if present trim the reads\nRemove low-quality and short reads\nProvide a quality report for the cleaned reads\nIdentify and separate both the ITS1 and 18S rRNA gene\nClassify the SSU and/or ITS1 gene using kraken2 and/or minimap2\nGenerate taxonomic barplots and OTU tables\n\nFor a more detailed explanation, check out the manual.\nBelow you can find the full workflow:\n\n\n\n\nTo run NanoITs, install conda and snakemake and clone the directory from github via:\n\ngit clone https://github.com/ndombrowski/NanoITS.git\n\nProvide your sample names and path to the samples as a comma-separated file, for example, a file looking similar as the one provided in example_files/mapping.csv. Sample names should be unique and consist of letter, numbers and - only. The barcode column can be left empty as it is not yet implemented. The path should contain the path to your demultiplexed, compressed fastq file.\nAdjust config/config.yaml to configure the location of your mapping file as well as specificy the parameters used by NanoITs.\nNanoITs can then be run with:\n\nsnakemake --use-conda --cores &lt;nr_cores&gt; \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile \\\n  --configfile config/config.yaml \\\n  --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  \\\n  --rerun-incomplete --nolock -np \n\nFor a more detailed explanation, check out the manual."
  },
  {
    "objectID": "source/metagenomics/readme.html",
    "href": "source/metagenomics/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Metagenomics is the study of genetic material recovered directly from environmental or other samples using sequencing technologies. Below you find some tutorials explaining how to work with sequencing data, assemble metagenome-assembled genomes (MAGs) from this sequencing data and how to analyse these MAGS. Notice: These tutorials where not developed at UvA and will be updated in the future but will nevertheless give a good starting point in these kind of analyses.\n\n\n\nAssembling a metagenome\nGenomic binning\nAnnotating microbial genomes\nHow to do phylogenetic analyses"
  },
  {
    "objectID": "source/metagenomics/readme.html#general",
    "href": "source/metagenomics/readme.html#general",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Metagenomics is the study of genetic material recovered directly from environmental or other samples using sequencing technologies. Below you find some tutorials explaining how to work with sequencing data, assemble metagenome-assembled genomes (MAGs) from this sequencing data and how to analyse these MAGS. Notice: These tutorials where not developed at UvA and will be updated in the future but will nevertheless give a good starting point in these kind of analyses.\n\n\n\nAssembling a metagenome\nGenomic binning\nAnnotating microbial genomes\nHow to do phylogenetic analyses"
  },
  {
    "objectID": "source/metagenomics/interproscan_readme.html",
    "href": "source/metagenomics/interproscan_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "InterPro is a database which integrates together predictive information about proteins’ function from a number of partner resources, giving an overview of the families that a protein belongs to and the domains and sites it contains (Blum et al. 2021).\nUsers who have novel nucleotide or protein sequences that they wish to functionally characterise can use the software package InterProScan to run the scanning algorithms from the InterPro database in an integrated way. Sequences are submitted in FASTA format. Matches are then calculated against all of the required member database’s signatures and the results are then output in a variety of formats (Jones et al. 2014).\n\n\n\n\n\nNotice: The newest version does NOT run on crunchomics unless you do some extra steps during the isntallation since we need a newer java version and we need to update a tool used to analyse Prosite specific databases.\nThese changes require to move a few things around, so if you feel uncomfortable with this feel free to install an older version (for installation, see section #### Version for Crunchomics below) that works well with the default java version installed on crunchomics.\nIf you want to work with the newest version and are not afraid of some extra steps do the following:\n\n#go into a folder in which you have all your software installed\ncd software_dir\n\n#make a new folder for the interproscan installation and go into the folder\nmkdir interproscan\ncd interproscan \n\n#download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum -c interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n#decompress the downloaded folder\ntar -pxvzf interproscan-5.64-96.0-*-bit.tar.gz\n\n#index hmm models\ncd interproscan-5.64-96.0\npython3 setup.py -f interproscan.properties\n\n#setup a conda environment with the newest java version\nconda deactivate\nmamba create --name java_f_iprscan -c conda-forge openjdk\n\n#activate teh environment and install some more dependencies\nconda activate java_f_iprscan\nmamba install -c conda-forge gfortran\n\n#test error outside of iprscan\n#bin/prosite/pfsearchV3\n\n#get new pftools version via conda\nmamba install -c bioconda pftools\n\n#remove old pftools \ncd bin/prosite\nrm pfscanV3\nrm pfsearchV3\n\n#replace with new pftools we have installed (and which are found in the mamba env folder)\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscan .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscanV3 .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearch .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearchV3 .\ncd ..\n\n\n#do testrun (ProSiteProfiles and ProSitePatterns not working yet)\nsrun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv -dp --appl TIGRFAM,SFLD,SUPERFAMILY,PANTHER,GENE3D,Hamap,Coils,SMART,CDD,PRINTS,PIRSR,AntiFam,Pfam,MobiDBLite,PIRSF,ProSiteProfiles \n\nsrun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv\nconda deactivate\n\n\n\n\nOn Crunchomics, newer versions of interproscan do not run due to an incompatibility with the installed Java version. To get it running without complicated environmental setups, you can install an older version:\n\n#download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n#decompress the folder\ntar -pxvzf interproscan-5.36-75.0-64-bit.tar.gz\n\n#index hmm models\ncd interproscan-5.36-75.0\n\n#do a test run to check the installation\nsrun -n1 --cpus-per-task 8 --mem=8G ./interproscan.sh -i test_proteins.fasta\n\n\n\n\n\nRequired inputs:\n\nProtein fasta file\n\nGenerated outputs:\n\nTSV: a simple tab-delimited file format\nXML: the new “IMPACT” XML format\nGFF3: The GFF 3.0 format\nJSON\nSVG\nHTML\n\nNotice:\n\nInterproscan does not like * symbols inside the protein sequence. Some tools for protein calling, like prokka, use * add the end of a protein to indicate that the full protein was found. If your files have such symbols, use the code below to remove it first. Beware: using sed -i overwrites the content of your file. If that behaviour is not wanted use sed 's/*//g' Proteins_of_interest.faa &gt; Proteins_of_interest_new.faa instead.\nIf you are on Crunchomics (or most other servers): DO NOT run jobs on the head node, but add something like srun -n 1 --cpus-per-task 4 before the actual command\n\nExample code:\n\n#clean faa file \n#remove `*` as interproscan does not like that symbol\nsed -i 's/*//g' Proteins_of_interest.faa\n\n#run interproscan\n&lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --cpu 4 -i Proteins_of_interest.faa -d outputfolder -T outputfolder/temp --iprlookup --goterms\n\nTo check available options use &lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --help or for more detailed information, see the documentation):"
  },
  {
    "objectID": "source/metagenomics/interproscan_readme.html#sec-interproscan",
    "href": "source/metagenomics/interproscan_readme.html#sec-interproscan",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "InterPro is a database which integrates together predictive information about proteins’ function from a number of partner resources, giving an overview of the families that a protein belongs to and the domains and sites it contains (Blum et al. 2021).\nUsers who have novel nucleotide or protein sequences that they wish to functionally characterise can use the software package InterProScan to run the scanning algorithms from the InterPro database in an integrated way. Sequences are submitted in FASTA format. Matches are then calculated against all of the required member database’s signatures and the results are then output in a variety of formats (Jones et al. 2014).\n\n\n\n\n\nNotice: The newest version does NOT run on crunchomics unless you do some extra steps during the isntallation since we need a newer java version and we need to update a tool used to analyse Prosite specific databases.\nThese changes require to move a few things around, so if you feel uncomfortable with this feel free to install an older version (for installation, see section #### Version for Crunchomics below) that works well with the default java version installed on crunchomics.\nIf you want to work with the newest version and are not afraid of some extra steps do the following:\n\n#go into a folder in which you have all your software installed\ncd software_dir\n\n#make a new folder for the interproscan installation and go into the folder\nmkdir interproscan\ncd interproscan \n\n#download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum -c interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n#decompress the downloaded folder\ntar -pxvzf interproscan-5.64-96.0-*-bit.tar.gz\n\n#index hmm models\ncd interproscan-5.64-96.0\npython3 setup.py -f interproscan.properties\n\n#setup a conda environment with the newest java version\nconda deactivate\nmamba create --name java_f_iprscan -c conda-forge openjdk\n\n#activate teh environment and install some more dependencies\nconda activate java_f_iprscan\nmamba install -c conda-forge gfortran\n\n#test error outside of iprscan\n#bin/prosite/pfsearchV3\n\n#get new pftools version via conda\nmamba install -c bioconda pftools\n\n#remove old pftools \ncd bin/prosite\nrm pfscanV3\nrm pfsearchV3\n\n#replace with new pftools we have installed (and which are found in the mamba env folder)\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscan .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscanV3 .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearch .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearchV3 .\ncd ..\n\n\n#do testrun (ProSiteProfiles and ProSitePatterns not working yet)\nsrun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv -dp --appl TIGRFAM,SFLD,SUPERFAMILY,PANTHER,GENE3D,Hamap,Coils,SMART,CDD,PRINTS,PIRSR,AntiFam,Pfam,MobiDBLite,PIRSF,ProSiteProfiles \n\nsrun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv\nconda deactivate\n\n\n\n\nOn Crunchomics, newer versions of interproscan do not run due to an incompatibility with the installed Java version. To get it running without complicated environmental setups, you can install an older version:\n\n#download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n#decompress the folder\ntar -pxvzf interproscan-5.36-75.0-64-bit.tar.gz\n\n#index hmm models\ncd interproscan-5.36-75.0\n\n#do a test run to check the installation\nsrun -n1 --cpus-per-task 8 --mem=8G ./interproscan.sh -i test_proteins.fasta\n\n\n\n\n\nRequired inputs:\n\nProtein fasta file\n\nGenerated outputs:\n\nTSV: a simple tab-delimited file format\nXML: the new “IMPACT” XML format\nGFF3: The GFF 3.0 format\nJSON\nSVG\nHTML\n\nNotice:\n\nInterproscan does not like * symbols inside the protein sequence. Some tools for protein calling, like prokka, use * add the end of a protein to indicate that the full protein was found. If your files have such symbols, use the code below to remove it first. Beware: using sed -i overwrites the content of your file. If that behaviour is not wanted use sed 's/*//g' Proteins_of_interest.faa &gt; Proteins_of_interest_new.faa instead.\nIf you are on Crunchomics (or most other servers): DO NOT run jobs on the head node, but add something like srun -n 1 --cpus-per-task 4 before the actual command\n\nExample code:\n\n#clean faa file \n#remove `*` as interproscan does not like that symbol\nsed -i 's/*//g' Proteins_of_interest.faa\n\n#run interproscan\n&lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --cpu 4 -i Proteins_of_interest.faa -d outputfolder -T outputfolder/temp --iprlookup --goterms\n\nTo check available options use &lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --help or for more detailed information, see the documentation):"
  },
  {
    "objectID": "source/metagenomics/fama_readme.html",
    "href": "source/metagenomics/fama_readme.html",
    "title": "FAMA",
    "section": "",
    "text": "FAMA\n\nIntroduction\nFama is a fast pipeline for functional and taxonomic analysis of shotgun metagenomic sequences.\n\n\nInstallation\nIf you want to install FAMA on your own, follow these instructions in the FAMA manual.\nAvailable on Crunchomics: Yes, via the metatools share. For access contact Anna Heintz Buschart a.u.s.heintzbuschart@uva.nl\n\n\nInstructions\n\n#activate environment\nconda activate /zfs/omics/projects/metatools/TOOLS/miniconda3/envs/FAMA\n\n#get template and add \n#(if you downloaded FAMA by yourself then the ini file can be found in the folder you downloaded)\ncp /zfs/omics/projects/metatools/DB/fama_new/project.ini.sample Annotations/Fama/my.project.config\n\n#run FAMA \npython3 /zfs/omics/projects/metatools/DB/fama_new/py/fama.py -c /zfs/omics/projects/metatools/DB/fama_new/config.ini -p my.project.config\n\nExample content of Annotations/Fama/my.project.config:\n[DEFAULT]\nproject_name = 'Test sample sulfur'\ncollection = test_sulfur_v1\nref_output_name = ref_tabular_output.txt\nbackground_output_name = bgr_tabular_output.txt\nref_hits_list_name = ref_hits.txt\nref_hits_fastq_name = ref_hits.fq\nreads_fastq_name = reads.fq\npe_reads_fastq_name = reads_pe.fq\noutput_subdir = out_sulfur\nreport_name = report.txt\nxml_name = krona.xml\nhtml_name = functional_profile.html\nreads_json_name = reads.json\nassembly_subdir = assembly\nwork_dir = /zfs/omics/projects/thiopac-mgx/nina_wdir/Annotations/Fama\n\n[test_sample]\nsample_id = TPS_fame_sulfur\nfastq_pe1 = /zfs/omics/projects/thiopac-mgx/nina_wdir/faa/All_genomes.faa\nsample_dir = /zfs/omics/projects/thiopac-mgx/nina_wdir/Annotations/Fama\nreplicate = 0\n#fastq_pe1_readcount = 30\n#fastq_pe1_basecount = 4509\n#fastq_pe2_readcount = 30\n#fastq_pe2_basecount = 4346\n#rpkg_scaling = 0.18962061540779365\n#insert_size = 233.4950097660472\n\n\nAdding more databases to FAMA\nIt is possible to add new databases to FAMA. Below is an example on how to add a custom sulfur database.\nSulfur database installation instructions:\n\nDownload and unpack the archive into a separate directory:https://iseq.lbl.gov/mydocs/fama_downloads/fama_sulfur_dataset.tar.gz\nCreate diamond databases:\n\n\ndiamond makedb --in classification_database.faa --db classification_database\ndiamond makedb --in selection_database_clustered.faa --db selection_database\n\n\nAppend your config.ini file with a new section and replace “” with real paths:\n\n\n[test_sulfur_v1]\nfunctions_file = &lt;path to db files&gt;/collection_functions.txt\ntaxonomy_file = &lt;path to db files&gt;/collection_taxonomy.txt\nreference_diamond_db = &lt;path to db files&gt;/selection_database.dmnd\nbackground_diamond_db = &lt;path to db files&gt;/classification_database.dmnd\nreference_db_size = 6990766\nbackground_db_size = 99120132"
  },
  {
    "objectID": "source/cli/hpc_usage.html",
    "href": "source/cli/hpc_usage.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "If you want to use the cli on the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED, please check out this documentation. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your uva-net id. Information about other resources available is documented on the computational support teams website under Scientific programming and HPC.\n\n\n\n\n\n\nImportant\n\n\n\nSince you share resources with other people when using the Crunchomics HPC (and any other HPC) make sure to read the documentation first!\n\n\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified shematic:\n\n\n\n\n\nTo login and submit jobs, have a look at the following guides:\n\nConnecting to Crunchomics\nFirst time setup when using Crunchomics\nSubmitting jobs via SLURM"
  },
  {
    "objectID": "source/cli/hpc_usage.html#using-a-hpc",
    "href": "source/cli/hpc_usage.html#using-a-hpc",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "If you want to use the cli on the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED, please check out this documentation. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your uva-net id. Information about other resources available is documented on the computational support teams website under Scientific programming and HPC.\n\n\n\n\n\n\nImportant\n\n\n\nSince you share resources with other people when using the Crunchomics HPC (and any other HPC) make sure to read the documentation first!\n\n\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified shematic:\n\n\n\n\n\nTo login and submit jobs, have a look at the following guides:\n\nConnecting to Crunchomics\nFirst time setup when using Crunchomics\nSubmitting jobs via SLURM"
  },
  {
    "objectID": "source/classification/minimap2.html",
    "href": "source/classification/minimap2.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Minimap2 (Li 2018) is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database.\nAvailable on Crunchomics: Minimap2 version 2.24-r1122 installed\n\n\n\nIf you want to install minimap2 on your own, its best to install it via mamba:\n\n#setup new conda environment, which we name kraken2\nmamba create --name minimap2 -c bioconda minimap2\n\n\n\n\nFor detailed usage information, check out the minimap2 manual.\nInput:\n\nMinimap takes both fastq as well as fasta sequences as input\n\n\n#example of mapping input sequences against sequences of the SILVA 16S database\nminimap2 -cx map-ont -t &lt;nr_of_threads&gt; \\\n        -N 10 -K 25M \\\n        silva-ref.fasta \\\n        my_seqs.fastq \\\n        -o output.paf\n\nUsed options:\n\n-x map-ont: Run minimap optimized for Nanopore reads\n-c: output is generated in PAF format, use -aif you prefer the output in SAM format\n-N: lso retain up to -N [=10] top secondary mappings\n-K: Read -K [=25M] query bases\n…\n\nThere are many different ways to run minimap2, so check out the minimap2 manual for all options."
  },
  {
    "objectID": "source/classification/minimap2.html#minimap2",
    "href": "source/classification/minimap2.html#minimap2",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Minimap2 (Li 2018) is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database.\nAvailable on Crunchomics: Minimap2 version 2.24-r1122 installed\n\n\n\nIf you want to install minimap2 on your own, its best to install it via mamba:\n\n#setup new conda environment, which we name kraken2\nmamba create --name minimap2 -c bioconda minimap2\n\n\n\n\nFor detailed usage information, check out the minimap2 manual.\nInput:\n\nMinimap takes both fastq as well as fasta sequences as input\n\n\n#example of mapping input sequences against sequences of the SILVA 16S database\nminimap2 -cx map-ont -t &lt;nr_of_threads&gt; \\\n        -N 10 -K 25M \\\n        silva-ref.fasta \\\n        my_seqs.fastq \\\n        -o output.paf\n\nUsed options:\n\n-x map-ont: Run minimap optimized for Nanopore reads\n-c: output is generated in PAF format, use -aif you prefer the output in SAM format\n-N: lso retain up to -N [=10] top secondary mappings\n-K: Read -K [=25M] query bases\n…\n\nThere are many different ways to run minimap2, so check out the minimap2 manual for all options."
  },
  {
    "objectID": "source/R/readme.html",
    "href": "source/R/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "R is a language and environment for statistical computing and graphics and is useful to analyse computational data.\nSome useful information to get started:\n\nInstallation guide for R and RStudio\nAn R cookbook including some example files\nTutorial on data manipulation with dplyr\nTutorial on data visualization with ggplot2"
  },
  {
    "objectID": "source/R/readme.html#using-r",
    "href": "source/R/readme.html#using-r",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "R is a language and environment for statistical computing and graphics and is useful to analyse computational data.\nSome useful information to get started:\n\nInstallation guide for R and RStudio\nAn R cookbook including some example files\nTutorial on data manipulation with dplyr\nTutorial on data visualization with ggplot2"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html",
    "href": "source/Qiime/qiime_cmi_tutorial.html",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Notice:\n\nThis tutorial was not written by myself but taken from QIIME tutorial. Additionally, the notes you find here were expended by copying from several spots in the QIIME documentation to explore things.\nThe code was run on my personal computer (Windows, WSL2).\nIf you follow the link for the tutorial, you will see that the tutorial is also available using the Galaxy interface and python API.\n\nOther useful resources to check out:\n\nQIIME 2 view: web-based viewer for .qza and .qzv files\nQIIME forum\nQIIME2 docs. Notice, the tutorial runs on v2021.2, which we will use for now and later update to the newest QIIME version\nQIIME2 Library. Useful to check for new plugins/functionality\nOld QIIME tutorials\nNEW QIIME tutorials\n\n\n\n\n\n\nAll files generated by QIIME 2 are either .qza or .qzv files, and these are simply zip files that store your data alongside some QIIME 2-specific metadata. You can unzip them with unzip xxx.qza\nThe .qza file extension is an abbreviation for QIIME Zipped Artifact, and the .qzv file extension is an abbreviation for QIIME Zipped Visualization. .qza files (which are often simply referred to as artifacts) are intermediary files in a QIIME 2 analysis, usually containing raw data of some sort.\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files (generally .qza and .qzv files) without requiring a QIIME installation.\nPlugins are software packages that can be developed by anyone and define actions, which are steps in an analysis workflow. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses.\nQIIME actions:\n\nA method, i.e.alpha-phylogenetic, accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts (qza file) as output.\nA visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input and generate qzv files. In contrast to a method, a visualizer produces exactly one visualization as output. An example of a QIIME 2 visualizer is the beta-group-significance action in the q2-diversity plugin.\nA pipeline can generate one or more .qza and/or .qzv as output. Pipelines are special in that they’re a type of action that can call other actions. They are often used by developers to define simplify common workflows so they can be run by users in a single step. For example, the core-metrics-phylogenetic action in the q2-diversity plugin is a Pipeline that runs both alpha-phylogenetic and beta-phylogenetic, as well as several other actions, in a single command.\n\nTypes used in QIIME 2:\n\nFile type: the format of a file used to store some data. For example, newick is a file type that is used for storing phylogenetic trees.\nData type: refer to how data is represented in a computer’s memory (i.e., RAM) while it’s actively in use.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. This is a representation of the meaning of the data. For example, two semantic types used in QIIME 2 are Phylogeny[Rooted] and Phylogeny[Unrooted], which are used to represent rooted and unrooted trees, respectively. QIIME 2 methods will describe what semantic types they take as input(s), and what semantic types they generate as output(s).\n\n\n\n\n\n\nFind out more also here.\n\nMetadata:\n\nSample metadata may include technical details, such as the DNA barcodes that were used for each sample in a multiplexed sequencing run, or descriptions of the samples, such as which subject, time point, and body site each sample came from\nFeature metadata is often a feature annotation, such as the taxonomy assigned to an amplicon sequence variant (ASV).\nQIIME 2 does not place restrictions on what types of metadata are expected to be present; there are no enforced “metadata standards”.\nthe MIxS and MIMARKS standards [1] provide recommendations for microbiome studies and may be helpful in determining what information to collect in your study. If you plan to deposit your data in a data archive (e.g. ENA or Qiita), it is also important to determine the types of metadata expected by that resource.\nIn QIIME 2, we always refer to these files as metadata files, but they are conceptually the same thing as QIIME 1 mapping files. QIIME 2 metadata files are backwards-compatible with QIIME 1 mapping files, meaning that you can use existing QIIME 1 mapping files in QIIME 2 without needing to make modifications to the file.\n\n\n\n\n\nUsually provided as TSV file (.tsv or .txt file extension) that provides data in form of rows and columns\nFirst row = non-comment, non-empty column headers containing a unique identifier for each metadata entry\nRows whose first cell begins with the pound sign (#) are interpreted as comments and may appear anywhere in the file. Comment rows are ignored by QIIME 2 and are for informational purposes only. Inline comments (i.e., comments that begin part-way through a row or at the end of a row) are not supported.\nEmpty rows (e.g. blank lines or rows consisting solely of empty cells) may appear anywhere in the file and are ignored.\nColumn 1: identifier (ID) column. This column defines the sample or feature IDs associated with your study. It is not recommended to mix sample and feature IDs in a single metadata file; keep sample and feature metadata stored in separate files. The ID column name (also referred to as the ID column header) must be:\n\nCase-insenitive (i.e., uppercase or lowercase, or a mixing of the two, is allowed): id, sampleid, sample id, sample-id, featureid feature id, feature-id\nIDs may consist of any Unicode characters, with the exception that IDs must not start with the pound sign (#), as those rows would be interpreted as comments and ignored.\nIDs cannot be empty (i.e. they must consist of at least one character).\nIDs must be unique (exact string matching is performed to detect duplicates).\nAt least one ID must be present in the file.\nIDs cannot be any of the reserved ID headers listed above.\n\nThe ID column is the first column in the metadata file, and can optionally be followed by additional columns defining metadata associated with each sample or feature ID. Metadata files are not required to have additional metadata columns, so a file containing only an ID column is a valid QIIME 2 metadata file.\nThe contents of a metadata file following the ID column and header row (excluding comments and empty lines) are referred to as the metadata values. A single metadata value, defined by an (ID, column) pair, is referred to as a cell. The following rules apply to metadata values and cells:\n\nMay consist of any Unicode characters.\nEmpty cells represent missing data. Other values such as NA are not interpreted as missing data; only the empty cell is recognized as “missing”. Note that cells consisting solely of whitespace characters are also interpreted as missing data\nIf any cell in the metadata contains leading or trailing whitespace characters (e.g. spaces, tabs), those characters will be ignored when the file is loaded.\n\n\nRecommendations for identifiers:\n\nIdentifiers should be 36 characters long or less.\nIdentifiers should contain only ASCII alphanumeric characters (i.e. in the range of [a-z], [A-Z], or [0-9]), the period (.) character, or the dash (-) character.\nNote that some bioinformatics tools may have more restrictive requirements on identifiers than the recommendations that are outlined here. For example, Illumina sample sheet identifiers cannot have . characters, while we do include those in our set of recommended characters\n[cual-id] (https://github.com/johnchase/cual-id) can be used to help create identifiers and the associated paper also includes a discussion on how to choose identifiers [2].\n\nColumn types:\n\nQIIME 2 currently supports categorical and numeric metadata columns and will automatically attempt to infer the type of each metadata column\nQIIME 2 supports an optional comment directive to allow users to explicitly state a column’s type.\nThe comment directive must appear directly below the header row. The value in the ID column in this row must be #q2:types to indicate the row is a comment directive. Subsequent cells in this row may contain the values categorical or numeric (both case-insensitive). The empty cell is also supported if you do not wish to assign a type to a column\n\nMetadata validation:\n\nQIIME 2 will automatically validate a metadata file anytime it is used. Loading your metadata in QIIME 2 will typically present only a single error at a time, which can make identifying and resolving validation issues cumbersome, especially if there are many issues with the metadata.\nSample and feature metadata files stored in Google Sheets can additionally be validated using Keemei [3]\n\n\n\n\n\nThis tutorial focuses on data reused a Compilation of longitudinal microbiota data and hospitalome from hematopoietic cell transplantation patients [4]\n\n\n\n\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help\n\n\n\n\n\n\n\nRun this if needed. If you are not using mamba yet, replace mamba with conda to use this as an alternative installer.\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n#mamba activate qiime2-2023.7\n\n\n\n\n\n#set wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/QIIME/qiime_cmi_tutorial\"\ncd $wdir \n\n#activate QIIME environment \nmamba activate qiime2-2023.7\n\n\n\n\n\n\nmkdir data\nmkdir visualizations\n\n#download metadata table\nwget \\\n  -O 'sample-metadata.tsv' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/020-metadata/sample-metadata.tsv'\n\n#prepare metadata for qiime view \nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv \\\n  --o-visualization visualizations/metadata-summ-1.qzv\n\n#download sequencing data (already demultiplexed)\nwget \\\n  -O 'data_to_import.zip' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/030-importing/data_to_import.zip'\n\nunzip -d data_to_import data_to_import.zip\n\nrm data_to_import.zip\n\n\n\n\n\n#import data into qiime\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-format CasavaOneEightSingleLanePerSampleDirFmt \\\n  --input-path data_to_import \\\n  --output-path demultiplexed-sequences.qza\n\n#generate a summary of the imported data\nqiime demux summarize \\\n  --i-data demultiplexed-sequences.qza \\\n  --o-visualization visualizations/demultiplexed-sequences-summ.qzv\n\nqiime demux: supports demultiplexing of single-end and paired-end sequence reads and visualization of sequence quality information.\n\n\n\n\nqiime tools peek  demultiplexed-sequences.qza\n\n\n\n\nIf you have reads from multiple samples in the same file, you’ll need to demultiplex your sequences.\nIf your barcodes are still in your sequences, you can use functions from the cutadapt plugin. The cutadapt demux-single method looks for barcode sequences at the beginning of your reads (5’ end) with a certain error tolerance, removes them, and returns sequence data separated by each sample. The QIIME 2 forum has a tutorial on various functions available in cutadapt, including demultiplexing. You can learn more about how cutadapt works under the hood by reading their documentation.\nNote: Currently q2-demux and q2-cutadapt do not support demultiplexing dual-barcoded paired-end sequences, but only can demultiplex with barcodes in the forward reads. So for the time being, this type of demultiplexing needs to be done outside of QIIME 2 using other tools, for example bcl2fastq.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here.\nThere are two plugins to check out:\n\nq2-demux\ncutadapt\n\n\n\n\nWhether or not you need to merge reads depends on how you plan to cluster or denoise your sequences into amplicon sequence variants (ASVs) or operational taxonomic units (OTUs). If you plan to use deblur or OTU clustering methods next, join your sequences now. If you plan to use dada2 to denoise your sequences, do not merge — dada2 performs read merging automatically after denoising each sequence.\nIf you need to merge your reads, you can use the QIIME 2 q2-vsearch plugin with the merge-pairs method.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here.\n\n\n\nIf your data contains any non-biological sequences (e.g. primers, sequencing adapters, PCR spacers, etc), you should remove these.\nThe q2-cutadapt plugin has comprehensive methods for removing non-biological sequences from paired-end or single-end data.\nIf you’re going to use DADA2 to denoise your sequences, you can remove biological sequences at the same time as you call the denoising function. All of DADA2’s denoise fuctions have some sort of –p-trim parameter you can specify to remove base pairs from the 5’ end of your reads. (Deblur does not have this functionality yet.)\n\n\n\nThe names for these steps are very descriptive:\n\nWe denoise our sequences to remove and/or correct noisy reads.\nWe dereplicate our sequences to reduce repetition and file size/memory requirements in downstream steps.\nWe cluster sequences to collapse similar sequences (e.g., those that are ≥ 97% similar to each other) into single replicate sequences. This process, also known as OTU picking, was once a common procedure, used to simultaneously dereplicate but also perform a sort of quick-and-dirty denoising procedure (to capture stochastic sequencing and PCR errors, which should be rare and similar to more abundant centroid sequences). Use denoising methods instead if you can.\n\nThe denoising methods currently available in QIIME 2 include DADA2 and Deblur. Note that deblur (and also vsearch dereplicate-sequences) should be preceded by basic quality-score-based filtering, but this is unnecessary for dada2. Both Deblur and DADA2 contain internal chimera checking methods and abundance filtering, so additional filtering should not be necessary following these methods.\nTo put it simply, these methods filter out noisy sequences, correct errors in marginal sequences (in the case of DADA2), remove chimeric sequences, remove singletons, join denoised paired-end reads (in the case of DADA2), and then dereplicate those sequences.\n\n\nThe final products of all denoising and clustering methods/workflows are a FeatureTable[Frequency] (feature table) artifact and a FeatureData[Sequence] (representative sequences) artifact. These are two of the most important artifacts in an amplicon sequencing workflow, and are used for many downstream analyses.\nMany operations on these tables can be performed with q2-feature-table.\nWant to see which sequences are associated with each feature ID? Use qiime metadata tabulate with your FeatureData[Sequence] artifact as input.\n\n\n\nQuality control or denoising of the sequence data will be performed with DADA2 [5]. DADA2 is an model-based approach for correcting amplicon errors without constructing OTUs. Can be applied to every gene of interest and is not limited to the 16S.\nThe DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).\nAlso check out this tutorial for using DADA2. One important parameter to check is truncLen to trim low quality read ends. Also check out this paper discussing quality filtering [6].\nIn QIIME the denoise_paired action in the q2-dada2 plugin. This performs quality filtering, chimera checking, and paired- end read joining.\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots and identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. Your reads must still overlap after truncation in order to merge them later!\nSome thoughts on this:\n\nReviewing the data we notice that the twenty-fifth percentile quality score drops below 30 at position 204 in the forward reads and 205 in the reverse reads. We chose to use those values for the required truncation lengths. This truncates the 3’/5’ end of the of the input sequences. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads.\nSince the first base of the reverse reads is slightly lower than those that follow, I choose to trim that first base in the reverse reads, but apply no trimming to the forward reads. This trimming is probably unnecessary here, but is useful here for illustrating how this works.\nFigaro is a tool to automatically choose the trunc_len\nChimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2\n\nDesired overlap length\n\nOften between 10 and 20 for most applications.\nDADA2 often recommends 20\nRemember: longer overlaps means more 3’ end must be kept at a cost of decreased overall sequence quality\n\nOther parameters:\n\n--p-max-ee-f/r {number}: Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]. If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads (eg. maxEE=c(2,5))\n--p-trunc-q {integeer}: Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-for trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap {INTEGER}: The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method {TEXT Choices(‘independent’, ‘pseudo’)}: he method used to pool samples for denoising. By default, the dada function processes each sample independently. However, pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method {TEXT Choices(‘consensus’, ‘none’, ‘pooled’)}: The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-n-threads {INTEGER}: default 1\n\nOutputs:\n\nThe feature table describes which amplicon sequence variants (ASVs) were observed in which samples, and how many times each ASV was observed in each sample.\nThe feature data in this case is the sequence that defines each ASV. Generate and explore the summaries of each of these files.\nSanity check: in stats-dada2 check that outside of filtering, there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nSequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the asv table.\n\n\n#denoise data\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs demultiplexed-sequences.qza \\\n  --p-trunc-len-f 204 \\\n  --p-trim-left-r 1 \\\n  --p-trunc-len-r 205 \\\n  --o-representative-sequences asv-sequences-0.qza \\\n  --o-table feature-table-0.qza \\\n  --o-denoising-stats dada2-stats.qza\n\n#generate summaries\nqiime feature-table summarize \\\n  --i-table feature-table-0.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-0-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data asv-sequences-0.qza \\\n  --o-visualization visualizations/asv-sequences-0-summ.qzv\n\nqiime metadata tabulate \\\n  --m-input-file dada2-stats.qza \\\n  --o-visualization visualizations/stats-dada2.qzv\n\nOutputs:\n\nASV table,\nthe representative sequences,\nstatistics on the procedure\n\n\n\n\n\nWe’ll next obtain a much larger feature table representing all of the samples included in the study dataset. These would take too much time to denoise in this course, so we’ll start with the feature table, sequences, and metadata provided by the authors and filter to samples that we’ll use for our analyses.\nA full description can be foun on the QIIME website\n\n#cleanup first dataset\nmkdir upstream_tutorial\nmv *qza upstream_tutorial/\n\n#get the data\nwget \\\n  -O 'feature-table.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/feature-table.qza'\n\nwget \\\n  -O 'rep-seqs.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/rep-seqs.qza'\n\n#summarize table\nqiime feature-table summarize \\\n  --i-table feature-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data rep-seqs.qza \\\n  --o-visualization visualizations/rep-seqs-summ.qzv\n\nOutput explanation:\n\nA feature is essentially any unit of observation, e.g., an OTU, a sequence variant, a gene, a metabolite, etc, and a feature table is a matrix of sample X feature abundances (the number of times each feature was observed in each sample).\nminimum frequency is 5342 - if you click the “Interactive Sample Detail” tab and scroll to the bottom, you will see that the sample with the lowest feature frequency has 5342 observations. Similarly, the sample with the highest frequency of features has 193491 total observations - meaning that many/most features were seen more than once.\nmean frequency of features (881)? Does it mean that one feature is found in average 881 times throughout all the samples\n\n\n\n\n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table autofmt-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/autofmt-table-summ.qzv\n\n#filter time window\nqiime feature-table filter-samples \\\n  --i-table autofmt-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'DayRelativeToNearestHCT BETWEEN -10 AND 70' \\\n  --o-filtered-table filtered-table-1.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\n#done to reduce runtime (so optional)\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table filtered-table-2.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-2-summ.qzv\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\nWe can also also filter tables by lists:\n\n#create imaginary list of ids to keep\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\n#filter \nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nSome examples using where:\n\n–p-where “[subject]=‘subject-1’”\n\n–p-where “[body-site] IN (‘left palm’, ‘right palm’)”\n\n–p-where “[subject]=‘subject-1’ AND [body-site]=‘gut’”\n\n–p-where “[body-site]=‘gut’ OR [reported-antibiotic-usage]=‘Yes’”\n\n–p-where “[subject]=‘subject-1’ AND NOT [body-site]=‘gut’”\n\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza\n\n\n\n\n\nTo identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods: - classify-consensus-blast and classify-consensus-vsearch are both alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained - Machine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! - Most users do not even need to follow that tutorial and perform that training step, because the lovely QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods we’ve tested based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nIn the example below we use a pre-trained Naive Bayes taxonomic classifier. This particular classifier was trained on the Greengenes 13-8 database, where sequences were trimmed to represent only the region between the 515F / 806R primers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [7].\nCheck out this study discussing reproducible sequence taxonomy reference database management [8].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [9].\n\n\n\n\n#get classifier\nwget \\\n  -O 'gg-13-8-99-nb-classifier.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/020-taxonomy/gg-13-8-99-nb-classifier.qza'\n\n#assign taxonomic info to ASV sequences\nqiime feature-classifier classify-sklearn \\\n  --i-classifier gg-13-8-99-nb-classifier.qza \\\n  --i-reads filtered-sequences-1.qza \\\n  --o-classification taxonomy.qza\n\n#generate summary\nqiime metadata tabulate \\\n  --m-input-file taxonomy.qza \\\n  --o-visualization visualizations/taxonomy.qzv\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza\n\n\n\n\nYou may have noticed when looking at feature table summaries earlier that some of the samples contained very few ASV sequences. These often represent samples which didn’t amplify or sequence well, and when we start visualizing our data low numbers of sequences can cause misleading results, because the observed composition of the sample may not be reflective of the sample’s actual composition. For this reason it can be helpful to exclude samples with low ASV sequence counts from our samples. Here, we’ll filter out samples from which we have obtained fewer than 10,000 sequences.\n\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#summarize data\nqiime feature-table summarize \\\n  --i-table filtered-table-4.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-4-summ.qzv\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-4.qza \\\n  --o-filtered-data filtered-sequences-2.qza\n\n\n\n\n\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/taxa-bar-plots-1.qzv\n\nNotice: We can also generate heatmaps with feature-table heatmap\n\n\n\n\n\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose\n\n\n\n\n\n\n\nNotice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [10]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [11].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [12]\nNormalization and microbial differential abundance strategies depend upon data characteristics [11]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible.\n\n\n\n\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)\n\n\n\nIf continuous sample metadata columns (e.g., days-since-experiment-start) are correlated with alpha diversity, we can test for those associations here. If you’re interested in performing those tests (for this data set, or for others), you can use the qiime diversity alpha-correlation command.\nWe can also analyze sample composition in the context of categorical metadata using PERMANOVA using the beta-group-significance command. The code below was taken from another test but would allow to test whether distances between samples within a group, such as samples from the same body site (e.g., gut), are more similar to each other then they are to samples from the other groups (e.g., tongue, left palm, and right palm). If you call this command with the --p-pairwise parameter, as we’ll do here, it will also perform pairwise tests that will allow you to determine which specific pairs of groups (e.g., tongue and gut) differ from one another, if any. This command can be slow to run, especially when passing --p-pairwise, since it is based on permutation tests. So, unlike the previous commands, we’ll run beta-group-significance on specific columns of metadata that we’re interested in exploring, rather than all metadata columns to which it is applicable. Here we’ll apply this to our unweighted UniFrac distances, using two sample metadata columns, as follows.\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column body-site \\\n  --o-visualization core-metrics-results/unweighted-unifrac-body-site-significance.qzv \\\n  --p-pairwise\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column subject \\\n  --o-visualization core-metrics-results/unweighted-unifrac-subject-group-significance.qzv \\\n  --p-pairwise\n\nCheck this link for an example output.\nIf continuous sample metadata are correlated with sample composition, we can use the qiime metadata distance-matrix in combination with qiime diversity mantel and qiime diversity bioenvcommands.\n\n\n\nIn order to manage the repeated measures, we will use a linear mixed-effects model. In a mixed-effects model, we combine fixed-effects (your typical linear regression coefficients) with random-effects. These random effects are some (ostensibly random) per-group coefficient which minimizes the error within that group. In our situation, we would want our random effect to be the PatientID as we can see each subject has a different baseline for richness (and we have multiple measures for each patient). By making that a random effect, we can more accurately ascribe associations to the fixed effects as we treat each sample as a “draw” from a per-group distribution.\nIt is called a random effect because the cause of the per-subject deviation from the population intercept is not known. Its source is “random” in the statistical sense. That is randomness is not introduced to the model, but is instead tolerated by it.\nThere are several ways to create a linear model with random effects, but we will be using a random-intercept, which allows for the per-subject intercept to take on a different average from the population intercept (modeling what we saw in the group-significance plot above).\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column DayRelativeToNearestHCT \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-HCT.qzv\n\nHere we see a significant association between richness and the bone marrow transplant.\nOptions:\n\n–p-state-column TEXT Metadata column containing state (time) variable information.\n–p-individual-id-column TEXT Metadata column containing IDs for individual subjects.\n–p-metric TEXT Dependent variable column name. Must be a column name located in the metadata or feature table files.\n\nWe may also be interested in the effect of the auto fecal microbiota transplant. It should be known that these are generally correlated, so choosing one model over the other will require external knowledge.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-FMT.qzv\n\nWe also see a downward trend from the FMT. Since the goal of the FMT was to ameliorate the impact of the bone marrow transplant protocol (which involves an extreme course of antibiotics) on gut health, and the timing of the FMT is related to the timing of the marrow transplant, we might deduce that the negative coefficient is primarily related to the bone marrow transplant procedure. (We can’t prove this with statistics alone however, in this case, we are using abductive reasoning).\nLooking at the log-likelihood, we also note that the HCT result is slightly better than the FMT in accounting for the loss of richness. But only slightly, if we were to perform model testing it may not prove significant.\nIn any case, we can ask a more targeted question to identify if the FMT was useful in recovering richness.\nBy adding the autoFmtGroup to our linear model, we can see if there are different slopes for the two groups, based on an interaction term.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-treatmentVScontrol.qzv\n\nHere we see that the autoFmtGroup is not on its own a significant predictor of richness, but its interaction term with Q(‘day-relative-to-fmt’) is. This implies that there are different slopes between these groups, and we note that given the coding of Q(‘day-relative-to-fmt’):autoFmtGroup[T.treatment] we have a positive coefficient which counteracts (to a degree) the negative coefficient of Q(‘day-relative-to-fmt’).\nNotice: The slopes of the regression scatterplot in this visualization are not the same fit as our mixed-effects model in the table. They are a naive OLS fit to give a sense of the situation. A proper visualization would be a partial regression plot which can condition on other terms to show some effect in “isolation”. This is not currently implemented in QIIME 2.\n\n\n\n\n#test group significance \nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-faith-pd.qzv\n\n#lme\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric faith_pd \\\n  --o-visualization visualizations/lme-faith-pd-treatmentVScontrol.qzv\n\n\n\n\nBeta diversity is between sample diversity. This is useful for answering the question, how different are these microbial communities?\nList of available indices:\n\nJaccard distance (a qualitative measure of community dissimilarity)\nBray-Curtis distance (a quantitative measure of community dissimilarity)\nunweighted UniFrac distance (a qualitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\nweighted UniFrac distance (a quantitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\n\n\nqiime diversity beta-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metric braycurtis \\\n  --p-clustering-method nj \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/braycurtis-rarefaction-plot.qzv\n\n\numap is an ordination method that can be used in place of PCoA and has been shown to better resolve differences between microbiome samples in ordination plots [13].\nLike PCoA, umap operates on distance matrices. We’ll compute this on our weighted and unweighted UniFrac distance matrices.\n\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/unweighted_unifrac_distance_matrix.qza \\\n  --o-umap uu-umap.qza\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/weighted_unifrac_distance_matrix.qza \\\n  --o-umap wu-umap.qza\n\nIn the next few steps, we’ll integrate our unweighted UniFrac umap axis 1 values, and our Faith PD, evenness, and Shannon diversity values, as metadata in visualizations. This will provide a few different ways of interpreting these values.\n\nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization expanded-metadata-summ.qzv\n\nTo see how this information can be used, let’s generate another version of our taxonomy barplots that includes these new metadata values.\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization visualizations/taxa-bar-plots-2.qzv\n\nWe’ll start by integrating these values as metadata in our ordination plots. We’ll also customize these plots in another way: in addition to plotting the ordination axes, we’ll add an explicit time axis to these plots. This is often useful for visualization patterns in ordination plots in time series studies. We’ll add an axis for week-relative-to-hct.\n\nqiime emperor plot \\\n  --i-pcoa uu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa wu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/unweighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-pcoa-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/weighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-pcoa-emperor-w-time.qzv\n\nTip:\nQIIME 2’s q2-diversity plugin provides visualizations for assessing whether microbiome composition differs across groups of independent samples (for example, individuals with a certain disease state and healthy controls) and for assessing whether differences in microbiome composition are correlated with differences in a continuous variable (for example, subjects’ body mass index). These tools assume that all samples are independent of one another, and therefore aren’t applicable to the data used in this tutorial where multiple samples are obtained from the same individual. We therefore don’t illustrate the use of these visualizations on this data, but you can learn about these approaches and view examples in the Moving Pictures tutorial. The Moving Pictures tutorial contains example data and commands, like this tutorial does, so you can experiment with generating these visualizations on your own.\n\n\n\n\nANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [14].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use.\n\n\n\n\n\nBefore applying these analyses, we’re going to perform some additional operations on the feature table that will make these analyses run quicker and make the results more interpretable.\nFirst, we are going to use the taxonomic information that we generated earlier to redefine our features as microbial genera. To do this, we group (or collapse) ASV features based on their taxonomic assignments through the genus level. This is achieved using the q2-taxa plugin’s collapse action.\n\nqiime taxa collapse \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-level 6 \\\n  --o-collapsed-table genus-table.qza\n\nThen, to focus on the genera that are likely to display the most interesting patterns over time (and to reduce the runtime of the steps that come next), we will perform even more filtering. This time we’ll apply prevalence and abudnance based filtering. Specifically, we’ll require that a genus’s abundance is at least 1% in at least 10% of the samples.\nNote: The prevalence-based filtering applied here is fairly stringent. In your own analyses you may want to experiment with relaxed settings of these parameters. Because we want the commands below to run quickly, stringent filtering is helpful for the tutorial.\n\nqiime feature-table filter-features-conditionally \\\n  --i-table genus-table.qza \\\n  --p-prevalence 0.1 \\\n  --p-abundance 0.01 \\\n  --o-filtered-table filtered-genus-table.qza\n\nFinally, we’ll convert the counts in our feature table to relative frequencies. This is required for some of the analyses that we’re about to perform.\n\nqiime feature-table relative-frequency \\\n  --i-table filtered-genus-table.qza \\\n  --o-relative-frequency-table genus-rf-table.qza\n\n\n\n\nThe first plots we’ll generate are volatility plots. We’ll generate these using two different time variables. First, we’ll plot based on week-relative-to-hct.\nThe volatility visualizer generates interactive line plots that allow us to assess how volatile a dependent variable is over a continuous, independent variable (e.g., time) in one or more groups. See also the QIIME notes\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-hct \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-1.qzv\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-2.qzv\n\n\n\n\nThe last plots we’ll generate in this section will come from a QIIME 2 pipeline called feature-volatility. These use supervised regression to identify features that are most associated with changes over time, and add plotting of those features to a volatility control chart.\nAgain, we’ll generate the same plots but using two different time variables on the x-axes. First, we’ll plot based on week-relative-to-hct.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-hct \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility-2\n\nOutputs:\n\nvolatility-plot contains an interactive feature volatility plot. This is very similar to the plots produced by the volatility visualizer described above, with a couple key differences. First, only features are viewable as “metrics” (plotted on the y-axis). Second, feature metadata (feature importances and descriptive statistics) are plotted as bar charts below the volatility plot. The relative frequencies of different features can be plotted in the volatility chart by either selecting the “metric” selection tool, or by clicking on one of the bars in the bar plot. This makes it convenient to select features for viewing based on importance or other feature metadata. By default, the most important feature is plotted in the volatility plot when the visualization is viewed. Different feature metadata can be selected and sorted using the control panel to the right of the bar charts. Most of these should be self-explanatory, except for “cumulative average change” (the cumulative magnitude of change, both positive and negative, across states, and averaged across samples at each state), and “net average change” (positive and negative “cumulative average change” is summed to determine whether a feature increased or decreased in abundance between baseline and end of study).\naccuracy-results display the predictive accuracy of the regression model. This is important to view, as important features are meaningless if the model is inaccurate. See the sample classifier tutorial for more description of regressor accuracy results.\nfeature-importance contains the importance scores of all features. This is viewable in the feature volatility plot, but this artifact is nonetheless output for convenience. See the sample classifier tutorial for more description of feature importance scores.\nfiltered-table is a FeatureTable[RelativeFrequency] artifact containing only important features. This is output for convenience.\nsample-estimator contains the trained sample regressor. This is output for convenience, just in case you plan to regress additional samples. See the sample classifier tutorial for more description of the SampleEstimator type.\n\n\n\n\n\nIf you’re a veteran microbiome scientist and don’t want to use QIIME 2 for your analyses, you can extract your feature table and sequences from the artifact using the export tool. While export only outputs the data, the extract tool allows you to also extract other metadata such as the citations, provenance etc.\nNote that this places generically named files (e.g. feature-table.txt) into the output directory, so you may want to immediately rename the files to something more information (or somehow ensure that they stay in their original directory)!\nYou can also use the handy qiime2R package to import QIIME 2 artifacts directly into R."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#introductory-notes",
    "href": "source/Qiime/qiime_cmi_tutorial.html#introductory-notes",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "All files generated by QIIME 2 are either .qza or .qzv files, and these are simply zip files that store your data alongside some QIIME 2-specific metadata. You can unzip them with unzip xxx.qza\nThe .qza file extension is an abbreviation for QIIME Zipped Artifact, and the .qzv file extension is an abbreviation for QIIME Zipped Visualization. .qza files (which are often simply referred to as artifacts) are intermediary files in a QIIME 2 analysis, usually containing raw data of some sort.\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files (generally .qza and .qzv files) without requiring a QIIME installation.\nPlugins are software packages that can be developed by anyone and define actions, which are steps in an analysis workflow. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses.\nQIIME actions:\n\nA method, i.e.alpha-phylogenetic, accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts (qza file) as output.\nA visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input and generate qzv files. In contrast to a method, a visualizer produces exactly one visualization as output. An example of a QIIME 2 visualizer is the beta-group-significance action in the q2-diversity plugin.\nA pipeline can generate one or more .qza and/or .qzv as output. Pipelines are special in that they’re a type of action that can call other actions. They are often used by developers to define simplify common workflows so they can be run by users in a single step. For example, the core-metrics-phylogenetic action in the q2-diversity plugin is a Pipeline that runs both alpha-phylogenetic and beta-phylogenetic, as well as several other actions, in a single command.\n\nTypes used in QIIME 2:\n\nFile type: the format of a file used to store some data. For example, newick is a file type that is used for storing phylogenetic trees.\nData type: refer to how data is represented in a computer’s memory (i.e., RAM) while it’s actively in use.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. This is a representation of the meaning of the data. For example, two semantic types used in QIIME 2 are Phylogeny[Rooted] and Phylogeny[Unrooted], which are used to represent rooted and unrooted trees, respectively. QIIME 2 methods will describe what semantic types they take as input(s), and what semantic types they generate as output(s)."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#metadata",
    "href": "source/Qiime/qiime_cmi_tutorial.html#metadata",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Find out more also here.\n\nMetadata:\n\nSample metadata may include technical details, such as the DNA barcodes that were used for each sample in a multiplexed sequencing run, or descriptions of the samples, such as which subject, time point, and body site each sample came from\nFeature metadata is often a feature annotation, such as the taxonomy assigned to an amplicon sequence variant (ASV).\nQIIME 2 does not place restrictions on what types of metadata are expected to be present; there are no enforced “metadata standards”.\nthe MIxS and MIMARKS standards [1] provide recommendations for microbiome studies and may be helpful in determining what information to collect in your study. If you plan to deposit your data in a data archive (e.g. ENA or Qiita), it is also important to determine the types of metadata expected by that resource.\nIn QIIME 2, we always refer to these files as metadata files, but they are conceptually the same thing as QIIME 1 mapping files. QIIME 2 metadata files are backwards-compatible with QIIME 1 mapping files, meaning that you can use existing QIIME 1 mapping files in QIIME 2 without needing to make modifications to the file.\n\n\n\n\n\nUsually provided as TSV file (.tsv or .txt file extension) that provides data in form of rows and columns\nFirst row = non-comment, non-empty column headers containing a unique identifier for each metadata entry\nRows whose first cell begins with the pound sign (#) are interpreted as comments and may appear anywhere in the file. Comment rows are ignored by QIIME 2 and are for informational purposes only. Inline comments (i.e., comments that begin part-way through a row or at the end of a row) are not supported.\nEmpty rows (e.g. blank lines or rows consisting solely of empty cells) may appear anywhere in the file and are ignored.\nColumn 1: identifier (ID) column. This column defines the sample or feature IDs associated with your study. It is not recommended to mix sample and feature IDs in a single metadata file; keep sample and feature metadata stored in separate files. The ID column name (also referred to as the ID column header) must be:\n\nCase-insenitive (i.e., uppercase or lowercase, or a mixing of the two, is allowed): id, sampleid, sample id, sample-id, featureid feature id, feature-id\nIDs may consist of any Unicode characters, with the exception that IDs must not start with the pound sign (#), as those rows would be interpreted as comments and ignored.\nIDs cannot be empty (i.e. they must consist of at least one character).\nIDs must be unique (exact string matching is performed to detect duplicates).\nAt least one ID must be present in the file.\nIDs cannot be any of the reserved ID headers listed above.\n\nThe ID column is the first column in the metadata file, and can optionally be followed by additional columns defining metadata associated with each sample or feature ID. Metadata files are not required to have additional metadata columns, so a file containing only an ID column is a valid QIIME 2 metadata file.\nThe contents of a metadata file following the ID column and header row (excluding comments and empty lines) are referred to as the metadata values. A single metadata value, defined by an (ID, column) pair, is referred to as a cell. The following rules apply to metadata values and cells:\n\nMay consist of any Unicode characters.\nEmpty cells represent missing data. Other values such as NA are not interpreted as missing data; only the empty cell is recognized as “missing”. Note that cells consisting solely of whitespace characters are also interpreted as missing data\nIf any cell in the metadata contains leading or trailing whitespace characters (e.g. spaces, tabs), those characters will be ignored when the file is loaded.\n\n\nRecommendations for identifiers:\n\nIdentifiers should be 36 characters long or less.\nIdentifiers should contain only ASCII alphanumeric characters (i.e. in the range of [a-z], [A-Z], or [0-9]), the period (.) character, or the dash (-) character.\nNote that some bioinformatics tools may have more restrictive requirements on identifiers than the recommendations that are outlined here. For example, Illumina sample sheet identifiers cannot have . characters, while we do include those in our set of recommended characters\n[cual-id] (https://github.com/johnchase/cual-id) can be used to help create identifiers and the associated paper also includes a discussion on how to choose identifiers [2].\n\nColumn types:\n\nQIIME 2 currently supports categorical and numeric metadata columns and will automatically attempt to infer the type of each metadata column\nQIIME 2 supports an optional comment directive to allow users to explicitly state a column’s type.\nThe comment directive must appear directly below the header row. The value in the ID column in this row must be #q2:types to indicate the row is a comment directive. Subsequent cells in this row may contain the values categorical or numeric (both case-insensitive). The empty cell is also supported if you do not wish to assign a type to a column\n\nMetadata validation:\n\nQIIME 2 will automatically validate a metadata file anytime it is used. Loading your metadata in QIIME 2 will typically present only a single error at a time, which can make identifying and resolving validation issues cumbersome, especially if there are many issues with the metadata.\nSample and feature metadata files stored in Google Sheets can additionally be validated using Keemei [3]\n\n\n\n\n\nThis tutorial focuses on data reused a Compilation of longitudinal microbiota data and hospitalome from hematopoietic cell transplantation patients [4]\n\n\n\n\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#setup",
    "href": "source/Qiime/qiime_cmi_tutorial.html#setup",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Run this if needed. If you are not using mamba yet, replace mamba with conda to use this as an alternative installer.\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n#mamba activate qiime2-2023.7\n\n\n\n\n\n#set wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/QIIME/qiime_cmi_tutorial\"\ncd $wdir \n\n#activate QIIME environment \nmamba activate qiime2-2023.7"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#download-data",
    "href": "source/Qiime/qiime_cmi_tutorial.html#download-data",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "mkdir data\nmkdir visualizations\n\n#download metadata table\nwget \\\n  -O 'sample-metadata.tsv' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/020-metadata/sample-metadata.tsv'\n\n#prepare metadata for qiime view \nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv \\\n  --o-visualization visualizations/metadata-summ-1.qzv\n\n#download sequencing data (already demultiplexed)\nwget \\\n  -O 'data_to_import.zip' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/030-importing/data_to_import.zip'\n\nunzip -d data_to_import data_to_import.zip\n\nrm data_to_import.zip"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-import-import-data-into-qiime",
    "href": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-import-import-data-into-qiime",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "#import data into qiime\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-format CasavaOneEightSingleLanePerSampleDirFmt \\\n  --input-path data_to_import \\\n  --output-path demultiplexed-sequences.qza\n\n#generate a summary of the imported data\nqiime demux summarize \\\n  --i-data demultiplexed-sequences.qza \\\n  --o-visualization visualizations/demultiplexed-sequences-summ.qzv\n\nqiime demux: supports demultiplexing of single-end and paired-end sequence reads and visualization of sequence quality information."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-peek-check-artifact-format",
    "href": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-peek-check-artifact-format",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "qiime tools peek  demultiplexed-sequences.qza"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#demultiplexing",
    "href": "source/Qiime/qiime_cmi_tutorial.html#demultiplexing",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "If you have reads from multiple samples in the same file, you’ll need to demultiplex your sequences.\nIf your barcodes are still in your sequences, you can use functions from the cutadapt plugin. The cutadapt demux-single method looks for barcode sequences at the beginning of your reads (5’ end) with a certain error tolerance, removes them, and returns sequence data separated by each sample. The QIIME 2 forum has a tutorial on various functions available in cutadapt, including demultiplexing. You can learn more about how cutadapt works under the hood by reading their documentation.\nNote: Currently q2-demux and q2-cutadapt do not support demultiplexing dual-barcoded paired-end sequences, but only can demultiplex with barcodes in the forward reads. So for the time being, this type of demultiplexing needs to be done outside of QIIME 2 using other tools, for example bcl2fastq.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here.\nThere are two plugins to check out:\n\nq2-demux\ncutadapt"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#merging-reads",
    "href": "source/Qiime/qiime_cmi_tutorial.html#merging-reads",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Whether or not you need to merge reads depends on how you plan to cluster or denoise your sequences into amplicon sequence variants (ASVs) or operational taxonomic units (OTUs). If you plan to use deblur or OTU clustering methods next, join your sequences now. If you plan to use dada2 to denoise your sequences, do not merge — dada2 performs read merging automatically after denoising each sequence.\nIf you need to merge your reads, you can use the QIIME 2 q2-vsearch plugin with the merge-pairs method.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#removing-non-biological-sequences",
    "href": "source/Qiime/qiime_cmi_tutorial.html#removing-non-biological-sequences",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "If your data contains any non-biological sequences (e.g. primers, sequencing adapters, PCR spacers, etc), you should remove these.\nThe q2-cutadapt plugin has comprehensive methods for removing non-biological sequences from paired-end or single-end data.\nIf you’re going to use DADA2 to denoise your sequences, you can remove biological sequences at the same time as you call the denoising function. All of DADA2’s denoise fuctions have some sort of –p-trim parameter you can specify to remove base pairs from the 5’ end of your reads. (Deblur does not have this functionality yet.)"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#denoising-and-clustering",
    "href": "source/Qiime/qiime_cmi_tutorial.html#denoising-and-clustering",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "The names for these steps are very descriptive:\n\nWe denoise our sequences to remove and/or correct noisy reads.\nWe dereplicate our sequences to reduce repetition and file size/memory requirements in downstream steps.\nWe cluster sequences to collapse similar sequences (e.g., those that are ≥ 97% similar to each other) into single replicate sequences. This process, also known as OTU picking, was once a common procedure, used to simultaneously dereplicate but also perform a sort of quick-and-dirty denoising procedure (to capture stochastic sequencing and PCR errors, which should be rare and similar to more abundant centroid sequences). Use denoising methods instead if you can.\n\nThe denoising methods currently available in QIIME 2 include DADA2 and Deblur. Note that deblur (and also vsearch dereplicate-sequences) should be preceded by basic quality-score-based filtering, but this is unnecessary for dada2. Both Deblur and DADA2 contain internal chimera checking methods and abundance filtering, so additional filtering should not be necessary following these methods.\nTo put it simply, these methods filter out noisy sequences, correct errors in marginal sequences (in the case of DADA2), remove chimeric sequences, remove singletons, join denoised paired-end reads (in the case of DADA2), and then dereplicate those sequences.\n\n\nThe final products of all denoising and clustering methods/workflows are a FeatureTable[Frequency] (feature table) artifact and a FeatureData[Sequence] (representative sequences) artifact. These are two of the most important artifacts in an amplicon sequencing workflow, and are used for many downstream analyses.\nMany operations on these tables can be performed with q2-feature-table.\nWant to see which sequences are associated with each feature ID? Use qiime metadata tabulate with your FeatureData[Sequence] artifact as input.\n\n\n\nQuality control or denoising of the sequence data will be performed with DADA2 [5]. DADA2 is an model-based approach for correcting amplicon errors without constructing OTUs. Can be applied to every gene of interest and is not limited to the 16S.\nThe DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).\nAlso check out this tutorial for using DADA2. One important parameter to check is truncLen to trim low quality read ends. Also check out this paper discussing quality filtering [6].\nIn QIIME the denoise_paired action in the q2-dada2 plugin. This performs quality filtering, chimera checking, and paired- end read joining.\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots and identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. Your reads must still overlap after truncation in order to merge them later!\nSome thoughts on this:\n\nReviewing the data we notice that the twenty-fifth percentile quality score drops below 30 at position 204 in the forward reads and 205 in the reverse reads. We chose to use those values for the required truncation lengths. This truncates the 3’/5’ end of the of the input sequences. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads.\nSince the first base of the reverse reads is slightly lower than those that follow, I choose to trim that first base in the reverse reads, but apply no trimming to the forward reads. This trimming is probably unnecessary here, but is useful here for illustrating how this works.\nFigaro is a tool to automatically choose the trunc_len\nChimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2\n\nDesired overlap length\n\nOften between 10 and 20 for most applications.\nDADA2 often recommends 20\nRemember: longer overlaps means more 3’ end must be kept at a cost of decreased overall sequence quality\n\nOther parameters:\n\n--p-max-ee-f/r {number}: Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]. If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads (eg. maxEE=c(2,5))\n--p-trunc-q {integeer}: Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-for trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap {INTEGER}: The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method {TEXT Choices(‘independent’, ‘pseudo’)}: he method used to pool samples for denoising. By default, the dada function processes each sample independently. However, pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method {TEXT Choices(‘consensus’, ‘none’, ‘pooled’)}: The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-n-threads {INTEGER}: default 1\n\nOutputs:\n\nThe feature table describes which amplicon sequence variants (ASVs) were observed in which samples, and how many times each ASV was observed in each sample.\nThe feature data in this case is the sequence that defines each ASV. Generate and explore the summaries of each of these files.\nSanity check: in stats-dada2 check that outside of filtering, there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nSequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the asv table.\n\n\n#denoise data\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs demultiplexed-sequences.qza \\\n  --p-trunc-len-f 204 \\\n  --p-trim-left-r 1 \\\n  --p-trunc-len-r 205 \\\n  --o-representative-sequences asv-sequences-0.qza \\\n  --o-table feature-table-0.qza \\\n  --o-denoising-stats dada2-stats.qza\n\n#generate summaries\nqiime feature-table summarize \\\n  --i-table feature-table-0.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-0-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data asv-sequences-0.qza \\\n  --o-visualization visualizations/asv-sequences-0-summ.qzv\n\nqiime metadata tabulate \\\n  --m-input-file dada2-stats.qza \\\n  --o-visualization visualizations/stats-dada2.qzv\n\nOutputs:\n\nASV table,\nthe representative sequences,\nstatistics on the procedure"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#filtering-the-feature-table",
    "href": "source/Qiime/qiime_cmi_tutorial.html#filtering-the-feature-table",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "We’ll next obtain a much larger feature table representing all of the samples included in the study dataset. These would take too much time to denoise in this course, so we’ll start with the feature table, sequences, and metadata provided by the authors and filter to samples that we’ll use for our analyses.\nA full description can be foun on the QIIME website\n\n#cleanup first dataset\nmkdir upstream_tutorial\nmv *qza upstream_tutorial/\n\n#get the data\nwget \\\n  -O 'feature-table.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/feature-table.qza'\n\nwget \\\n  -O 'rep-seqs.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/rep-seqs.qza'\n\n#summarize table\nqiime feature-table summarize \\\n  --i-table feature-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data rep-seqs.qza \\\n  --o-visualization visualizations/rep-seqs-summ.qzv\n\nOutput explanation:\n\nA feature is essentially any unit of observation, e.g., an OTU, a sequence variant, a gene, a metabolite, etc, and a feature table is a matrix of sample X feature abundances (the number of times each feature was observed in each sample).\nminimum frequency is 5342 - if you click the “Interactive Sample Detail” tab and scroll to the bottom, you will see that the sample with the lowest feature frequency has 5342 observations. Similarly, the sample with the highest frequency of features has 193491 total observations - meaning that many/most features were seen more than once.\nmean frequency of features (881)? Does it mean that one feature is found in average 881 times throughout all the samples\n\n\n\n\n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table autofmt-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/autofmt-table-summ.qzv\n\n#filter time window\nqiime feature-table filter-samples \\\n  --i-table autofmt-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'DayRelativeToNearestHCT BETWEEN -10 AND 70' \\\n  --o-filtered-table filtered-table-1.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\n#done to reduce runtime (so optional)\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table filtered-table-2.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-2-summ.qzv\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\nWe can also also filter tables by lists:\n\n#create imaginary list of ids to keep\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\n#filter \nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nSome examples using where:\n\n–p-where “[subject]=‘subject-1’”\n\n–p-where “[body-site] IN (‘left palm’, ‘right palm’)”\n\n–p-where “[subject]=‘subject-1’ AND [body-site]=‘gut’”\n\n–p-where “[body-site]=‘gut’ OR [reported-antibiotic-usage]=‘Yes’”\n\n–p-where “[subject]=‘subject-1’ AND NOT [body-site]=‘gut’”\n\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#taxonomic-annotations",
    "href": "source/Qiime/qiime_cmi_tutorial.html#taxonomic-annotations",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "To identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods: - classify-consensus-blast and classify-consensus-vsearch are both alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained - Machine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! - Most users do not even need to follow that tutorial and perform that training step, because the lovely QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods we’ve tested based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nIn the example below we use a pre-trained Naive Bayes taxonomic classifier. This particular classifier was trained on the Greengenes 13-8 database, where sequences were trimmed to represent only the region between the 515F / 806R primers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [7].\nCheck out this study discussing reproducible sequence taxonomy reference database management [8].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [9].\n\n\n\n\n#get classifier\nwget \\\n  -O 'gg-13-8-99-nb-classifier.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/020-taxonomy/gg-13-8-99-nb-classifier.qza'\n\n#assign taxonomic info to ASV sequences\nqiime feature-classifier classify-sklearn \\\n  --i-classifier gg-13-8-99-nb-classifier.qza \\\n  --i-reads filtered-sequences-1.qza \\\n  --o-classification taxonomy.qza\n\n#generate summary\nqiime metadata tabulate \\\n  --m-input-file taxonomy.qza \\\n  --o-visualization visualizations/taxonomy.qzv\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza\n\n\n\n\nYou may have noticed when looking at feature table summaries earlier that some of the samples contained very few ASV sequences. These often represent samples which didn’t amplify or sequence well, and when we start visualizing our data low numbers of sequences can cause misleading results, because the observed composition of the sample may not be reflective of the sample’s actual composition. For this reason it can be helpful to exclude samples with low ASV sequence counts from our samples. Here, we’ll filter out samples from which we have obtained fewer than 10,000 sequences.\n\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#summarize data\nqiime feature-table summarize \\\n  --i-table filtered-table-4.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-4-summ.qzv\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-4.qza \\\n  --o-filtered-data filtered-sequences-2.qza"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#generate-taxonomic-composition-barplots",
    "href": "source/Qiime/qiime_cmi_tutorial.html#generate-taxonomic-composition-barplots",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "qiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/taxa-bar-plots-1.qzv\n\nNotice: We can also generate heatmaps with feature-table heatmap"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#phylogenetic-tree-construction",
    "href": "source/Qiime/qiime_cmi_tutorial.html#phylogenetic-tree-construction",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "One advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "href": "source/Qiime/qiime_cmi_tutorial.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Notice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [10]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [11].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [12]\nNormalization and microbial differential abundance strategies depend upon data characteristics [11]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#computing-diversity-metrics",
    "href": "source/Qiime/qiime_cmi_tutorial.html#computing-diversity-metrics",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "core-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)\n\n\n\nIf continuous sample metadata columns (e.g., days-since-experiment-start) are correlated with alpha diversity, we can test for those associations here. If you’re interested in performing those tests (for this data set, or for others), you can use the qiime diversity alpha-correlation command.\nWe can also analyze sample composition in the context of categorical metadata using PERMANOVA using the beta-group-significance command. The code below was taken from another test but would allow to test whether distances between samples within a group, such as samples from the same body site (e.g., gut), are more similar to each other then they are to samples from the other groups (e.g., tongue, left palm, and right palm). If you call this command with the --p-pairwise parameter, as we’ll do here, it will also perform pairwise tests that will allow you to determine which specific pairs of groups (e.g., tongue and gut) differ from one another, if any. This command can be slow to run, especially when passing --p-pairwise, since it is based on permutation tests. So, unlike the previous commands, we’ll run beta-group-significance on specific columns of metadata that we’re interested in exploring, rather than all metadata columns to which it is applicable. Here we’ll apply this to our unweighted UniFrac distances, using two sample metadata columns, as follows.\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column body-site \\\n  --o-visualization core-metrics-results/unweighted-unifrac-body-site-significance.qzv \\\n  --p-pairwise\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column subject \\\n  --o-visualization core-metrics-results/unweighted-unifrac-subject-group-significance.qzv \\\n  --p-pairwise\n\nCheck this link for an example output.\nIf continuous sample metadata are correlated with sample composition, we can use the qiime metadata distance-matrix in combination with qiime diversity mantel and qiime diversity bioenvcommands.\n\n\n\nIn order to manage the repeated measures, we will use a linear mixed-effects model. In a mixed-effects model, we combine fixed-effects (your typical linear regression coefficients) with random-effects. These random effects are some (ostensibly random) per-group coefficient which minimizes the error within that group. In our situation, we would want our random effect to be the PatientID as we can see each subject has a different baseline for richness (and we have multiple measures for each patient). By making that a random effect, we can more accurately ascribe associations to the fixed effects as we treat each sample as a “draw” from a per-group distribution.\nIt is called a random effect because the cause of the per-subject deviation from the population intercept is not known. Its source is “random” in the statistical sense. That is randomness is not introduced to the model, but is instead tolerated by it.\nThere are several ways to create a linear model with random effects, but we will be using a random-intercept, which allows for the per-subject intercept to take on a different average from the population intercept (modeling what we saw in the group-significance plot above).\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column DayRelativeToNearestHCT \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-HCT.qzv\n\nHere we see a significant association between richness and the bone marrow transplant.\nOptions:\n\n–p-state-column TEXT Metadata column containing state (time) variable information.\n–p-individual-id-column TEXT Metadata column containing IDs for individual subjects.\n–p-metric TEXT Dependent variable column name. Must be a column name located in the metadata or feature table files.\n\nWe may also be interested in the effect of the auto fecal microbiota transplant. It should be known that these are generally correlated, so choosing one model over the other will require external knowledge.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-FMT.qzv\n\nWe also see a downward trend from the FMT. Since the goal of the FMT was to ameliorate the impact of the bone marrow transplant protocol (which involves an extreme course of antibiotics) on gut health, and the timing of the FMT is related to the timing of the marrow transplant, we might deduce that the negative coefficient is primarily related to the bone marrow transplant procedure. (We can’t prove this with statistics alone however, in this case, we are using abductive reasoning).\nLooking at the log-likelihood, we also note that the HCT result is slightly better than the FMT in accounting for the loss of richness. But only slightly, if we were to perform model testing it may not prove significant.\nIn any case, we can ask a more targeted question to identify if the FMT was useful in recovering richness.\nBy adding the autoFmtGroup to our linear model, we can see if there are different slopes for the two groups, based on an interaction term.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-treatmentVScontrol.qzv\n\nHere we see that the autoFmtGroup is not on its own a significant predictor of richness, but its interaction term with Q(‘day-relative-to-fmt’) is. This implies that there are different slopes between these groups, and we note that given the coding of Q(‘day-relative-to-fmt’):autoFmtGroup[T.treatment] we have a positive coefficient which counteracts (to a degree) the negative coefficient of Q(‘day-relative-to-fmt’).\nNotice: The slopes of the regression scatterplot in this visualization are not the same fit as our mixed-effects model in the table. They are a naive OLS fit to give a sense of the situation. A proper visualization would be a partial regression plot which can condition on other terms to show some effect in “isolation”. This is not currently implemented in QIIME 2.\n\n\n\n\n#test group significance \nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-faith-pd.qzv\n\n#lme\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric faith_pd \\\n  --o-visualization visualizations/lme-faith-pd-treatmentVScontrol.qzv\n\n\n\n\nBeta diversity is between sample diversity. This is useful for answering the question, how different are these microbial communities?\nList of available indices:\n\nJaccard distance (a qualitative measure of community dissimilarity)\nBray-Curtis distance (a quantitative measure of community dissimilarity)\nunweighted UniFrac distance (a qualitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\nweighted UniFrac distance (a quantitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\n\n\nqiime diversity beta-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metric braycurtis \\\n  --p-clustering-method nj \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/braycurtis-rarefaction-plot.qzv\n\n\numap is an ordination method that can be used in place of PCoA and has been shown to better resolve differences between microbiome samples in ordination plots [13].\nLike PCoA, umap operates on distance matrices. We’ll compute this on our weighted and unweighted UniFrac distance matrices.\n\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/unweighted_unifrac_distance_matrix.qza \\\n  --o-umap uu-umap.qza\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/weighted_unifrac_distance_matrix.qza \\\n  --o-umap wu-umap.qza\n\nIn the next few steps, we’ll integrate our unweighted UniFrac umap axis 1 values, and our Faith PD, evenness, and Shannon diversity values, as metadata in visualizations. This will provide a few different ways of interpreting these values.\n\nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization expanded-metadata-summ.qzv\n\nTo see how this information can be used, let’s generate another version of our taxonomy barplots that includes these new metadata values.\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization visualizations/taxa-bar-plots-2.qzv\n\nWe’ll start by integrating these values as metadata in our ordination plots. We’ll also customize these plots in another way: in addition to plotting the ordination axes, we’ll add an explicit time axis to these plots. This is often useful for visualization patterns in ordination plots in time series studies. We’ll add an axis for week-relative-to-hct.\n\nqiime emperor plot \\\n  --i-pcoa uu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa wu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/unweighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-pcoa-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/weighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-pcoa-emperor-w-time.qzv\n\nTip:\nQIIME 2’s q2-diversity plugin provides visualizations for assessing whether microbiome composition differs across groups of independent samples (for example, individuals with a certain disease state and healthy controls) and for assessing whether differences in microbiome composition are correlated with differences in a continuous variable (for example, subjects’ body mass index). These tools assume that all samples are independent of one another, and therefore aren’t applicable to the data used in this tutorial where multiple samples are obtained from the same individual. We therefore don’t illustrate the use of these visualizations on this data, but you can learn about these approaches and view examples in the Moving Pictures tutorial. The Moving Pictures tutorial contains example data and commands, like this tutorial does, so you can experiment with generating these visualizations on your own."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#differential-abundance-testing",
    "href": "source/Qiime/qiime_cmi_tutorial.html#differential-abundance-testing",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "ANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [14].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#longitudinal-microbiome-analysis",
    "href": "source/Qiime/qiime_cmi_tutorial.html#longitudinal-microbiome-analysis",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Before applying these analyses, we’re going to perform some additional operations on the feature table that will make these analyses run quicker and make the results more interpretable.\nFirst, we are going to use the taxonomic information that we generated earlier to redefine our features as microbial genera. To do this, we group (or collapse) ASV features based on their taxonomic assignments through the genus level. This is achieved using the q2-taxa plugin’s collapse action.\n\nqiime taxa collapse \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-level 6 \\\n  --o-collapsed-table genus-table.qza\n\nThen, to focus on the genera that are likely to display the most interesting patterns over time (and to reduce the runtime of the steps that come next), we will perform even more filtering. This time we’ll apply prevalence and abudnance based filtering. Specifically, we’ll require that a genus’s abundance is at least 1% in at least 10% of the samples.\nNote: The prevalence-based filtering applied here is fairly stringent. In your own analyses you may want to experiment with relaxed settings of these parameters. Because we want the commands below to run quickly, stringent filtering is helpful for the tutorial.\n\nqiime feature-table filter-features-conditionally \\\n  --i-table genus-table.qza \\\n  --p-prevalence 0.1 \\\n  --p-abundance 0.01 \\\n  --o-filtered-table filtered-genus-table.qza\n\nFinally, we’ll convert the counts in our feature table to relative frequencies. This is required for some of the analyses that we’re about to perform.\n\nqiime feature-table relative-frequency \\\n  --i-table filtered-genus-table.qza \\\n  --o-relative-frequency-table genus-rf-table.qza\n\n\n\n\nThe first plots we’ll generate are volatility plots. We’ll generate these using two different time variables. First, we’ll plot based on week-relative-to-hct.\nThe volatility visualizer generates interactive line plots that allow us to assess how volatile a dependent variable is over a continuous, independent variable (e.g., time) in one or more groups. See also the QIIME notes\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-hct \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-1.qzv\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-2.qzv\n\n\n\n\nThe last plots we’ll generate in this section will come from a QIIME 2 pipeline called feature-volatility. These use supervised regression to identify features that are most associated with changes over time, and add plotting of those features to a volatility control chart.\nAgain, we’ll generate the same plots but using two different time variables on the x-axes. First, we’ll plot based on week-relative-to-hct.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-hct \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility-2\n\nOutputs:\n\nvolatility-plot contains an interactive feature volatility plot. This is very similar to the plots produced by the volatility visualizer described above, with a couple key differences. First, only features are viewable as “metrics” (plotted on the y-axis). Second, feature metadata (feature importances and descriptive statistics) are plotted as bar charts below the volatility plot. The relative frequencies of different features can be plotted in the volatility chart by either selecting the “metric” selection tool, or by clicking on one of the bars in the bar plot. This makes it convenient to select features for viewing based on importance or other feature metadata. By default, the most important feature is plotted in the volatility plot when the visualization is viewed. Different feature metadata can be selected and sorted using the control panel to the right of the bar charts. Most of these should be self-explanatory, except for “cumulative average change” (the cumulative magnitude of change, both positive and negative, across states, and averaged across samples at each state), and “net average change” (positive and negative “cumulative average change” is summed to determine whether a feature increased or decreased in abundance between baseline and end of study).\naccuracy-results display the predictive accuracy of the regression model. This is important to view, as important features are meaningless if the model is inaccurate. See the sample classifier tutorial for more description of regressor accuracy results.\nfeature-importance contains the importance scores of all features. This is viewable in the feature volatility plot, but this artifact is nonetheless output for convenience. See the sample classifier tutorial for more description of feature importance scores.\nfiltered-table is a FeatureTable[RelativeFrequency] artifact containing only important features. This is output for convenience.\nsample-estimator contains the trained sample regressor. This is output for convenience, just in case you plan to regress additional samples. See the sample classifier tutorial for more description of the SampleEstimator type."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-export-export-data",
    "href": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-export-export-data",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "If you’re a veteran microbiome scientist and don’t want to use QIIME 2 for your analyses, you can extract your feature table and sequences from the artifact using the export tool. While export only outputs the data, the extract tool allows you to also extract other metadata such as the citations, provenance etc.\nNote that this places generically named files (e.g. feature-table.txt) into the output directory, so you may want to immediately rename the files to something more information (or somehow ensure that they stay in their original directory)!\nYou can also use the handy qiime2R package to import QIIME 2 artifacts directly into R."
  },
  {
    "objectID": "source/ITSx/itsx_readme.html",
    "href": "source/ITSx/itsx_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "“ITSx is an open source software utility to extract the highly variable ITS1 and ITS2 subregions from ITS sequences, which is commonly used as a molecular barcode for e.g. fungi.”\n\nWebsite\nManual\nPaper. Please do not forget to cite the ITSx paper whenever you use the software (Bengtsson-Palme et al. 2013).\n\nAvailable on Crunchomics: No\n\n\n\nITSx can be installed from scratch but below is the code needed to install the software with mamba.\nIf there is an issue with the mamba/conda installation, the software can also be downloaded with wget https://microbiology.se/sw/ITSx_1.1.3.tar.gz. After the download, decompress the folder and follow the information in the readme.txt. The download also comes with a test.fasta which can be used to test either installation.\nData for testing can also be found here.\n\nmamba create -n fungi_its\nmamba install -n fungi_its -c bioconda itsx\n\n\n\n\n\nRequired input: FASTA format (aligned or unaligned, DNA or RNA)\nGenerated output:\n\none summary file of the entire run\none more detailed table containing the positions in the respective sequences where the ITS subregions were found\none “semi-graphical” representation of hits\none FASTA file of all identified ITS sequences\none FASTA file for the ITS1 and ITS2 regions\nif entries that did not contain any ITS region are found, a list of sequence IDs representing those entries (optional)\n\nUseful arguments (not extensive, check manual for all arguments):\n\n--save_regions: Get all regions of interest, not only ITS1/2\n-E {value}: E-value cutoff (default 1e-5)\n-S {value}: Domain score cutoff (default 0)\n--cpu {value }: Number of cpus to use (default 1)\n--multi_thread {T/F}: Multi-thread the HMMER-search. On (T) by default if the number of CPUs/cores is larger than one (–cpu option &gt; 1), else off (F)\n--preserve {T/F}: If on, ITSx will preserve the sequence headers from the input file instead of replacing them with ITSx headers in the output. Off (F) by default.\n--only_full {T/F}: If true, the output is limited to full-length ITS1 and ITS2 regions only. Off (F) by default.\n--minlen {value} Minimum length the ITS regions must be to be outputted in the concatenated file (see –concat above). Default is zero (0).\n\n\n\n\n\n#activate the right environment\nmamba activate fungi_its\n\n#download data for testing the installation (optional)\nmkdir testing\nwget -P testing https://raw.githubusercontent.com/ScienceParkStudyGroup/software_information/main/data/itsx/test.fasta\n\n#testrun (adjust path of test.fasta to where ever you downloaded the software)\nITSx -i testing/test.fasta --save_regions all -o testing/ITS_test_v1\n\n\n#deactivate environment (if using environment)\nmamba deactivate\n\nRegions extracted from test file (notice how the full fasta ONLY contains sequences with all regions):\n\ntesting/ITS_test_v1.5_8S.fasta:50\ntesting/ITS_test_v1.ITS1.fasta:50\ntesting/ITS_test_v1.ITS2.fasta:50\ntesting/ITS_test_v1.LSU.fasta:32\ntesting/ITS_test_v1.SSU.fasta:31\ntesting/ITS_test_v1.full.fasta:19\ntesting/ITS_test_v1_no_detections.fasta:0\ntesting/test.fasta:50"
  },
  {
    "objectID": "source/ITSx/itsx_readme.html#itsx",
    "href": "source/ITSx/itsx_readme.html#itsx",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "“ITSx is an open source software utility to extract the highly variable ITS1 and ITS2 subregions from ITS sequences, which is commonly used as a molecular barcode for e.g. fungi.”\n\nWebsite\nManual\nPaper. Please do not forget to cite the ITSx paper whenever you use the software (Bengtsson-Palme et al. 2013).\n\nAvailable on Crunchomics: No\n\n\n\nITSx can be installed from scratch but below is the code needed to install the software with mamba.\nIf there is an issue with the mamba/conda installation, the software can also be downloaded with wget https://microbiology.se/sw/ITSx_1.1.3.tar.gz. After the download, decompress the folder and follow the information in the readme.txt. The download also comes with a test.fasta which can be used to test either installation.\nData for testing can also be found here.\n\nmamba create -n fungi_its\nmamba install -n fungi_its -c bioconda itsx\n\n\n\n\n\nRequired input: FASTA format (aligned or unaligned, DNA or RNA)\nGenerated output:\n\none summary file of the entire run\none more detailed table containing the positions in the respective sequences where the ITS subregions were found\none “semi-graphical” representation of hits\none FASTA file of all identified ITS sequences\none FASTA file for the ITS1 and ITS2 regions\nif entries that did not contain any ITS region are found, a list of sequence IDs representing those entries (optional)\n\nUseful arguments (not extensive, check manual for all arguments):\n\n--save_regions: Get all regions of interest, not only ITS1/2\n-E {value}: E-value cutoff (default 1e-5)\n-S {value}: Domain score cutoff (default 0)\n--cpu {value }: Number of cpus to use (default 1)\n--multi_thread {T/F}: Multi-thread the HMMER-search. On (T) by default if the number of CPUs/cores is larger than one (–cpu option &gt; 1), else off (F)\n--preserve {T/F}: If on, ITSx will preserve the sequence headers from the input file instead of replacing them with ITSx headers in the output. Off (F) by default.\n--only_full {T/F}: If true, the output is limited to full-length ITS1 and ITS2 regions only. Off (F) by default.\n--minlen {value} Minimum length the ITS regions must be to be outputted in the concatenated file (see –concat above). Default is zero (0).\n\n\n\n\n\n#activate the right environment\nmamba activate fungi_its\n\n#download data for testing the installation (optional)\nmkdir testing\nwget -P testing https://raw.githubusercontent.com/ScienceParkStudyGroup/software_information/main/data/itsx/test.fasta\n\n#testrun (adjust path of test.fasta to where ever you downloaded the software)\nITSx -i testing/test.fasta --save_regions all -o testing/ITS_test_v1\n\n\n#deactivate environment (if using environment)\nmamba deactivate\n\nRegions extracted from test file (notice how the full fasta ONLY contains sequences with all regions):\n\ntesting/ITS_test_v1.5_8S.fasta:50\ntesting/ITS_test_v1.ITS1.fasta:50\ntesting/ITS_test_v1.ITS2.fasta:50\ntesting/ITS_test_v1.LSU.fasta:32\ntesting/ITS_test_v1.SSU.fasta:31\ntesting/ITS_test_v1.full.fasta:19\ntesting/ITS_test_v1_no_detections.fasta:0\ntesting/test.fasta:50"
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "These tutorial closely follows the tutorial from IBEDs former bioinformatician, Evelyn Jongepier. A large part of the text found here was taken directly from this tutorial but extended as was seen fit and updated to work with a newer QIIME version.\nA test data set is provided with this tutorial and consists of already demultiplexed sequencing data. The files we are working with are:\n\nThe forward, R1, reads\nThe reverse, R2, reads\nThe metadata file describing what kind of data we work with (in our case that is simply a sample ID)\n\nOther comments on the files:\n\nThe barcodes used are ~20 bp long\nEach read is 250 bp long\nThe primers used are called 515F and 926R and target the V4 region of the 16S SSU rRNA resulting in a fragment length or amplicon size of 411 bp\nWhen removing the primer, we have a target region of around 371 bp\nHow to calculating whether or not there is an overlap between our forward and reverse read: (length of forward read) + (length of reverse read) − (length of amplicon) = length of overlap\n\n250 + 250 - 411 = ~90 bp overlap\n\n\n\n\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (i.e. provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. Semantic types enable QIIME 2 to identify artifacts that are suitable inputs to an analysis. For example, if an analysis expects a distance matrix as input, QIIME 2 can determine which artifacts have a distance matrix semantic type and prevent incompatible artifacts from being used in the analysis (e.g. an artifact representing a phylogenetic tree).\nPlugins are software packages that can be developed by anyone. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses. A method accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts as output. A visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input. In contrast to a method, a visualizer produces exactly one visualization as output.\n\n\n\n\n\nWe can install a QIIME 2 environment with with mamba:\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n\nIf you run into problem, have a look at QIIME 2 installation guide.\n\n\n\nNext, we download the example files for this tutorial. The folder contains:\n\nDe-multiplexed sequence fastq files in the data folder\nA manifest.csv file that lists the sampleID, sample location and read orientation in the data folder\nFiles generated during the workflow\n\n\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\ncd metabarcoding-qiime2-datapackage-v2021.06.2\n\n\n\n\nNotice:\n\nChange the path stored in wdir to wherever you downloaded the data\nSetting an environmental variable to your working directory is not needed but useful to have if you resume an analysis after closing the terminal or simply to remind yourself where you analysed your data\n\n\n#set working environment\nwdir=\"&lt;path_to_your_downloaded_folder&gt;/metabarcoding-qiime2-datapackage-v2021.06.2/\"\ncd $wdir\n\n#activate QIIME environment \nconda deactivate\nmamba activate qiime2-2023.7\n\n\n\n\nThe help argument allows you to get an explanation about what each plugin in QIIME 2 does:\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help\n\n\n\n\n\n\n\nThe manifest file includes the sample ids, the path to where each sequence read file, i.e. fastq.gz file, is stored, and the read orientation. Because we have paired-end data and thus two files for each sample, we will list each sample twice, once for the forward and once for the reverse orientation. This is what the first few lines of the manifest file look like if you open the file manually or run head data/MANIFEST.csv in the terminal:\nsample-id,absolute-filepath,direction\nDUMMY10,$PWD/data/Sample-DUMMY10_R1.fastq.gz,forward\nDUMMY10,$PWD/data/Sample-DUMMY10_R2.fastq.gz,reverse\nDUMMY11,$PWD/data/Sample-DUMMY11_R1.fastq.gz,forward\nDUMMY11,$PWD/data/Sample-DUMMY11_R2.fastq.gz,reverse\nDUMMY12,$PWD/data/Sample-DUMMY12_R1.fastq.gz,forward\nDUMMY12,$PWD/data/Sample-DUMMY12_R2.fastq.gz,reverse\nNotice:\n\nThe manifest file is a csv file, i.e. a comma-separated file\nWe use $PWD, standing for print working directory, to say that the input files are in the data folder which is found in our working directory.\n\n\n\n\nWe can import our data into QIIME as follows:\n\n#prepare a folder in which we store our imported data\nmkdir -p prep\n\n#import the fastq files into QIIME\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path data/MANIFEST.csv \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#check artifact type\nqiime tools peek prep/demux-seqs.qza \n\n#generate summary\nqiime demux summarize \\\n    --i-data prep/demux-seqs.qza \\\n    --o-visualization prep/demux-seqs.qzv\n\nIf we visualize prep/demux-seqs.qzv with QIIME view, we get an overview about our samples:\n\nSome additonal comments on importing data:\n\nA general tutorial on importing data can be found here\nYou can import data by:\n\nproviding the path to the sequences with --input-path\nOr you can import sequences using the maifest file, which maps the sample identifiers to a fastq.gz or fastq file. It requires a file path, which as to be an absolute filepath (but can use $PWD/) as well as the direction of the reads. The manifest file is compatible with the metadata format\n\nThere are different phred scores used in the input-format: Phred33 and Phred64. In Phred 64 files, you should not be seeing phred values &lt;64. But you can also look at upper case and lower case of quality representations (ascii letters). In general, lower case means 64.\n\nUseful Options:\n\n--type {TEXT} The semantic type of the artifact that will be created upon importing. Use –show-importable-types to see what importable semantic types are available in the current deployment. [required]\n--input-path {PATH} Path to file or directory that should be imported. [required]\n--output-path {ARTIFACT} Path where output artifact should be written. [required]\n--input-format {TEXT} The format of the data to be imported. If not provided, data must be in the format expected by the semantic type provided via –type.\n--show-importable-types Show the semantic types that can be supplied to –type to import data into an artifact.\n\n\n\n\n\n\n\nFastQC is a tool to judge the quality of sequencing data (in a visual way). It is installed on Crunchomics and if you want to install it on your own machine follow the instructions found here.\n\nmkdir prep/fastqc\nfastqc data/*gz -o prep/fastqc\n\n\n\n\nThe next step is to remove any primer sequences. We will use the cutadapt QIIME2 plugin for that. Because we have paired-end data, there is a forward and reverse primer, referenced by the parameters --p-front-f and --p-front-r.\n\nqiime cutadapt trim-paired \\\n    --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f GTGYCAGCMGCCGCGGTAA \\\n  --p-front-r CCGYCAATTYMTTTRAGTTT \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose\n\n#generate summary \nqiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#export data (not necessary but to give an example how to do this)\nqiime tools extract \\\n    --output-path prep/trimmed_remove_primers \\\n    --input-path prep/trimmed-seqs.qza\n\nUseful options (for the full list, check the help function):\n\nThere are many ways an adaptor can be ligated. Check the help function for all options and if you do not know where the adaptor is contact your sequencing center.\n--p-front-f TEXT… Sequence of an adapter ligated to the 5’ end. Searched in forward read\n--p-front-r TEXT… Sequence of an adapter ligated to the 5’ end. Searched in reverse read\n--p-error-rate PROPORTION Range(0, 1, inclusive_end=True) Maximum allowed error rate. [default: 0.1]\n--p-indels / --p-no-indels Allow insertions or deletions of bases when matching adapters. [default: True]\n--p-times INTEGER Remove multiple occurrences of an adapter if it is Range(1, None) repeated, up to times times. [default: 1]\n--p-match-adapter-wildcards / --p-no-match-adapter-wildcards Interpret IUPAC wildcards (e.g., N) in adapters. [default: True]\n--p-minimum-length INTEGER Range(1, None) Discard reads shorter than specified value. Note, the cutadapt default of 0 has been overridden, because that value produces empty sequence records. [default: 1]\n--p-max-expected-errors NUMBER Range(0, None) Discard reads that exceed maximum expected erroneous nucleotides. [optional] --p-max-n NUMBER Discard reads with more than COUNT N bases. If Range(0, None) COUNT_or_FRACTION is a number between 0 and 1, it is interpreted as a fraction of the read length. [optional] --p-quality-cutoff-5end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 5 prime end. [default: 0] --p-quality-cutoff-3end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 3 prime end. [default: 0] --p-quality-base INTEGER Range(0, None) How the Phred score is encoded (33 or 64). [default: 33]\n\n\n\n\n\nIn order to minimize the risks of sequencer error in targeted sequencing, clustering approaches were initially developed. Clustering approaches are based upon the idea that related/similar organisms will have similar gene sequences and that rare sequencing errors will have a trivial contribution, if any, to the consensus sequence for these clusters, or operating taxonomic units (OTUs).\nIn contrast, ASV methods generate an error model tailored to an individual sequencing run and employing algorithms that use the model to distinguish between true biological sequences and those generated by error. The two ASV methods we explore in this tutorial are Deblur and DADA2.\nDADA2 implements an algorithm that models the errors introduced during amplicon sequencing, and uses that error model to infer the true sample composition. DADA2 replaces the traditional OTU-picking step in amplicon sequencing workflows and instead produces tables of amplicon sequence variants (ASVs).\nDeblur uses pre-computed error profiles to obtain putative error-free sequences from Illumina MiSeq and HiSeq sequencing platforms. Unlike DADA2, Deblur operates on each sample independently. Additionally, reads are depleted from sequencing artifacts either using a set of known sequencing artifacts (such as PhiX) (negative filtering) or using a set of known 16S sequences (positive filtering).\n\n\n\n\nNotice:\n\nDepending on the QIIME 2 version you use, you might need to change qiime vsearch join-pairs to vsearch merge-pairs.\nSome more details on merging reads can be found here\nIf you plan to use DADA2 to join and denoise your paired end data, do not join your reads prior to denoising with DADA2; DADA2 expects reads that have not yet been joined, and will join the reads for you during the denoising process.\nSome notes on overlap calculation for dada2\nAnother option for joining is fastq-join\n\n\nmkdir -p deblur\n\n#merge reads\nqiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 2 \\\n  --verbose\n\n#summarize data\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nUseful options:\n\n--p-minlen {INTEGER} Sequences shorter than minlen after truncation are Range(0, None) discarded. [default: 1]\n--p-maxns {INTEGER} Sequences with more than maxns N characters are Range(0, None) discarded. [optional]\n--p-maxdiffs {INTEGER} Maximum number of mismatches in the area of overlap Range(0, None) during merging. [default: 10]\n--p-minmergelen {INTEGER Range(0, None) } Minimum length of the merged read to be retained. [optional]\n--p-maxee {NUMBER} Maximum number of expected errors in the merged read Range(0.0, None) to be retained. [optional]\n--p-threads {INTEGER Range(0, 8, inclusive_end=True)} The number of threads to use for computation. Does not scale much past 4 threads. [default: 1]\n\nWhen running this, we will get a lot of information printed to the screen. It is useful to look at this to know if the merge worked well. For example, we see below that ~90% of our sequences merged well, telling us that everything went fine. Much lower values might indicate some problems with the trimmed data.\nMerging reads 100%\n     45195  Pairs\n     40730  Merged (90.1%)\n      4465  Not merged (9.9%)\n\nPairs that failed merging due to various reasons:\n        50  too few kmers found on same diagonal\n        30  multiple potential alignments\n      1878  too many differences\n      1790  alignment score too low, or score drop too high\n       717  staggered read pairs\n\nStatistics of all reads:\n    233.02  Mean read length\n\nStatistics of merged reads:\n    374.39  Mean fragment length\n     14.67  Standard deviation of fragment length\n      0.32  Mean expected error in forward sequences\n      0.78  Mean expected error in reverse sequences\n      0.51  Mean expected error in merged sequences\n      0.21  Mean observed errors in merged region of forward sequences\n      0.71  Mean observed errors in merged region of reverse sequences\n      0.92  Mean observed errors in merged region\nNotice that if we look at deblur/joined-seqs.qzv with QIIME view we see an increased quality score in the middle of the region:\n\nWhen merging reads with tools like vsearch, the quality scores often increase, not decrease, in the region of overlap. The reason being, if the forward and reverse read call the same base at the same position (even if both are low quality), then the quality estimate for the base in that position goes up. That is, you have two independent observations of that base in that position. This is often the benefit of being able to merge paired reads, as you can recover / increase your confidence of the sequence in the region of overlap.\n\n\n\nNotice: Both DADA and deblur will do some quality filtering, so we do not need to be too strict here. One could consider whether to even include this step, however, running this might be useful to reduce the dataset site.\n\nqiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\n#summarize stats\nqiime metadata tabulate \\\n   --m-input-file deblur/filt-stats.qza \\\n   --o-visualization deblur/filt-stats.qzv\n\nDefault settings:\n\n--p-min-quality4 All PHRED scores less that this value are considered to be low PHRED scores.\n--p-quality-window 3: The maximum number of low PHRED scores that can be observed in direct succession before truncating a sequence read.\n--p-min-length-fraction 0.75: The minimum length that a sequence read can be following truncation and still be retained. This length should be provided as a fraction of the input sequence length. --p-max-ambiguous 0: The maximum number of ambiguous (i.e., N) base calls.\n\n\n\n\n\n#default settings\nqiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length 370 \\\n  --o-representative-sequences deblur/deblur-reprseqs2.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 2 \\\n  --verbose\n\nHow to set the parameters:\n\nWhen you use deblur-denoise, you need to truncate your fragments such that they are all of the same length. The position at which sequences are truncated is specified by the --p-trim-length parameter. Any sequence that is shorter than this value will be lost from your analyses. Any sequence that is longer will be truncated at this position.\nTo decide on what value to choose, inspect deblur/filt-seqs.qzv with QIIME view. Once you open the interactive quality plot and scroll down we see the following:\n\n\n\n\n\n\n\nWe can set --p-trim-length of 370, because that resulted in minimal data loss. That is, only &lt;9% of the reads were discarded for being too short, and only 4 bases were trimmed off from sequences of median length.\n\nUseful options:\n\n--p-trim-length INTEGER Sequence trim length, specify -1 to disable trimming. [required]\n--p-left-trim-len {INTEGER} Range(0, None) Sequence trimming from the 5’ end. A value of 0 will disable this trim. [default: 0]\n--p-mean-error NUMBER The mean per nucleotide error, used for original sequence estimate. [default: 0.005]\n--p-indel-probNUMBER Insertion/deletion (indel) probability (same for N indels). [default: 0.01]\n--p-indel-max INTEGER Maximum number of insertion/deletions. [default: 3]\n--p-min-reads INTEGER Retain only features appearing at least min-reads times across all samples in the resulting feature table. [default: 10]\n--p-min-size INTEGER In each sample, discard all features with an abundance less than min-size. [default: 2]\n--p-jobs-to-start INTEGER Number of jobs to start (if to run in parallel). [default: 1]\n\nGenerated outputs:\n\nA feature table artifact with the frequencies per sample and feature.\nA representative sequences artifact with one single fasta sequence for each of the features in the feature table.\nA stats artifact with details of how many reads passed each filtering step of the deblur procedure.\n\nWe can create visualizations from the different outputs as follows:\n\n#look at stats\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats_noremoval.qza \\\n  --o-visualization deblur/deblur-stats_noremova.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table_noremoval.qza \\\n  --o-visualization deblur/deblur-table_noremova.qzv\n\nIf we check the visualization with QIIME 2 view (or qiime tools view deblur/deblur-table.qzv) we get some feelings about how many ASVs were generated and where we lost things. Often times the Interactive sample detail page can be particularily interesting. This shows you the frequency per sample. If there is large variation in this number between samples, it means it is difficult to directly compare samples. In that case it is often recommended to standardize your data by for instance rarefaction. Rarefaction will not be treated in detail in the tutorial, but the idea is laid out below.\n\nOut of the 1,346,802 filtered and merged reads we get 3,636 features with a total frequency of 290,671. So we only retained ~21%. If we compare this to the QIIME CMI tutorial we have 662 features with a total frequency of 1,535,691 (~84) so this is definitely something to watch out for.\nWith this dataset we loose a lot to “fraction-artifact-with-minsize” . This value is related to the --p-min-size parameter which sets the min abundance of a read to be retained (default 2). So we simply might loose things because it is a subsetted dataset and it means that most of your reads are singletons and so are discarded. For a real dataset this could be due to improper merging, not having removed primer/barcodes from your reads, or that there just is that many singletons (though unlikely imo)\nloss of sequences with merged reads in deblur compared to just the forward reads has been observed and discussed before see here\n\n\n\n\n\nThe code below is not required to be run. However, the code is useful in case something goes wrong and you need to check if the file you work with is corrupted.\n\n#check file: Result deblur/filt-seqs.qza appears to be valid at level=max.\nqiime tools validate deblur/filt-seqs.qza\n\n\n\n\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots to: - identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. - If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. - If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads - Figaro is a tool to automatically choose the trunc_len - Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2 - Your reads must still overlap after truncation in order to merge them later!\n\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trim-left-f 0 \\\n  --p-trim-left-r 0 \\\n  --p-trunc-len-f 230 \\\n  --p-trunc-len-r 220 \\\n  --p-n-threads 4 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose\n\nUseful options:\n\n--p-trunc-len-f INTEGER Position at which forward read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trunc-len-r INTEGER Position at which reverse read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trim-left-f INTEGER Position at which forward read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-trim-left-r INTEGER Position at which reverse read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-max-ee-f NUMBER Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-max-ee-r NUMBER Reverse reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-trunc-q INTEGER Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-f or trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap INTEGER Range(4, None) The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method TEXT Choices(‘independent’, ‘pseudo’) The method used to pool samples for denoising. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method TEXT Choices(‘consensus’, ‘none’, ‘pooled’) The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-min-fold-parent-over-abundance NUMBER The minimum abundance of potential parents of a sequence being tested as chimeric, expressed as a fold-change versus the abundance of the sequence being tested. Values should be greater than or equal to 1 (i.e. parents should be more abundant than the sequence being tested). This parameter has no effect if chimera-method is “none”. [default: 1.0]\n--p-n-threads INTEGER The number of threads to use for multithreaded processing. If 0 is provided, all available cores will be used. [default: 1]\n--p-n-reads-learn INTEGER The number of reads to use when training the error model. Smaller numbers will result in a shorter run time but a less reliable error model. [default: 1000000]\n\nLet’s now summarize the DADA2 output:\n\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nNotes:\n\nSanity check: in the stats visualizations for both DADA2 and deblur check that outside of filtering, there should be no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nsequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the ASV table.\n\nIf we compare the two tools, we should see something like this:\n\nDeblur:\n\n3,636 features\n290,671 total frequency (out of 1,346,802 filtered and merged reads, ca 21%)\nnothing at frequencies below 2500, most samples with frequencies between 2500-7500, 1 sample with 20 000\n\nDada2:\n\n13,934 features\n648,666 total frequency (out of 1554929 trimmed reads, ~41%)\nmost samples at 10,000 frequency/sample peak, histrogram looks closer to normal distribution, 1 sample with ~37 000 frequencies\n\n\nOveral Dada2 seems beneficial in that less reads are lost but without some deeper dive into the data the qzv files are not informative enough to decide what happened to the sequences. The reason why more DADA2 retains more reads is related to the quality filtering steps. To better compare sequences, we could compare them at a sampling depth of 2000 and should see: - Retained 94,000 (32.34%) features in 47 (100.00%) samples (deblur) instead of 290,671 (100.00%) features in 47 (100.00%) - Retained 94,000 (14.49%) features in 47 (95.92%) samples (dada2) instead of 648,666 (100.00%) features in 49 (100.00%)\nBased on that both methods produce very similar results.\n\n\n\n\nTo identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods:\n\nclassify-consensus-blast\nclassify-consensus-vsearch\nMachine-learning-based classification methods\n\nThe first two are alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained\nMachine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [@bokulich2018].\nCheck out this study discussing reproducible sequence taxonomy reference database management [@ii2021].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [@kaehler2019].\n\nFor the steps below a classifier using SILVA_138_99 is pre-computed the classifier. Steps outlining how to do this can be found here.\nRequirements:\n\nThe reference sequences\nThe reference taxonomy\n\n\n\nFirst, we import the SILVA database (consisting of fasta sequences and ta taxonomy table) into QIIME:\n\nqiime tools import \\\n  --type \"FeatureData[Sequence]\" \\\n  --input-path db/SILVA_138_99_16S-ref-seqs.fna \\\n  --output-path db/SILVA_138_99_16S-ref-seqs.qza\n\nqiime tools import \\\n  --type \"FeatureData[Taxonomy]\" \\\n  --input-format HeaderlessTSVTaxonomyFormat \\\n  --input-path db/SILVA_138_99_16S-ref-taxonomy.txt \\\n  --output-path db/SILVA_138_99_16S-ref-taxonomy.qza\n\n\n\n\nWe can now use these same sequences we used to amplify our 16S sequences to extract the corresponding region of the 16S rRNA sequences from the SILVA database, like so:\n\nqiime feature-classifier extract-reads \\\n  --i-sequences db/SILVA_138_99_16S-ref-seqs.qza \\\n  --p-f-primer GTGYCAGCMGCCGCGGTAA \\\n  --p-r-primer CCGYCAATTYMTTTRAGTTT \\\n  --o-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --p-n-jobs 2\n\nOptions:\n\n--p-trim-right 0\n--p-trunc-len 0\n--p-identity {NUMBER} minimum combined primer match identity threshold. [default: 0.8] –p-min-length {INTEGER} Minimum amplicon length. Shorter amplicons are Range(0, None) discarded. Applied after trimming and truncation, so be aware that trimming may impact sequence retention. Set to zero to disable min length filtering. [default: 50]\n--p-max-length 0\n\nThis results in :\n\nMin length: 56 (we have partial sequences)\nMax length: 2316 (indicates we might have some non-16S seqs or some 16S sequences have introns)\nMean length: 394 (What we would expect based on our 16S data)\n\n\n\n\nNext, we use the reference fragments you just created to train your classifier specifically on your region of interest.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have &gt;32GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the db folder.\n\n\n\nqiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --i-reference-taxonomy db/SILVA_138_99_16S-ref-taxonomy.qza \\\n  --o-classifier db/SILVA_138_99_16S-ref-classifier.qza\n\n\n\n\nNow that we have a trained classifier and a set of representative sequences from our DADA2-denoise analyses we can finally classify the sequences in our dataset.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have ~50 GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the taxonomy folder.\n\n\n\nqiime feature-classifier classify-sklearn \\\n  --i-classifier db/SILVA_138_99_16S-ref-classifier.qza \\\n  --p-n-jobs 16 \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza\n\nArguments:\n\n--p-confidence {VALUE Float % Range(0, 1, inclusive_end=True) | Str % Choices(‘disable’)} Confidence threshold for limiting taxonomic depth. Set to “disable” to disable confidence calculation, or 0 to calculate confidence but not apply it to limit the taxonomic depth of the assignments. [default: 0.7]\n--p-read-orientation {TEXT Choices(‘same’, ‘reverse-complement’, ‘auto’)} Direction of reads with respect to reference sequences. same will cause reads to be classified unchanged; reverse-complement will cause reads to be reversed and complemented prior to classification. “auto” will autodetect orientation based on the confidence estimates for the first 100 reads. [default: ‘auto’]\n\nNext, view the data with:\n\nqiime metadata tabulate \\\n --m-input-file taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nqiime2 tools view taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nWe see:\n\nThat this also reports a confidence score ranging between 0.7 and 1. The lower limit of 0.7 is the default value (see also the qiime feature-classifier classify-sklearn help function). You can opt for a lower value to increase the number of features with a classification, but beware that that will also increase the risk of spurious classifcations!\n\n\n\n\nBefore running the command, we will need to prepare a metadata file. This metadata file should contain information on the samples. For instance, at which depth was the sample taken, from which location does it come, was it subjected to experimental or control treatment etc. etc. This information is of course very specific to the study design but at the very least it should look like this (see also data/META.tsv):\n#SampleID\n#q2:types\nDUMMY1\nDUMMY10\n...\nbut we can add any variables, like so:\n#SampleID    BarcodeSequence Location        depth   location        treatment       grainsize       flowrate        age\n#q2:types    categorical     categorical     categorical     catechorical    categorical     numeric numeric numeric\n&lt;your data&gt;\nNext, we can generate a barplot like this:\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n  --m-metadata-file data/META.tsv \\\n  --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv\n\nqiime tools view taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv\n\n\n\n\n\n\n\nThese are just some general comments on filtering ASV tables and are not yet implemented in this specific tutorial. A full description of filtering methods can be found on the QIIME website\n\n#filter using the metadata info \n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#filter out samples from which we have obtained fewer than 10,000 sequences\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#create imaginary list of ids to keep and use this to filter\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\n\n\n\n\n#filter the representative sequences based on a filtered ASV table\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza\n\n\n\n\n\nNote: This are general notes and not yet implemented in this tutorial\n\n\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose\n\n\n\n\n\n\n\nNotice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [@gloor2017]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [@weiss2017].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [@mcmurdie2014]\nNormalization and microbial differential abundance strategies depend upon data characteristics [@weiss2017]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible.\n\n\n\n\nNote: These are general notes and not yet implemented in this tutorial\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)\n\n\n\n\nANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [@mandal2015].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use.\n\n\n\nIf submitting jobs via the srun command it can be useful to submit them via a screen for long running jobs. Some basics on using a screen can be found here.\n\n\nGeneral notice: In Evelyn’s tutorial qiime2-2021.2 was used. For this run, qiime2 was updated to a newer version and the tutorial code adjusted if needed.\nTo be able to access the amplicomics share, and the QIIME 2 installation and necessary databases, please contact Nina Dombrowski with your UvAnetID.\n\n#check what environments we already have\nconda env list\n\n#add amplicomics environment to be able to use the QIIME conda install\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\n\n\n\nIf you run this the first time, start by downloading the tutorial data.\n\n#download data data\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\n\n\n\n\nBelow, we set some environmental variables describing the location for key files we want to use. The only thing that you need to change yourself is the path to the working directory, ie. wdir, and the path_to_where_you_downloaded_the_data.\n\nwdir=\"/home/ndombro/tutorials/evelyn_qiime\"\ncd $wdir\n\n#make symbolic link to have the structures in the tutorial work\nln -s path_to_where_you_downloaded_the_data/metabarcoding-qiime2-datapackage-v2021.06.2/data ./\n\n#set environmental variables\nexport MANIFEST=\"data/MANIFEST.csv\"\nexport META=\"data/META.tsv\"\nexport PRIMFWD=\"GTGYCAGCMGCCGCGGTAA\"\nexport PRIMREV=\"CCGYCAATTYMTTTRAGTTT\"\nexport TRIMLENG=370\nexport TRUNKFWD=230\nexport TRUNKREV=220\n\nexport DBSEQ=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-seqs.qza\"\nexport DBTAX=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-tax.qza\"\nexport DBPREFIX=\"SILVA_138_99_16S\"\n\n#activate qiime environment\nmamba activate qiime2-2023.7\n\n\n\n\n\nmkdir -p logs\nmkdir -p prep\nmkdir -p deblur\nmkdir -p dada2\nmkdir -p db\nmkdir -p taxonomy\n\n\n\n\nWhen using Slurm we use the following settings to say that:\n\nthe job consists of 1 task (-n 1)\nthe job needs to be run on 1 cpu (–cpus-per-task 1)\n\n\nsrun -n 1 --cpus-per-task 1 qiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path $MANIFEST \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#create visualization\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/demux-seqs.qza \\\n  --o-visualization prep/demux-seqs.qzv\n\n#instructions to view qvz (tool not available, need to check if I can find it) \n#how-to-view-this-qzv prep/demux-seqs.qzv\n\nSummary of statistics:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\nsrun -n 1 --cpus-per-task 4 qiime cutadapt trim-paired \\\n  --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f $PRIMFWD \\\n  --p-front-r $PRIMREV \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-cutadapt-trim-paired.log\n\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#make a table out of this (quick, so ok to run on headnode)\nsrun -n 1 --cpus-per-task 1 qiime tools extract \\\n   --input-path prep/trimmed-seqs.qzv \\\n   --output-path visualizations/trimmed-seqs\n\n\nInformation about the 2&gt;&1 is a redirection operator :\n\nIt says to “redirect stderr (file descriptor 2) to the same location as stdout (file descriptor 1).” This means that both stdout and stderr will be combined and sent to the same destination\n\nThe tee command is used to:\n\nLogging: It captures the standard output (stdout) and standard error (stderr) of the command that precedes it and saves it to a file specified as its argument. In this case, it saves the output and errors to the file logs/qiime-cutadapt-trim-paired.log.\nDisplaying Output: In addition to saving the output to a file, tee also displays the captured output and errors on the terminal in real-time. This allows you to see the progress and any error messages while the command is running.\n\nIf we would want to run the command without tee we could do 2&gt;&1 &gt; logs/qiime-cutadapt-trim-paired.log but this won’t print output to the screen\n\nSummary of run:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\n\nNote that for the qiime cutadapt trim-paired-command you used p-cores while here you need p-threads\n\nsrun -n 1 --cpus-per-task 4 qiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 4 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-vsearch-join-pairs.log\n\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nNotice: This step cannot be sped up because the qiime quality-filter q-score command does not have an option to use more cpus or threads.\n\nsrun -n 1 --cpus-per-task 1 qiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-quality-filter-q-score.log\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\nqiime metadata tabulate \\\n  --m-input-file deblur/filt-stats.qza \\\n  --o-visualization deblur/filt-stats.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nBelow we:\n\nWe defined a memory allocation of 16GB. This is the total amount of RAM you expect to need for this job (+ a bit more to be on the safe side). How much you need is not always easy to predict and requires some experience/trial and error. As a rule of thumb: if you get an Out Of Memory error, double it and try again. Note that you can also define mem-per-cpu, which may be easier to work with if you often change the number of cpus between analyses.\nWe get a warning warn' method is deprecated, use 'warning' instead, but we can ignore that\n\n\n#job id: 398288\nsrun -n 1 --cpus-per-task 10 --mem=16GB qiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length $TRIMLENG \\\n  --o-representative-sequences deblur/deblur-reprseqs.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --p-min-size 2 \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 10 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-deblur-denoise-16S.log\n\n#check memory requirements: 1.6GB\n#maxrss = maximum amount of memory used at any time by any process in that job\nsacct -j 398288 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\ncat deblur.log &gt;&gt; logs/qiime_deblur_denoise-16S.log && rm deblur.log\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data deblur/deblur-reprseqs.qza \\\n  --o-visualization deblur/deblur-reprseqs.qzv\n\n\n\n\n\nDADA2 performs filtering, joining and denoising all with one single command, and therefor takes longer to run. Here, it takes ca. 30m5.334s using 8 cpus. It is tempting to just increase the number of cpus when impatient, but please note that not all commands are very efficient at running more jobs in parallel (i.e. on multiple cpus). The reason often is that the rate-limiting step in the workflow is unable to use multiple cpus, so all but one are idle.\n\n\n#job id: 398291\ntime srun -n 1 --cpus-per-task 8 --mem=32GB qiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trunc-len-f $TRUNKFWD \\\n  --p-trunc-len-r $TRUNKREV \\\n  --p-n-threads 8 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-dada2-denoise-paired.log\n\n#get mem usage\nsacct -j 398291 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#get summaries\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nUsage info –&gt; 7.64878 GB needed and 50 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398291            qiime          8              00:49:55\n398291.exte+     extern          8          0   00:49:55\n398291.0          qiime          8   7648780K   00:49:55\n\n\n\n\n\n\n\n#job id: 398333\ntime srun -n 1 --cpus-per-task 8 --mem=4GB qiime feature-classifier extract-reads \\\n  --i-sequences $DBSEQ \\\n  --p-f-primer $PRIMFWD \\\n  --p-r-primer $PRIMREV \\\n  --o-reads db/$DBPREFIX-ref-frags.qza \\\n  --p-n-jobs 8 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398333 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nUsage info –&gt; 1.43846 GB needed in 4 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398333            qiime          8              00:04:33\n398333.exte+     extern          8          0   00:04:33\n398333.0          qiime          8   1438460K   00:04:33\n\n\n\nTraining a classifier takes quite some time, especially because the qiime feature-classifier fit-classifier-naive-bayes- command cannot use multiple cpus in parallel (see qiime feature-classifier fit-classifier-naive-bayes –help). Here, it took ca. 145m56.836s. You can however often re-use your classifier, provided you used the same primers and db.\n\n#job id: 398335\ntime srun -n 1 --cpus-per-task 1 --mem=32GB qiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/$DBPREFIX-ref-frags.qza \\\n  --i-reference-taxonomy $DBTAX \\\n  --o-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --p-verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398335 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nResources needed –&gt; 33.4GB\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398335            qiime          2              02:19:24\n398335.exte+     extern          2          0   02:19:24\n398335.0          qiime          1  33469976K   02:19:24\n\n\n\nImportant\nThe classifier takes up quite some disc space. If you ‘just’ run it you are likely to get a No space left on device-error. You can avoid this by changing the directory where temporary files are written to, but make sure you have sufficient space there as well of course.\n\n#prepare tmp dir \nmkdir -p tmptmp\nexport TMPDIR=$PWD/tmptmp/\n\n#job id: 398339\ntime srun -n 1 --cpus-per-task 32 --mem=160GB qiime feature-classifier classify-sklearn \\\n  --i-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --p-n-jobs 32 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-classify-sklearn.log\n\n#get mem usage\nsacct -j 398339 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#cleanup\nrm -fr tmptmp\n\nqiime metadata tabulate \\\n  --m-input-file taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxonomy.qzv\n\nResource need –&gt; 158.7 GB:\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398339            qiime         32              00:13:57\n398339.exte+     extern         32          0   00:13:57\n398339.0          qiime         32 158749076K   00:13:57\n\n\n\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --m-metadata-file $META \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxplot.qzv\n\n\n\n\n\n\nscp crunchomics:'/home/ndombro/tutorials/evelyn_qiime/*/*.qzv' crunchomics_files/"
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#qiime2-terminology",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#qiime2-terminology",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "Data produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (i.e. provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. Semantic types enable QIIME 2 to identify artifacts that are suitable inputs to an analysis. For example, if an analysis expects a distance matrix as input, QIIME 2 can determine which artifacts have a distance matrix semantic type and prevent incompatible artifacts from being used in the analysis (e.g. an artifact representing a phylogenetic tree).\nPlugins are software packages that can be developed by anyone. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses. A method accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts as output. A visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input. In contrast to a method, a visualizer produces exactly one visualization as output."
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#setup",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#setup",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "We can install a QIIME 2 environment with with mamba:\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n\nIf you run into problem, have a look at QIIME 2 installation guide.\n\n\n\nNext, we download the example files for this tutorial. The folder contains:\n\nDe-multiplexed sequence fastq files in the data folder\nA manifest.csv file that lists the sampleID, sample location and read orientation in the data folder\nFiles generated during the workflow\n\n\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\ncd metabarcoding-qiime2-datapackage-v2021.06.2\n\n\n\n\nNotice:\n\nChange the path stored in wdir to wherever you downloaded the data\nSetting an environmental variable to your working directory is not needed but useful to have if you resume an analysis after closing the terminal or simply to remind yourself where you analysed your data\n\n\n#set working environment\nwdir=\"&lt;path_to_your_downloaded_folder&gt;/metabarcoding-qiime2-datapackage-v2021.06.2/\"\ncd $wdir\n\n#activate QIIME environment \nconda deactivate\nmamba activate qiime2-2023.7\n\n\n\n\nThe help argument allows you to get an explanation about what each plugin in QIIME 2 does:\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help"
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#import-data",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#import-data",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "The manifest file includes the sample ids, the path to where each sequence read file, i.e. fastq.gz file, is stored, and the read orientation. Because we have paired-end data and thus two files for each sample, we will list each sample twice, once for the forward and once for the reverse orientation. This is what the first few lines of the manifest file look like if you open the file manually or run head data/MANIFEST.csv in the terminal:\nsample-id,absolute-filepath,direction\nDUMMY10,$PWD/data/Sample-DUMMY10_R1.fastq.gz,forward\nDUMMY10,$PWD/data/Sample-DUMMY10_R2.fastq.gz,reverse\nDUMMY11,$PWD/data/Sample-DUMMY11_R1.fastq.gz,forward\nDUMMY11,$PWD/data/Sample-DUMMY11_R2.fastq.gz,reverse\nDUMMY12,$PWD/data/Sample-DUMMY12_R1.fastq.gz,forward\nDUMMY12,$PWD/data/Sample-DUMMY12_R2.fastq.gz,reverse\nNotice:\n\nThe manifest file is a csv file, i.e. a comma-separated file\nWe use $PWD, standing for print working directory, to say that the input files are in the data folder which is found in our working directory.\n\n\n\n\nWe can import our data into QIIME as follows:\n\n#prepare a folder in which we store our imported data\nmkdir -p prep\n\n#import the fastq files into QIIME\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path data/MANIFEST.csv \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#check artifact type\nqiime tools peek prep/demux-seqs.qza \n\n#generate summary\nqiime demux summarize \\\n    --i-data prep/demux-seqs.qza \\\n    --o-visualization prep/demux-seqs.qzv\n\nIf we visualize prep/demux-seqs.qzv with QIIME view, we get an overview about our samples:\n\nSome additonal comments on importing data:\n\nA general tutorial on importing data can be found here\nYou can import data by:\n\nproviding the path to the sequences with --input-path\nOr you can import sequences using the maifest file, which maps the sample identifiers to a fastq.gz or fastq file. It requires a file path, which as to be an absolute filepath (but can use $PWD/) as well as the direction of the reads. The manifest file is compatible with the metadata format\n\nThere are different phred scores used in the input-format: Phred33 and Phred64. In Phred 64 files, you should not be seeing phred values &lt;64. But you can also look at upper case and lower case of quality representations (ascii letters). In general, lower case means 64.\n\nUseful Options:\n\n--type {TEXT} The semantic type of the artifact that will be created upon importing. Use –show-importable-types to see what importable semantic types are available in the current deployment. [required]\n--input-path {PATH} Path to file or directory that should be imported. [required]\n--output-path {ARTIFACT} Path where output artifact should be written. [required]\n--input-format {TEXT} The format of the data to be imported. If not provided, data must be in the format expected by the semantic type provided via –type.\n--show-importable-types Show the semantic types that can be supplied to –type to import data into an artifact."
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#quality-controls",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#quality-controls",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "FastQC is a tool to judge the quality of sequencing data (in a visual way). It is installed on Crunchomics and if you want to install it on your own machine follow the instructions found here.\n\nmkdir prep/fastqc\nfastqc data/*gz -o prep/fastqc\n\n\n\n\nThe next step is to remove any primer sequences. We will use the cutadapt QIIME2 plugin for that. Because we have paired-end data, there is a forward and reverse primer, referenced by the parameters --p-front-f and --p-front-r.\n\nqiime cutadapt trim-paired \\\n    --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f GTGYCAGCMGCCGCGGTAA \\\n  --p-front-r CCGYCAATTYMTTTRAGTTT \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose\n\n#generate summary \nqiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#export data (not necessary but to give an example how to do this)\nqiime tools extract \\\n    --output-path prep/trimmed_remove_primers \\\n    --input-path prep/trimmed-seqs.qza\n\nUseful options (for the full list, check the help function):\n\nThere are many ways an adaptor can be ligated. Check the help function for all options and if you do not know where the adaptor is contact your sequencing center.\n--p-front-f TEXT… Sequence of an adapter ligated to the 5’ end. Searched in forward read\n--p-front-r TEXT… Sequence of an adapter ligated to the 5’ end. Searched in reverse read\n--p-error-rate PROPORTION Range(0, 1, inclusive_end=True) Maximum allowed error rate. [default: 0.1]\n--p-indels / --p-no-indels Allow insertions or deletions of bases when matching adapters. [default: True]\n--p-times INTEGER Remove multiple occurrences of an adapter if it is Range(1, None) repeated, up to times times. [default: 1]\n--p-match-adapter-wildcards / --p-no-match-adapter-wildcards Interpret IUPAC wildcards (e.g., N) in adapters. [default: True]\n--p-minimum-length INTEGER Range(1, None) Discard reads shorter than specified value. Note, the cutadapt default of 0 has been overridden, because that value produces empty sequence records. [default: 1]\n--p-max-expected-errors NUMBER Range(0, None) Discard reads that exceed maximum expected erroneous nucleotides. [optional] --p-max-n NUMBER Discard reads with more than COUNT N bases. If Range(0, None) COUNT_or_FRACTION is a number between 0 and 1, it is interpreted as a fraction of the read length. [optional] --p-quality-cutoff-5end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 5 prime end. [default: 0] --p-quality-cutoff-3end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 3 prime end. [default: 0] --p-quality-base INTEGER Range(0, None) How the Phred score is encoded (33 or 64). [default: 33]"
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#generate-asvs-using-deblur-or-dada2",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#generate-asvs-using-deblur-or-dada2",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "In order to minimize the risks of sequencer error in targeted sequencing, clustering approaches were initially developed. Clustering approaches are based upon the idea that related/similar organisms will have similar gene sequences and that rare sequencing errors will have a trivial contribution, if any, to the consensus sequence for these clusters, or operating taxonomic units (OTUs).\nIn contrast, ASV methods generate an error model tailored to an individual sequencing run and employing algorithms that use the model to distinguish between true biological sequences and those generated by error. The two ASV methods we explore in this tutorial are Deblur and DADA2.\nDADA2 implements an algorithm that models the errors introduced during amplicon sequencing, and uses that error model to infer the true sample composition. DADA2 replaces the traditional OTU-picking step in amplicon sequencing workflows and instead produces tables of amplicon sequence variants (ASVs).\nDeblur uses pre-computed error profiles to obtain putative error-free sequences from Illumina MiSeq and HiSeq sequencing platforms. Unlike DADA2, Deblur operates on each sample independently. Additionally, reads are depleted from sequencing artifacts either using a set of known sequencing artifacts (such as PhiX) (negative filtering) or using a set of known 16S sequences (positive filtering).\n\n\n\n\nNotice:\n\nDepending on the QIIME 2 version you use, you might need to change qiime vsearch join-pairs to vsearch merge-pairs.\nSome more details on merging reads can be found here\nIf you plan to use DADA2 to join and denoise your paired end data, do not join your reads prior to denoising with DADA2; DADA2 expects reads that have not yet been joined, and will join the reads for you during the denoising process.\nSome notes on overlap calculation for dada2\nAnother option for joining is fastq-join\n\n\nmkdir -p deblur\n\n#merge reads\nqiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 2 \\\n  --verbose\n\n#summarize data\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nUseful options:\n\n--p-minlen {INTEGER} Sequences shorter than minlen after truncation are Range(0, None) discarded. [default: 1]\n--p-maxns {INTEGER} Sequences with more than maxns N characters are Range(0, None) discarded. [optional]\n--p-maxdiffs {INTEGER} Maximum number of mismatches in the area of overlap Range(0, None) during merging. [default: 10]\n--p-minmergelen {INTEGER Range(0, None) } Minimum length of the merged read to be retained. [optional]\n--p-maxee {NUMBER} Maximum number of expected errors in the merged read Range(0.0, None) to be retained. [optional]\n--p-threads {INTEGER Range(0, 8, inclusive_end=True)} The number of threads to use for computation. Does not scale much past 4 threads. [default: 1]\n\nWhen running this, we will get a lot of information printed to the screen. It is useful to look at this to know if the merge worked well. For example, we see below that ~90% of our sequences merged well, telling us that everything went fine. Much lower values might indicate some problems with the trimmed data.\nMerging reads 100%\n     45195  Pairs\n     40730  Merged (90.1%)\n      4465  Not merged (9.9%)\n\nPairs that failed merging due to various reasons:\n        50  too few kmers found on same diagonal\n        30  multiple potential alignments\n      1878  too many differences\n      1790  alignment score too low, or score drop too high\n       717  staggered read pairs\n\nStatistics of all reads:\n    233.02  Mean read length\n\nStatistics of merged reads:\n    374.39  Mean fragment length\n     14.67  Standard deviation of fragment length\n      0.32  Mean expected error in forward sequences\n      0.78  Mean expected error in reverse sequences\n      0.51  Mean expected error in merged sequences\n      0.21  Mean observed errors in merged region of forward sequences\n      0.71  Mean observed errors in merged region of reverse sequences\n      0.92  Mean observed errors in merged region\nNotice that if we look at deblur/joined-seqs.qzv with QIIME view we see an increased quality score in the middle of the region:\n\nWhen merging reads with tools like vsearch, the quality scores often increase, not decrease, in the region of overlap. The reason being, if the forward and reverse read call the same base at the same position (even if both are low quality), then the quality estimate for the base in that position goes up. That is, you have two independent observations of that base in that position. This is often the benefit of being able to merge paired reads, as you can recover / increase your confidence of the sequence in the region of overlap.\n\n\n\nNotice: Both DADA and deblur will do some quality filtering, so we do not need to be too strict here. One could consider whether to even include this step, however, running this might be useful to reduce the dataset site.\n\nqiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\n#summarize stats\nqiime metadata tabulate \\\n   --m-input-file deblur/filt-stats.qza \\\n   --o-visualization deblur/filt-stats.qzv\n\nDefault settings:\n\n--p-min-quality4 All PHRED scores less that this value are considered to be low PHRED scores.\n--p-quality-window 3: The maximum number of low PHRED scores that can be observed in direct succession before truncating a sequence read.\n--p-min-length-fraction 0.75: The minimum length that a sequence read can be following truncation and still be retained. This length should be provided as a fraction of the input sequence length. --p-max-ambiguous 0: The maximum number of ambiguous (i.e., N) base calls.\n\n\n\n\n\n#default settings\nqiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length 370 \\\n  --o-representative-sequences deblur/deblur-reprseqs2.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 2 \\\n  --verbose\n\nHow to set the parameters:\n\nWhen you use deblur-denoise, you need to truncate your fragments such that they are all of the same length. The position at which sequences are truncated is specified by the --p-trim-length parameter. Any sequence that is shorter than this value will be lost from your analyses. Any sequence that is longer will be truncated at this position.\nTo decide on what value to choose, inspect deblur/filt-seqs.qzv with QIIME view. Once you open the interactive quality plot and scroll down we see the following:\n\n\n\n\n\n\n\nWe can set --p-trim-length of 370, because that resulted in minimal data loss. That is, only &lt;9% of the reads were discarded for being too short, and only 4 bases were trimmed off from sequences of median length.\n\nUseful options:\n\n--p-trim-length INTEGER Sequence trim length, specify -1 to disable trimming. [required]\n--p-left-trim-len {INTEGER} Range(0, None) Sequence trimming from the 5’ end. A value of 0 will disable this trim. [default: 0]\n--p-mean-error NUMBER The mean per nucleotide error, used for original sequence estimate. [default: 0.005]\n--p-indel-probNUMBER Insertion/deletion (indel) probability (same for N indels). [default: 0.01]\n--p-indel-max INTEGER Maximum number of insertion/deletions. [default: 3]\n--p-min-reads INTEGER Retain only features appearing at least min-reads times across all samples in the resulting feature table. [default: 10]\n--p-min-size INTEGER In each sample, discard all features with an abundance less than min-size. [default: 2]\n--p-jobs-to-start INTEGER Number of jobs to start (if to run in parallel). [default: 1]\n\nGenerated outputs:\n\nA feature table artifact with the frequencies per sample and feature.\nA representative sequences artifact with one single fasta sequence for each of the features in the feature table.\nA stats artifact with details of how many reads passed each filtering step of the deblur procedure.\n\nWe can create visualizations from the different outputs as follows:\n\n#look at stats\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats_noremoval.qza \\\n  --o-visualization deblur/deblur-stats_noremova.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table_noremoval.qza \\\n  --o-visualization deblur/deblur-table_noremova.qzv\n\nIf we check the visualization with QIIME 2 view (or qiime tools view deblur/deblur-table.qzv) we get some feelings about how many ASVs were generated and where we lost things. Often times the Interactive sample detail page can be particularily interesting. This shows you the frequency per sample. If there is large variation in this number between samples, it means it is difficult to directly compare samples. In that case it is often recommended to standardize your data by for instance rarefaction. Rarefaction will not be treated in detail in the tutorial, but the idea is laid out below.\n\nOut of the 1,346,802 filtered and merged reads we get 3,636 features with a total frequency of 290,671. So we only retained ~21%. If we compare this to the QIIME CMI tutorial we have 662 features with a total frequency of 1,535,691 (~84) so this is definitely something to watch out for.\nWith this dataset we loose a lot to “fraction-artifact-with-minsize” . This value is related to the --p-min-size parameter which sets the min abundance of a read to be retained (default 2). So we simply might loose things because it is a subsetted dataset and it means that most of your reads are singletons and so are discarded. For a real dataset this could be due to improper merging, not having removed primer/barcodes from your reads, or that there just is that many singletons (though unlikely imo)\nloss of sequences with merged reads in deblur compared to just the forward reads has been observed and discussed before see here\n\n\n\n\n\nThe code below is not required to be run. However, the code is useful in case something goes wrong and you need to check if the file you work with is corrupted.\n\n#check file: Result deblur/filt-seqs.qza appears to be valid at level=max.\nqiime tools validate deblur/filt-seqs.qza\n\n\n\n\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots to: - identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. - If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. - If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads - Figaro is a tool to automatically choose the trunc_len - Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2 - Your reads must still overlap after truncation in order to merge them later!\n\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trim-left-f 0 \\\n  --p-trim-left-r 0 \\\n  --p-trunc-len-f 230 \\\n  --p-trunc-len-r 220 \\\n  --p-n-threads 4 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose\n\nUseful options:\n\n--p-trunc-len-f INTEGER Position at which forward read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trunc-len-r INTEGER Position at which reverse read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trim-left-f INTEGER Position at which forward read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-trim-left-r INTEGER Position at which reverse read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-max-ee-f NUMBER Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-max-ee-r NUMBER Reverse reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-trunc-q INTEGER Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-f or trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap INTEGER Range(4, None) The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method TEXT Choices(‘independent’, ‘pseudo’) The method used to pool samples for denoising. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method TEXT Choices(‘consensus’, ‘none’, ‘pooled’) The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-min-fold-parent-over-abundance NUMBER The minimum abundance of potential parents of a sequence being tested as chimeric, expressed as a fold-change versus the abundance of the sequence being tested. Values should be greater than or equal to 1 (i.e. parents should be more abundant than the sequence being tested). This parameter has no effect if chimera-method is “none”. [default: 1.0]\n--p-n-threads INTEGER The number of threads to use for multithreaded processing. If 0 is provided, all available cores will be used. [default: 1]\n--p-n-reads-learn INTEGER The number of reads to use when training the error model. Smaller numbers will result in a shorter run time but a less reliable error model. [default: 1000000]\n\nLet’s now summarize the DADA2 output:\n\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nNotes:\n\nSanity check: in the stats visualizations for both DADA2 and deblur check that outside of filtering, there should be no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nsequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the ASV table.\n\nIf we compare the two tools, we should see something like this:\n\nDeblur:\n\n3,636 features\n290,671 total frequency (out of 1,346,802 filtered and merged reads, ca 21%)\nnothing at frequencies below 2500, most samples with frequencies between 2500-7500, 1 sample with 20 000\n\nDada2:\n\n13,934 features\n648,666 total frequency (out of 1554929 trimmed reads, ~41%)\nmost samples at 10,000 frequency/sample peak, histrogram looks closer to normal distribution, 1 sample with ~37 000 frequencies\n\n\nOveral Dada2 seems beneficial in that less reads are lost but without some deeper dive into the data the qzv files are not informative enough to decide what happened to the sequences. The reason why more DADA2 retains more reads is related to the quality filtering steps. To better compare sequences, we could compare them at a sampling depth of 2000 and should see: - Retained 94,000 (32.34%) features in 47 (100.00%) samples (deblur) instead of 290,671 (100.00%) features in 47 (100.00%) - Retained 94,000 (14.49%) features in 47 (95.92%) samples (dada2) instead of 648,666 (100.00%) features in 49 (100.00%)\nBased on that both methods produce very similar results."
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#taxonomic-classification-naive-bayes",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#taxonomic-classification-naive-bayes",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "To identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods:\n\nclassify-consensus-blast\nclassify-consensus-vsearch\nMachine-learning-based classification methods\n\nThe first two are alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained\nMachine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [@bokulich2018].\nCheck out this study discussing reproducible sequence taxonomy reference database management [@ii2021].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [@kaehler2019].\n\nFor the steps below a classifier using SILVA_138_99 is pre-computed the classifier. Steps outlining how to do this can be found here.\nRequirements:\n\nThe reference sequences\nThe reference taxonomy\n\n\n\nFirst, we import the SILVA database (consisting of fasta sequences and ta taxonomy table) into QIIME:\n\nqiime tools import \\\n  --type \"FeatureData[Sequence]\" \\\n  --input-path db/SILVA_138_99_16S-ref-seqs.fna \\\n  --output-path db/SILVA_138_99_16S-ref-seqs.qza\n\nqiime tools import \\\n  --type \"FeatureData[Taxonomy]\" \\\n  --input-format HeaderlessTSVTaxonomyFormat \\\n  --input-path db/SILVA_138_99_16S-ref-taxonomy.txt \\\n  --output-path db/SILVA_138_99_16S-ref-taxonomy.qza\n\n\n\n\nWe can now use these same sequences we used to amplify our 16S sequences to extract the corresponding region of the 16S rRNA sequences from the SILVA database, like so:\n\nqiime feature-classifier extract-reads \\\n  --i-sequences db/SILVA_138_99_16S-ref-seqs.qza \\\n  --p-f-primer GTGYCAGCMGCCGCGGTAA \\\n  --p-r-primer CCGYCAATTYMTTTRAGTTT \\\n  --o-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --p-n-jobs 2\n\nOptions:\n\n--p-trim-right 0\n--p-trunc-len 0\n--p-identity {NUMBER} minimum combined primer match identity threshold. [default: 0.8] –p-min-length {INTEGER} Minimum amplicon length. Shorter amplicons are Range(0, None) discarded. Applied after trimming and truncation, so be aware that trimming may impact sequence retention. Set to zero to disable min length filtering. [default: 50]\n--p-max-length 0\n\nThis results in :\n\nMin length: 56 (we have partial sequences)\nMax length: 2316 (indicates we might have some non-16S seqs or some 16S sequences have introns)\nMean length: 394 (What we would expect based on our 16S data)\n\n\n\n\nNext, we use the reference fragments you just created to train your classifier specifically on your region of interest.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have &gt;32GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the db folder.\n\n\n\nqiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --i-reference-taxonomy db/SILVA_138_99_16S-ref-taxonomy.qza \\\n  --o-classifier db/SILVA_138_99_16S-ref-classifier.qza\n\n\n\n\nNow that we have a trained classifier and a set of representative sequences from our DADA2-denoise analyses we can finally classify the sequences in our dataset.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have ~50 GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the taxonomy folder.\n\n\n\nqiime feature-classifier classify-sklearn \\\n  --i-classifier db/SILVA_138_99_16S-ref-classifier.qza \\\n  --p-n-jobs 16 \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza\n\nArguments:\n\n--p-confidence {VALUE Float % Range(0, 1, inclusive_end=True) | Str % Choices(‘disable’)} Confidence threshold for limiting taxonomic depth. Set to “disable” to disable confidence calculation, or 0 to calculate confidence but not apply it to limit the taxonomic depth of the assignments. [default: 0.7]\n--p-read-orientation {TEXT Choices(‘same’, ‘reverse-complement’, ‘auto’)} Direction of reads with respect to reference sequences. same will cause reads to be classified unchanged; reverse-complement will cause reads to be reversed and complemented prior to classification. “auto” will autodetect orientation based on the confidence estimates for the first 100 reads. [default: ‘auto’]\n\nNext, view the data with:\n\nqiime metadata tabulate \\\n --m-input-file taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nqiime2 tools view taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nWe see:\n\nThat this also reports a confidence score ranging between 0.7 and 1. The lower limit of 0.7 is the default value (see also the qiime feature-classifier classify-sklearn help function). You can opt for a lower value to increase the number of features with a classification, but beware that that will also increase the risk of spurious classifcations!\n\n\n\n\nBefore running the command, we will need to prepare a metadata file. This metadata file should contain information on the samples. For instance, at which depth was the sample taken, from which location does it come, was it subjected to experimental or control treatment etc. etc. This information is of course very specific to the study design but at the very least it should look like this (see also data/META.tsv):\n#SampleID\n#q2:types\nDUMMY1\nDUMMY10\n...\nbut we can add any variables, like so:\n#SampleID    BarcodeSequence Location        depth   location        treatment       grainsize       flowrate        age\n#q2:types    categorical     categorical     categorical     catechorical    categorical     numeric numeric numeric\n&lt;your data&gt;\nNext, we can generate a barplot like this:\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n  --m-metadata-file data/META.tsv \\\n  --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv\n\nqiime tools view taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv"
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#filtering",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#filtering",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "These are just some general comments on filtering ASV tables and are not yet implemented in this specific tutorial. A full description of filtering methods can be found on the QIIME website\n\n#filter using the metadata info \n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#filter out samples from which we have obtained fewer than 10,000 sequences\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#create imaginary list of ids to keep and use this to filter\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\n\n\n\n\n#filter the representative sequences based on a filtered ASV table\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza"
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#phylogenetic-tree-construction",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#phylogenetic-tree-construction",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "Note: This are general notes and not yet implemented in this tutorial\n\n\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose"
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "Notice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [@gloor2017]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [@weiss2017].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [@mcmurdie2014]\nNormalization and microbial differential abundance strategies depend upon data characteristics [@weiss2017]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible."
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#computing-diversity-metrics",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#computing-diversity-metrics",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "Note: These are general notes and not yet implemented in this tutorial\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)"
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#differential-abundance-testing",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#differential-abundance-testing",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "ANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [@mandal2015].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use."
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#sec-crunchomics",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#sec-crunchomics",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "If submitting jobs via the srun command it can be useful to submit them via a screen for long running jobs. Some basics on using a screen can be found here.\n\n\nGeneral notice: In Evelyn’s tutorial qiime2-2021.2 was used. For this run, qiime2 was updated to a newer version and the tutorial code adjusted if needed.\nTo be able to access the amplicomics share, and the QIIME 2 installation and necessary databases, please contact Nina Dombrowski with your UvAnetID.\n\n#check what environments we already have\nconda env list\n\n#add amplicomics environment to be able to use the QIIME conda install\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\n\n\n\nIf you run this the first time, start by downloading the tutorial data.\n\n#download data data\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\n\n\n\n\nBelow, we set some environmental variables describing the location for key files we want to use. The only thing that you need to change yourself is the path to the working directory, ie. wdir, and the path_to_where_you_downloaded_the_data.\n\nwdir=\"/home/ndombro/tutorials/evelyn_qiime\"\ncd $wdir\n\n#make symbolic link to have the structures in the tutorial work\nln -s path_to_where_you_downloaded_the_data/metabarcoding-qiime2-datapackage-v2021.06.2/data ./\n\n#set environmental variables\nexport MANIFEST=\"data/MANIFEST.csv\"\nexport META=\"data/META.tsv\"\nexport PRIMFWD=\"GTGYCAGCMGCCGCGGTAA\"\nexport PRIMREV=\"CCGYCAATTYMTTTRAGTTT\"\nexport TRIMLENG=370\nexport TRUNKFWD=230\nexport TRUNKREV=220\n\nexport DBSEQ=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-seqs.qza\"\nexport DBTAX=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-tax.qza\"\nexport DBPREFIX=\"SILVA_138_99_16S\"\n\n#activate qiime environment\nmamba activate qiime2-2023.7\n\n\n\n\n\nmkdir -p logs\nmkdir -p prep\nmkdir -p deblur\nmkdir -p dada2\nmkdir -p db\nmkdir -p taxonomy\n\n\n\n\nWhen using Slurm we use the following settings to say that:\n\nthe job consists of 1 task (-n 1)\nthe job needs to be run on 1 cpu (–cpus-per-task 1)\n\n\nsrun -n 1 --cpus-per-task 1 qiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path $MANIFEST \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#create visualization\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/demux-seqs.qza \\\n  --o-visualization prep/demux-seqs.qzv\n\n#instructions to view qvz (tool not available, need to check if I can find it) \n#how-to-view-this-qzv prep/demux-seqs.qzv\n\nSummary of statistics:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\nsrun -n 1 --cpus-per-task 4 qiime cutadapt trim-paired \\\n  --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f $PRIMFWD \\\n  --p-front-r $PRIMREV \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-cutadapt-trim-paired.log\n\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#make a table out of this (quick, so ok to run on headnode)\nsrun -n 1 --cpus-per-task 1 qiime tools extract \\\n   --input-path prep/trimmed-seqs.qzv \\\n   --output-path visualizations/trimmed-seqs\n\n\nInformation about the 2&gt;&1 is a redirection operator :\n\nIt says to “redirect stderr (file descriptor 2) to the same location as stdout (file descriptor 1).” This means that both stdout and stderr will be combined and sent to the same destination\n\nThe tee command is used to:\n\nLogging: It captures the standard output (stdout) and standard error (stderr) of the command that precedes it and saves it to a file specified as its argument. In this case, it saves the output and errors to the file logs/qiime-cutadapt-trim-paired.log.\nDisplaying Output: In addition to saving the output to a file, tee also displays the captured output and errors on the terminal in real-time. This allows you to see the progress and any error messages while the command is running.\n\nIf we would want to run the command without tee we could do 2&gt;&1 &gt; logs/qiime-cutadapt-trim-paired.log but this won’t print output to the screen\n\nSummary of run:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\n\nNote that for the qiime cutadapt trim-paired-command you used p-cores while here you need p-threads\n\nsrun -n 1 --cpus-per-task 4 qiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 4 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-vsearch-join-pairs.log\n\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nNotice: This step cannot be sped up because the qiime quality-filter q-score command does not have an option to use more cpus or threads.\n\nsrun -n 1 --cpus-per-task 1 qiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-quality-filter-q-score.log\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\nqiime metadata tabulate \\\n  --m-input-file deblur/filt-stats.qza \\\n  --o-visualization deblur/filt-stats.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nBelow we:\n\nWe defined a memory allocation of 16GB. This is the total amount of RAM you expect to need for this job (+ a bit more to be on the safe side). How much you need is not always easy to predict and requires some experience/trial and error. As a rule of thumb: if you get an Out Of Memory error, double it and try again. Note that you can also define mem-per-cpu, which may be easier to work with if you often change the number of cpus between analyses.\nWe get a warning warn' method is deprecated, use 'warning' instead, but we can ignore that\n\n\n#job id: 398288\nsrun -n 1 --cpus-per-task 10 --mem=16GB qiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length $TRIMLENG \\\n  --o-representative-sequences deblur/deblur-reprseqs.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --p-min-size 2 \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 10 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-deblur-denoise-16S.log\n\n#check memory requirements: 1.6GB\n#maxrss = maximum amount of memory used at any time by any process in that job\nsacct -j 398288 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\ncat deblur.log &gt;&gt; logs/qiime_deblur_denoise-16S.log && rm deblur.log\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data deblur/deblur-reprseqs.qza \\\n  --o-visualization deblur/deblur-reprseqs.qzv\n\n\n\n\n\nDADA2 performs filtering, joining and denoising all with one single command, and therefor takes longer to run. Here, it takes ca. 30m5.334s using 8 cpus. It is tempting to just increase the number of cpus when impatient, but please note that not all commands are very efficient at running more jobs in parallel (i.e. on multiple cpus). The reason often is that the rate-limiting step in the workflow is unable to use multiple cpus, so all but one are idle.\n\n\n#job id: 398291\ntime srun -n 1 --cpus-per-task 8 --mem=32GB qiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trunc-len-f $TRUNKFWD \\\n  --p-trunc-len-r $TRUNKREV \\\n  --p-n-threads 8 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-dada2-denoise-paired.log\n\n#get mem usage\nsacct -j 398291 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#get summaries\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nUsage info –&gt; 7.64878 GB needed and 50 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398291            qiime          8              00:49:55\n398291.exte+     extern          8          0   00:49:55\n398291.0          qiime          8   7648780K   00:49:55\n\n\n\n\n\n\n\n#job id: 398333\ntime srun -n 1 --cpus-per-task 8 --mem=4GB qiime feature-classifier extract-reads \\\n  --i-sequences $DBSEQ \\\n  --p-f-primer $PRIMFWD \\\n  --p-r-primer $PRIMREV \\\n  --o-reads db/$DBPREFIX-ref-frags.qza \\\n  --p-n-jobs 8 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398333 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nUsage info –&gt; 1.43846 GB needed in 4 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398333            qiime          8              00:04:33\n398333.exte+     extern          8          0   00:04:33\n398333.0          qiime          8   1438460K   00:04:33\n\n\n\nTraining a classifier takes quite some time, especially because the qiime feature-classifier fit-classifier-naive-bayes- command cannot use multiple cpus in parallel (see qiime feature-classifier fit-classifier-naive-bayes –help). Here, it took ca. 145m56.836s. You can however often re-use your classifier, provided you used the same primers and db.\n\n#job id: 398335\ntime srun -n 1 --cpus-per-task 1 --mem=32GB qiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/$DBPREFIX-ref-frags.qza \\\n  --i-reference-taxonomy $DBTAX \\\n  --o-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --p-verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398335 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nResources needed –&gt; 33.4GB\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398335            qiime          2              02:19:24\n398335.exte+     extern          2          0   02:19:24\n398335.0          qiime          1  33469976K   02:19:24\n\n\n\nImportant\nThe classifier takes up quite some disc space. If you ‘just’ run it you are likely to get a No space left on device-error. You can avoid this by changing the directory where temporary files are written to, but make sure you have sufficient space there as well of course.\n\n#prepare tmp dir \nmkdir -p tmptmp\nexport TMPDIR=$PWD/tmptmp/\n\n#job id: 398339\ntime srun -n 1 --cpus-per-task 32 --mem=160GB qiime feature-classifier classify-sklearn \\\n  --i-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --p-n-jobs 32 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-classify-sklearn.log\n\n#get mem usage\nsacct -j 398339 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#cleanup\nrm -fr tmptmp\n\nqiime metadata tabulate \\\n  --m-input-file taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxonomy.qzv\n\nResource need –&gt; 158.7 GB:\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398339            qiime         32              00:13:57\n398339.exte+     extern         32          0   00:13:57\n398339.0          qiime         32 158749076K   00:13:57\n\n\n\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --m-metadata-file $META \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxplot.qzv\n\n\n\n\n\n\nscp crunchomics:'/home/ndombro/tutorials/evelyn_qiime/*/*.qzv' crunchomics_files/"
  },
  {
    "objectID": "source/Qiime/readme.html",
    "href": "source/Qiime/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "QIIME 2 is a platform for microbial community analysis such as 16S rRNA gene amplicon analysis. The QIIME 2 website comes with a wealth of documentation and tutorials that are worthwhile to check out before running QIIME 2 on your own data:\n\nInstallation instructions\nThe QIIME 2 documentation.\nThe cancer microbiome tutorial\nq2book a very detailed, but still unfinished, tutorial on QIIME 2."
  },
  {
    "objectID": "source/Qiime/readme.html#qiime-2",
    "href": "source/Qiime/readme.html#qiime-2",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "QIIME 2 is a platform for microbial community analysis such as 16S rRNA gene amplicon analysis. The QIIME 2 website comes with a wealth of documentation and tutorials that are worthwhile to check out before running QIIME 2 on your own data:\n\nInstallation instructions\nThe QIIME 2 documentation.\nThe cancer microbiome tutorial\nq2book a very detailed, but still unfinished, tutorial on QIIME 2."
  },
  {
    "objectID": "source/classification/kraken2.html",
    "href": "source/classification/kraken2.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Kraken2 (Wood, Lu, and Langmead 2019) is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.\nAvailable on Crunchomics: Kraken version 2.0.8-beta installed\n\n\n\nIf you want to install kraken2 on your own, its best to install it via mamba:\n\n#setup new conda environment, which we name kraken2\nmamba create --name kraken2 -c bioconda kraken2\n\n\n\n\nFor detailed usage information, check out the kraken2 manual.\n\n\nThe command below will download NCBI taxonomic information, as well as the complete genomes in RefSeq for the bacterial, archaeal, and viral domains, along with the human genome and a collection of known vectors (UniVec_Core). After downloading all this data, the build process begins; this can be the most time-consuming step. If you have multiple processing cores, you can run this process with multiple threads.\nAddtionally, kraken2 comes with several custom databases, such as the SILVA database for 16S rRNA gene analyses. Check the kraken2 manual for detailed information on how to download custom things..\n\n#create a kraken2 database \nkraken2-build --standard --threads 24 --db $DBNAME\n\n\n\n\n\nkraken2 --db $DBNAME seqs.fa --output output.out --report output.report"
  },
  {
    "objectID": "source/classification/kraken2.html#kraken2",
    "href": "source/classification/kraken2.html#kraken2",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Kraken2 (Wood, Lu, and Langmead 2019) is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.\nAvailable on Crunchomics: Kraken version 2.0.8-beta installed\n\n\n\nIf you want to install kraken2 on your own, its best to install it via mamba:\n\n#setup new conda environment, which we name kraken2\nmamba create --name kraken2 -c bioconda kraken2\n\n\n\n\nFor detailed usage information, check out the kraken2 manual.\n\n\nThe command below will download NCBI taxonomic information, as well as the complete genomes in RefSeq for the bacterial, archaeal, and viral domains, along with the human genome and a collection of known vectors (UniVec_Core). After downloading all this data, the build process begins; this can be the most time-consuming step. If you have multiple processing cores, you can run this process with multiple threads.\nAddtionally, kraken2 comes with several custom databases, such as the SILVA database for 16S rRNA gene analyses. Check the kraken2 manual for detailed information on how to download custom things..\n\n#create a kraken2 database \nkraken2-build --standard --threads 24 --db $DBNAME\n\n\n\n\n\nkraken2 --db $DBNAME seqs.fa --output output.out --report output.report"
  },
  {
    "objectID": "source/cli/cli.html",
    "href": "source/cli/cli.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "The command line interface (cli) is not a software but nevertheless useful for bioinformaticians since it is a text-based user interface used to run programs, manage computer files and interact with the computer.\nYou can use cli as follows:\n\nMac: Use the terminal tool (easy to find with spotlight)\nLinux: Use xterm or konsole\nWindows: Use putty, mobaxterm or windows subsystem for linux (WSL)\n\nIf you are completely unfamiliar with using the command line check out:\n\nAn older session on using the command line. Will be updated to work on Crunchomics in the future\nA tutorial on using AWK, an excellent command line tool for filtering tables, extracting patterns, etc… If you want to follow this tutorial then you can download the required input files from here"
  },
  {
    "objectID": "source/cli/cli.html#using-the-command-line-interface",
    "href": "source/cli/cli.html#using-the-command-line-interface",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "The command line interface (cli) is not a software but nevertheless useful for bioinformaticians since it is a text-based user interface used to run programs, manage computer files and interact with the computer.\nYou can use cli as follows:\n\nMac: Use the terminal tool (easy to find with spotlight)\nLinux: Use xterm or konsole\nWindows: Use putty, mobaxterm or windows subsystem for linux (WSL)\n\nIf you are completely unfamiliar with using the command line check out:\n\nAn older session on using the command line. Will be updated to work on Crunchomics in the future\nA tutorial on using AWK, an excellent command line tool for filtering tables, extracting patterns, etc… If you want to follow this tutorial then you can download the required input files from here"
  },
  {
    "objectID": "source/conda/conda.html",
    "href": "source/conda/conda.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "A lot of bioinformatic workflows start with installing software. Since this often means not only installing the software but several dependencies, we recommend the use of a package management system, such as conda or mamba. These tools allow you to find and install packages in their own environment without administrator privileges.\nThis is especially useful if you require different software versions, such as python3.6 versus python3.10, for different workflows. With package management systems you can easily setup different python versions in different environments.\n\n\nA lot of system already come with conda installed, however, if possible we recommend working with mamba instead of conda. mamba is a drop-in replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\nTo install mamba, follow the instructions here. This should look something like this for mac and linux-systems. If you are on windows, the easiest is to setup up Windows Subsystem for Linux (WSL) first and then use the code below.\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\n\n\n\nLet’s assume we want to install a tool, ITSx, into an environment called fungal_genomics. We can do this as follows:\n\n#check if the tool is installed (should return command not found)\nITSx -h\n\n#create an empty environment and name it fungal_genomics\nmamba create -n fungal_genomics\n\n#install some software, i.e. itsx, into the fungal_genomics environment\nmamba install -n fungal_genomics -c bioconda itsx\n\n#to run the tool activate the environment\nconda activate fungal_genomics\n\n#check if tool is installed\nITSx -h\n\n#leave the environment\nconda deactivate\n\nA full set of mamba/conda commands can be found here\n\n\n\nOn crunchomics other people might have already installed environments that might be useful for your work. One example is the amplicomics share, which comes with several QIIME 2 installations. To use this, first ask for access to the amplicomics share by contacting n.dombrowski@uva.nl with your uva net id. After you got access, you can add conda environments in the amplicomics share with:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/"
  },
  {
    "objectID": "source/conda/conda.html#installing-software",
    "href": "source/conda/conda.html#installing-software",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "A lot of bioinformatic workflows start with installing software. Since this often means not only installing the software but several dependencies, we recommend the use of a package management system, such as conda or mamba. These tools allow you to find and install packages in their own environment without administrator privileges.\nThis is especially useful if you require different software versions, such as python3.6 versus python3.10, for different workflows. With package management systems you can easily setup different python versions in different environments.\n\n\nA lot of system already come with conda installed, however, if possible we recommend working with mamba instead of conda. mamba is a drop-in replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\nTo install mamba, follow the instructions here. This should look something like this for mac and linux-systems. If you are on windows, the easiest is to setup up Windows Subsystem for Linux (WSL) first and then use the code below.\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\n\n\n\nLet’s assume we want to install a tool, ITSx, into an environment called fungal_genomics. We can do this as follows:\n\n#check if the tool is installed (should return command not found)\nITSx -h\n\n#create an empty environment and name it fungal_genomics\nmamba create -n fungal_genomics\n\n#install some software, i.e. itsx, into the fungal_genomics environment\nmamba install -n fungal_genomics -c bioconda itsx\n\n#to run the tool activate the environment\nconda activate fungal_genomics\n\n#check if tool is installed\nITSx -h\n\n#leave the environment\nconda deactivate\n\nA full set of mamba/conda commands can be found here\n\n\n\nOn crunchomics other people might have already installed environments that might be useful for your work. One example is the amplicomics share, which comes with several QIIME 2 installations. To use this, first ask for access to the amplicomics share by contacting n.dombrowski@uva.nl with your uva net id. After you got access, you can add conda environments in the amplicomics share with:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/"
  },
  {
    "objectID": "source/metagenomics/fastqc_readme.html",
    "href": "source/metagenomics/fastqc_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "FastQC is a quality control tool for high throughput sequence data. For help with interpreting the output, please visit the website with some very good examples.\nFastQC can be run both on short- and long-read data.\nAvailable on Crunchomics: Yes\n\n\n\nFastQC already is installed on Crunchomics, if you want to install it on your own system check out the instructions found here.\n\n\n\n\nInputs: FastQC can process bam,sam,bam_mapped,sam_mapped and fastq files\nOutput: An HTML quality report\n\nExample code:\n\n#get help \nfastqc --help\n\n#run on a single file\nfastqc myfile.fastq.gz -o outputfolder --threads 1"
  },
  {
    "objectID": "source/metagenomics/fastqc_readme.html#fastqc",
    "href": "source/metagenomics/fastqc_readme.html#fastqc",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "FastQC is a quality control tool for high throughput sequence data. For help with interpreting the output, please visit the website with some very good examples.\nFastQC can be run both on short- and long-read data.\nAvailable on Crunchomics: Yes\n\n\n\nFastQC already is installed on Crunchomics, if you want to install it on your own system check out the instructions found here.\n\n\n\n\nInputs: FastQC can process bam,sam,bam_mapped,sam_mapped and fastq files\nOutput: An HTML quality report\n\nExample code:\n\n#get help \nfastqc --help\n\n#run on a single file\nfastqc myfile.fastq.gz -o outputfolder --threads 1"
  },
  {
    "objectID": "source/metagenomics/metabolic.html",
    "href": "source/metagenomics/metabolic.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Metabolic is a workflow developed by the AnantharamanLab. This software enables the prediction of metabolic and biogeochemical functional trait profiles to any given genome datasets. These genome datasets can either be metagenome-assembled genomes (MAGs), single-cell amplified genomes (SAGs) or isolated strain sequenced genomes.\nMETABOLIC has two main implementations, which are METABOLIC-G and METABOLIC-C. METABOLIC-G.pl allows for generation of metabolic profiles and biogeochemical cycling diagrams of input genomes and does not require input of sequencing reads. METABOLIC-C.pl generates the same output as METABOLIC-G.pl, but as it allows for the input of metagenomic read data, it will generate information pertaining to community metabolism.\nCheck out the manual for all usage options (Zhou et al. 2022).\nAvailable on Crunchomics: No\n\n\n\n\n\n\n#go into folder in which to install software\ncd ~/personal/software/METABOLIC_4.0\n\n#download environmental yaml (which tells conda what software to install)\nmkdir envs\nwget https://raw.githubusercontent.com/AnantharamanLab/METABOLIC/master/METABOLIC_v4.0_env.yml -P envs/\n\n#install dependencies via the environmental yaml (this env will be named METABOLIC_v4.0)\nmamba env create -f envs/METABOLIC_v4.0_env.yml\n\n#activate environment (NEEDS to be active to run the setup steps below)\nconda activate METABOLIC_v4.0\n\n#download a git clone of the METABOLIC workflow\ngit clone https://github.com/AnantharamanLab/METABOLIC.git\n\n#run bash setup script (needs some time, have patience)\ncd METABOLIC\nbash run_to_setup.sh\n\n\n\n\n\n\n\nNote\n\n\n\nThe command bash run_to_setup.sh installs some public databases, which might be useful to use other things. These are:\n\nThe kofam database: KOfams are a customized HMM database of KEGG Orthologs (KOs). The KO (KEGG Orthology) database is a database of molecular functions represented in terms of functional orthologs and is useful to assign functions to your proteins of interest. The script will download the database from scratch and you will therefore always have the newest version installed when installing METABOLIC.\nThe dbCAN2 database: A database that can be used for carbohydrate-active enzyme annotation. The script downloads dbCAN v10.\nThe Meropds database: A database for peptidases (also termed proteases, proteinases and proteolytic enzymes) and the proteins that inhibit them. The script will download the most recent version from the internet.\n\n\n\n\n\n\nMETABOLIC uses GTDB_tk for taxonomic assignment. This database is very big, so instead of installing this database every single time in a conda environment folder that uses GTDB_tk, it is better to use one global installation.\n\n\nFor UvA people using crunchomics, we have a global install of GTDB r207 you can use, which is installed on the metatools share. To get access to the share, please contact Anna Heintz Buschart via a.u.s.heintzbuschart@uva.nl with your UvAnetID and a description why you need access\n\n#tell conda where the gtdb database is installed\nconda env config vars set GTDBTK_DATA_PATH=\"/zfs/omics/projects/metatools/DB/GTDB_tk/GTDB_tk_r207\"\n\n#reactivate env to make the changes work\nconda deactivate\nconda activate METABOLIC_v4.0\n\n\n\n\nIf desired, you can install the GTDB database yourself as follows:\n\n#Manually download the latest reference data ()\nwget https://data.gtdb.ecogenomic.org/releases/release207/207.0/auxillary_files/gtdbtk_r207_v2_data.tar.gz\n\n#Extract the archive to a target directory:\n#change `/path/to/target/db` to whereever you want to install the db\ntar -xvzf gtdbtk_r207_v2_data.tar.gz -c \"/path/to/target/db\" --strip 1 &gt; /dev/null\n\n#cleanup\nrm gtdbtk_r207_v2_data.tar.gz\n\n#while the conda env for METABOLIC is activate link gtdb database\nconda env config vars set GTDBTK_DATA_PATH=\"/path/to/target/db\"\n\n#reactivate env \nconda deactivate\nconda activate METABOLIC_v4.0\n\n\n\n\n\n\nMETABOLIC has two scripts:\n\nMETABOLIC-G.pl: Allows for classification of the metabolic capabilities of input genomes.\nMETABOLIC-C.pl: Allows for classification of the metabolic capabilities of input genomes, calculation of genome coverage, creation of biogeochemical cycling diagrams, and visualization of community metabolic interactions and contribution to biogeochemical processes by each microbial group.\n\nInput:\n\nNucleotide fasta files (use -in-gn in the perl script)\nProtein faa files (use -in in the perl script)\nIllumina reads (use -r flag in the metabolic-c perl script) provided as un-compressed fastq files. This option requires you to provide the full path to your paired reads. Note that the two different sets of paired reads are separated by a line return (new line), and two reads in each line are separated by a “,” but not ” ,” or ” , ” (no spaces before or after comma). Blank lines are not allowed\nNanopore/PacBio long-reds (use r togher with -st illumina/pacbio/pacbio_hifi/nanopore to provide information about the sequencing type)\n\n#Read pairs: \n/path/to/your/reads/file/SRR3577362_sub_1.fastq,/path/to/your/reads/file/SRR3577362_sub_2.fastq\n/path/to/your/reads/file/SRR3577362_sub2_1.fastq,/path/to/your/reads/file/SRR3577362_sub2_2.fastq\nOutputs (for more detail, check the manual):\n\nAll_gene_collections_mapped.depth.txt: The gene depth of all input genes (METABOLIC-C only)\n\nEach_HMM_Amino_Acid_Sequence/: The faa collection for each hmm file\nintermediate_files/: The hmmsearch, peptides (MEROPS), CAZymes (dbCAN2), and GTDB-Tk (only for METABOLIC-C) running intermediate files\nKEGG_identifier_result/: The hit and result of each genome by Kofam database\nMETABOLIC_Figures/: All figures output from the running of METABOLIC\nMETABOLIC_Figures_Input/: All input files for R-generated diagrams\nMETABOLIC_result_each_spreadsheet/: TSV files representing each sheet of the created METABOLIC_result.xlsx file\nMW-score_result/: The resulted table for MW-score (METABOLIC-C only)\n\nMETABOLIC_result.xlsx: The resulting excel file of METABOLIC\n\nRequired/Optional flags:\n\n-in-gn [required if you are starting from nucleotide fasta files] Defines the location of the FOLDER containing the genome nucleotide fasta files ending with “.fasta” to be run by this program\n-in [required if you are starting from faa files] Defines the location of the FOLDER containing the genome amino acid files ending with “.faa” to be run by this program\n-r [required] Defines the path to a text file containing the location of paried reads\n-rt [optional] Defines the option to use “metaG” or “metaT” to indicate whether you use the metagenomic reads or metatranscriptomic reads (default: ‘metaG’). Only required when using METABOLIC-C\n-st [optional] To use “illumina” (for Illumina short reads), or “pacbio” (for PacBio CLR reads), or “pacbio_hifi” (for PacBio HiFi/CCS genomic reads (v2.19 or later)), or “pacbio_asm20” (for PacBio HiFi/CCS genomic reads (v2.18 or earlier)), or “nanopore” (for Oxford Nanopore reads) to indicate the sequencing type of metagenomes or metatranscriptomes (default: ‘illumina’; Note that all “illumina”, “pacbio”, “pacbio_hifi”, “pacbio_asm20”, and “nanopore” should be provided as lowercase letters and the underscore “_” should not be typed as “-” or any other marks)\n-t [optional] Defines the number of threads to run the program with (Default: 20)\n-m-cutoff [optional] Defines the fraction of KEGG module steps present to designate a KEGG module as present (Default: 0.75)\n-kofam-db [optional] Defines the use of the full (“full”) or reduced (“small”) KOfam database by the program (Default: ‘full’). “small” KOfam database only contains KOs present in KEGG module, using this setting will significantly reduce hmmsearch running time.\n-tax [optional] To calculate MW-score contribution of microbial groups at the resolution of which taxonomical level (default: “phylum”; other options: “class”, “order”, “family”, “genus”, “species”, and “bin” (MAG itself)). Only required when using METABOLIC-C\n-p [optional] Defines the prodigal method used to annotate ORFs (“meta” or “single”)(Default: “meta”)\n-o [optional] Defines the output directory to be created by the program (Default: current directory)\n\nSome example files for testing can be found in METABOLIC_test_files/.\n\n#print the help information\nperl METABOLIC-C.pl -help\n\n#run workflow on test data (5 genomes provided as nucleotides)\nmkdir -p testing/genomes\nperl METABOLIC-G.pl -in-gn METABOLIC_test_files/Guaymas_Basin_genome_files/ -o testing/genomes\n\n#run workflow on metagenomic fastq files\nperl METABOLIC-C.pl -test true"
  },
  {
    "objectID": "source/metagenomics/metabolic.html#metabolic",
    "href": "source/metagenomics/metabolic.html#metabolic",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Metabolic is a workflow developed by the AnantharamanLab. This software enables the prediction of metabolic and biogeochemical functional trait profiles to any given genome datasets. These genome datasets can either be metagenome-assembled genomes (MAGs), single-cell amplified genomes (SAGs) or isolated strain sequenced genomes.\nMETABOLIC has two main implementations, which are METABOLIC-G and METABOLIC-C. METABOLIC-G.pl allows for generation of metabolic profiles and biogeochemical cycling diagrams of input genomes and does not require input of sequencing reads. METABOLIC-C.pl generates the same output as METABOLIC-G.pl, but as it allows for the input of metagenomic read data, it will generate information pertaining to community metabolism.\nCheck out the manual for all usage options (Zhou et al. 2022).\nAvailable on Crunchomics: No\n\n\n\n\n\n\n#go into folder in which to install software\ncd ~/personal/software/METABOLIC_4.0\n\n#download environmental yaml (which tells conda what software to install)\nmkdir envs\nwget https://raw.githubusercontent.com/AnantharamanLab/METABOLIC/master/METABOLIC_v4.0_env.yml -P envs/\n\n#install dependencies via the environmental yaml (this env will be named METABOLIC_v4.0)\nmamba env create -f envs/METABOLIC_v4.0_env.yml\n\n#activate environment (NEEDS to be active to run the setup steps below)\nconda activate METABOLIC_v4.0\n\n#download a git clone of the METABOLIC workflow\ngit clone https://github.com/AnantharamanLab/METABOLIC.git\n\n#run bash setup script (needs some time, have patience)\ncd METABOLIC\nbash run_to_setup.sh\n\n\n\n\n\n\n\nNote\n\n\n\nThe command bash run_to_setup.sh installs some public databases, which might be useful to use other things. These are:\n\nThe kofam database: KOfams are a customized HMM database of KEGG Orthologs (KOs). The KO (KEGG Orthology) database is a database of molecular functions represented in terms of functional orthologs and is useful to assign functions to your proteins of interest. The script will download the database from scratch and you will therefore always have the newest version installed when installing METABOLIC.\nThe dbCAN2 database: A database that can be used for carbohydrate-active enzyme annotation. The script downloads dbCAN v10.\nThe Meropds database: A database for peptidases (also termed proteases, proteinases and proteolytic enzymes) and the proteins that inhibit them. The script will download the most recent version from the internet.\n\n\n\n\n\n\nMETABOLIC uses GTDB_tk for taxonomic assignment. This database is very big, so instead of installing this database every single time in a conda environment folder that uses GTDB_tk, it is better to use one global installation.\n\n\nFor UvA people using crunchomics, we have a global install of GTDB r207 you can use, which is installed on the metatools share. To get access to the share, please contact Anna Heintz Buschart via a.u.s.heintzbuschart@uva.nl with your UvAnetID and a description why you need access\n\n#tell conda where the gtdb database is installed\nconda env config vars set GTDBTK_DATA_PATH=\"/zfs/omics/projects/metatools/DB/GTDB_tk/GTDB_tk_r207\"\n\n#reactivate env to make the changes work\nconda deactivate\nconda activate METABOLIC_v4.0\n\n\n\n\nIf desired, you can install the GTDB database yourself as follows:\n\n#Manually download the latest reference data ()\nwget https://data.gtdb.ecogenomic.org/releases/release207/207.0/auxillary_files/gtdbtk_r207_v2_data.tar.gz\n\n#Extract the archive to a target directory:\n#change `/path/to/target/db` to whereever you want to install the db\ntar -xvzf gtdbtk_r207_v2_data.tar.gz -c \"/path/to/target/db\" --strip 1 &gt; /dev/null\n\n#cleanup\nrm gtdbtk_r207_v2_data.tar.gz\n\n#while the conda env for METABOLIC is activate link gtdb database\nconda env config vars set GTDBTK_DATA_PATH=\"/path/to/target/db\"\n\n#reactivate env \nconda deactivate\nconda activate METABOLIC_v4.0\n\n\n\n\n\n\nMETABOLIC has two scripts:\n\nMETABOLIC-G.pl: Allows for classification of the metabolic capabilities of input genomes.\nMETABOLIC-C.pl: Allows for classification of the metabolic capabilities of input genomes, calculation of genome coverage, creation of biogeochemical cycling diagrams, and visualization of community metabolic interactions and contribution to biogeochemical processes by each microbial group.\n\nInput:\n\nNucleotide fasta files (use -in-gn in the perl script)\nProtein faa files (use -in in the perl script)\nIllumina reads (use -r flag in the metabolic-c perl script) provided as un-compressed fastq files. This option requires you to provide the full path to your paired reads. Note that the two different sets of paired reads are separated by a line return (new line), and two reads in each line are separated by a “,” but not ” ,” or ” , ” (no spaces before or after comma). Blank lines are not allowed\nNanopore/PacBio long-reds (use r togher with -st illumina/pacbio/pacbio_hifi/nanopore to provide information about the sequencing type)\n\n#Read pairs: \n/path/to/your/reads/file/SRR3577362_sub_1.fastq,/path/to/your/reads/file/SRR3577362_sub_2.fastq\n/path/to/your/reads/file/SRR3577362_sub2_1.fastq,/path/to/your/reads/file/SRR3577362_sub2_2.fastq\nOutputs (for more detail, check the manual):\n\nAll_gene_collections_mapped.depth.txt: The gene depth of all input genes (METABOLIC-C only)\n\nEach_HMM_Amino_Acid_Sequence/: The faa collection for each hmm file\nintermediate_files/: The hmmsearch, peptides (MEROPS), CAZymes (dbCAN2), and GTDB-Tk (only for METABOLIC-C) running intermediate files\nKEGG_identifier_result/: The hit and result of each genome by Kofam database\nMETABOLIC_Figures/: All figures output from the running of METABOLIC\nMETABOLIC_Figures_Input/: All input files for R-generated diagrams\nMETABOLIC_result_each_spreadsheet/: TSV files representing each sheet of the created METABOLIC_result.xlsx file\nMW-score_result/: The resulted table for MW-score (METABOLIC-C only)\n\nMETABOLIC_result.xlsx: The resulting excel file of METABOLIC\n\nRequired/Optional flags:\n\n-in-gn [required if you are starting from nucleotide fasta files] Defines the location of the FOLDER containing the genome nucleotide fasta files ending with “.fasta” to be run by this program\n-in [required if you are starting from faa files] Defines the location of the FOLDER containing the genome amino acid files ending with “.faa” to be run by this program\n-r [required] Defines the path to a text file containing the location of paried reads\n-rt [optional] Defines the option to use “metaG” or “metaT” to indicate whether you use the metagenomic reads or metatranscriptomic reads (default: ‘metaG’). Only required when using METABOLIC-C\n-st [optional] To use “illumina” (for Illumina short reads), or “pacbio” (for PacBio CLR reads), or “pacbio_hifi” (for PacBio HiFi/CCS genomic reads (v2.19 or later)), or “pacbio_asm20” (for PacBio HiFi/CCS genomic reads (v2.18 or earlier)), or “nanopore” (for Oxford Nanopore reads) to indicate the sequencing type of metagenomes or metatranscriptomes (default: ‘illumina’; Note that all “illumina”, “pacbio”, “pacbio_hifi”, “pacbio_asm20”, and “nanopore” should be provided as lowercase letters and the underscore “_” should not be typed as “-” or any other marks)\n-t [optional] Defines the number of threads to run the program with (Default: 20)\n-m-cutoff [optional] Defines the fraction of KEGG module steps present to designate a KEGG module as present (Default: 0.75)\n-kofam-db [optional] Defines the use of the full (“full”) or reduced (“small”) KOfam database by the program (Default: ‘full’). “small” KOfam database only contains KOs present in KEGG module, using this setting will significantly reduce hmmsearch running time.\n-tax [optional] To calculate MW-score contribution of microbial groups at the resolution of which taxonomical level (default: “phylum”; other options: “class”, “order”, “family”, “genus”, “species”, and “bin” (MAG itself)). Only required when using METABOLIC-C\n-p [optional] Defines the prodigal method used to annotate ORFs (“meta” or “single”)(Default: “meta”)\n-o [optional] Defines the output directory to be created by the program (Default: current directory)\n\nSome example files for testing can be found in METABOLIC_test_files/.\n\n#print the help information\nperl METABOLIC-C.pl -help\n\n#run workflow on test data (5 genomes provided as nucleotides)\nmkdir -p testing/genomes\nperl METABOLIC-G.pl -in-gn METABOLIC_test_files/Guaymas_Basin_genome_files/ -o testing/genomes\n\n#run workflow on metagenomic fastq files\nperl METABOLIC-C.pl -test true"
  },
  {
    "objectID": "source/nanopore/chopper.html",
    "href": "source/nanopore/chopper.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Chopper is a tool for quality filtering of long read data. It is a Rust implementation of two other tools for long-read quality filtering, NanoFilt and NanoLyse, both originally written in Python. This tool, intended for long read sequencing such as PacBio or ONT, filters and trims a fastq file (De Coster and Rademakers 2023).\nAvailable on Crunchomics: Not by default\n\n\n\nIt is easiest to install chopper with conda. If you do not have mamba (or conda) installed. Check out the installation instructions for mamba and conda. Mamba is a newer version of conda and from experience faster but since its newer this might mean that not all software can yet be installed with mamba.\n\nmamba create --name chopper -c bioconda chopper\n\n\n\n\nRequired input:\n\nFASTQ files\n\nOutput:\n\nFASTQ files\n\nExample usage:\n\nchopper -q 10 \\\n        --headcrop 0 \\\n        --tailcrop 0   \\\n        -l 250 \\\n        --maxlength 3000  \\\n        --threads 1 \n\nUseful arguments:\n\n--headcrop Trim N nucleotides from the start of a read [default: 0]\n--maxlength Sets a maximum read length [default: 2147483647]\n-l, --minlength Sets a minimum read length [default: 1]\n-q, --quality Sets a minimum Phred average quality score [default: 0]\n--tailcrop Trim N nucleotides from the end of a read [default: 0]\n--threads Number of parallel threads to use [default: 4]\n--contam Fasta file with reference to check potential contaminants against [default None]}"
  },
  {
    "objectID": "source/nanopore/chopper.html#chopper",
    "href": "source/nanopore/chopper.html#chopper",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Chopper is a tool for quality filtering of long read data. It is a Rust implementation of two other tools for long-read quality filtering, NanoFilt and NanoLyse, both originally written in Python. This tool, intended for long read sequencing such as PacBio or ONT, filters and trims a fastq file (De Coster and Rademakers 2023).\nAvailable on Crunchomics: Not by default\n\n\n\nIt is easiest to install chopper with conda. If you do not have mamba (or conda) installed. Check out the installation instructions for mamba and conda. Mamba is a newer version of conda and from experience faster but since its newer this might mean that not all software can yet be installed with mamba.\n\nmamba create --name chopper -c bioconda chopper\n\n\n\n\nRequired input:\n\nFASTQ files\n\nOutput:\n\nFASTQ files\n\nExample usage:\n\nchopper -q 10 \\\n        --headcrop 0 \\\n        --tailcrop 0   \\\n        -l 250 \\\n        --maxlength 3000  \\\n        --threads 1 \n\nUseful arguments:\n\n--headcrop Trim N nucleotides from the start of a read [default: 0]\n--maxlength Sets a maximum read length [default: 2147483647]\n-l, --minlength Sets a minimum read length [default: 1]\n-q, --quality Sets a minimum Phred average quality score [default: 0]\n--tailcrop Trim N nucleotides from the end of a read [default: 0]\n--threads Number of parallel threads to use [default: 4]\n--contam Fasta file with reference to check potential contaminants against [default None]}"
  },
  {
    "objectID": "source/nanopore/nanoclass.html",
    "href": "source/nanopore/nanoclass.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoClass is a snakemake workflow developed by IBED’s former biofinformatician Evelien Jongepier.\nNanoClass is a taxonomic meta-classifier for meta-barcoding 16S rRNA gene sequencing data generated with the Oxford Nanopore Technology. With a single command, this Snakemake pipeline installs all programs and databases and runs and evaluates 10 popular taxonomic classification tools.\nTo find out more check out the manual."
  },
  {
    "objectID": "source/nanopore/nanoclass.html#nanoclass",
    "href": "source/nanopore/nanoclass.html#nanoclass",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoClass is a snakemake workflow developed by IBED’s former biofinformatician Evelien Jongepier.\nNanoClass is a taxonomic meta-classifier for meta-barcoding 16S rRNA gene sequencing data generated with the Oxford Nanopore Technology. With a single command, this Snakemake pipeline installs all programs and databases and runs and evaluates 10 popular taxonomic classification tools.\nTo find out more check out the manual."
  },
  {
    "objectID": "source/nanopore/nanoqc_readme.html",
    "href": "source/nanopore/nanoqc_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoQC is a quality control tool for long read sequencing data aiming to replicate some of the plots made by fastQC (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\n\nNanoQC is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools using pip\n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\n\n\nInputs: Fastqc.gz file\nOutput: An HTML with quality information\n\nExample code:\n\n#start environment\nconda activate nanopack\n\n#run on a single file\nnanoQC myfile.fastq.gz -o outputfolder\n\nUseful arguments (for the full version, check the manual):\n\n-l, --minlen {int} Minimum length of reads to be included in the plots. This also controls the length plotted in the graphs from the beginning and end of reads (length plotted = minlen / 2)"
  },
  {
    "objectID": "source/nanopore/nanoqc_readme.html#nanoqc",
    "href": "source/nanopore/nanoqc_readme.html#nanoqc",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoQC is a quality control tool for long read sequencing data aiming to replicate some of the plots made by fastQC (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\n\nNanoQC is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools using pip\n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\n\n\nInputs: Fastqc.gz file\nOutput: An HTML with quality information\n\nExample code:\n\n#start environment\nconda activate nanopack\n\n#run on a single file\nnanoQC myfile.fastq.gz -o outputfolder\n\nUseful arguments (for the full version, check the manual):\n\n-l, --minlen {int} Minimum length of reads to be included in the plots. This also controls the length plotted in the graphs from the beginning and end of reads (length plotted = minlen / 2)"
  },
  {
    "objectID": "source/nanopore/readme.html",
    "href": "source/nanopore/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "In this section you find tools to analyse long-read sequencing data. There might be a bit of overlap with the section “Amplicon sequencing data” but in the Nanopore analysis section you specifically will find tools suitable for dealing with long read data generated with Nanopore."
  },
  {
    "objectID": "source/nanopore/readme.html#general",
    "href": "source/nanopore/readme.html#general",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "In this section you find tools to analyse long-read sequencing data. There might be a bit of overlap with the section “Amplicon sequencing data” but in the Nanopore analysis section you specifically will find tools suitable for dealing with long read data generated with Nanopore."
  }
]