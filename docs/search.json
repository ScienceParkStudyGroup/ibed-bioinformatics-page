[
  {
    "objectID": "source/snakemake/tutorial.html",
    "href": "source/snakemake/tutorial.html",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "Notes following the snakemake tutorial outlined here. Also, check out this snakemake presentation for more information.\nformat: pdf: toc: true number-sections: true colorlinks: true\n\n\n\nPython-based workflow system\nCommand-line interface\nScheduling algorithm suitable to work on clusters and batch systems allowing scaling of workflows\nReproducibility:\n\nInstall required software and all dependencies in exact versions\nSoftware install from different sources\nNormalize installation via build script\nNo admin rights needed\nIsolated environments\n\nIntegrates with Conda package manager and the Singularity container engine\nGeneration of self-contained HTML reports\nWorkflows are defined in terms of rules that define how to create output files from input files. The are executed in three phases:\n\nInitialization (parsing)\nDAG phase (DAG is build)\nScheduling phase (execution of DAG)\n\nDependencies\n\nbetween the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be automatically parallelized\nDependencies are determined top-down, i.e. the rule on top which defines the input gets passed to a rule further down were files are for example sorted\n\nJob execution only if:\n\nOutput file is target and does notexist\nInput file newer than output file\nRule has been modified\nExecution is enforced\n\nEasy distribution of workflows via git repositories with standardized folder structure\n\n\n\n\n\nIt is best practice to have subsequent steps of a workflow in separate, unique, output folders. This keeps the working directory structured. Further, such unique prefixes allow Snakemake to quickly discard most rules in its search for rules that can provide the requested input. This accelerates the resolution of the rule dependencies in a workflow.\nSnakemake (&gt;=5.11) comes with a code quality checker (a so called linter), that analyzes your workflow and highlights issues that should be solved in order to follow best practices, achieve maximum readability, and reproducibility. The linter can be invoked with snakemake --lint\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nFurther, scripts should be stored in a subfolder workflow/scripts and notebooks in a subfolder workflow/notebooks\nConda environments (see Integrated Package Management) should be stored in a subfolder workflow/envs\nWhen publishing your workflow in a Github repository, it is a good idea to add some minimal test data and configure Github Actions for continuously testing the workflow on each new commit. For this purpose, we provide predefined Github actions for both running tests and linting here, as well as formatting here.\nConfiguration of a workflow should be handled via config files and, if needed, tabular configuration like sample sheets (either via Pandas or PEPs). Use such configuration for metadata and experiement information, not for runtime specific configuration like threads, resources and output folders. For those, just rely on Snakemake’s CLI arguments like –set-threads, –set-resources, –set-default-resources, and –directory. This makes workflows more readable, scalable, and portable.\nTry to keep filenames short (thus easier on the eye), but informative. Avoid mixing of too many special characters (e.g. decide whether to use _ or - as a separator and do that consistently throughout the workflow).\nTry to keep Python code like helper functions separate from rules (e.g. in a workflow/rules/common.smk file). This way, you help non-experts to read the workflow without needing to parse internals that are irrelevant for them. The helper function names should be chosen in a way that makes them sufficiently informative without looking at their content. Also avoid lambda expressions inside of rules.\nMake use of Snakemake wrappers whenever possible. Consider contributing to the wrapper repo whenever you have a rule that reoccurs in at least two of your workflows. More about wrappers can be found here.\n\nFollow this link to view the recommended structure to store a workflow in a git repository. A snakemake workflow template can be found here. Conda determines the priority of channels depending on their order of appearance in the channels list. For instance, the channel that comes first in the list gets the highest priority.\n├── .gitignore\n├── README.md\n├── LICENSE.md\n├── workflow\n│   ├── rules\n|   │   ├── module1.smk\n|   │   └── module2.smk\n│   ├── envs\n|   │   ├── tool1.yaml\n|   │   └── tool2.yaml\n│   ├── scripts\n|   │   ├── script1.py\n|   │   └── script2.R\n│   ├── notebooks\n|   │   ├── notebook1.py.ipynb\n|   │   └── notebook2.r.ipynb\n│   ├── report\n|   │   ├── plot1.rst\n|   │   └── plot2.rst\n|   └── Snakefile\n├── config\n│   ├── config.yaml\n│   └── some-sheet.tsv\n├── results\n└── resources\n\n\n\n\n-n --dry-run: Snakemake will only show the execution plan instead of actually performing the steps\n-p: print the resulting shell command for illustration.\n--cores {INTEGER}: The numbers of course to use. Must be part of every executed workflow\n--forcerun allows to repeat a run even if the output files already exist from a given rule\n--foreall enforces a complete re-execution of the workflow.\n\n\n\n\nA Snakemake workflow is defined by specifying rules in a Snakefile. Rules decompose the workflow into small steps (for example, the application of a single tool) by specifying how to create sets of output files from sets of input files. Snakemake automatically determines the dependencies between the rules by matching file names.\nAll added syntactic structures begin with a keyword followed by a code block that is either in the same line or indented and consisting of multiple lines. The resulting syntax resembles that of original Python constructs.\nIn this workflow we perform a read mapping workflow.\n\n\n\n\n\nNotice:\n\nSetup done in WSL\nWe will use Conda to create an isolated environment with all the required software for this tutorial.\nMambaforge already installed, if instructions are needed go here\n\n\n\n\n\n#define wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/Snakemake\"\ncd $wdir\n\n#activate conda enviornment (for this to work, see code cell below)\nconda activate snakemake-tutorial\n\n\n#download the example data\ncurl -L https://api.github.com/repos/snakemake/snakemake-tutorial-data/tarball -o snakemake-tutorial-data.tar.gz\n\n#extract the data\ntar --wildcards -xf snakemake-tutorial-data.tar.gz --strip 1 \"*/data\" \"*/environment.yaml\"\n\n#create conda env (/home/ndombrowski/mambaforge/envs/snakemake-tutorial)\nmamba env create --name snakemake-tutorial --file environment.yaml\n\n\n\n\n\n\n\n\n\nTo write our first rule, we create a new document and use it to map a sample to a reference genome using bwa mem.\nA snakemake rule:\n\nHas a name, i.e. bwa_map\nHas a number of directives (i.e. input, output, shell)\ninput can provide a list of files that are expected to be used. Important: have a comma between input/output items.\nHas a shell command, i.e. the shell command to be executed\nIf a rule has multiple input files, they will be concatenated and separated by a whitespace (i.e. we get data/genome.fa data/samples/A.fastq)\n\nBasics: - Snakemake applies the rules given in the Snakefile in a top-down way - The application of a rule to generate a set of output files is called job - Note that Snakemake automatically creates missing directories before jobs are executed. - Snakemake works backwards from requested output, and not from available input. Thus, it does not automatically infer all possible output from, for example, the fastq files in the data folder.\n\nnano Snakefile\n\nContent:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/A.fastq\"\n    output:\n        \"mapped_reads/A.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#perform dry-run workflow\nsnakemake -np mapped_reads/A.bam\n\n#execute workflow \nsnakemake --cores 1 mapped_reads/A.bam\n\nSince mapped_reads/A.bam now exists snakemake WILL NOT try to create this file again. Snakemake only re-runs jobs if one of the input files is newer than one of the output files or one of the input files will be updated by another job.\n\n\n\n\nIf we want to work on more than one sample not only data/samples/A.fastq then we can generalize rules using named wildcards. Below, we for example replace the A with {sample}\nNote that you can have multiple wildcards in your file paths, however, to avoid conflicts with other jobs of the same rule, all output files of a rule have to contain exactly the same wildcards.\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#execute command (dry-run)\nsnakemake -np mapped_reads/B.bam\n\nIf we run this, we will see that bwa will use an input name that is based on the B.bam name, i.e. data/samples/B.fastq. You can see how the character B is propagaged through the input file names and the names in the shell command.\n\n#specify more than one target\nsnakemake -np mapped_reads/A.bam mapped_reads/B.bam\n\n#same command but more condensed using bash magic \nsnakemake -np mapped_reads/{A,B}.bam\n\nIn both cases, you will see that Snakemake only proposes to create the output file mapped_reads/B.bam. This is because you already executed the workflow before (see the previous step) and no input file is newer than the output file mapped_reads/A.bam.\n\n\n\nAdd a new rule:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam\"\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nComments:\n\nIn the shell command above we split the string into two lines, which are however automatically concatenated into one by Python. This is a handy pattern to avoid too long shell command lines. When using this, make sure to have a trailing whitespace in each line but the last, in order to avoid arguments to become not properly separated.\nWith sorted_reads/{wildcards.sample} we extract the value of the sample wildcard, i.e. we get sorted_reads/B\n\n\n#dry run\nsnakemake -np sorted_reads/B.bam\n\n\n\n\nAdd a new rule:\n\nrule samtools_index:\n    input:\n        \"sorted_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam.bai\"\n    shell:\n        \"samtools index {input}\"\n\n\n#dry run \nsnakemake -np sorted_reads/B.bam.bai\n\n#create a DAG\nsnakemake --dag sorted_reads/{A,B}.bam.bai | dot -Tsvg &gt; dag.svg\n\nThe last command will create a visualization of the DAG using the dot command provided by Graphviz. For the given target files, Snakemake specifies the DAG in the dot language and pipes it into the dot command, which renders the definition into SVG format. The rendered DAG is piped into the file dag.svg.\nThe DAG contains a node for each job with the edges connecting them representing the dependencies. The frames of jobs that don’t need to be run (because their output is up-to-date) are dashed. For rules with wildcards, the value of the wildcard for the particular job is displayed in the job node.\n\n\n\nNext, we will aggregate the mapped reads from all samples and jointly call genomic variants on them. Therefore, we will combine the two utilities samtools and bcftools. Snakemake provides a helper function for collecting input files that helps us to describe the aggregation in this step. With\nexpand(\"sorted_reads/{sample}.bam\", sample=SAMPLES)\nwe obtain a list of files where the given pattern “sorted_reads/{sample}.bam” was formatted with the values in a given list of samples SAMPLES, i.e.\n[\"sorted_reads/A.bam\", \"sorted_reads/B.bam\"]\nThis can also be used if multiple wildcards are used i.e.\nexpand(\"sorted_reads/{sample}.{replicate}.bam\", sample=SAMPLES, replicate=[0, 1])\nwould create\n[\"sorted_reads/A.0.bam\", \"sorted_reads/A.1.bam\", \"sorted_reads/B.0.bam\", \"sorted_reads/B.1.bam\"]\nLet’s start with defining the list of samples ad-hoc in plain Python at the top of the Snakefile and add a rule at the end of our Snakefile:\n\nSAMPLES = [\"A\", \"B\"]\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=SAMPLES),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=SAMPLES)\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\nWith multiple input or output files, it is sometimes handy to refer to them separately in the shell command. This can be done by specifying names for input or output files, for example with fa=.... The files can then be referred to in the shell command by name, for example with {input.fa}.\nFor long shell commands like this one, it is advisable to split the string over multiple indented lines. Python will automatically merge it into one. Further, you will notice that the input or output file lists can contain arbitrary Python statements, as long as it returns a string, or a list of strings. Here, we invoke our expand function to aggregate over the aligned reads of all samples.\n\n#create a DAG\nsnakemake --dag calls/all.vcf | dot -Tsvg &gt; dag.svg\n\n\n\n\nUsually, a workflow not only consists of invoking various tools, but also contains custom code to for example calculate summary statistics or create plots. While Snakemake also allows you to directly write Python code inside a rule, it is usually reasonable to move such logic into separate scripts. For this purpose, Snakemake offers the script directive. Add the following rule to your Snakefile:\nWith this rule, we will eventually generate a histogram of the quality scores that have been assigned to the variant calls in the file calls/all.vcf. The actual Python code to generate the plot is hidden in the script scripts/plot-quals.py.\nScript paths are always relative to the referring Snakefile. In the script, all properties of the rule like input, output, wildcards, etc. are available as attributes of a global snakemake object.\n\nrule plot_quals:\n    input:\n        \"calls/all.vcf\"\n    output:\n        \"plots/quals.svg\"\n    script:\n        \"scripts/plot-quals.py\"\n\nCreate the file scripts/plot-quals.py, with the following content:\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom pysam import VariantFile\n\nquals = [record.qual for record in VariantFile(snakemake.input[0])]\nplt.hist(quals)\n\nplt.savefig(snakemake.output[0])\n\nAlthough there are other strategies to invoke separate scripts from your workflow (for example, invoking them via shell commands), the benefit of this is obvious: the script logic is separated from the workflow logic (and can even be shared between workflows), but boilerplate code like the parsing of command line arguments is unnecessary.\nApart from Python scripts, it is also possible to use R scripts. In R scripts, an S4 object named snakemake analogous to the Python case above is available and allows access to input and output files and other parameters. Here, the syntax follows that of S4 classes with attributes that are R lists, for example we can access the first input file with snakemake@input[[1]] (note that the first file does not have index 0 here, because R starts counting from 1). Named input and output files can be accessed in the same way, by just providing the name instead of an index, for example snakemake@input[[“myfile”]].\nFor details and examples, see the External scripts section in the Documentation.\n\n\n\nSo far, we always executed the workflow by specifying a target file at the command line. Apart from filenames, Snakemake also accepts rule names as targets if the requested rule does not have wildcards.\nIt is possible to write target rules collecting particular subsets of the desired results or all results. Moreover, if no target is given at the command line, Snakemake will define the first rule of the Snakefile as the target. Hence, it is best practice to have a rule all at the top of the workflow which has all typically desired target files as input files.\nTo do this add the following to the top of the workflow:\n\nrule all:\n    input:\n        \"plots/quals.svg\"\n\nAnd execute snakemake with:\n\nsnakemake -n\n\nIf we run this the execution plan for creating the file plots/quals.svg, which contains and summarizes all our results, will be shown.\nNote that, apart from Snakemake considering the first rule of the workflow as the default target, the order of rules in the Snakefile is arbitrary and does not influence the DAG of jobs.\n\n\n\n\n#execution \nsnakemake --cores 1\n\n#view output\nwslview plots/quals.svg\n\n#force to re-execute samtools_sort\nsnakemake --cores 1 --forcerun samtools_sort\n\n\n\n\n\n\n\nSnakemake can be made aware of the threads a rule needs with the threads directive. Ie change the bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    threads: 2\n    shell:\n        \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\nThe number of threads can be propagated to the shell command with the familiar braces notation (i.e. {threads}). If no threads directive is given, a rule is assumed to need 1 thread.\nWhen a workflow is executed, the number of threads the jobs need is considered by the Snakemake scheduler. In particular, the scheduler ensures that the sum of the threads of all jobs running at the same time does not exceed a given number of available CPU cores. This number is given with the –cores command line argument, which is mandatory for snakemake calls that actually run the workflow.\nFor example snakemake --cores 10 would execute the workflow with 10 cores. Since the rule bwa_map needs 8 threads, only one job of the rule can run at a time, and the Snakemake scheduler will try to saturate the remaining cores with other jobs like, e.g., samtools_sort.\nThe threads directive in a rule is interpreted as a maximum: when less cores than threads are provided, the number of threads a rule uses will be reduced to the number of given cores.\nIf --cores is given without a number, all available cores are used.\n\nsnakemake --forceall --cores 2\nsnakemake --forceall --cores 4\n\n\n\n\nWe can use a configuration file if we want your workflow to be customizable:\n\nCheck out the full readme here\nCan be provided as JSON or YAML\nThey are used with the configfile direction\nvariable names in the rule and the key name in the configuration file do not have to be identical. Snakemake matches variables in the rule based on their position within the curly braces {}.\n\nA config file is specified by adding this to the top of our Snakemake workflow:\n\nconfigfile: \"config.yaml\"\n\nSnakemake will load the config file and store its contents into a globally available dictionary named config. In our case, it makes sense to specify the samples in config.yaml as:\nsamples:\n    A: data/samples/A.fastq\n    B: data/samples/B.fastq\nAnd remove the statement defining the SAMPLES in the Snakemake file. And change the rule of bcftools_call to:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\n\n\n\nSince we have stored the path to the FASTQ files in the config file, we can also generalize the rule bwa_map to use these paths. This case is different to the rule bcftools_call we modified above. To understand this, it is important to know that Snakemake workflows are executed in three phases.\n\nIn the initialization phase, the files defining the workflow are parsed and all rules are instantiated.\nIn the DAG phase, the directed acyclic dependency graph of all jobs is built by filling wildcards and matching input files to output files.\nIn the scheduling phase, the DAG of jobs is executed, with jobs started according to the available resources.\n\nThe expand functions in the list of input files of the rule bcftools_call are executed during the initialization phase.\nIn this phase, we don’t know about jobs, wildcard values and rule dependencies. Hence, we cannot determine the FASTQ paths for rule bwa_map from the config file in this phase, because we don’t even know which jobs will be generated from that rule.\nInstead, we need to defer the determination of input files to the DAG phase. This can be achieved by specifying an input function instead of a string as inside of the input directive. For the rule bwa_map this works as follows:\n::: {.cell}\ndef get_bwa_map_input_fastqs(wildcards):\n   return config[\"samples\"][wildcards.sample]\n\nrule bwa_map:\n   input:\n       \"data/genome.fa\",\n       get_bwa_map_input_fastqs\n   output:\n       \"mapped_reads/{sample}.bam\"\n   threads: 8\n   shell:\n       \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n:::\nHow wild cards work here:\n\nThe wildcards object is a special Snakemake variable that represents placeholders in rule input and output file names. In your case, you are using {sample} as a placeholder in the rule’s input section, and Snakemake automatically parses this placeholder into a wildcards object.\nWhen you define the function get_bwa_map_input_fastqs(wildcards), you are essentially telling Snakemake that this function accepts a wildcards object as an argument. This allows the function to access and use the wildcards to determine which sample’s input files to fetch.\nWhen Snakemake processes the rule bwa_map, it looks at the rule’s input section and identifies that it contains the {sample} placeholder.\nSnakemake then matches the placeholder to the wildcards object. If you have multiple samples defined in your config.yaml, Snakemake will create separate jobs for each sample, each with a different wildcards.sample value.\nFor each job, the wildcards.sample attribute will be replaced with the actual sample name being processed. For example, if Snakemake is processing the sample “A,” then wildcards.sample will be “A.”\nThe function get_bwa_map_input_fastqs uses config[“samples”][wildcards.sample] to fetch the corresponding input file path for the sample being processed. So, for sample “A,” it would retrieve “data/samples/A.fastq” as the input file path.\n\nBenefits of using python function:\n\nModularity and Reusability: By defining a separate Python function (get_bwa_map_input_fastqs) to retrieve the input files, you create a modular and reusable piece of code. This can be particularly useful if you have multiple rules that need to access the same input files, as you can reuse the function across those rules.\nAbstraction: The function abstracts the details of where the input files come from. In your first version, you had to specify the exact path to the input files in the rule itself. With the function, you only need to specify how to obtain the input files for a given sample, making your rule more abstract and easier to maintain.\nFlexibility: If you need to change the way you obtain input files in the future (for example, if you decide to store them in a different directory or use a different naming convention), you can update the function in one place, and all the rules that use it will automatically adapt.\nReduced Redundancy: Your original version of the rule had repetitive code for specifying input and output paths for each sample. With the function, you reduce redundancy and the potential for errors, as the input file retrieval logic is centralized.\nEasier Testing: You can more easily write unit tests for the input file retrieval function to ensure it works correctly, which can be challenging when input paths are hard-coded in multiple rules.\n\n\n\nIn the data/samples folder, there is an additional sample C.fastq. Add that sample to the config file and see how Snakemake wants to recompute the part of the workflow belonging to the new sample, when invoking with snakemake -n --forcerun bcftools_call\nNote: Snakemake does not automatically rerun jobs when new input files are added as in the excercise below. However, you can get a list of output files that are affected by such changes with snakemake –list-input-changes. To trigger a rerun, this bit of bash magic helps:\n\nsnakemake -n --forcerun $(snakemake --list-input-changes)\n\n\n\n\n\nSometimes shell commands are not only composed of input and output files but require additional parameters.\nWe can set additional parameters need to be set depending on the wildcard values of the job. For this, Snakemake allows to define arbitrary parameters for rules with the params directive. I.e. edit bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    threads: 8\n    shell:\n        \"bwa mem -R '{params.rg}' -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\n\nsnakemake -np --forceall\n\nNext, consider another important parameter, the prior mutation rate (1e-3 per default). It is set via the flag -P of the bcftools call command. Consider making this flag configurable via adding a new key to the config file and using the params directive in the rule bcftools_call to propagate it to the shell command.\nAdd this in the config file:\nbcftools:\n    evalue: 0.001\nChange this in the Snakemakefile:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}\"\n\n\nsnakemake -p --cores 1 --forceall\n\n\n\n\nWhen executing a large workflow, it is usually desirable to store the logging output of each job into a separate file, instead of just printing all logging output to the terminal—when multiple jobs are run in parallel, this would result in chaotic output. For this purpose, Snakemake allows to specify log files for rules. Log files are defined via the log directive and handled similarly to output files, but they are not subject of rule matching and are not cleaned up when a job fails. We modify our rule bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThe shell command is modified to collect STDERR output of both bwa and samtools and pipe it into the file referred to by {log}. Log files must contain exactly the same wildcards as the output files to avoid file name clashes between different jobs of the same rule.\nLet’s add a log to the to the bcftools_call rule as well.\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    log:\n        \"logs/bcftools/all.log\"\n    shell:\n        \"(bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}) 2&gt; {log}\"\n\n\nsnakemake -p --cores 1 --forceall\n\nThe ability to track the provenance of each generated result is an important step towards reproducible analyses. Apart from the report functionality discussed before, Snakemake can summarize various provenance information for all output files of the workflow. The flag –summary prints a table associating each output file with the rule used to generate it, the creation date and optionally the version of the tool used for creation is provided. Further, the table informs about updated input files and changes to the source code of the rule after creation of the output file.\n\nsnakemake -p --cores 1 --forceall --summary\n\n\n\n\nIn our workflow, we create two BAM files for each sample, namely the output of the rules bwa_map and samtools_sort. When not dealing with examples, the underlying data is usually huge. Hence, the resulting BAM files need a lot of disk space and their creation takes some time. To save disk space, you can mark output files as temporary. Snakemake will delete the marked files for you, once all the consuming jobs (that need it as input) have been executed. We use this mechanism for the output file of the rule bwa_map:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThis results in the deletion of the BAM file once the corresponding samtools_sort job has been executed. Since the creation of BAM files via read mapping and sorting is computationally expensive, it is reasonable to protect the final BAM file from accidental deletion or modification. We modify the rule samtools_sort to mark its output file as protected:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        protected(\"sorted_reads/{sample}.bam\")\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nRun Snakemake with the target mapped_reads/A.bam. Although the file is marked as temporary, you will see that Snakemake does not delete it because it is specified as a target file.\n\nsnakemake --forceall --cores 1 mapped_reads/A.bam \n\n\n\n\n\nFor more, check out this documentation.\n\n\nIn order to re-use building blocks or simply to structure large workflows, it is sometimes reasonable to split a workflow into modules. For this, Snakemake provides the include directive to include another Snakefile into the current one, e.g.:\ninclude: \"path/to/other.snakefile\"\nAlternatively, Snakemake allows to define sub-workflows. A sub-workflow refers to a working directory with a complete Snakemake workflow. Output files of that sub-workflow can be used in the current Snakefile. When executing, Snakemake ensures that the output files of the sub-workflow are up-to-date before executing the current workflow. This mechanism is particularly useful when you want to extend a previous analysis without modifying it. For details about sub-workflows, see the documentation.\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nLets put the read mapping into a separate snakefile and use include to make it available in our workflow:\nIn rules add a new file bwa_mem.smk with the following content:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 2\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nRemove the bwa_mem rule from the workflow and add include: \"rules/bwa_mem.smk and run the workflow with :\n\nsnakemake --forceall -p --cores 1\n\n\n\n\nIn order to get a fully reproducible data analysis, it is not sufficient to be able to execute each step and document all used parameters. The used software tools and libraries have to be documented as well. In this tutorial, you have already seen how Conda can be used to specify an isolated software environment for a whole workflow. With Snakemake, you can go one step further and specify Conda environments per rule. This way, you can even make use of conflicting software versions (e.g. combine Python 2 with Python 3).\nIn our example, instead of using an external environment we can specify environments per rule, e.g.:\n\nrule samtools_index:\n  input:\n      \"sorted_reads/{sample}.bam\"\n  output:\n      \"sorted_reads/{sample}.bam.bai\"\n  conda:\n      \"envs/samtools.yaml\"\n  shell:\n      \"samtools index {input}\"\n\nwith envs/samtools.yaml defined as:\n\nchannels:\n  - bioconda\n  - conda-forge\ndependencies:\n  - samtools =1.9\n\nWhen executing the workflow with snakemake --use-conda --cores 1 it will automatically create required environments and activate them before a job is executed. It is best practice to specify at least the major and minor version of any packages in the environment definition.\nSpecifying environments per rule in this way has two advantages: - First, the workflow definition also documents all used software versions. - Second, a workflow can be re-executed (without admin rights) on a vanilla system, without installing any prerequisites apart from Snakemake and Miniconda.\n\n\n\nIn order to simplify the utilization of popular tools, Snakemake provides a repository of so-called wrappers (the Snakemake wrapper repository). A wrapper is a short script that wraps (typically) a command line application and makes it directly addressable from within Snakemake. For this, Snakemake provides the wrapper directive that can be used instead of shell, script, or run. For example, the rule bwa_map could alternatively look like this:\n\nrule bwa_mem:\n  input:\n      ref=\"data/genome.fa\",\n      sample=lambda wildcards: config[\"samples\"][wildcards.sample]\n  output:\n      temp(\"mapped_reads/{sample}.bam\")\n  log:\n      \"logs/bwa_mem/{sample}.log\"\n  params:\n      \"-R '@RG\\tID:{sample}\\tSM:{sample}'\"\n  threads: 8\n  wrapper:\n      \"0.15.3/bio/bwa/mem\"\n\n\n\n\nBy default, Snakemake executes jobs on the local machine it is invoked on. Alternatively, it can execute jobs in distributed environments, e.g., compute clusters or batch systems. If the nodes share a common file system, Snakemake supports three alternative execution modes.\nIn cluster environments, compute jobs are usually submitted as shell scripts via commands like qsub. Snakemake provides a generic mode to execute on such clusters. By invoking Snakemake with snakemake --cluster qsub --jobs 100 each job will be compiled into a shell script that is submitted with the given command (here qsub). The –jobs flag limits the number of concurrently submitted jobs to 100.\nFind out more for SLURM submissions.\n\n\n\nSometimes it is useful to constrain the values a wildcard can have. This can be achieved by adding a regular expression that describes the set of allowed wildcard values. For example, the wildcard sample in the output file \"sorted_reads/{sample}.bam\" can be constrained to only allow alphanumeric sample names as \"sorted_reads/{sample,[A-Za-z0-9]+}.bam\".\nConstraints may be defined per rule or globally using the wildcard_constraints keyword, as demonstrated in Wildcards. This mechanism helps to solve two kinds of ambiguity.\n\nIt can help to avoid ambiguous rules, i.e. two or more rules that can be applied to generate the same output file. Other ways of handling ambiguous rules are described in the Section Handling Ambiguous Rules.\nIt can help to guide the regular expression based matching so that wildcards are assigned to the right parts of a file name. Consider the output file {sample}.{group}.txt and assume that the target file is A.1.normal.txt. It is not clear whether dataset=“A.1” and group=“normal” or dataset=“A” and group=“1.normal” is the right assignment. Here, constraining the dataset wildcard by {sample,[A-Z]+}.{group}solves the problem.\n\nWhen dealing with ambiguous rules, it is best practice to first try to solve the ambiguity by using a proper file structure, for example, by separating the output files of different steps in different directories.\n\n\n\nA basic report contains runtime statistics, a visualization of the workflow topology, used software and data provenance information.\n\nsnakemake --report report.html --rerun-incomplete\n\nIn addition, you can mark any output file generated in your workflow for inclusion into the report. It will be encoded directly into the report, such that it can be, e.g., emailed as a self-contained document. The reader (e.g., a collaborator of yours) can at any time download the enclosed results from the report for further use, e.g., in a manuscript you write together.\nLet’s mark the output file “results/plots/quals.svg” for inclusion by replacing it with report(“results/plots/quals.svg”, caption=“report/calling.rst”) and adding a file report/calling.rst, containing some description of the output file. This description will be presented as caption in the resulting report."
  },
  {
    "objectID": "source/snakemake/tutorial.html#general",
    "href": "source/snakemake/tutorial.html#general",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "Python-based workflow system\nCommand-line interface\nScheduling algorithm suitable to work on clusters and batch systems allowing scaling of workflows\nReproducibility:\n\nInstall required software and all dependencies in exact versions\nSoftware install from different sources\nNormalize installation via build script\nNo admin rights needed\nIsolated environments\n\nIntegrates with Conda package manager and the Singularity container engine\nGeneration of self-contained HTML reports\nWorkflows are defined in terms of rules that define how to create output files from input files. The are executed in three phases:\n\nInitialization (parsing)\nDAG phase (DAG is build)\nScheduling phase (execution of DAG)\n\nDependencies\n\nbetween the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be automatically parallelized\nDependencies are determined top-down, i.e. the rule on top which defines the input gets passed to a rule further down were files are for example sorted\n\nJob execution only if:\n\nOutput file is target and does notexist\nInput file newer than output file\nRule has been modified\nExecution is enforced\n\nEasy distribution of workflows via git repositories with standardized folder structure"
  },
  {
    "objectID": "source/snakemake/tutorial.html#best-practices",
    "href": "source/snakemake/tutorial.html#best-practices",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "It is best practice to have subsequent steps of a workflow in separate, unique, output folders. This keeps the working directory structured. Further, such unique prefixes allow Snakemake to quickly discard most rules in its search for rules that can provide the requested input. This accelerates the resolution of the rule dependencies in a workflow.\nSnakemake (&gt;=5.11) comes with a code quality checker (a so called linter), that analyzes your workflow and highlights issues that should be solved in order to follow best practices, achieve maximum readability, and reproducibility. The linter can be invoked with snakemake --lint\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nFurther, scripts should be stored in a subfolder workflow/scripts and notebooks in a subfolder workflow/notebooks\nConda environments (see Integrated Package Management) should be stored in a subfolder workflow/envs\nWhen publishing your workflow in a Github repository, it is a good idea to add some minimal test data and configure Github Actions for continuously testing the workflow on each new commit. For this purpose, we provide predefined Github actions for both running tests and linting here, as well as formatting here.\nConfiguration of a workflow should be handled via config files and, if needed, tabular configuration like sample sheets (either via Pandas or PEPs). Use such configuration for metadata and experiement information, not for runtime specific configuration like threads, resources and output folders. For those, just rely on Snakemake’s CLI arguments like –set-threads, –set-resources, –set-default-resources, and –directory. This makes workflows more readable, scalable, and portable.\nTry to keep filenames short (thus easier on the eye), but informative. Avoid mixing of too many special characters (e.g. decide whether to use _ or - as a separator and do that consistently throughout the workflow).\nTry to keep Python code like helper functions separate from rules (e.g. in a workflow/rules/common.smk file). This way, you help non-experts to read the workflow without needing to parse internals that are irrelevant for them. The helper function names should be chosen in a way that makes them sufficiently informative without looking at their content. Also avoid lambda expressions inside of rules.\nMake use of Snakemake wrappers whenever possible. Consider contributing to the wrapper repo whenever you have a rule that reoccurs in at least two of your workflows. More about wrappers can be found here.\n\nFollow this link to view the recommended structure to store a workflow in a git repository. A snakemake workflow template can be found here. Conda determines the priority of channels depending on their order of appearance in the channels list. For instance, the channel that comes first in the list gets the highest priority.\n├── .gitignore\n├── README.md\n├── LICENSE.md\n├── workflow\n│   ├── rules\n|   │   ├── module1.smk\n|   │   └── module2.smk\n│   ├── envs\n|   │   ├── tool1.yaml\n|   │   └── tool2.yaml\n│   ├── scripts\n|   │   ├── script1.py\n|   │   └── script2.R\n│   ├── notebooks\n|   │   ├── notebook1.py.ipynb\n|   │   └── notebook2.r.ipynb\n│   ├── report\n|   │   ├── plot1.rst\n|   │   └── plot2.rst\n|   └── Snakefile\n├── config\n│   ├── config.yaml\n│   └── some-sheet.tsv\n├── results\n└── resources"
  },
  {
    "objectID": "source/snakemake/tutorial.html#useful-options",
    "href": "source/snakemake/tutorial.html#useful-options",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "-n --dry-run: Snakemake will only show the execution plan instead of actually performing the steps\n-p: print the resulting shell command for illustration.\n--cores {INTEGER}: The numbers of course to use. Must be part of every executed workflow\n--forcerun allows to repeat a run even if the output files already exist from a given rule\n--foreall enforces a complete re-execution of the workflow."
  },
  {
    "objectID": "source/snakemake/tutorial.html#description-of-example-workflow",
    "href": "source/snakemake/tutorial.html#description-of-example-workflow",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "A Snakemake workflow is defined by specifying rules in a Snakefile. Rules decompose the workflow into small steps (for example, the application of a single tool) by specifying how to create sets of output files from sets of input files. Snakemake automatically determines the dependencies between the rules by matching file names.\nAll added syntactic structures begin with a keyword followed by a code block that is either in the same line or indented and consisting of multiple lines. The resulting syntax resembles that of original Python constructs.\nIn this workflow we perform a read mapping workflow."
  },
  {
    "objectID": "source/snakemake/tutorial.html#setup",
    "href": "source/snakemake/tutorial.html#setup",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "Notice:\n\nSetup done in WSL\nWe will use Conda to create an isolated environment with all the required software for this tutorial.\nMambaforge already installed, if instructions are needed go here\n\n\n\n\n\n#define wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/Snakemake\"\ncd $wdir\n\n#activate conda enviornment (for this to work, see code cell below)\nconda activate snakemake-tutorial\n\n\n#download the example data\ncurl -L https://api.github.com/repos/snakemake/snakemake-tutorial-data/tarball -o snakemake-tutorial-data.tar.gz\n\n#extract the data\ntar --wildcards -xf snakemake-tutorial-data.tar.gz --strip 1 \"*/data\" \"*/environment.yaml\"\n\n#create conda env (/home/ndombrowski/mambaforge/envs/snakemake-tutorial)\nmamba env create --name snakemake-tutorial --file environment.yaml"
  },
  {
    "objectID": "source/snakemake/tutorial.html#basic-workflow",
    "href": "source/snakemake/tutorial.html#basic-workflow",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "To write our first rule, we create a new document and use it to map a sample to a reference genome using bwa mem.\nA snakemake rule:\n\nHas a name, i.e. bwa_map\nHas a number of directives (i.e. input, output, shell)\ninput can provide a list of files that are expected to be used. Important: have a comma between input/output items.\nHas a shell command, i.e. the shell command to be executed\nIf a rule has multiple input files, they will be concatenated and separated by a whitespace (i.e. we get data/genome.fa data/samples/A.fastq)\n\nBasics: - Snakemake applies the rules given in the Snakefile in a top-down way - The application of a rule to generate a set of output files is called job - Note that Snakemake automatically creates missing directories before jobs are executed. - Snakemake works backwards from requested output, and not from available input. Thus, it does not automatically infer all possible output from, for example, the fastq files in the data folder.\n\nnano Snakefile\n\nContent:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/A.fastq\"\n    output:\n        \"mapped_reads/A.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#perform dry-run workflow\nsnakemake -np mapped_reads/A.bam\n\n#execute workflow \nsnakemake --cores 1 mapped_reads/A.bam\n\nSince mapped_reads/A.bam now exists snakemake WILL NOT try to create this file again. Snakemake only re-runs jobs if one of the input files is newer than one of the output files or one of the input files will be updated by another job.\n\n\n\n\nIf we want to work on more than one sample not only data/samples/A.fastq then we can generalize rules using named wildcards. Below, we for example replace the A with {sample}\nNote that you can have multiple wildcards in your file paths, however, to avoid conflicts with other jobs of the same rule, all output files of a rule have to contain exactly the same wildcards.\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#execute command (dry-run)\nsnakemake -np mapped_reads/B.bam\n\nIf we run this, we will see that bwa will use an input name that is based on the B.bam name, i.e. data/samples/B.fastq. You can see how the character B is propagaged through the input file names and the names in the shell command.\n\n#specify more than one target\nsnakemake -np mapped_reads/A.bam mapped_reads/B.bam\n\n#same command but more condensed using bash magic \nsnakemake -np mapped_reads/{A,B}.bam\n\nIn both cases, you will see that Snakemake only proposes to create the output file mapped_reads/B.bam. This is because you already executed the workflow before (see the previous step) and no input file is newer than the output file mapped_reads/A.bam.\n\n\n\nAdd a new rule:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam\"\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nComments:\n\nIn the shell command above we split the string into two lines, which are however automatically concatenated into one by Python. This is a handy pattern to avoid too long shell command lines. When using this, make sure to have a trailing whitespace in each line but the last, in order to avoid arguments to become not properly separated.\nWith sorted_reads/{wildcards.sample} we extract the value of the sample wildcard, i.e. we get sorted_reads/B\n\n\n#dry run\nsnakemake -np sorted_reads/B.bam\n\n\n\n\nAdd a new rule:\n\nrule samtools_index:\n    input:\n        \"sorted_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam.bai\"\n    shell:\n        \"samtools index {input}\"\n\n\n#dry run \nsnakemake -np sorted_reads/B.bam.bai\n\n#create a DAG\nsnakemake --dag sorted_reads/{A,B}.bam.bai | dot -Tsvg &gt; dag.svg\n\nThe last command will create a visualization of the DAG using the dot command provided by Graphviz. For the given target files, Snakemake specifies the DAG in the dot language and pipes it into the dot command, which renders the definition into SVG format. The rendered DAG is piped into the file dag.svg.\nThe DAG contains a node for each job with the edges connecting them representing the dependencies. The frames of jobs that don’t need to be run (because their output is up-to-date) are dashed. For rules with wildcards, the value of the wildcard for the particular job is displayed in the job node.\n\n\n\nNext, we will aggregate the mapped reads from all samples and jointly call genomic variants on them. Therefore, we will combine the two utilities samtools and bcftools. Snakemake provides a helper function for collecting input files that helps us to describe the aggregation in this step. With\nexpand(\"sorted_reads/{sample}.bam\", sample=SAMPLES)\nwe obtain a list of files where the given pattern “sorted_reads/{sample}.bam” was formatted with the values in a given list of samples SAMPLES, i.e.\n[\"sorted_reads/A.bam\", \"sorted_reads/B.bam\"]\nThis can also be used if multiple wildcards are used i.e.\nexpand(\"sorted_reads/{sample}.{replicate}.bam\", sample=SAMPLES, replicate=[0, 1])\nwould create\n[\"sorted_reads/A.0.bam\", \"sorted_reads/A.1.bam\", \"sorted_reads/B.0.bam\", \"sorted_reads/B.1.bam\"]\nLet’s start with defining the list of samples ad-hoc in plain Python at the top of the Snakefile and add a rule at the end of our Snakefile:\n\nSAMPLES = [\"A\", \"B\"]\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=SAMPLES),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=SAMPLES)\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\nWith multiple input or output files, it is sometimes handy to refer to them separately in the shell command. This can be done by specifying names for input or output files, for example with fa=.... The files can then be referred to in the shell command by name, for example with {input.fa}.\nFor long shell commands like this one, it is advisable to split the string over multiple indented lines. Python will automatically merge it into one. Further, you will notice that the input or output file lists can contain arbitrary Python statements, as long as it returns a string, or a list of strings. Here, we invoke our expand function to aggregate over the aligned reads of all samples.\n\n#create a DAG\nsnakemake --dag calls/all.vcf | dot -Tsvg &gt; dag.svg\n\n\n\n\nUsually, a workflow not only consists of invoking various tools, but also contains custom code to for example calculate summary statistics or create plots. While Snakemake also allows you to directly write Python code inside a rule, it is usually reasonable to move such logic into separate scripts. For this purpose, Snakemake offers the script directive. Add the following rule to your Snakefile:\nWith this rule, we will eventually generate a histogram of the quality scores that have been assigned to the variant calls in the file calls/all.vcf. The actual Python code to generate the plot is hidden in the script scripts/plot-quals.py.\nScript paths are always relative to the referring Snakefile. In the script, all properties of the rule like input, output, wildcards, etc. are available as attributes of a global snakemake object.\n\nrule plot_quals:\n    input:\n        \"calls/all.vcf\"\n    output:\n        \"plots/quals.svg\"\n    script:\n        \"scripts/plot-quals.py\"\n\nCreate the file scripts/plot-quals.py, with the following content:\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom pysam import VariantFile\n\nquals = [record.qual for record in VariantFile(snakemake.input[0])]\nplt.hist(quals)\n\nplt.savefig(snakemake.output[0])\n\nAlthough there are other strategies to invoke separate scripts from your workflow (for example, invoking them via shell commands), the benefit of this is obvious: the script logic is separated from the workflow logic (and can even be shared between workflows), but boilerplate code like the parsing of command line arguments is unnecessary.\nApart from Python scripts, it is also possible to use R scripts. In R scripts, an S4 object named snakemake analogous to the Python case above is available and allows access to input and output files and other parameters. Here, the syntax follows that of S4 classes with attributes that are R lists, for example we can access the first input file with snakemake@input[[1]] (note that the first file does not have index 0 here, because R starts counting from 1). Named input and output files can be accessed in the same way, by just providing the name instead of an index, for example snakemake@input[[“myfile”]].\nFor details and examples, see the External scripts section in the Documentation.\n\n\n\nSo far, we always executed the workflow by specifying a target file at the command line. Apart from filenames, Snakemake also accepts rule names as targets if the requested rule does not have wildcards.\nIt is possible to write target rules collecting particular subsets of the desired results or all results. Moreover, if no target is given at the command line, Snakemake will define the first rule of the Snakefile as the target. Hence, it is best practice to have a rule all at the top of the workflow which has all typically desired target files as input files.\nTo do this add the following to the top of the workflow:\n\nrule all:\n    input:\n        \"plots/quals.svg\"\n\nAnd execute snakemake with:\n\nsnakemake -n\n\nIf we run this the execution plan for creating the file plots/quals.svg, which contains and summarizes all our results, will be shown.\nNote that, apart from Snakemake considering the first rule of the workflow as the default target, the order of rules in the Snakefile is arbitrary and does not influence the DAG of jobs.\n\n\n\n\n#execution \nsnakemake --cores 1\n\n#view output\nwslview plots/quals.svg\n\n#force to re-execute samtools_sort\nsnakemake --cores 1 --forcerun samtools_sort"
  },
  {
    "objectID": "source/snakemake/tutorial.html#advanced-options",
    "href": "source/snakemake/tutorial.html#advanced-options",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "Snakemake can be made aware of the threads a rule needs with the threads directive. Ie change the bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    threads: 2\n    shell:\n        \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\nThe number of threads can be propagated to the shell command with the familiar braces notation (i.e. {threads}). If no threads directive is given, a rule is assumed to need 1 thread.\nWhen a workflow is executed, the number of threads the jobs need is considered by the Snakemake scheduler. In particular, the scheduler ensures that the sum of the threads of all jobs running at the same time does not exceed a given number of available CPU cores. This number is given with the –cores command line argument, which is mandatory for snakemake calls that actually run the workflow.\nFor example snakemake --cores 10 would execute the workflow with 10 cores. Since the rule bwa_map needs 8 threads, only one job of the rule can run at a time, and the Snakemake scheduler will try to saturate the remaining cores with other jobs like, e.g., samtools_sort.\nThe threads directive in a rule is interpreted as a maximum: when less cores than threads are provided, the number of threads a rule uses will be reduced to the number of given cores.\nIf --cores is given without a number, all available cores are used.\n\nsnakemake --forceall --cores 2\nsnakemake --forceall --cores 4\n\n\n\n\nWe can use a configuration file if we want your workflow to be customizable:\n\nCheck out the full readme here\nCan be provided as JSON or YAML\nThey are used with the configfile direction\nvariable names in the rule and the key name in the configuration file do not have to be identical. Snakemake matches variables in the rule based on their position within the curly braces {}.\n\nA config file is specified by adding this to the top of our Snakemake workflow:\n\nconfigfile: \"config.yaml\"\n\nSnakemake will load the config file and store its contents into a globally available dictionary named config. In our case, it makes sense to specify the samples in config.yaml as:\nsamples:\n    A: data/samples/A.fastq\n    B: data/samples/B.fastq\nAnd remove the statement defining the SAMPLES in the Snakemake file. And change the rule of bcftools_call to:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\n\n\n\nSince we have stored the path to the FASTQ files in the config file, we can also generalize the rule bwa_map to use these paths. This case is different to the rule bcftools_call we modified above. To understand this, it is important to know that Snakemake workflows are executed in three phases.\n\nIn the initialization phase, the files defining the workflow are parsed and all rules are instantiated.\nIn the DAG phase, the directed acyclic dependency graph of all jobs is built by filling wildcards and matching input files to output files.\nIn the scheduling phase, the DAG of jobs is executed, with jobs started according to the available resources.\n\nThe expand functions in the list of input files of the rule bcftools_call are executed during the initialization phase.\nIn this phase, we don’t know about jobs, wildcard values and rule dependencies. Hence, we cannot determine the FASTQ paths for rule bwa_map from the config file in this phase, because we don’t even know which jobs will be generated from that rule.\nInstead, we need to defer the determination of input files to the DAG phase. This can be achieved by specifying an input function instead of a string as inside of the input directive. For the rule bwa_map this works as follows:\n::: {.cell}\ndef get_bwa_map_input_fastqs(wildcards):\n   return config[\"samples\"][wildcards.sample]\n\nrule bwa_map:\n   input:\n       \"data/genome.fa\",\n       get_bwa_map_input_fastqs\n   output:\n       \"mapped_reads/{sample}.bam\"\n   threads: 8\n   shell:\n       \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n:::\nHow wild cards work here:\n\nThe wildcards object is a special Snakemake variable that represents placeholders in rule input and output file names. In your case, you are using {sample} as a placeholder in the rule’s input section, and Snakemake automatically parses this placeholder into a wildcards object.\nWhen you define the function get_bwa_map_input_fastqs(wildcards), you are essentially telling Snakemake that this function accepts a wildcards object as an argument. This allows the function to access and use the wildcards to determine which sample’s input files to fetch.\nWhen Snakemake processes the rule bwa_map, it looks at the rule’s input section and identifies that it contains the {sample} placeholder.\nSnakemake then matches the placeholder to the wildcards object. If you have multiple samples defined in your config.yaml, Snakemake will create separate jobs for each sample, each with a different wildcards.sample value.\nFor each job, the wildcards.sample attribute will be replaced with the actual sample name being processed. For example, if Snakemake is processing the sample “A,” then wildcards.sample will be “A.”\nThe function get_bwa_map_input_fastqs uses config[“samples”][wildcards.sample] to fetch the corresponding input file path for the sample being processed. So, for sample “A,” it would retrieve “data/samples/A.fastq” as the input file path.\n\nBenefits of using python function:\n\nModularity and Reusability: By defining a separate Python function (get_bwa_map_input_fastqs) to retrieve the input files, you create a modular and reusable piece of code. This can be particularly useful if you have multiple rules that need to access the same input files, as you can reuse the function across those rules.\nAbstraction: The function abstracts the details of where the input files come from. In your first version, you had to specify the exact path to the input files in the rule itself. With the function, you only need to specify how to obtain the input files for a given sample, making your rule more abstract and easier to maintain.\nFlexibility: If you need to change the way you obtain input files in the future (for example, if you decide to store them in a different directory or use a different naming convention), you can update the function in one place, and all the rules that use it will automatically adapt.\nReduced Redundancy: Your original version of the rule had repetitive code for specifying input and output paths for each sample. With the function, you reduce redundancy and the potential for errors, as the input file retrieval logic is centralized.\nEasier Testing: You can more easily write unit tests for the input file retrieval function to ensure it works correctly, which can be challenging when input paths are hard-coded in multiple rules.\n\n\n\nIn the data/samples folder, there is an additional sample C.fastq. Add that sample to the config file and see how Snakemake wants to recompute the part of the workflow belonging to the new sample, when invoking with snakemake -n --forcerun bcftools_call\nNote: Snakemake does not automatically rerun jobs when new input files are added as in the excercise below. However, you can get a list of output files that are affected by such changes with snakemake –list-input-changes. To trigger a rerun, this bit of bash magic helps:\n\nsnakemake -n --forcerun $(snakemake --list-input-changes)\n\n\n\n\n\nSometimes shell commands are not only composed of input and output files but require additional parameters.\nWe can set additional parameters need to be set depending on the wildcard values of the job. For this, Snakemake allows to define arbitrary parameters for rules with the params directive. I.e. edit bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    threads: 8\n    shell:\n        \"bwa mem -R '{params.rg}' -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\n\nsnakemake -np --forceall\n\nNext, consider another important parameter, the prior mutation rate (1e-3 per default). It is set via the flag -P of the bcftools call command. Consider making this flag configurable via adding a new key to the config file and using the params directive in the rule bcftools_call to propagate it to the shell command.\nAdd this in the config file:\nbcftools:\n    evalue: 0.001\nChange this in the Snakemakefile:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}\"\n\n\nsnakemake -p --cores 1 --forceall\n\n\n\n\nWhen executing a large workflow, it is usually desirable to store the logging output of each job into a separate file, instead of just printing all logging output to the terminal—when multiple jobs are run in parallel, this would result in chaotic output. For this purpose, Snakemake allows to specify log files for rules. Log files are defined via the log directive and handled similarly to output files, but they are not subject of rule matching and are not cleaned up when a job fails. We modify our rule bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThe shell command is modified to collect STDERR output of both bwa and samtools and pipe it into the file referred to by {log}. Log files must contain exactly the same wildcards as the output files to avoid file name clashes between different jobs of the same rule.\nLet’s add a log to the to the bcftools_call rule as well.\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    log:\n        \"logs/bcftools/all.log\"\n    shell:\n        \"(bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}) 2&gt; {log}\"\n\n\nsnakemake -p --cores 1 --forceall\n\nThe ability to track the provenance of each generated result is an important step towards reproducible analyses. Apart from the report functionality discussed before, Snakemake can summarize various provenance information for all output files of the workflow. The flag –summary prints a table associating each output file with the rule used to generate it, the creation date and optionally the version of the tool used for creation is provided. Further, the table informs about updated input files and changes to the source code of the rule after creation of the output file.\n\nsnakemake -p --cores 1 --forceall --summary\n\n\n\n\nIn our workflow, we create two BAM files for each sample, namely the output of the rules bwa_map and samtools_sort. When not dealing with examples, the underlying data is usually huge. Hence, the resulting BAM files need a lot of disk space and their creation takes some time. To save disk space, you can mark output files as temporary. Snakemake will delete the marked files for you, once all the consuming jobs (that need it as input) have been executed. We use this mechanism for the output file of the rule bwa_map:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThis results in the deletion of the BAM file once the corresponding samtools_sort job has been executed. Since the creation of BAM files via read mapping and sorting is computationally expensive, it is reasonable to protect the final BAM file from accidental deletion or modification. We modify the rule samtools_sort to mark its output file as protected:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        protected(\"sorted_reads/{sample}.bam\")\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nRun Snakemake with the target mapped_reads/A.bam. Although the file is marked as temporary, you will see that Snakemake does not delete it because it is specified as a target file.\n\nsnakemake --forceall --cores 1 mapped_reads/A.bam"
  },
  {
    "objectID": "source/snakemake/tutorial.html#additional-features",
    "href": "source/snakemake/tutorial.html#additional-features",
    "title": "Snakemake tutorial",
    "section": "",
    "text": "For more, check out this documentation.\n\n\nIn order to re-use building blocks or simply to structure large workflows, it is sometimes reasonable to split a workflow into modules. For this, Snakemake provides the include directive to include another Snakefile into the current one, e.g.:\ninclude: \"path/to/other.snakefile\"\nAlternatively, Snakemake allows to define sub-workflows. A sub-workflow refers to a working directory with a complete Snakemake workflow. Output files of that sub-workflow can be used in the current Snakefile. When executing, Snakemake ensures that the output files of the sub-workflow are up-to-date before executing the current workflow. This mechanism is particularly useful when you want to extend a previous analysis without modifying it. For details about sub-workflows, see the documentation.\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nLets put the read mapping into a separate snakefile and use include to make it available in our workflow:\nIn rules add a new file bwa_mem.smk with the following content:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 2\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nRemove the bwa_mem rule from the workflow and add include: \"rules/bwa_mem.smk and run the workflow with :\n\nsnakemake --forceall -p --cores 1\n\n\n\n\nIn order to get a fully reproducible data analysis, it is not sufficient to be able to execute each step and document all used parameters. The used software tools and libraries have to be documented as well. In this tutorial, you have already seen how Conda can be used to specify an isolated software environment for a whole workflow. With Snakemake, you can go one step further and specify Conda environments per rule. This way, you can even make use of conflicting software versions (e.g. combine Python 2 with Python 3).\nIn our example, instead of using an external environment we can specify environments per rule, e.g.:\n\nrule samtools_index:\n  input:\n      \"sorted_reads/{sample}.bam\"\n  output:\n      \"sorted_reads/{sample}.bam.bai\"\n  conda:\n      \"envs/samtools.yaml\"\n  shell:\n      \"samtools index {input}\"\n\nwith envs/samtools.yaml defined as:\n\nchannels:\n  - bioconda\n  - conda-forge\ndependencies:\n  - samtools =1.9\n\nWhen executing the workflow with snakemake --use-conda --cores 1 it will automatically create required environments and activate them before a job is executed. It is best practice to specify at least the major and minor version of any packages in the environment definition.\nSpecifying environments per rule in this way has two advantages: - First, the workflow definition also documents all used software versions. - Second, a workflow can be re-executed (without admin rights) on a vanilla system, without installing any prerequisites apart from Snakemake and Miniconda.\n\n\n\nIn order to simplify the utilization of popular tools, Snakemake provides a repository of so-called wrappers (the Snakemake wrapper repository). A wrapper is a short script that wraps (typically) a command line application and makes it directly addressable from within Snakemake. For this, Snakemake provides the wrapper directive that can be used instead of shell, script, or run. For example, the rule bwa_map could alternatively look like this:\n\nrule bwa_mem:\n  input:\n      ref=\"data/genome.fa\",\n      sample=lambda wildcards: config[\"samples\"][wildcards.sample]\n  output:\n      temp(\"mapped_reads/{sample}.bam\")\n  log:\n      \"logs/bwa_mem/{sample}.log\"\n  params:\n      \"-R '@RG\\tID:{sample}\\tSM:{sample}'\"\n  threads: 8\n  wrapper:\n      \"0.15.3/bio/bwa/mem\"\n\n\n\n\nBy default, Snakemake executes jobs on the local machine it is invoked on. Alternatively, it can execute jobs in distributed environments, e.g., compute clusters or batch systems. If the nodes share a common file system, Snakemake supports three alternative execution modes.\nIn cluster environments, compute jobs are usually submitted as shell scripts via commands like qsub. Snakemake provides a generic mode to execute on such clusters. By invoking Snakemake with snakemake --cluster qsub --jobs 100 each job will be compiled into a shell script that is submitted with the given command (here qsub). The –jobs flag limits the number of concurrently submitted jobs to 100.\nFind out more for SLURM submissions.\n\n\n\nSometimes it is useful to constrain the values a wildcard can have. This can be achieved by adding a regular expression that describes the set of allowed wildcard values. For example, the wildcard sample in the output file \"sorted_reads/{sample}.bam\" can be constrained to only allow alphanumeric sample names as \"sorted_reads/{sample,[A-Za-z0-9]+}.bam\".\nConstraints may be defined per rule or globally using the wildcard_constraints keyword, as demonstrated in Wildcards. This mechanism helps to solve two kinds of ambiguity.\n\nIt can help to avoid ambiguous rules, i.e. two or more rules that can be applied to generate the same output file. Other ways of handling ambiguous rules are described in the Section Handling Ambiguous Rules.\nIt can help to guide the regular expression based matching so that wildcards are assigned to the right parts of a file name. Consider the output file {sample}.{group}.txt and assume that the target file is A.1.normal.txt. It is not clear whether dataset=“A.1” and group=“normal” or dataset=“A” and group=“1.normal” is the right assignment. Here, constraining the dataset wildcard by {sample,[A-Z]+}.{group}solves the problem.\n\nWhen dealing with ambiguous rules, it is best practice to first try to solve the ambiguity by using a proper file structure, for example, by separating the output files of different steps in different directories.\n\n\n\nA basic report contains runtime statistics, a visualization of the workflow topology, used software and data provenance information.\n\nsnakemake --report report.html --rerun-incomplete\n\nIn addition, you can mark any output file generated in your workflow for inclusion into the report. It will be encoded directly into the report, such that it can be, e.g., emailed as a self-contained document. The reader (e.g., a collaborator of yours) can at any time download the enclosed results from the report for further use, e.g., in a manuscript you write together.\nLet’s mark the output file “results/plots/quals.svg” for inclusion by replacing it with report(“results/plots/quals.svg”, caption=“report/calling.rst”) and adding a file report/calling.rst, containing some description of the output file. This description will be presented as caption in the resulting report."
  },
  {
    "objectID": "source/nanopore/porechop_readme.html",
    "href": "source/nanopore/porechop_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Porechop is a tool for finding and removing adapters from Oxford Nanopore reads. Adapters on the ends of reads are trimmed off, and when a read has an adapter in its middle, it is treated as chimeric and chopped into separate reads. Porechop performs thorough alignments to effectively find adapters, even at low sequence identity.\nNotice: From 2018 on, porechop is not actively maintained anymore. It runs perfectly fine, but that is something to keep in mind when running into bugs.\nAvailable on Crunchomics: Not by default\n\n\n\nIt is easiest to install porechop with conda:\n\nmamba create --name porechop -c conda-forge -c bioconda porechop=0.2.4\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAdaptor trimming very much depends on how the sequencing library was generated. Therefore, I recommend to carefully read through the How it works section of the softwares manual to know what to expect and look out for.\nSimilarly, porechop works with both demultiplexed and non-demultiplexed sequences. Also here, the manual explains in more detail how to perform barcode demultiplexing.\n\n\nRequired input:\n\nFASTA/FASTQ of input reads or a directory which will be recursively searched for FASTQ files (required and can be fasta,fastq,fasta.gz,fastq.gz)\n\nOutput:\n\nFASTA or FASTQ of trimmed reads\n\nExample code:\n\nporechop --input myfile.fastq.gz \\\n  --output outputfolder/myfile_filtered.fastq.gz \\\n  --threads 1 \\\n  --discard_middle\n\nUseful arguments (for the full version, check the manual):\n\n-b {BARCODE_DIR}, --barcode_dir {BARCODE_DIR}: Reads will be binned based on their barcode and saved to separate files in this directory (incompatible with –output)\n--barcode_threshold {BARCODE_THRESHOLD} A read must have at least this percent identity to a barcode to be binned (default: 75.0)\n--barcode_diff {BARCODE_DIFF} If the difference between a read’s best barcode identity and its second-best barcode identity is less than this value, it will not be put in a barcode bin (to exclude cases which are too close to call) (default: 5.0)\n--adapter_threshold {ADAPTER_THRESHOLD} An adapter set has to have at least this percent identity to be labelled as present and trimmed off (0 to 100) (default: 90.0)\n--check_reads{CHECK_READS} This many reads will be aligned to all possible adapters to determine which adapter sets are present (default: 10000)\n--no_split Skip splitting reads based on middle adapters (default: split reads when an adapter is found in the middle)\n--discard_middle Reads with middle adapters will be discarded (default: reads with middle adapters are split) (required for reads to be used with Nanopolish, this option is on by default when outputting reads into barcode bins)",
    "crumbs": [
      "Omics Analyses",
      "Nanopore analyses",
      "Porechop"
    ]
  },
  {
    "objectID": "source/nanopore/porechop_readme.html#porechop",
    "href": "source/nanopore/porechop_readme.html#porechop",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Porechop is a tool for finding and removing adapters from Oxford Nanopore reads. Adapters on the ends of reads are trimmed off, and when a read has an adapter in its middle, it is treated as chimeric and chopped into separate reads. Porechop performs thorough alignments to effectively find adapters, even at low sequence identity.\nNotice: From 2018 on, porechop is not actively maintained anymore. It runs perfectly fine, but that is something to keep in mind when running into bugs.\nAvailable on Crunchomics: Not by default\n\n\n\nIt is easiest to install porechop with conda:\n\nmamba create --name porechop -c conda-forge -c bioconda porechop=0.2.4\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAdaptor trimming very much depends on how the sequencing library was generated. Therefore, I recommend to carefully read through the How it works section of the softwares manual to know what to expect and look out for.\nSimilarly, porechop works with both demultiplexed and non-demultiplexed sequences. Also here, the manual explains in more detail how to perform barcode demultiplexing.\n\n\nRequired input:\n\nFASTA/FASTQ of input reads or a directory which will be recursively searched for FASTQ files (required and can be fasta,fastq,fasta.gz,fastq.gz)\n\nOutput:\n\nFASTA or FASTQ of trimmed reads\n\nExample code:\n\nporechop --input myfile.fastq.gz \\\n  --output outputfolder/myfile_filtered.fastq.gz \\\n  --threads 1 \\\n  --discard_middle\n\nUseful arguments (for the full version, check the manual):\n\n-b {BARCODE_DIR}, --barcode_dir {BARCODE_DIR}: Reads will be binned based on their barcode and saved to separate files in this directory (incompatible with –output)\n--barcode_threshold {BARCODE_THRESHOLD} A read must have at least this percent identity to a barcode to be binned (default: 75.0)\n--barcode_diff {BARCODE_DIFF} If the difference between a read’s best barcode identity and its second-best barcode identity is less than this value, it will not be put in a barcode bin (to exclude cases which are too close to call) (default: 5.0)\n--adapter_threshold {ADAPTER_THRESHOLD} An adapter set has to have at least this percent identity to be labelled as present and trimmed off (0 to 100) (default: 90.0)\n--check_reads{CHECK_READS} This many reads will be aligned to all possible adapters to determine which adapter sets are present (default: 10000)\n--no_split Skip splitting reads based on middle adapters (default: split reads when an adapter is found in the middle)\n--discard_middle Reads with middle adapters will be discarded (default: reads with middle adapters are split) (required for reads to be used with Nanopolish, this option is on by default when outputting reads into barcode bins)",
    "crumbs": [
      "Omics Analyses",
      "Nanopore analyses",
      "Porechop"
    ]
  },
  {
    "objectID": "source/nanopore/nanoplot_readme.html",
    "href": "source/nanopore/nanoplot_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoPlot is a plotting tool for long read sequencing data and alignment (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\n\nNanoPlot is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools \n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\n\nPossible input formats :\n\nfastq files (can be bgzip, bzip2 or gzip compressed)\nfastq files generated by albacore, guppy or MinKNOW containing additional information (can be bgzip, bzip2 or gzip compressed)\n\nsorted bam files\n\nsequencing_summary.txt output table generated by albacore, guppy or MinKnow basecalling (can be gzip, bz2, zip and xz compressed)\nfasta files (can be bgzip, bzip2 or gzip compressed); Multiple files of the same type can be offered simultaneously\n\nOutput:\n\na statistical summary\na number of plots\na html summary file\n\nExample code:\n\n#start environment\nconda activate nanopack\n\n#run on a single file (if you are using other inputs, check the readme for the appropriate flag)\nNanoPlot --fastq myfile.fastq.gz -o outputfolder --threads 1\n\nUseful arguments (for the full version, check the manual):\n\n--tsv_stats Output the stats file as a properly formatted TSV.\n--info_in_report Add NanoPlot run info in the report.\n--barcoded Use if you want to split the summary file by barcode\n-f, --format {[{png,jpg,jpeg,webp,svg,pdf,eps,json} …] } Specify the output format of the plots, which are in addition to the html files",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "Nanoplot"
    ]
  },
  {
    "objectID": "source/nanopore/nanoplot_readme.html#nanoplot",
    "href": "source/nanopore/nanoplot_readme.html#nanoplot",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoPlot is a plotting tool for long read sequencing data and alignment (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\n\nNanoPlot is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools \n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\n\nPossible input formats :\n\nfastq files (can be bgzip, bzip2 or gzip compressed)\nfastq files generated by albacore, guppy or MinKNOW containing additional information (can be bgzip, bzip2 or gzip compressed)\n\nsorted bam files\n\nsequencing_summary.txt output table generated by albacore, guppy or MinKnow basecalling (can be gzip, bz2, zip and xz compressed)\nfasta files (can be bgzip, bzip2 or gzip compressed); Multiple files of the same type can be offered simultaneously\n\nOutput:\n\na statistical summary\na number of plots\na html summary file\n\nExample code:\n\n#start environment\nconda activate nanopack\n\n#run on a single file (if you are using other inputs, check the readme for the appropriate flag)\nNanoPlot --fastq myfile.fastq.gz -o outputfolder --threads 1\n\nUseful arguments (for the full version, check the manual):\n\n--tsv_stats Output the stats file as a properly formatted TSV.\n--info_in_report Add NanoPlot run info in the report.\n--barcoded Use if you want to split the summary file by barcode\n-f, --format {[{png,jpg,jpeg,webp,svg,pdf,eps,json} …] } Specify the output format of the plots, which are in addition to the html files",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "Nanoplot"
    ]
  },
  {
    "objectID": "source/nanopore/nanoITS.html",
    "href": "source/nanopore/nanoITS.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoITS is a classifier for long-read Oxford Nanopore data of the eukaryotic 18S/SSU-ITS1 operon.\nWhen giving the tool some nanopore long-read data it will:\n\nProvide a quality report of the raw reads\nCheck the reads for adaptors and barcodes and if present trim the reads\nRemove low-quality and short reads\nProvide a quality report for the cleaned reads\nIdentify and separate both the ITS1 and 18S rRNA gene\nClassify the SSU and/or ITS1 gene using kraken2 and/or minimap2\nGenerate taxonomic barplots and OTU tables\n\nFor a more detailed explanation, check out the manual.\nBelow you can find the full workflow:\n\n\n\n\nTo run NanoITs, install conda and snakemake and clone the directory from github via:\n\ngit clone https://github.com/ndombrowski/NanoITS.git\n\nProvide your sample names and path to the samples as a comma-separated file, for example, a file looking similar as the one provided in example_files/mapping.csv. Sample names should be unique and consist of letter, numbers and - only. The barcode column can be left empty as it is not yet implemented. The path should contain the path to your demultiplexed, compressed fastq file.\nAdjust config/config.yaml to configure the location of your mapping file as well as specificy the parameters used by NanoITs.\nNanoITs can then be run with:\n\nsnakemake --use-conda --cores &lt;nr_cores&gt; \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile \\\n  --configfile config/config.yaml \\\n  --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  \\\n  --rerun-incomplete --nolock -np \n\nFor a more detailed explanation, check out the manual.",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "NanoITS"
    ]
  },
  {
    "objectID": "source/nanopore/nanoITS.html#nanoits",
    "href": "source/nanopore/nanoITS.html#nanoits",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoITS is a classifier for long-read Oxford Nanopore data of the eukaryotic 18S/SSU-ITS1 operon.\nWhen giving the tool some nanopore long-read data it will:\n\nProvide a quality report of the raw reads\nCheck the reads for adaptors and barcodes and if present trim the reads\nRemove low-quality and short reads\nProvide a quality report for the cleaned reads\nIdentify and separate both the ITS1 and 18S rRNA gene\nClassify the SSU and/or ITS1 gene using kraken2 and/or minimap2\nGenerate taxonomic barplots and OTU tables\n\nFor a more detailed explanation, check out the manual.\nBelow you can find the full workflow:\n\n\n\n\nTo run NanoITs, install conda and snakemake and clone the directory from github via:\n\ngit clone https://github.com/ndombrowski/NanoITS.git\n\nProvide your sample names and path to the samples as a comma-separated file, for example, a file looking similar as the one provided in example_files/mapping.csv. Sample names should be unique and consist of letter, numbers and - only. The barcode column can be left empty as it is not yet implemented. The path should contain the path to your demultiplexed, compressed fastq file.\nAdjust config/config.yaml to configure the location of your mapping file as well as specificy the parameters used by NanoITs.\nNanoITs can then be run with:\n\nsnakemake --use-conda --cores &lt;nr_cores&gt; \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile \\\n  --configfile config/config.yaml \\\n  --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  \\\n  --rerun-incomplete --nolock -np \n\nFor a more detailed explanation, check out the manual.",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "NanoITS"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/trinity.html",
    "href": "source/metatranscriptomics/trinity.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Trinity is a tool to assemble transcript sequences from Illumina RNA-Seq data de novo (without a reference genome). Additionally, it comes with several scripts that can be used to compare replicates, identify differentially expressed genes or functional annotation.\n\nManual\nPaper: Please do not forget to cite the paper whenever you use the software (Grabherr et al. 2011).\n\nAvailable on Crunchomics: No\n\n\n\nTrinity can be easily installed with conda/mamba:\n\nmamba create -n trinity\nmamba install -n trinity -c bioconda trinity \n\n\n\n\nTrinity can not only be used to assemble reads but also do many down-stream analyses. Since its not the scope of this page to give an in-depth overview about these functionalities, we recommend that an in-depth look at the manual.\n\nRequired input:\n\nPaired-reads (fa or fq)\nSingle-reads (fa or fq)\nNotice: Trinity performs best with strand-specific data, in which case sense and antisense transcripts can be resolved\n\nGenerated outputs:\n\nTrinity.fasta: An assembled transcriptome\n\n\nRecommendations:\n\nA basic recommendation is to have 1G of RAM per 1M pairs of Illumina reads in order to run some of the steps in the workflow\nThe entire process can require ~1 hour per million pairs of reads\nMost (not all) parts of Trinity are parallelized. It therefore, makes sense to use most available CPUs and also due to high mem requirement to run such a job on a complete node\n\nPossible settings to adjust when dealing with deeply sequenced data:\n\n--min_kmer_cov 2 (singleton K-mers will not be included in initial Inchworm contigs)\nPerform K-mer based insilico read set normalization (–-normalize_max_read_cov 50), this is adjusted compared to the default of 200 , see figure 4 for some benchmarking. As you see applying this coverage thresholds can easily result in ~70% of read reduction. Notice, that some genes may be missed when reads are removed\n--JM allows the user to control the amount of RAM used during Jellyfish kmer counting\n\n\n\n\nTrinity can do a lot of things and the goal of this page is not to go through everyone of them, for this, visit the manual. However, below you find the key commands to generate the assembly, get some quality assessment scores and identify differentially expressed genes\n\n\nIn a first example, we work with a single sample and have paired-end reads.\n\nTrinity --seqType fq --max_memory 50G \\\n         --left sample1_f.fq.gz  --right sample1_r.fq.gz --CPU 6\n\nMore often you however will work with multiple samples, for example you might work with 6 samples: 3 replicates for control conditions and three replicates for sulfur-treatment. Trinity can work with multiple samples by using a text file that gives the tool all relevant information. For example in our case samples.txt provides the condition, the replicates, the location of the forward reads and the location of the reverse reads in a tab-delimited file:\nC   C_rep1  sortmerna/C_rep1/other_fwd.fq.gz    sortmerna/C_rep1/other_rev.fq.gz\nC   C_rep2  sortmerna/C_rep2/other_fwd.fq.gz    sortmerna/C_rep2/other_rev.fq.gz\nC   C_rep3  sortmerna/C_rep3/other_fwd.fq.gz    sortmerna/C_rep3/other_rev.fq.gz\nS   S_rep1  sortmerna/S_rep3/other_fwd.fq.gz    sortmerna/S_rep1/other_rev.fq.gz\nS   S_rep2  sortmerna/S-3C/out/other_fwd.fq.gz  sortmerna/S_rep1//other_rev.fq.gz\nS   S_rep3  sortmerna/S-5C/out/other_fwd.fq.gz  sortmerna/S_rep3/other_rev.fq.gz\nOnce we have this file, we can run the assembly as follows:\n\n#make folders for better file organization\nmkdir trinity_output \n\n#generate an assembly\n#using min_kmer_cov and normalize_max_read_cov can be useful for large assemblies\nTrinity \\\n  --seqType fq \\\n  --samples_file samples.txt \\\n  --max_memory 50G \\\n  --CPU 6 \\\n  --output trinity_output\n\n\n\n\nTo assess the quality, we can use a script that comes with Trinity as follows:\n\nTrinityStats.pl \\\n   trinity_output/Trinity.fasta \\\n  &gt; trinity_output/trinity_stats.txt\n\nThe output of trinity_output/trinity_stats.txt might look something like this:\n- Total trinity 'genes':  931,787\n- Total trinity transcripts:      1,130,657\n- Percent GC: 48.25\n\n- Stats based on ALL transcript contigs::\n  - Contig N10: 3008\n  - Contig N20: 1492\n  - Contig N30: 1065\n  - Contig N40: 799\n  - Contig N50: 607 ***might be a bit short\n  - Median contig length: 302\n  - Average contig: 491.91\n  - Total assembled bases: 556178948\n\n- Stats based on ONLY LONGEST ISOFORM per 'GENE:\n  - Contig N10: 1480\n  - Contig N20: 1000\n  - Contig N30: 738\n  - Contig N40: 561\n  - Contig N50: 434\n  - Median contig length: 276\n  - Average contig: 406.06\n  - Total assembled bases: 378358462\nIf the N50 statistics falls in the right area expected for a gene (about 1000-1,500), then N50 can be used as a rough check on overall “sanity” of the transcriptome assembly.\n\n\n\nWe will next use some code to ask how many of our reads can be mapped back to our transcriptome. The results can be used to construct an expression matrix as well as estimate how many reads map to our assembly.\nThere are three options we can use to align our reads to the de novo assembly: bowtie, kallisto and salmon. Due to its speed we will use salmon here but feel free to have a look at the other methods as well by checking out the manual.\nThe command below will print some useful information to the screen. In order to capture this in a file instead we redirect this output by using &&gt; logs/salmon.info.\n\n#get a folder for better organization \nmkdir logs\n\n#run the pseudo-alignment with salmon\nalign_and_estimate_abundance.pl \\\n  --transcripts trinity_output/Trinity.fasta \\\n  --seqType fq \\\n  --thread_count 10 \\\n  --samples_file samples.txt \\\n  --est_method salmon \\\n  --trinity_mode --prep_reference \\\n  --output_dir trinity_output/salmon &&gt; logs/salmon.info\n\nIf you check logs/salmon.info you will see how many of our reads mapped back to the transcriptome. A typical “good” assembly has ~80% reads mapping to the assembly and ~80% are properly paired.\n\n\n\nNext, we use the results from aligning our reads to the transcriptome to build an expression matrix.\nNotice, that we talk about isoforms when we talk about transcripts and genes when talking about genes. Depending on what you want to look at you can most of the following steps on either the transcripts or genes. The examples below will only be looking at the transcripts.\n\n#make files containing a list of all the target files\nls trinity_output/salmon/*/quant.sf &gt; isoform-file-paths.txt\nls trinity_output/salmon/*/quant.sf.genes &gt; gene-file-paths.txt\n\n#make matrices for transcripts\nabundance_estimates_to_matrix.pl \\\n            --est_method salmon \\\n            --out_prefix trinity_output/salmon \\\n            --gene_trans_map trinity_output/trinity.Trinity.fasta.gene_trans_map \\\n            --name_sample_by_basedir \\\n            --quant_files isoform-file-paths.txt\n\nThe abundance_estimates_to_matrix script generates the following files:\n\nsalmon.[gene|isoform].counts.matrix: the estimated RNA-Seq fragment counts (raw counts). This file is used for downstream analyses of differential expression.\nsalmon.[gene|isoform].TMM.EXPR.matrix: a matrix of TPM expression values (not cross-sample normalized). This file is used as the gene expression matrix in most other analyses\nsalmon.[gene|isoform].TPM.not_cross_norm : a matrix of TMM-normalized expression values\n\n\n\n\nThe PtR script can be used to check the variation across samples and replicates. This is useful to do to ensure that your replicates are more similar to each other compared to any treatments.\n\n\n\n\n\n\nWarning\n\n\n\nThe ptr script might run into an error “‘length = 2’ in coercion to ‘logical(1)’”, this has to do with an incompatibility with a script with a newer R version, if that happens, you need to make the following changes in one of the script. The script will be found in the location trinity will be installed with and should be something like \\$HOME/personal/mambaforge/envs/metatranscriptomics/opt/trinity-2.15.1/Analysis/DifferentialExpression/R/heatmap.3.R. You can edit this scripts according to these instructions.\nAlternatively, you can install R v4.2.2 in metatranscriptomic environment (not tested myself).\n\n\nLet’s first see how to compare replicates:\n\nPtR --matrix trinity_output/salmon.gene.counts.matrix \\\n    --samples &lt;(awk '{print $1\"\\t\"$2}' samples.txt) \\\n    --log2 --CPM \\\n    --min_rowSums 10 \\\n    --compare_replicates\n\n#view the files\n#since the files are large they likely will not open, so best transfer to your own computer\ndisplay N.rep_compare.pdf\ndisplay S.rep_compare.pdf\n\nWe can also compare replicates across all samples. Run PtR to generate a correlation matrix for all sample replicates like so:\n\nPtR \\\n  --matrix trinity_output/salmon.isoform.counts.matrix \\\n  --min_rowSums 10 \\\n  -s &lt;(awk '{print $1\"\\t\"$2}' samples.txt) \\\n  --log2 --CPM --sample_cor_matrix \n\nAs before you can use display to view the files. Ideally, we want to see that replicates are more highly correlated within samples than between samples.\nAnother important analysis method to explore relationships among the sample replicates is Principal Component Analysis (PCA). You can generate a PCA plot showing the first 3 principal components like so:\n\nPtR \\\n    --matrix trinity_output/salmon.isoform.counts.matrix \\\n    -s &lt;(awk '{print $1\"\\t\"$2}' samples.txt) \\\n    --min_rowSums 10 --log2 \\\n    --CPM --center_rows \\\n    --prin_comp 3 \n\nTo keep things organized, lets move these pdfs into a new folder:\n\nmkdir trinity_output/plots\nmv *pdf trinity_output/plots\nrm salmon*\n\n\n\n\nThe script below will perform pairwise comparisons among each of your sample types. To analyze transcripts, use the ‘transcripts.counts.matrix’ (or isoform.counts.matrix in later software versions) file. To perform an analysis at the ‘gene’ level, use the ‘genes.counts.matrix’.\nTrinity comes with three methods for this analysis: edgeR, DESeq2 and voom. View the manual for more information about each of these approaches.\n\n#look at differential expression  for transcripts\nrun_DE_analysis.pl \\\n        --matrix trinity_output/salmon.isoform.counts.matrix \\\n        --method edgeR \\\n        --samples_file samples.txt \\\n        --output trinity_output/edgeR-transcript\n\n#view vulcano plots\ndisplay trinity_output/edgeR-transcript/salmon.gene.counts.matrix.N_vs_S.edgeR.DE_results.MA_n_Volcano.pdf\n\ndisplay trinity_output/edgeR-transcript/salmon.isoform.counts.matrix.N_vs_S.edgeR.DE_results.MA_n_Volcano.pdf\n\nThis will generate some information including differentially expressed genes. The output will look something like this:\n\n${prefix}.sampleA_vs_sampleB.${method}.DE_results : the DE analysis results,\nincluding log fold change and statistical significance (see FDR column).\n${prefix}.sampleA_vs_sampleB.${method}.MA_n_Volcano.pdf : MA and Volcano plots\nfeatures found DE at FDR &lt;0.05 will be colored red. Plots are shown\nwith large (top) or small (bottom) points only for choice of aesthetics.\n${prefix}.sampleA_vs_sampleB.${method}.Rscript : the R-script executed to perform the DE analysis.\n\nNow that we did this, we can compare our samples and identify differentially expressed features as follows:\n\n#go into the edge R output folder \ncd trinity_output/edgeR-transcript\n\n#extract differentially expressed genes\nanalyze_diff_expr.pl \\\n    --matrix ../salmon.gene.counts.matrix  \\\n    --samples ../../samples.txt \\\n    -P 0.001 \\\n    -C 2\n\n#view results \ndisplay diffExpr.P0.001_C2.matrix.log2.centered.sample_cor_matrix.pdf\ndisplay diffExpr.P0.001_C2.matrix.log2.centered.genes_vs_samples_heatmap.pdf\n\nOptions:\n\n-P &lt;float&gt;: p-value cutoff for FDR (default: 0.001)\n-C &lt;float&gt;: min abs(log2(a/b)) fold change (default: 2 (meaning 2^(2) or 4-fold)).\n\nBy default, each pairwise sample comparison will be performed. If you want to restrict the pairwise comparisons, provide the list of the comparisons to perform to the --contrasts parameter.\nIn this output directory, you’ll find the following files for each of the pairwise comparisons performed:\n\n${prefix}.sampleA_vs_sampleB.voom.DE_results.P0.001_C2.sampleA-UP.subset : the expression matrix subset\nfor features up-regulated in sampleA\n${prefix}.sampleA_vs_sampleB.voom.DE_results.P0.001_C2.sampleB-UP.subset : the expression matrix subset\nfor features up-regulated in sampleB\ndiffExpr.P0.001_C2.matrix.log2.dat : All features found DE in any of these pairwise comparisons\nconsolidated into a single expression matrix:\ndiffExpr.P0.001_C2.matrix.log2.sample_cor.dat : A Pearson correlation matrix for pairwise sample comparisons\nbased on this set of DE features.\ndiffExpr.P0.001_C2.matrix.log2.sample_cor_matrix.pdf : clustered heatmap showing the above sample correlation matrix.\ndiffExpr.P0.001_C2.matrix.log2.centered.genes_vs_samples_heatmap.pdf : clustered heatmap of DE genes vs. sample replicates.",
    "crumbs": [
      "Welcome page",
      "Metatranscriptomics",
      "Trinity"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/trinity.html#trinity",
    "href": "source/metatranscriptomics/trinity.html#trinity",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Trinity is a tool to assemble transcript sequences from Illumina RNA-Seq data de novo (without a reference genome). Additionally, it comes with several scripts that can be used to compare replicates, identify differentially expressed genes or functional annotation.\n\nManual\nPaper: Please do not forget to cite the paper whenever you use the software (Grabherr et al. 2011).\n\nAvailable on Crunchomics: No\n\n\n\nTrinity can be easily installed with conda/mamba:\n\nmamba create -n trinity\nmamba install -n trinity -c bioconda trinity \n\n\n\n\nTrinity can not only be used to assemble reads but also do many down-stream analyses. Since its not the scope of this page to give an in-depth overview about these functionalities, we recommend that an in-depth look at the manual.\n\nRequired input:\n\nPaired-reads (fa or fq)\nSingle-reads (fa or fq)\nNotice: Trinity performs best with strand-specific data, in which case sense and antisense transcripts can be resolved\n\nGenerated outputs:\n\nTrinity.fasta: An assembled transcriptome\n\n\nRecommendations:\n\nA basic recommendation is to have 1G of RAM per 1M pairs of Illumina reads in order to run some of the steps in the workflow\nThe entire process can require ~1 hour per million pairs of reads\nMost (not all) parts of Trinity are parallelized. It therefore, makes sense to use most available CPUs and also due to high mem requirement to run such a job on a complete node\n\nPossible settings to adjust when dealing with deeply sequenced data:\n\n--min_kmer_cov 2 (singleton K-mers will not be included in initial Inchworm contigs)\nPerform K-mer based insilico read set normalization (–-normalize_max_read_cov 50), this is adjusted compared to the default of 200 , see figure 4 for some benchmarking. As you see applying this coverage thresholds can easily result in ~70% of read reduction. Notice, that some genes may be missed when reads are removed\n--JM allows the user to control the amount of RAM used during Jellyfish kmer counting\n\n\n\n\nTrinity can do a lot of things and the goal of this page is not to go through everyone of them, for this, visit the manual. However, below you find the key commands to generate the assembly, get some quality assessment scores and identify differentially expressed genes\n\n\nIn a first example, we work with a single sample and have paired-end reads.\n\nTrinity --seqType fq --max_memory 50G \\\n         --left sample1_f.fq.gz  --right sample1_r.fq.gz --CPU 6\n\nMore often you however will work with multiple samples, for example you might work with 6 samples: 3 replicates for control conditions and three replicates for sulfur-treatment. Trinity can work with multiple samples by using a text file that gives the tool all relevant information. For example in our case samples.txt provides the condition, the replicates, the location of the forward reads and the location of the reverse reads in a tab-delimited file:\nC   C_rep1  sortmerna/C_rep1/other_fwd.fq.gz    sortmerna/C_rep1/other_rev.fq.gz\nC   C_rep2  sortmerna/C_rep2/other_fwd.fq.gz    sortmerna/C_rep2/other_rev.fq.gz\nC   C_rep3  sortmerna/C_rep3/other_fwd.fq.gz    sortmerna/C_rep3/other_rev.fq.gz\nS   S_rep1  sortmerna/S_rep3/other_fwd.fq.gz    sortmerna/S_rep1/other_rev.fq.gz\nS   S_rep2  sortmerna/S-3C/out/other_fwd.fq.gz  sortmerna/S_rep1//other_rev.fq.gz\nS   S_rep3  sortmerna/S-5C/out/other_fwd.fq.gz  sortmerna/S_rep3/other_rev.fq.gz\nOnce we have this file, we can run the assembly as follows:\n\n#make folders for better file organization\nmkdir trinity_output \n\n#generate an assembly\n#using min_kmer_cov and normalize_max_read_cov can be useful for large assemblies\nTrinity \\\n  --seqType fq \\\n  --samples_file samples.txt \\\n  --max_memory 50G \\\n  --CPU 6 \\\n  --output trinity_output\n\n\n\n\nTo assess the quality, we can use a script that comes with Trinity as follows:\n\nTrinityStats.pl \\\n   trinity_output/Trinity.fasta \\\n  &gt; trinity_output/trinity_stats.txt\n\nThe output of trinity_output/trinity_stats.txt might look something like this:\n- Total trinity 'genes':  931,787\n- Total trinity transcripts:      1,130,657\n- Percent GC: 48.25\n\n- Stats based on ALL transcript contigs::\n  - Contig N10: 3008\n  - Contig N20: 1492\n  - Contig N30: 1065\n  - Contig N40: 799\n  - Contig N50: 607 ***might be a bit short\n  - Median contig length: 302\n  - Average contig: 491.91\n  - Total assembled bases: 556178948\n\n- Stats based on ONLY LONGEST ISOFORM per 'GENE:\n  - Contig N10: 1480\n  - Contig N20: 1000\n  - Contig N30: 738\n  - Contig N40: 561\n  - Contig N50: 434\n  - Median contig length: 276\n  - Average contig: 406.06\n  - Total assembled bases: 378358462\nIf the N50 statistics falls in the right area expected for a gene (about 1000-1,500), then N50 can be used as a rough check on overall “sanity” of the transcriptome assembly.\n\n\n\nWe will next use some code to ask how many of our reads can be mapped back to our transcriptome. The results can be used to construct an expression matrix as well as estimate how many reads map to our assembly.\nThere are three options we can use to align our reads to the de novo assembly: bowtie, kallisto and salmon. Due to its speed we will use salmon here but feel free to have a look at the other methods as well by checking out the manual.\nThe command below will print some useful information to the screen. In order to capture this in a file instead we redirect this output by using &&gt; logs/salmon.info.\n\n#get a folder for better organization \nmkdir logs\n\n#run the pseudo-alignment with salmon\nalign_and_estimate_abundance.pl \\\n  --transcripts trinity_output/Trinity.fasta \\\n  --seqType fq \\\n  --thread_count 10 \\\n  --samples_file samples.txt \\\n  --est_method salmon \\\n  --trinity_mode --prep_reference \\\n  --output_dir trinity_output/salmon &&gt; logs/salmon.info\n\nIf you check logs/salmon.info you will see how many of our reads mapped back to the transcriptome. A typical “good” assembly has ~80% reads mapping to the assembly and ~80% are properly paired.\n\n\n\nNext, we use the results from aligning our reads to the transcriptome to build an expression matrix.\nNotice, that we talk about isoforms when we talk about transcripts and genes when talking about genes. Depending on what you want to look at you can most of the following steps on either the transcripts or genes. The examples below will only be looking at the transcripts.\n\n#make files containing a list of all the target files\nls trinity_output/salmon/*/quant.sf &gt; isoform-file-paths.txt\nls trinity_output/salmon/*/quant.sf.genes &gt; gene-file-paths.txt\n\n#make matrices for transcripts\nabundance_estimates_to_matrix.pl \\\n            --est_method salmon \\\n            --out_prefix trinity_output/salmon \\\n            --gene_trans_map trinity_output/trinity.Trinity.fasta.gene_trans_map \\\n            --name_sample_by_basedir \\\n            --quant_files isoform-file-paths.txt\n\nThe abundance_estimates_to_matrix script generates the following files:\n\nsalmon.[gene|isoform].counts.matrix: the estimated RNA-Seq fragment counts (raw counts). This file is used for downstream analyses of differential expression.\nsalmon.[gene|isoform].TMM.EXPR.matrix: a matrix of TPM expression values (not cross-sample normalized). This file is used as the gene expression matrix in most other analyses\nsalmon.[gene|isoform].TPM.not_cross_norm : a matrix of TMM-normalized expression values\n\n\n\n\nThe PtR script can be used to check the variation across samples and replicates. This is useful to do to ensure that your replicates are more similar to each other compared to any treatments.\n\n\n\n\n\n\nWarning\n\n\n\nThe ptr script might run into an error “‘length = 2’ in coercion to ‘logical(1)’”, this has to do with an incompatibility with a script with a newer R version, if that happens, you need to make the following changes in one of the script. The script will be found in the location trinity will be installed with and should be something like \\$HOME/personal/mambaforge/envs/metatranscriptomics/opt/trinity-2.15.1/Analysis/DifferentialExpression/R/heatmap.3.R. You can edit this scripts according to these instructions.\nAlternatively, you can install R v4.2.2 in metatranscriptomic environment (not tested myself).\n\n\nLet’s first see how to compare replicates:\n\nPtR --matrix trinity_output/salmon.gene.counts.matrix \\\n    --samples &lt;(awk '{print $1\"\\t\"$2}' samples.txt) \\\n    --log2 --CPM \\\n    --min_rowSums 10 \\\n    --compare_replicates\n\n#view the files\n#since the files are large they likely will not open, so best transfer to your own computer\ndisplay N.rep_compare.pdf\ndisplay S.rep_compare.pdf\n\nWe can also compare replicates across all samples. Run PtR to generate a correlation matrix for all sample replicates like so:\n\nPtR \\\n  --matrix trinity_output/salmon.isoform.counts.matrix \\\n  --min_rowSums 10 \\\n  -s &lt;(awk '{print $1\"\\t\"$2}' samples.txt) \\\n  --log2 --CPM --sample_cor_matrix \n\nAs before you can use display to view the files. Ideally, we want to see that replicates are more highly correlated within samples than between samples.\nAnother important analysis method to explore relationships among the sample replicates is Principal Component Analysis (PCA). You can generate a PCA plot showing the first 3 principal components like so:\n\nPtR \\\n    --matrix trinity_output/salmon.isoform.counts.matrix \\\n    -s &lt;(awk '{print $1\"\\t\"$2}' samples.txt) \\\n    --min_rowSums 10 --log2 \\\n    --CPM --center_rows \\\n    --prin_comp 3 \n\nTo keep things organized, lets move these pdfs into a new folder:\n\nmkdir trinity_output/plots\nmv *pdf trinity_output/plots\nrm salmon*\n\n\n\n\nThe script below will perform pairwise comparisons among each of your sample types. To analyze transcripts, use the ‘transcripts.counts.matrix’ (or isoform.counts.matrix in later software versions) file. To perform an analysis at the ‘gene’ level, use the ‘genes.counts.matrix’.\nTrinity comes with three methods for this analysis: edgeR, DESeq2 and voom. View the manual for more information about each of these approaches.\n\n#look at differential expression  for transcripts\nrun_DE_analysis.pl \\\n        --matrix trinity_output/salmon.isoform.counts.matrix \\\n        --method edgeR \\\n        --samples_file samples.txt \\\n        --output trinity_output/edgeR-transcript\n\n#view vulcano plots\ndisplay trinity_output/edgeR-transcript/salmon.gene.counts.matrix.N_vs_S.edgeR.DE_results.MA_n_Volcano.pdf\n\ndisplay trinity_output/edgeR-transcript/salmon.isoform.counts.matrix.N_vs_S.edgeR.DE_results.MA_n_Volcano.pdf\n\nThis will generate some information including differentially expressed genes. The output will look something like this:\n\n${prefix}.sampleA_vs_sampleB.${method}.DE_results : the DE analysis results,\nincluding log fold change and statistical significance (see FDR column).\n${prefix}.sampleA_vs_sampleB.${method}.MA_n_Volcano.pdf : MA and Volcano plots\nfeatures found DE at FDR &lt;0.05 will be colored red. Plots are shown\nwith large (top) or small (bottom) points only for choice of aesthetics.\n${prefix}.sampleA_vs_sampleB.${method}.Rscript : the R-script executed to perform the DE analysis.\n\nNow that we did this, we can compare our samples and identify differentially expressed features as follows:\n\n#go into the edge R output folder \ncd trinity_output/edgeR-transcript\n\n#extract differentially expressed genes\nanalyze_diff_expr.pl \\\n    --matrix ../salmon.gene.counts.matrix  \\\n    --samples ../../samples.txt \\\n    -P 0.001 \\\n    -C 2\n\n#view results \ndisplay diffExpr.P0.001_C2.matrix.log2.centered.sample_cor_matrix.pdf\ndisplay diffExpr.P0.001_C2.matrix.log2.centered.genes_vs_samples_heatmap.pdf\n\nOptions:\n\n-P &lt;float&gt;: p-value cutoff for FDR (default: 0.001)\n-C &lt;float&gt;: min abs(log2(a/b)) fold change (default: 2 (meaning 2^(2) or 4-fold)).\n\nBy default, each pairwise sample comparison will be performed. If you want to restrict the pairwise comparisons, provide the list of the comparisons to perform to the --contrasts parameter.\nIn this output directory, you’ll find the following files for each of the pairwise comparisons performed:\n\n${prefix}.sampleA_vs_sampleB.voom.DE_results.P0.001_C2.sampleA-UP.subset : the expression matrix subset\nfor features up-regulated in sampleA\n${prefix}.sampleA_vs_sampleB.voom.DE_results.P0.001_C2.sampleB-UP.subset : the expression matrix subset\nfor features up-regulated in sampleB\ndiffExpr.P0.001_C2.matrix.log2.dat : All features found DE in any of these pairwise comparisons\nconsolidated into a single expression matrix:\ndiffExpr.P0.001_C2.matrix.log2.sample_cor.dat : A Pearson correlation matrix for pairwise sample comparisons\nbased on this set of DE features.\ndiffExpr.P0.001_C2.matrix.log2.sample_cor_matrix.pdf : clustered heatmap showing the above sample correlation matrix.\ndiffExpr.P0.001_C2.matrix.log2.centered.genes_vs_samples_heatmap.pdf : clustered heatmap of DE genes vs. sample replicates.",
    "crumbs": [
      "Welcome page",
      "Metatranscriptomics",
      "Trinity"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/fastp.html",
    "href": "source/metatranscriptomics/fastp.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "FastP is a tool designed to provide fast all-in-one preprocessing for FastQ files.\n\nManual\nPaper. Please do not forget to cite the paper whenever you use the software (Chen 2023).\n\nAvailable on Crunchomics: Yes, v0.23.2\n\n\n\nFastP can be easily installed with conda/mamba\n\nmamba create -n fastp\nmamba install -n fastp -c bioconda fastp\n\n\n\n\nFastP has many different options, so its best to also have a look at the manual. Below you find a quick example to try out.\n\nRequired input: Single-end or paired-end FastQ files (can be compressed)\nGenerated output: Quality filtered fastq files\nUseful arguments (not extensive, check manual for all arguments):\n\n-q, --qualified_quality_phred: the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified\n-l, --length_required: reads shorter than length_required will be discarded, default is 15\nLow complexity filter: The low complexity filter is disabled by default, and you can enable it by -y or --low_complexity_filter. The complexity is defined as the percentage of base that is different from its next base (base[i] != base[i+1]).\nAdaptor removal: Adaptors are removed by default and fastp contains some built-in known adapter sequences for better auto-detection. For SE data, the adapters are evaluated by analyzing the tails of first ~1M reads. This evaluation may be inacurrate, and you can specify the adapter sequence by -a or --adapter_sequence option. For PE data, the adapters can be detected by per-read overlap analysis, which seeks for the overlap of each pair of reads. This method is robust and fast, so normally you don’t have to input the adapter sequence even you know it. But you can still specify the adapter sequences for read1 by --adapter_sequence, and for read2 by --adapter_sequence_r2.\nRead cutting by quality score: fastp supports per read sliding window cutting by evaluating the mean quality scores in the sliding window. There are 3 different operations, and you enable one or all of them:\n\n-5, --cut_front move a sliding window from front (5’) to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The leading N bases are also trimmed. Use cut_front_window_size to set the widnow size, and cut_front_mean_quality to set the mean quality threshold. If the window size is 1, this is similar as the Trimmomatic LEADING method.\n-3, --cut_tail move a sliding window from tail (3’) to front, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The trailing N bases are also trimmed. Use cut_tail_window_size to set the widnow size, and cut_tail_mean_quality to set the mean quality threshold. If the window size is 1, this is similar as the Trimmomatic TRAILING method.\n-r, --cut_right move a sliding window from front to tail, if meet one window with mean quality &lt; threshold, drop the bases in the window and the right part, and then stop. Use cut_right_window_size to set the widnow size, and cut_right_mean_quality to set the mean quality threshold. This is similar as the Trimmomatic SLIDINGWINDOW method.\nIf you don’t set window size and mean quality threshold for these function respectively, fastp will use the values from -W, --cut_window_size(Range: 1~1000, default: 4) and -M, --cut_mean_quality (Range: 1~36 default: 20)\n\nGlobal trimming: fastp supports global trimming, which means trim all reads in the front or the tail. This function is useful since sometimes you want to drop some cycles of a sequencing run.\n\nFor read1 or SE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1.\nFor read2 of PE data, the front/tail trimming settings are given with -F, --trim_front2 and -T, --trim_tail2. But if these options are not specified, they will be as same as read1 options, which means trim_front2 = trim_front1 and trim_tail2 = trim_tail1.\n\n-D, --dedup: enable deduplication to drop the duplicated reads/pairs\n\n\n\n\nTo give a simple example for using FastP, assume we work with paired-end (i.e. reverse and forward) reads from a sample and we want to remove adaptors, reads with a quality using a phred-score cutoff of 20 and remove reads shorter than 100 bp.\n\n#activate the right environment\nmamba activate fastp\n\n#run fastp\nfastp \\\n  -i data/sample1_F.fastq.gz -I data/sample1_R.fastq.gz \\\n  -o filtered_data/sample1_F_filtered.fq.gz -O filtered_data/sample1_R_filtered.fq.gz \\\n  --thread 5 -q 20 -l 100\n\n#deactivate environment (if using environment)\nmamba deactivate",
    "crumbs": [
      "Welcome page",
      "Metatranscriptomics",
      "FastP"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/fastp.html#fastp",
    "href": "source/metatranscriptomics/fastp.html#fastp",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "FastP is a tool designed to provide fast all-in-one preprocessing for FastQ files.\n\nManual\nPaper. Please do not forget to cite the paper whenever you use the software (Chen 2023).\n\nAvailable on Crunchomics: Yes, v0.23.2\n\n\n\nFastP can be easily installed with conda/mamba\n\nmamba create -n fastp\nmamba install -n fastp -c bioconda fastp\n\n\n\n\nFastP has many different options, so its best to also have a look at the manual. Below you find a quick example to try out.\n\nRequired input: Single-end or paired-end FastQ files (can be compressed)\nGenerated output: Quality filtered fastq files\nUseful arguments (not extensive, check manual for all arguments):\n\n-q, --qualified_quality_phred: the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified\n-l, --length_required: reads shorter than length_required will be discarded, default is 15\nLow complexity filter: The low complexity filter is disabled by default, and you can enable it by -y or --low_complexity_filter. The complexity is defined as the percentage of base that is different from its next base (base[i] != base[i+1]).\nAdaptor removal: Adaptors are removed by default and fastp contains some built-in known adapter sequences for better auto-detection. For SE data, the adapters are evaluated by analyzing the tails of first ~1M reads. This evaluation may be inacurrate, and you can specify the adapter sequence by -a or --adapter_sequence option. For PE data, the adapters can be detected by per-read overlap analysis, which seeks for the overlap of each pair of reads. This method is robust and fast, so normally you don’t have to input the adapter sequence even you know it. But you can still specify the adapter sequences for read1 by --adapter_sequence, and for read2 by --adapter_sequence_r2.\nRead cutting by quality score: fastp supports per read sliding window cutting by evaluating the mean quality scores in the sliding window. There are 3 different operations, and you enable one or all of them:\n\n-5, --cut_front move a sliding window from front (5’) to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The leading N bases are also trimmed. Use cut_front_window_size to set the widnow size, and cut_front_mean_quality to set the mean quality threshold. If the window size is 1, this is similar as the Trimmomatic LEADING method.\n-3, --cut_tail move a sliding window from tail (3’) to front, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The trailing N bases are also trimmed. Use cut_tail_window_size to set the widnow size, and cut_tail_mean_quality to set the mean quality threshold. If the window size is 1, this is similar as the Trimmomatic TRAILING method.\n-r, --cut_right move a sliding window from front to tail, if meet one window with mean quality &lt; threshold, drop the bases in the window and the right part, and then stop. Use cut_right_window_size to set the widnow size, and cut_right_mean_quality to set the mean quality threshold. This is similar as the Trimmomatic SLIDINGWINDOW method.\nIf you don’t set window size and mean quality threshold for these function respectively, fastp will use the values from -W, --cut_window_size(Range: 1~1000, default: 4) and -M, --cut_mean_quality (Range: 1~36 default: 20)\n\nGlobal trimming: fastp supports global trimming, which means trim all reads in the front or the tail. This function is useful since sometimes you want to drop some cycles of a sequencing run.\n\nFor read1 or SE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1.\nFor read2 of PE data, the front/tail trimming settings are given with -F, --trim_front2 and -T, --trim_tail2. But if these options are not specified, they will be as same as read1 options, which means trim_front2 = trim_front1 and trim_tail2 = trim_tail1.\n\n-D, --dedup: enable deduplication to drop the duplicated reads/pairs\n\n\n\n\nTo give a simple example for using FastP, assume we work with paired-end (i.e. reverse and forward) reads from a sample and we want to remove adaptors, reads with a quality using a phred-score cutoff of 20 and remove reads shorter than 100 bp.\n\n#activate the right environment\nmamba activate fastp\n\n#run fastp\nfastp \\\n  -i data/sample1_F.fastq.gz -I data/sample1_R.fastq.gz \\\n  -o filtered_data/sample1_F_filtered.fq.gz -O filtered_data/sample1_R_filtered.fq.gz \\\n  --thread 5 -q 20 -l 100\n\n#deactivate environment (if using environment)\nmamba deactivate",
    "crumbs": [
      "Welcome page",
      "Metatranscriptomics",
      "FastP"
    ]
  },
  {
    "objectID": "source/metagenomics/metabolic.html",
    "href": "source/metagenomics/metabolic.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Metabolic is a workflow developed by the AnantharamanLab. This software enables the prediction of metabolic and biogeochemical functional trait profiles to any given genome datasets. These genome datasets can either be metagenome-assembled genomes (MAGs), single-cell amplified genomes (SAGs) or isolated strain sequenced genomes.\nMETABOLIC has two main implementations, which are METABOLIC-G and METABOLIC-C. METABOLIC-G.pl allows for generation of metabolic profiles and biogeochemical cycling diagrams of input genomes and does not require input of sequencing reads. METABOLIC-C.pl generates the same output as METABOLIC-G.pl, but as it allows for the input of metagenomic read data, it will generate information pertaining to community metabolism.\nCheck out the manual for all usage options (Zhou et al. 2022).\nAvailable on Crunchomics: No\n\n\n\n\n\n\n#go into folder in which to install software\ncd ~/personal/software/METABOLIC_4.0\n\n#download environmental yaml (which tells conda what software to install)\nmkdir envs\nwget https://raw.githubusercontent.com/AnantharamanLab/METABOLIC/master/METABOLIC_v4.0_env.yml -P envs/\n\n#install dependencies via the environmental yaml (this env will be named METABOLIC_v4.0)\nmamba env create -f envs/METABOLIC_v4.0_env.yml\n\n#activate environment (NEEDS to be active to run the setup steps below)\nconda activate METABOLIC_v4.0\n\n#download a git clone of the METABOLIC workflow\ngit clone https://github.com/AnantharamanLab/METABOLIC.git\n\n#run bash setup script (needs some time, have patience)\ncd METABOLIC\nbash run_to_setup.sh\n\n\n\n\n\n\n\nNote\n\n\n\nThe command bash run_to_setup.sh installs some public databases, which might be useful to use other things. These are:\n\nThe kofam database: KOfams are a customized HMM database of KEGG Orthologs (KOs). The KO (KEGG Orthology) database is a database of molecular functions represented in terms of functional orthologs and is useful to assign functions to your proteins of interest. The script will download the database from scratch and you will therefore always have the newest version installed when installing METABOLIC.\nThe dbCAN2 database: A database that can be used for carbohydrate-active enzyme annotation. The script downloads dbCAN v10.\nThe Meropds database: A database for peptidases (also termed proteases, proteinases and proteolytic enzymes) and the proteins that inhibit them. The script will download the most recent version from the internet.\n\n\n\n\n\n\nMETABOLIC uses GTDB_tk for taxonomic assignment. This database is very big, so instead of installing this database every single time in a conda environment folder that uses GTDB_tk, it is better to use one global installation.\n\n\nFor UvA people using crunchomics, we have a global install of GTDB r207 you can use, which is installed on the metatools share. To get access to the share, please contact Anna Heintz Buschart via a.u.s.heintzbuschart@uva.nl with your UvAnetID and a description why you need access\n\n#tell conda where the gtdb database is installed\nconda env config vars set GTDBTK_DATA_PATH=\"/zfs/omics/projects/metatools/DB/GTDB_tk/GTDB_tk_r207\"\n\n#reactivate env to make the changes work\nconda deactivate\nconda activate METABOLIC_v4.0\n\n\n\n\nIf desired, you can install the GTDB database yourself as follows:\n\n#Manually download the latest reference data ()\nwget https://data.gtdb.ecogenomic.org/releases/release207/207.0/auxillary_files/gtdbtk_r207_v2_data.tar.gz\n\n#Extract the archive to a target directory:\n#change `/path/to/target/db` to whereever you want to install the db\ntar -xvzf gtdbtk_r207_v2_data.tar.gz -c \"/path/to/target/db\" --strip 1 &gt; /dev/null\n\n#cleanup\nrm gtdbtk_r207_v2_data.tar.gz\n\n#while the conda env for METABOLIC is activate link gtdb database\nconda env config vars set GTDBTK_DATA_PATH=\"/path/to/target/db\"\n\n#reactivate env \nconda deactivate\nconda activate METABOLIC_v4.0\n\n\n\n\n\n\nMETABOLIC has two scripts:\n\nMETABOLIC-G.pl: Allows for classification of the metabolic capabilities of input genomes.\nMETABOLIC-C.pl: Allows for classification of the metabolic capabilities of input genomes, calculation of genome coverage, creation of biogeochemical cycling diagrams, and visualization of community metabolic interactions and contribution to biogeochemical processes by each microbial group.\n\nInput:\n\nNucleotide fasta files (use -in-gn in the perl script)\nProtein faa files (use -in in the perl script)\nIllumina reads (use -r flag in the metabolic-c perl script) provided as un-compressed fastq files. This option requires you to provide the full path to your paired reads. Note that the two different sets of paired reads are separated by a line return (new line), and two reads in each line are separated by a “,” but not ” ,” or ” , ” (no spaces before or after comma). Blank lines are not allowed\nNanopore/PacBio long-reds (use r togher with -st illumina/pacbio/pacbio_hifi/nanopore to provide information about the sequencing type)\n\n#Read pairs: \n/path/to/your/reads/file/SRR3577362_sub_1.fastq,/path/to/your/reads/file/SRR3577362_sub_2.fastq\n/path/to/your/reads/file/SRR3577362_sub2_1.fastq,/path/to/your/reads/file/SRR3577362_sub2_2.fastq\nOutputs (for more detail, check the manual):\n\nAll_gene_collections_mapped.depth.txt: The gene depth of all input genes (METABOLIC-C only)\n\nEach_HMM_Amino_Acid_Sequence/: The faa collection for each hmm file\nintermediate_files/: The hmmsearch, peptides (MEROPS), CAZymes (dbCAN2), and GTDB-Tk (only for METABOLIC-C) running intermediate files\nKEGG_identifier_result/: The hit and result of each genome by Kofam database\nMETABOLIC_Figures/: All figures output from the running of METABOLIC\nMETABOLIC_Figures_Input/: All input files for R-generated diagrams\nMETABOLIC_result_each_spreadsheet/: TSV files representing each sheet of the created METABOLIC_result.xlsx file\nMW-score_result/: The resulted table for MW-score (METABOLIC-C only)\n\nMETABOLIC_result.xlsx: The resulting excel file of METABOLIC\n\nRequired/Optional flags:\n\n-in-gn [required if you are starting from nucleotide fasta files] Defines the location of the FOLDER containing the genome nucleotide fasta files ending with “.fasta” to be run by this program\n-in [required if you are starting from faa files] Defines the location of the FOLDER containing the genome amino acid files ending with “.faa” to be run by this program\n-r [required] Defines the path to a text file containing the location of paried reads\n-rt [optional] Defines the option to use “metaG” or “metaT” to indicate whether you use the metagenomic reads or metatranscriptomic reads (default: ‘metaG’). Only required when using METABOLIC-C\n-st [optional] To use “illumina” (for Illumina short reads), or “pacbio” (for PacBio CLR reads), or “pacbio_hifi” (for PacBio HiFi/CCS genomic reads (v2.19 or later)), or “pacbio_asm20” (for PacBio HiFi/CCS genomic reads (v2.18 or earlier)), or “nanopore” (for Oxford Nanopore reads) to indicate the sequencing type of metagenomes or metatranscriptomes (default: ‘illumina’; Note that all “illumina”, “pacbio”, “pacbio_hifi”, “pacbio_asm20”, and “nanopore” should be provided as lowercase letters and the underscore “_” should not be typed as “-” or any other marks)\n-t [optional] Defines the number of threads to run the program with (Default: 20)\n-m-cutoff [optional] Defines the fraction of KEGG module steps present to designate a KEGG module as present (Default: 0.75)\n-kofam-db [optional] Defines the use of the full (“full”) or reduced (“small”) KOfam database by the program (Default: ‘full’). “small” KOfam database only contains KOs present in KEGG module, using this setting will significantly reduce hmmsearch running time.\n-tax [optional] To calculate MW-score contribution of microbial groups at the resolution of which taxonomical level (default: “phylum”; other options: “class”, “order”, “family”, “genus”, “species”, and “bin” (MAG itself)). Only required when using METABOLIC-C\n-p [optional] Defines the prodigal method used to annotate ORFs (“meta” or “single”)(Default: “meta”)\n-o [optional] Defines the output directory to be created by the program (Default: current directory)\n\nSome example files for testing can be found in METABOLIC_test_files/.\n\n#print the help information\nperl METABOLIC-C.pl -help\n\n#run workflow on test data (5 genomes provided as nucleotides)\nmkdir -p testing/genomes\nperl METABOLIC-G.pl -in-gn METABOLIC_test_files/Guaymas_Basin_genome_files/ -o testing/genomes\n\n#run workflow on metagenomic fastq files\nperl METABOLIC-C.pl -test true",
    "crumbs": [
      "Welcome page",
      "Metagenomics",
      "Metabolic"
    ]
  },
  {
    "objectID": "source/metagenomics/metabolic.html#metabolic",
    "href": "source/metagenomics/metabolic.html#metabolic",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Metabolic is a workflow developed by the AnantharamanLab. This software enables the prediction of metabolic and biogeochemical functional trait profiles to any given genome datasets. These genome datasets can either be metagenome-assembled genomes (MAGs), single-cell amplified genomes (SAGs) or isolated strain sequenced genomes.\nMETABOLIC has two main implementations, which are METABOLIC-G and METABOLIC-C. METABOLIC-G.pl allows for generation of metabolic profiles and biogeochemical cycling diagrams of input genomes and does not require input of sequencing reads. METABOLIC-C.pl generates the same output as METABOLIC-G.pl, but as it allows for the input of metagenomic read data, it will generate information pertaining to community metabolism.\nCheck out the manual for all usage options (Zhou et al. 2022).\nAvailable on Crunchomics: No\n\n\n\n\n\n\n#go into folder in which to install software\ncd ~/personal/software/METABOLIC_4.0\n\n#download environmental yaml (which tells conda what software to install)\nmkdir envs\nwget https://raw.githubusercontent.com/AnantharamanLab/METABOLIC/master/METABOLIC_v4.0_env.yml -P envs/\n\n#install dependencies via the environmental yaml (this env will be named METABOLIC_v4.0)\nmamba env create -f envs/METABOLIC_v4.0_env.yml\n\n#activate environment (NEEDS to be active to run the setup steps below)\nconda activate METABOLIC_v4.0\n\n#download a git clone of the METABOLIC workflow\ngit clone https://github.com/AnantharamanLab/METABOLIC.git\n\n#run bash setup script (needs some time, have patience)\ncd METABOLIC\nbash run_to_setup.sh\n\n\n\n\n\n\n\nNote\n\n\n\nThe command bash run_to_setup.sh installs some public databases, which might be useful to use other things. These are:\n\nThe kofam database: KOfams are a customized HMM database of KEGG Orthologs (KOs). The KO (KEGG Orthology) database is a database of molecular functions represented in terms of functional orthologs and is useful to assign functions to your proteins of interest. The script will download the database from scratch and you will therefore always have the newest version installed when installing METABOLIC.\nThe dbCAN2 database: A database that can be used for carbohydrate-active enzyme annotation. The script downloads dbCAN v10.\nThe Meropds database: A database for peptidases (also termed proteases, proteinases and proteolytic enzymes) and the proteins that inhibit them. The script will download the most recent version from the internet.\n\n\n\n\n\n\nMETABOLIC uses GTDB_tk for taxonomic assignment. This database is very big, so instead of installing this database every single time in a conda environment folder that uses GTDB_tk, it is better to use one global installation.\n\n\nFor UvA people using crunchomics, we have a global install of GTDB r207 you can use, which is installed on the metatools share. To get access to the share, please contact Anna Heintz Buschart via a.u.s.heintzbuschart@uva.nl with your UvAnetID and a description why you need access\n\n#tell conda where the gtdb database is installed\nconda env config vars set GTDBTK_DATA_PATH=\"/zfs/omics/projects/metatools/DB/GTDB_tk/GTDB_tk_r207\"\n\n#reactivate env to make the changes work\nconda deactivate\nconda activate METABOLIC_v4.0\n\n\n\n\nIf desired, you can install the GTDB database yourself as follows:\n\n#Manually download the latest reference data ()\nwget https://data.gtdb.ecogenomic.org/releases/release207/207.0/auxillary_files/gtdbtk_r207_v2_data.tar.gz\n\n#Extract the archive to a target directory:\n#change `/path/to/target/db` to whereever you want to install the db\ntar -xvzf gtdbtk_r207_v2_data.tar.gz -c \"/path/to/target/db\" --strip 1 &gt; /dev/null\n\n#cleanup\nrm gtdbtk_r207_v2_data.tar.gz\n\n#while the conda env for METABOLIC is activate link gtdb database\nconda env config vars set GTDBTK_DATA_PATH=\"/path/to/target/db\"\n\n#reactivate env \nconda deactivate\nconda activate METABOLIC_v4.0\n\n\n\n\n\n\nMETABOLIC has two scripts:\n\nMETABOLIC-G.pl: Allows for classification of the metabolic capabilities of input genomes.\nMETABOLIC-C.pl: Allows for classification of the metabolic capabilities of input genomes, calculation of genome coverage, creation of biogeochemical cycling diagrams, and visualization of community metabolic interactions and contribution to biogeochemical processes by each microbial group.\n\nInput:\n\nNucleotide fasta files (use -in-gn in the perl script)\nProtein faa files (use -in in the perl script)\nIllumina reads (use -r flag in the metabolic-c perl script) provided as un-compressed fastq files. This option requires you to provide the full path to your paired reads. Note that the two different sets of paired reads are separated by a line return (new line), and two reads in each line are separated by a “,” but not ” ,” or ” , ” (no spaces before or after comma). Blank lines are not allowed\nNanopore/PacBio long-reds (use r togher with -st illumina/pacbio/pacbio_hifi/nanopore to provide information about the sequencing type)\n\n#Read pairs: \n/path/to/your/reads/file/SRR3577362_sub_1.fastq,/path/to/your/reads/file/SRR3577362_sub_2.fastq\n/path/to/your/reads/file/SRR3577362_sub2_1.fastq,/path/to/your/reads/file/SRR3577362_sub2_2.fastq\nOutputs (for more detail, check the manual):\n\nAll_gene_collections_mapped.depth.txt: The gene depth of all input genes (METABOLIC-C only)\n\nEach_HMM_Amino_Acid_Sequence/: The faa collection for each hmm file\nintermediate_files/: The hmmsearch, peptides (MEROPS), CAZymes (dbCAN2), and GTDB-Tk (only for METABOLIC-C) running intermediate files\nKEGG_identifier_result/: The hit and result of each genome by Kofam database\nMETABOLIC_Figures/: All figures output from the running of METABOLIC\nMETABOLIC_Figures_Input/: All input files for R-generated diagrams\nMETABOLIC_result_each_spreadsheet/: TSV files representing each sheet of the created METABOLIC_result.xlsx file\nMW-score_result/: The resulted table for MW-score (METABOLIC-C only)\n\nMETABOLIC_result.xlsx: The resulting excel file of METABOLIC\n\nRequired/Optional flags:\n\n-in-gn [required if you are starting from nucleotide fasta files] Defines the location of the FOLDER containing the genome nucleotide fasta files ending with “.fasta” to be run by this program\n-in [required if you are starting from faa files] Defines the location of the FOLDER containing the genome amino acid files ending with “.faa” to be run by this program\n-r [required] Defines the path to a text file containing the location of paried reads\n-rt [optional] Defines the option to use “metaG” or “metaT” to indicate whether you use the metagenomic reads or metatranscriptomic reads (default: ‘metaG’). Only required when using METABOLIC-C\n-st [optional] To use “illumina” (for Illumina short reads), or “pacbio” (for PacBio CLR reads), or “pacbio_hifi” (for PacBio HiFi/CCS genomic reads (v2.19 or later)), or “pacbio_asm20” (for PacBio HiFi/CCS genomic reads (v2.18 or earlier)), or “nanopore” (for Oxford Nanopore reads) to indicate the sequencing type of metagenomes or metatranscriptomes (default: ‘illumina’; Note that all “illumina”, “pacbio”, “pacbio_hifi”, “pacbio_asm20”, and “nanopore” should be provided as lowercase letters and the underscore “_” should not be typed as “-” or any other marks)\n-t [optional] Defines the number of threads to run the program with (Default: 20)\n-m-cutoff [optional] Defines the fraction of KEGG module steps present to designate a KEGG module as present (Default: 0.75)\n-kofam-db [optional] Defines the use of the full (“full”) or reduced (“small”) KOfam database by the program (Default: ‘full’). “small” KOfam database only contains KOs present in KEGG module, using this setting will significantly reduce hmmsearch running time.\n-tax [optional] To calculate MW-score contribution of microbial groups at the resolution of which taxonomical level (default: “phylum”; other options: “class”, “order”, “family”, “genus”, “species”, and “bin” (MAG itself)). Only required when using METABOLIC-C\n-p [optional] Defines the prodigal method used to annotate ORFs (“meta” or “single”)(Default: “meta”)\n-o [optional] Defines the output directory to be created by the program (Default: current directory)\n\nSome example files for testing can be found in METABOLIC_test_files/.\n\n#print the help information\nperl METABOLIC-C.pl -help\n\n#run workflow on test data (5 genomes provided as nucleotides)\nmkdir -p testing/genomes\nperl METABOLIC-G.pl -in-gn METABOLIC_test_files/Guaymas_Basin_genome_files/ -o testing/genomes\n\n#run workflow on metagenomic fastq files\nperl METABOLIC-C.pl -test true",
    "crumbs": [
      "Welcome page",
      "Metagenomics",
      "Metabolic"
    ]
  },
  {
    "objectID": "source/metagenomics/fastqc_readme.html",
    "href": "source/metagenomics/fastqc_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "FastQC is a quality control tool for high throughput sequence data. For help with interpreting the output, please visit the website with some very good examples.\nFastQC can be run both on short- and long-read data.\nAvailable on Crunchomics: Yes\n\n\n\nFastQC already is installed on Crunchomics, if you want to install it on your own system check out the instructions found here.\n\n\n\n\nInputs: FastQC can process bam,sam,bam_mapped,sam_mapped and fastq files\nOutput: An HTML quality report\n\nExample code:\n\n#get help \nfastqc --help\n\n#run on a single file\nfastqc myfile.fastq.gz -o outputfolder --threads 1",
    "crumbs": [
      "Welcome page",
      "Metagenomics",
      "FastQC"
    ]
  },
  {
    "objectID": "source/metagenomics/fastqc_readme.html#fastqc",
    "href": "source/metagenomics/fastqc_readme.html#fastqc",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "FastQC is a quality control tool for high throughput sequence data. For help with interpreting the output, please visit the website with some very good examples.\nFastQC can be run both on short- and long-read data.\nAvailable on Crunchomics: Yes\n\n\n\nFastQC already is installed on Crunchomics, if you want to install it on your own system check out the instructions found here.\n\n\n\n\nInputs: FastQC can process bam,sam,bam_mapped,sam_mapped and fastq files\nOutput: An HTML quality report\n\nExample code:\n\n#get help \nfastqc --help\n\n#run on a single file\nfastqc myfile.fastq.gz -o outputfolder --threads 1",
    "crumbs": [
      "Welcome page",
      "Metagenomics",
      "FastQC"
    ]
  },
  {
    "objectID": "source/conda/conda.html",
    "href": "source/conda/conda.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "A lot of bioinformatic workflows start with installing software. Since this often means not only installing the software but several dependencies, we recommend the use of a package management system, such as conda or mamba. These tools allow you to find and install packages in their own environment without administrator privileges.\nThis is especially useful if you require different software versions, such as python3.6 versus python3.10, for different workflows. With package management systems you can easily setup different python versions in different environments.\n\n\nA lot of system already come with conda installed, however, if possible we recommend working with mamba instead of conda. mamba is a drop-in replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\nTo install mamba, follow the instructions here. This should look something like this for mac and linux-systems. If you are on windows, the easiest is to setup up Windows Subsystem for Linux (WSL) first and then use the code below.\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\n\n\n\nLet’s assume we want to install a tool, ITSx, into an environment called fungal_genomics. We can do this as follows:\n\n#check if the tool is installed (should return command not found)\nITSx -h\n\n#create an empty environment and name it fungal_genomics\nmamba create -n fungal_genomics\n\n#install some software, i.e. itsx, into the fungal_genomics environment\nmamba install -n fungal_genomics -c bioconda itsx\n\n#to run the tool activate the environment\nconda activate fungal_genomics\n\n#check if tool is installed\nITSx -h\n\n#leave the environment\nconda deactivate\n\nA full set of mamba/conda commands can be found here\n\n\n\nOn crunchomics other people might have already installed environments that might be useful for your work. One example is the amplicomics share, which comes with several QIIME 2 installations. To use this, first ask for access to the amplicomics share by contacting n.dombrowski@uva.nl with your uva net id. After you got access, you can add conda environments in the amplicomics share with:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/",
    "crumbs": [
      "Welcome page",
      "Installing software"
    ]
  },
  {
    "objectID": "source/conda/conda.html#installing-software",
    "href": "source/conda/conda.html#installing-software",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "A lot of bioinformatic workflows start with installing software. Since this often means not only installing the software but several dependencies, we recommend the use of a package management system, such as conda or mamba. These tools allow you to find and install packages in their own environment without administrator privileges.\nThis is especially useful if you require different software versions, such as python3.6 versus python3.10, for different workflows. With package management systems you can easily setup different python versions in different environments.\n\n\nA lot of system already come with conda installed, however, if possible we recommend working with mamba instead of conda. mamba is a drop-in replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\nTo install mamba, follow the instructions here. This should look something like this for mac and linux-systems. If you are on windows, the easiest is to setup up Windows Subsystem for Linux (WSL) first and then use the code below.\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\n\n\n\nLet’s assume we want to install a tool, ITSx, into an environment called fungal_genomics. We can do this as follows:\n\n#check if the tool is installed (should return command not found)\nITSx -h\n\n#create an empty environment and name it fungal_genomics\nmamba create -n fungal_genomics\n\n#install some software, i.e. itsx, into the fungal_genomics environment\nmamba install -n fungal_genomics -c bioconda itsx\n\n#to run the tool activate the environment\nconda activate fungal_genomics\n\n#check if tool is installed\nITSx -h\n\n#leave the environment\nconda deactivate\n\nA full set of mamba/conda commands can be found here\n\n\n\nOn crunchomics other people might have already installed environments that might be useful for your work. One example is the amplicomics share, which comes with several QIIME 2 installations. To use this, first ask for access to the amplicomics share by contacting n.dombrowski@uva.nl with your uva net id. After you got access, you can add conda environments in the amplicomics share with:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/",
    "crumbs": [
      "Welcome page",
      "Installing software"
    ]
  },
  {
    "objectID": "source/cli/hpc_usage.html",
    "href": "source/cli/hpc_usage.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "If you work at IBED and want to use the cli on the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED, please check out this documentation. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your uva-net id.\nFor information about other computational resources please visit the computational support teams website under Scientific programming and HPC.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified shematic:\n\n\n\n\n\nVery briefly, the purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files and compiling, preparing a job script). The compute nodes on the other hand are used to actually run a program and we submit jobs from the login to the compute nodes via Slurm. Slurm is an open-source workload manager/scheduler that is used on many big HPCs. Slurm has three key functions:\n\nprovide the users access to the resources on the compute nodes for a certain amount of time to perform any computation\nprovide a framework to start, execute, and check the work on the set of allocated compute nodes\nmange the queue of submitted jobs based on the availability of resources\n\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind you are sharing the system with other users. Some rules of thumb:\n\nDon’t run jobs that request many cpus and lots of memory on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005) for this\nDon’t allocate more than 20% (cpu or memory) of the cluster for more than a day\nDo not leave allocations unused and set reasonable time limits on you jobs\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nare granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nhave a 25 G quotum on your home directory which is on a fast NVMe ssd-drive. You can store up to 500 GB data in /zfs/omics/personal/$USER\nyou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nfind information and documentation about the cluster here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\nAccess to two directories:\n\nThe home directory with 25 GB of storage\nyour personal directory, with 500 GB of storage\n\n\nSome information about storage:\n\nSnapshots are made daily at 00.00.00 and kept for 2 weeks. This means that if you accidentally remove a file it can be restored up to 2 weeks after removal.\nData on Crunchomics is stored on multiple disks. Therefore, there is protection against disk failure. However, the data is not replicated and you are responsible for backing up and/or archiving your data.",
    "crumbs": [
      "Getting Started",
      "Using an HPC at UvA"
    ]
  },
  {
    "objectID": "source/cli/hpc_usage.html#introduction",
    "href": "source/cli/hpc_usage.html#introduction",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "If you work at IBED and want to use the cli on the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED, please check out this documentation. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your uva-net id.\nFor information about other computational resources please visit the computational support teams website under Scientific programming and HPC.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified shematic:\n\n\n\n\n\nVery briefly, the purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files and compiling, preparing a job script). The compute nodes on the other hand are used to actually run a program and we submit jobs from the login to the compute nodes via Slurm. Slurm is an open-source workload manager/scheduler that is used on many big HPCs. Slurm has three key functions:\n\nprovide the users access to the resources on the compute nodes for a certain amount of time to perform any computation\nprovide a framework to start, execute, and check the work on the set of allocated compute nodes\nmange the queue of submitted jobs based on the availability of resources\n\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind you are sharing the system with other users. Some rules of thumb:\n\nDon’t run jobs that request many cpus and lots of memory on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005) for this\nDon’t allocate more than 20% (cpu or memory) of the cluster for more than a day\nDo not leave allocations unused and set reasonable time limits on you jobs\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nare granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nhave a 25 G quotum on your home directory which is on a fast NVMe ssd-drive. You can store up to 500 GB data in /zfs/omics/personal/$USER\nyou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nfind information and documentation about the cluster here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\nAccess to two directories:\n\nThe home directory with 25 GB of storage\nyour personal directory, with 500 GB of storage\n\n\nSome information about storage:\n\nSnapshots are made daily at 00.00.00 and kept for 2 weeks. This means that if you accidentally remove a file it can be restored up to 2 weeks after removal.\nData on Crunchomics is stored on multiple disks. Therefore, there is protection against disk failure. However, the data is not replicated and you are responsible for backing up and/or archiving your data.",
    "crumbs": [
      "Getting Started",
      "Using an HPC at UvA"
    ]
  },
  {
    "objectID": "source/cli/cli_installation.html",
    "href": "source/cli/cli_installation.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Below you find some links that help you in setting up a Bash command line depending on your operating system:\n\n\nThe default shell is usually Bash and there is no need to install anything to be able to follow this tutorial. On most versions of Linux, the shell accessible by running the Gnome Terminal or KDE Konsole or xterm, which can be found via the applications menu or the search bar. If your machine is set up to use something other than Bash, you should be able to switch the shell by opening a terminal and typing bash.\n\n\n\nFor Mac running macOS Mojave or earlier releases, the default Unix Shell is Bash. For a Mac computer running macOS Catalina or later releases, the default Unix Shell is Zsh. To open a terminal, try one or both of the following:\n\nIn Finder, select the Go menu, then select Utilities. Locate Terminal in the Utilities folder and open it.\nUse the Mac ‘Spotlight’ computer search function. Search for: Terminal and press Return.\n\nTo ensure that you work with a consistent shell and to check if your machine is set up to use something other than Bash, type echo $SHELL in your terminal window. The name of the current shell should be printed to the terminal window.\nIf your machine is set up to use something other than Bash, you can try switching to Bash by opening a terminal and typing bash. To check if that worked type echo $SHELL again.\n\n\n\nOperating systems like macOS and Linux come with a native command-line terminal, making it straightforward to run bash commands. However, for Windows users you need to install some software first to be able to use bash, below you find three options:\nOne option to access the bash shell commands is using Git Bash, for detailed installation instructions please have a look at the carpenties website.\nA second option is Mobaxterm, which enables Windows users to execute basic Linux/Unix commands on their local machine, connect to an HPC with SSH and to transfer files with SCP/SFTP (more on that later). Installation instructions can be found here.\nA final option is to use Windows and Linux at the same time on a Windows machine. The Windows Subsystem for Linux (WSL2) lets users install a Linux distribution (such as Ubuntu, which is the default Linux distribution, which we recommend to use) and use Linux applications, utilities, and Bash command-line tools directly on Windows. This option allows you to use all the tools available but since you more or less are installing a separating system on your PC needs to have enough memory to run this. Installation instructions can be found here.\nNotice: The code found on these pages was run on a windows machine using WSL2 and on a Linux machine.\nAfter you set everything up and opened a terminal you should see something like this and are good to go if you want to follow the tutorial:",
    "crumbs": [
      "Getting Started",
      "Setting up a bash command line"
    ]
  },
  {
    "objectID": "source/cli/cli_installation.html#setting-up-a-terminal",
    "href": "source/cli/cli_installation.html#setting-up-a-terminal",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Below you find some links that help you in setting up a Bash command line depending on your operating system:\n\n\nThe default shell is usually Bash and there is no need to install anything to be able to follow this tutorial. On most versions of Linux, the shell accessible by running the Gnome Terminal or KDE Konsole or xterm, which can be found via the applications menu or the search bar. If your machine is set up to use something other than Bash, you should be able to switch the shell by opening a terminal and typing bash.\n\n\n\nFor Mac running macOS Mojave or earlier releases, the default Unix Shell is Bash. For a Mac computer running macOS Catalina or later releases, the default Unix Shell is Zsh. To open a terminal, try one or both of the following:\n\nIn Finder, select the Go menu, then select Utilities. Locate Terminal in the Utilities folder and open it.\nUse the Mac ‘Spotlight’ computer search function. Search for: Terminal and press Return.\n\nTo ensure that you work with a consistent shell and to check if your machine is set up to use something other than Bash, type echo $SHELL in your terminal window. The name of the current shell should be printed to the terminal window.\nIf your machine is set up to use something other than Bash, you can try switching to Bash by opening a terminal and typing bash. To check if that worked type echo $SHELL again.\n\n\n\nOperating systems like macOS and Linux come with a native command-line terminal, making it straightforward to run bash commands. However, for Windows users you need to install some software first to be able to use bash, below you find three options:\nOne option to access the bash shell commands is using Git Bash, for detailed installation instructions please have a look at the carpenties website.\nA second option is Mobaxterm, which enables Windows users to execute basic Linux/Unix commands on their local machine, connect to an HPC with SSH and to transfer files with SCP/SFTP (more on that later). Installation instructions can be found here.\nA final option is to use Windows and Linux at the same time on a Windows machine. The Windows Subsystem for Linux (WSL2) lets users install a Linux distribution (such as Ubuntu, which is the default Linux distribution, which we recommend to use) and use Linux applications, utilities, and Bash command-line tools directly on Windows. This option allows you to use all the tools available but since you more or less are installing a separating system on your PC needs to have enough memory to run this. Installation instructions can be found here.\nNotice: The code found on these pages was run on a windows machine using WSL2 and on a Linux machine.\nAfter you set everything up and opened a terminal you should see something like this and are good to go if you want to follow the tutorial:",
    "crumbs": [
      "Getting Started",
      "Setting up a bash command line"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html",
    "href": "source/cli/cli_basics.html",
    "title": "Unix Basics",
    "section": "",
    "text": "Before getting started with the CLI, lets first understand the file system used by Unix, which might look some thing like this:\n\n\n\nThe Unix File System is a logical method of organizing and storing large amounts of information in a way that makes it easy to manage. Some key features are:\n\nDifferent to what you might be used to from Windows, Unix does not use Drives but appears as a rooted tree of directories\n\nThe first directory in the file system is called the root directory represented by /\n\nDifferent storage devices may contain different branches of the tree, but there is always a single tree. e.g., /users/john/ and / users/mary/\n\n\n\n\nNow that we know how the file system looks like, we do want to move around. Since we do not use a GUI we can not use the mouse to point and click but instead have to write down what we want to do.\npwd prints the location of the current working directory and basically tells you where exactly you are. Usually when we login we start from what is called our home directory. This generally will be something like /Users/username but might be slightly different depending on your operating system.\n\npwd\n\nNow that we know where we are, let’s see how to move around by first seeing what other folders there are. For this we can use the ls command, which stands for list directory contents\n\nls\n\nIn my case this returns something like this:\n\n\n\nThis might look a bit different for your system in terms of color and file/folder names but what we basically see are the files (in bold text) and folders (green-highlighted text).\n\n\n\nNext, let’s go into how commands, like ls can be used a bit more precisely. Let´s start with looking at the general structure of a command:\ncommand [options] [arguments]\n\ncommand is the name of the command you want to use, i.e. ls would be an example for a very basic Unix command\narguments is one or more adjustments to the command’s behavior. I.e. for ls we could use the following arguments\n\nShort notation: -a\nLong notation: —all\n\n\nLet’s explore more about how to use this arguments in the next section.\n\n\n\nLet’s look a bit closer into the ls command and use it with an argument -l.\nAs a reminder:\n\nls stands for list directory contents\neverything starting with a minus symbol is an optional argument we can use\n-l is an additional argument we can use that makes ls use a long listing format\n\n\nls -l\n\nAfter running this, we should see our files and folders but in what is called the long format (which gives way more information):\n\n\n\nIf you are unsure what options come with a program its always a good idea to check out the manual. You can do this with:\n\nman ls\n\nYou can exit the manual by pressing q.\nIn case you want to check what a program does or what options there are, depending on the program there might be different ways how to do this. These most common ways are:\n\nman ls\nls --help\nls -h\n\n\n\n\nNext, lets move around these folders using the cd command, which let’s you change the directory. If you run this on your computer exchange source to a folder name that you see when using ls. If there are no directories, we will explain a bit later how to generate new directories.\n\ncd source/\n\nIf you use pwd afterwards, then you should see that we moved into another directory. We can also move back to our original directory as follows:\n\ncd ..\n\nWe can also move around multiple levels for example here i am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nAnother useful way to move around quickly is using the tilde symbol, i.e. ~, that can be used as a shortcut to move directly into our home directory:\n\ncd ~\n\n\n\n\nNext, let’s discuss an important distinction between absolute and relative pathnames.\nAbsolute pathnames\nAn absolute pathname begins with the root directory and follows the tree branch by branch until the path to the desired directory or file is completed. For example on your computer full path to our Desktop might be /Users/username/Desktop\nIf we want to go to the desktop using an absolute path we do the following:\n\ncd /Users/username/Desktop\n\nRelative pathnames\nA relative pathname starts from the directory you are currently in and you traverse to other directories relative from this directory. When using the relative pathway allows you to use a couple of special notations to represent relative positions in the file system tree.\n\n. (dot) The working directory itself\n.. (dot dot) Refers to the working directory’s parent directory\n\nIf we want to go to the source directory using an relative path we do the following (again change this to whatever folder you see when using ls):\n\ncd Desktop\n\n\n\n\nNow that we know how to explore our surroundings, let us make a new folder in which we can generate some new files later on. For this we use the mkdir command.\nTo do this, we will first move into our home directory and create a new folder from which we later run all further commands\n\n#go into the folder from which you want to work (i.e. the home directory)\ncd ~\n\n#make a new folder (in the directory we currently are in, name it new_older)\nmkdir playground\n\n#check if new folder was generated\nls\n\n#next we move into the newly generated folder\ncd playground\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nEnsure that you run the following steps while being in the playground folder you just generated.\n\n\n\n\nNow, let’s make some files of our own using nano. Nano is a basic text editor that lets us view and generate text.\n\n#ensure that we work in the right folder\npwd\n\n#open a new document and name it random.txt\nnano random.txt\n\nOnce we type nano random.txt and press enter then a text document will open. In there:\n\nType something into this document\nClose the document with control + X\nType y to save changes and press enter\n\nIf we now use ls -l again, we see that a new file was generated.\nWe can use nano to open whatever document we want, but for larger files it might take a long time to read everything into memory. For such cases, let’s look at some alternative text viewers.\n\n\n\nless is a program that lets you view a file’s contents one screen at a time. This is useful when dealing with a large text file because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless random.txt\n\nOnce started, less will display the text file one page at a time.\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\nFor files with a lots of columns or long strings we can use:\n\nless -S random.txt\n\nIn this mode we can also use the arrow right and left keys, to view columns that are further on the right.\n\n\n\nAnother, very quick way to check the first 10 rows is head:\n\nhead random.txt\n\n\n\n\nIf you want to check the last 10 rows use tail\n\ntail random.txt\n\n\n\n\n\nBy using some special notations we can redirect the output of many commands to files, devices, and even to the input of other commands.\nStandard output (stdout)\nBy default, standard output directs its contents to the display, i.e. when we use ls the list of files and folders is printed to the screen. However, we can also redirect the standard output to a file by using &gt; character is used like this:\n\n#redirect the output from ls to a new file\nls -l &gt; file_list.txt\n\n#check what happened (feel free to also use the other methods we have discussed)\nnano file_list.txt\n\nIf we would use ls -l &gt; file_list.txt again we would overwrite the content of any existing files. However, there might be instances were we want to append something to an existing file. We can do this by using &gt;&gt;:\n\nls -l &gt;&gt; file_list.txt\n\n#check what happened\nnano file_list.txt\n\n\n\n\nNext, let’s move the text file we generated before around. To do this, we mainly need two commands:\n\ncp - copy files and directories\nmv - move or rename files and directories\n\n\n#make a new folder\nmkdir our_files\n\n#copy our random file into our new folder\ncp random.txt our_files\n\nIf we check the content of our working directory and the newly generated folder after doing this, we should see that random.txt exists now twice. Once in our working directory and once in the our_files folder. If we want to move a file, instead of copying, we can do the following:\n\n#copy our random file into our new_folder\nmv random.txt our_files\n\nNow, random.txt should only exist once in the our_files folder.\n\n\n\nTo remove files and folders, we use the rm command. Let’s first remove the text file we generated:\n\n#rm a file\nrm our_files/random.txt\n\n#check if that worked\nls -l our_files\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an argument. To do this, we use the -r argument (to remove directories and their contents recursively).\n\n\n\n\n\n\nWarning\n\n\n\nUnix does not have an undelete command.\nTherefore, if you delete something with rm, it’s gone.\nSo use rm with care and check what you wrote twice before pressing enter!\n\n\n\n#rm a directory\nrm -r our_files\n\n#check if that worked \nls -l\n\n\n\n\nNext, let’s download some data to learn how to manipulate files. One way to do this is using the wget. With -P we specify were to download the data:\n\n#make a folder for our downloads\nmkdir downloads\n\n#download a genome from ncbi using wget\nwget -P downloads ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/002/728/275/GCA_002728275.1_ASM272827v1/GCA_002728275.1_ASM272827v1_genomic.fna.gz\n\n\n\n\n\n\nAfter downloading and exploring the content of the downloads folder, you see that the file we downloaded ends with gz. This indicates that we work with a gzip-compressed file. Gzip is a tool used to compress the size of files.\nIn order to work with files, we sometimes need to de-compress them first. We can do this as follows (by using the decompress, -d argument):\n\n#decompress gz data (-d = decompress)\ngzip -d downloads/GCA_002728275.1_ASM272827v1_genomic.fna.gz\n\nConversely, if you want to compress a file, you can do this as such:\n\n#compress\ngzip downloads/GCA_002728275.1_ASM272827v1_genomic.fna\n\n\n\n\nAnother file compression you might encounter is tar:\n\nShort for Tape Archive, and sometimes referred to as tarball, is a file in the Consolidated Unix Archive format.\nThe TAR file format is common in Unix and Unix-like systems when storing data (however, we do not compress our files when using the tar files)\nTAR files are often compressed after being created and then become TGZ files, using the tgz, tar.gz, or gz extension.\n\nLet’s try to make a compressed tarball with the file we just downloaded.\n\n#decompress gz data\ngzip -d downloads/GCA_002728275.1_ASM272827v1_genomic.fna.gz\n\n#create a tar file, to_compress.tar.gz, from our downloads directory\ntar -cvzf to_compress.tar.gz downloads\n\nIf we print the content of our working directory with ls we now see an additional file, to_compress.tar.gz. We can uncompress this file into a new folder as follows:\n\n#make a folder into with to compress the data\nmkdir to_compress \n\n#decompress\ntar -xvf to_compress.tar.gz -C to_compress\n\n#check what happened\nls to_compress\n\nWe can see that there is one folder in to_compress, our downloads folder.",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#the-file-system",
    "href": "source/cli/cli_basics.html#the-file-system",
    "title": "Unix Basics",
    "section": "",
    "text": "Before getting started with the CLI, lets first understand the file system used by Unix, which might look some thing like this:\n\n\n\nThe Unix File System is a logical method of organizing and storing large amounts of information in a way that makes it easy to manage. Some key features are:\n\nDifferent to what you might be used to from Windows, Unix does not use Drives but appears as a rooted tree of directories\n\nThe first directory in the file system is called the root directory represented by /\n\nDifferent storage devices may contain different branches of the tree, but there is always a single tree. e.g., /users/john/ and / users/mary/",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#pwd-finding-out-where-we-are",
    "href": "source/cli/cli_basics.html#pwd-finding-out-where-we-are",
    "title": "Unix Basics",
    "section": "",
    "text": "Now that we know how the file system looks like, we do want to move around. Since we do not use a GUI we can not use the mouse to point and click but instead have to write down what we want to do.\npwd prints the location of the current working directory and basically tells you where exactly you are. Usually when we login we start from what is called our home directory. This generally will be something like /Users/username but might be slightly different depending on your operating system.\n\npwd\n\nNow that we know where we are, let’s see how to move around by first seeing what other folders there are. For this we can use the ls command, which stands for list directory contents\n\nls\n\nIn my case this returns something like this:\n\n\n\nThis might look a bit different for your system in terms of color and file/folder names but what we basically see are the files (in bold text) and folders (green-highlighted text).",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#general-structure-of-a-unix-command",
    "href": "source/cli/cli_basics.html#general-structure-of-a-unix-command",
    "title": "Unix Basics",
    "section": "",
    "text": "Next, let’s go into how commands, like ls can be used a bit more precisely. Let´s start with looking at the general structure of a command:\ncommand [options] [arguments]\n\ncommand is the name of the command you want to use, i.e. ls would be an example for a very basic Unix command\narguments is one or more adjustments to the command’s behavior. I.e. for ls we could use the following arguments\n\nShort notation: -a\nLong notation: —all\n\n\nLet’s explore more about how to use this arguments in the next section.",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#ls-viewing-the-content-of-a-folder",
    "href": "source/cli/cli_basics.html#ls-viewing-the-content-of-a-folder",
    "title": "Unix Basics",
    "section": "",
    "text": "Let’s look a bit closer into the ls command and use it with an argument -l.\nAs a reminder:\n\nls stands for list directory contents\neverything starting with a minus symbol is an optional argument we can use\n-l is an additional argument we can use that makes ls use a long listing format\n\n\nls -l\n\nAfter running this, we should see our files and folders but in what is called the long format (which gives way more information):\n\n\n\nIf you are unsure what options come with a program its always a good idea to check out the manual. You can do this with:\n\nman ls\n\nYou can exit the manual by pressing q.\nIn case you want to check what a program does or what options there are, depending on the program there might be different ways how to do this. These most common ways are:\n\nman ls\nls --help\nls -h",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#cd-moving-around-folders",
    "href": "source/cli/cli_basics.html#cd-moving-around-folders",
    "title": "Unix Basics",
    "section": "",
    "text": "Next, lets move around these folders using the cd command, which let’s you change the directory. If you run this on your computer exchange source to a folder name that you see when using ls. If there are no directories, we will explain a bit later how to generate new directories.\n\ncd source/\n\nIf you use pwd afterwards, then you should see that we moved into another directory. We can also move back to our original directory as follows:\n\ncd ..\n\nWe can also move around multiple levels for example here i am going into the source folder, then back to the home directory and then into the docs folder.\n\ncd source/../docs\n\nAnother useful way to move around quickly is using the tilde symbol, i.e. ~, that can be used as a shortcut to move directly into our home directory:\n\ncd ~",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#pathnames",
    "href": "source/cli/cli_basics.html#pathnames",
    "title": "Unix Basics",
    "section": "",
    "text": "Next, let’s discuss an important distinction between absolute and relative pathnames.\nAbsolute pathnames\nAn absolute pathname begins with the root directory and follows the tree branch by branch until the path to the desired directory or file is completed. For example on your computer full path to our Desktop might be /Users/username/Desktop\nIf we want to go to the desktop using an absolute path we do the following:\n\ncd /Users/username/Desktop\n\nRelative pathnames\nA relative pathname starts from the directory you are currently in and you traverse to other directories relative from this directory. When using the relative pathway allows you to use a couple of special notations to represent relative positions in the file system tree.\n\n. (dot) The working directory itself\n.. (dot dot) Refers to the working directory’s parent directory\n\nIf we want to go to the source directory using an relative path we do the following (again change this to whatever folder you see when using ls):\n\ncd Desktop",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#mkdir-making-new-folders",
    "href": "source/cli/cli_basics.html#mkdir-making-new-folders",
    "title": "Unix Basics",
    "section": "",
    "text": "Now that we know how to explore our surroundings, let us make a new folder in which we can generate some new files later on. For this we use the mkdir command.\nTo do this, we will first move into our home directory and create a new folder from which we later run all further commands\n\n#go into the folder from which you want to work (i.e. the home directory)\ncd ~\n\n#make a new folder (in the directory we currently are in, name it new_older)\nmkdir playground\n\n#check if new folder was generated\nls\n\n#next we move into the newly generated folder\ncd playground",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#generating-and-viewing-files",
    "href": "source/cli/cli_basics.html#generating-and-viewing-files",
    "title": "Unix Basics",
    "section": "",
    "text": "Important\n\n\n\nEnsure that you run the following steps while being in the playground folder you just generated.\n\n\n\n\nNow, let’s make some files of our own using nano. Nano is a basic text editor that lets us view and generate text.\n\n#ensure that we work in the right folder\npwd\n\n#open a new document and name it random.txt\nnano random.txt\n\nOnce we type nano random.txt and press enter then a text document will open. In there:\n\nType something into this document\nClose the document with control + X\nType y to save changes and press enter\n\nIf we now use ls -l again, we see that a new file was generated.\nWe can use nano to open whatever document we want, but for larger files it might take a long time to read everything into memory. For such cases, let’s look at some alternative text viewers.\n\n\n\nless is a program that lets you view a file’s contents one screen at a time. This is useful when dealing with a large text file because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless random.txt\n\nOnce started, less will display the text file one page at a time.\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\nFor files with a lots of columns or long strings we can use:\n\nless -S random.txt\n\nIn this mode we can also use the arrow right and left keys, to view columns that are further on the right.\n\n\n\nAnother, very quick way to check the first 10 rows is head:\n\nhead random.txt\n\n\n\n\nIf you want to check the last 10 rows use tail\n\ntail random.txt",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#io-redirection-to-new-files",
    "href": "source/cli/cli_basics.html#io-redirection-to-new-files",
    "title": "Unix Basics",
    "section": "",
    "text": "By using some special notations we can redirect the output of many commands to files, devices, and even to the input of other commands.\nStandard output (stdout)\nBy default, standard output directs its contents to the display, i.e. when we use ls the list of files and folders is printed to the screen. However, we can also redirect the standard output to a file by using &gt; character is used like this:\n\n#redirect the output from ls to a new file\nls -l &gt; file_list.txt\n\n#check what happened (feel free to also use the other methods we have discussed)\nnano file_list.txt\n\nIf we would use ls -l &gt; file_list.txt again we would overwrite the content of any existing files. However, there might be instances were we want to append something to an existing file. We can do this by using &gt;&gt;:\n\nls -l &gt;&gt; file_list.txt\n\n#check what happened\nnano file_list.txt",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#cp-and-mv-copying-and-moving-files",
    "href": "source/cli/cli_basics.html#cp-and-mv-copying-and-moving-files",
    "title": "Unix Basics",
    "section": "",
    "text": "Next, let’s move the text file we generated before around. To do this, we mainly need two commands:\n\ncp - copy files and directories\nmv - move or rename files and directories\n\n\n#make a new folder\nmkdir our_files\n\n#copy our random file into our new folder\ncp random.txt our_files\n\nIf we check the content of our working directory and the newly generated folder after doing this, we should see that random.txt exists now twice. Once in our working directory and once in the our_files folder. If we want to move a file, instead of copying, we can do the following:\n\n#copy our random file into our new_folder\nmv random.txt our_files\n\nNow, random.txt should only exist once in the our_files folder.",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#rm-removing-files-and-folders",
    "href": "source/cli/cli_basics.html#rm-removing-files-and-folders",
    "title": "Unix Basics",
    "section": "",
    "text": "To remove files and folders, we use the rm command. Let’s first remove the text file we generated:\n\n#rm a file\nrm our_files/random.txt\n\n#check if that worked\nls -l our_files\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an argument. To do this, we use the -r argument (to remove directories and their contents recursively).\n\n\n\n\n\n\nWarning\n\n\n\nUnix does not have an undelete command.\nTherefore, if you delete something with rm, it’s gone.\nSo use rm with care and check what you wrote twice before pressing enter!\n\n\n\n#rm a directory\nrm -r our_files\n\n#check if that worked \nls -l",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#wget-downloading-data",
    "href": "source/cli/cli_basics.html#wget-downloading-data",
    "title": "Unix Basics",
    "section": "",
    "text": "Next, let’s download some data to learn how to manipulate files. One way to do this is using the wget. With -P we specify were to download the data:\n\n#make a folder for our downloads\nmkdir downloads\n\n#download a genome from ncbi using wget\nwget -P downloads ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/002/728/275/GCA_002728275.1_ASM272827v1/GCA_002728275.1_ASM272827v1_genomic.fna.gz",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/cli/cli_basics.html#gzip-compressing-files",
    "href": "source/cli/cli_basics.html#gzip-compressing-files",
    "title": "Unix Basics",
    "section": "",
    "text": "After downloading and exploring the content of the downloads folder, you see that the file we downloaded ends with gz. This indicates that we work with a gzip-compressed file. Gzip is a tool used to compress the size of files.\nIn order to work with files, we sometimes need to de-compress them first. We can do this as follows (by using the decompress, -d argument):\n\n#decompress gz data (-d = decompress)\ngzip -d downloads/GCA_002728275.1_ASM272827v1_genomic.fna.gz\n\nConversely, if you want to compress a file, you can do this as such:\n\n#compress\ngzip downloads/GCA_002728275.1_ASM272827v1_genomic.fna\n\n\n\n\nAnother file compression you might encounter is tar:\n\nShort for Tape Archive, and sometimes referred to as tarball, is a file in the Consolidated Unix Archive format.\nThe TAR file format is common in Unix and Unix-like systems when storing data (however, we do not compress our files when using the tar files)\nTAR files are often compressed after being created and then become TGZ files, using the tgz, tar.gz, or gz extension.\n\nLet’s try to make a compressed tarball with the file we just downloaded.\n\n#decompress gz data\ngzip -d downloads/GCA_002728275.1_ASM272827v1_genomic.fna.gz\n\n#create a tar file, to_compress.tar.gz, from our downloads directory\ntar -cvzf to_compress.tar.gz downloads\n\nIf we print the content of our working directory with ls we now see an additional file, to_compress.tar.gz. We can uncompress this file into a new folder as follows:\n\n#make a folder into with to compress the data\nmkdir to_compress \n\n#decompress\ntar -xvf to_compress.tar.gz -C to_compress\n\n#check what happened\nls to_compress\n\nWe can see that there is one folder in to_compress, our downloads folder.",
    "crumbs": [
      "Getting Started",
      "Command Line Basics"
    ]
  },
  {
    "objectID": "source/classification/minimap2.html",
    "href": "source/classification/minimap2.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Minimap2 (Li 2018) is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database.\nAvailable on Crunchomics: Minimap2 version 2.24-r1122 installed\n\n\n\nIf you want to install minimap2 on your own, its best to install it via mamba:\n\n#setup new conda environment, which we name kraken2\nmamba create --name minimap2 -c bioconda minimap2\n\n\n\n\nFor detailed usage information, check out the minimap2 manual.\nInput:\n\nMinimap takes both fastq as well as fasta sequences as input\n\n\n#example of mapping input sequences against sequences of the SILVA 16S database\nminimap2 -cx map-ont -t &lt;nr_of_threads&gt; \\\n        -N 10 -K 25M \\\n        silva-ref.fasta \\\n        my_seqs.fastq \\\n        -o output.paf\n\nUsed options:\n\n-x map-ont: Run minimap optimized for Nanopore reads\n-c: output is generated in PAF format, use -aif you prefer the output in SAM format\n-N: lso retain up to -N [=10] top secondary mappings\n-K: Read -K [=25M] query bases\n…\n\nThere are many different ways to run minimap2, so check out the minimap2 manual for all options.",
    "crumbs": [
      "Welcome page",
      "Sequence classification",
      "Minimap2"
    ]
  },
  {
    "objectID": "source/classification/minimap2.html#minimap2",
    "href": "source/classification/minimap2.html#minimap2",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Minimap2 (Li 2018) is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database.\nAvailable on Crunchomics: Minimap2 version 2.24-r1122 installed\n\n\n\nIf you want to install minimap2 on your own, its best to install it via mamba:\n\n#setup new conda environment, which we name kraken2\nmamba create --name minimap2 -c bioconda minimap2\n\n\n\n\nFor detailed usage information, check out the minimap2 manual.\nInput:\n\nMinimap takes both fastq as well as fasta sequences as input\n\n\n#example of mapping input sequences against sequences of the SILVA 16S database\nminimap2 -cx map-ont -t &lt;nr_of_threads&gt; \\\n        -N 10 -K 25M \\\n        silva-ref.fasta \\\n        my_seqs.fastq \\\n        -o output.paf\n\nUsed options:\n\n-x map-ont: Run minimap optimized for Nanopore reads\n-c: output is generated in PAF format, use -aif you prefer the output in SAM format\n-N: lso retain up to -N [=10] top secondary mappings\n-K: Read -K [=25M] query bases\n…\n\nThere are many different ways to run minimap2, so check out the minimap2 manual for all options.",
    "crumbs": [
      "Welcome page",
      "Sequence classification",
      "Minimap2"
    ]
  },
  {
    "objectID": "source/R/readme.html",
    "href": "source/R/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "R is a language and environment for statistical computing and graphics and is useful to analyse computational data.\nSome useful information to get started:\n\nInstallation guide for R and RStudio\nAn R cookbook including some example files\nTutorial on data manipulation with dplyr\nTutorial on data visualization with ggplot2",
    "crumbs": [
      "Welcome page",
      "R"
    ]
  },
  {
    "objectID": "source/R/readme.html#using-r",
    "href": "source/R/readme.html#using-r",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "R is a language and environment for statistical computing and graphics and is useful to analyse computational data.\nSome useful information to get started:\n\nInstallation guide for R and RStudio\nAn R cookbook including some example files\nTutorial on data manipulation with dplyr\nTutorial on data visualization with ggplot2",
    "crumbs": [
      "Welcome page",
      "R"
    ]
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html",
    "href": "source/Qiime/qiime_cmi_tutorial.html",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Notice:\n\nThis tutorial was not written by myself but taken from QIIME tutorial. Additionally, the notes you find here were expended by copying from several spots in the QIIME documentation to explore things.\nThe code was run on my personal computer (Windows, WSL2).\nIf you follow the link for the tutorial, you will see that the tutorial is also available using the Galaxy interface and python API.\n\nOther useful resources to check out:\n\nQIIME 2 view: web-based viewer for .qza and .qzv files\nQIIME forum\nQIIME2 docs. Notice, the tutorial runs on v2021.2, which we will use for now and later update to the newest QIIME version\nQIIME2 Library. Useful to check for new plugins/functionality\nOld QIIME tutorials\nNEW QIIME tutorials\n\n\n\n\n\n\nAll files generated by QIIME 2 are either .qza or .qzv files, and these are simply zip files that store your data alongside some QIIME 2-specific metadata. You can unzip them with unzip xxx.qza\nThe .qza file extension is an abbreviation for QIIME Zipped Artifact, and the .qzv file extension is an abbreviation for QIIME Zipped Visualization. .qza files (which are often simply referred to as artifacts) are intermediary files in a QIIME 2 analysis, usually containing raw data of some sort.\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files (generally .qza and .qzv files) without requiring a QIIME installation.\nPlugins are software packages that can be developed by anyone and define actions, which are steps in an analysis workflow. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses.\nQIIME actions:\n\nA method, i.e.alpha-phylogenetic, accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts (qza file) as output.\nA visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input and generate qzv files. In contrast to a method, a visualizer produces exactly one visualization as output. An example of a QIIME 2 visualizer is the beta-group-significance action in the q2-diversity plugin.\nA pipeline can generate one or more .qza and/or .qzv as output. Pipelines are special in that they’re a type of action that can call other actions. They are often used by developers to define simplify common workflows so they can be run by users in a single step. For example, the core-metrics-phylogenetic action in the q2-diversity plugin is a Pipeline that runs both alpha-phylogenetic and beta-phylogenetic, as well as several other actions, in a single command.\n\nTypes used in QIIME 2:\n\nFile type: the format of a file used to store some data. For example, newick is a file type that is used for storing phylogenetic trees.\nData type: refer to how data is represented in a computer’s memory (i.e., RAM) while it’s actively in use.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. This is a representation of the meaning of the data. For example, two semantic types used in QIIME 2 are Phylogeny[Rooted] and Phylogeny[Unrooted], which are used to represent rooted and unrooted trees, respectively. QIIME 2 methods will describe what semantic types they take as input(s), and what semantic types they generate as output(s).\n\n\n\n\n\n\nFind out more also here.\n\nMetadata:\n\nSample metadata may include technical details, such as the DNA barcodes that were used for each sample in a multiplexed sequencing run, or descriptions of the samples, such as which subject, time point, and body site each sample came from\nFeature metadata is often a feature annotation, such as the taxonomy assigned to an amplicon sequence variant (ASV).\nQIIME 2 does not place restrictions on what types of metadata are expected to be present; there are no enforced “metadata standards”.\nthe MIxS and MIMARKS standards [1] provide recommendations for microbiome studies and may be helpful in determining what information to collect in your study. If you plan to deposit your data in a data archive (e.g. ENA or Qiita), it is also important to determine the types of metadata expected by that resource.\nIn QIIME 2, we always refer to these files as metadata files, but they are conceptually the same thing as QIIME 1 mapping files. QIIME 2 metadata files are backwards-compatible with QIIME 1 mapping files, meaning that you can use existing QIIME 1 mapping files in QIIME 2 without needing to make modifications to the file.\n\n\n\n\n\nUsually provided as TSV file (.tsv or .txt file extension) that provides data in form of rows and columns\nFirst row = non-comment, non-empty column headers containing a unique identifier for each metadata entry\nRows whose first cell begins with the pound sign (#) are interpreted as comments and may appear anywhere in the file. Comment rows are ignored by QIIME 2 and are for informational purposes only. Inline comments (i.e., comments that begin part-way through a row or at the end of a row) are not supported.\nEmpty rows (e.g. blank lines or rows consisting solely of empty cells) may appear anywhere in the file and are ignored.\nColumn 1: identifier (ID) column. This column defines the sample or feature IDs associated with your study. It is not recommended to mix sample and feature IDs in a single metadata file; keep sample and feature metadata stored in separate files. The ID column name (also referred to as the ID column header) must be:\n\nCase-insenitive (i.e., uppercase or lowercase, or a mixing of the two, is allowed): id, sampleid, sample id, sample-id, featureid feature id, feature-id\nIDs may consist of any Unicode characters, with the exception that IDs must not start with the pound sign (#), as those rows would be interpreted as comments and ignored.\nIDs cannot be empty (i.e. they must consist of at least one character).\nIDs must be unique (exact string matching is performed to detect duplicates).\nAt least one ID must be present in the file.\nIDs cannot be any of the reserved ID headers listed above.\n\nThe ID column is the first column in the metadata file, and can optionally be followed by additional columns defining metadata associated with each sample or feature ID. Metadata files are not required to have additional metadata columns, so a file containing only an ID column is a valid QIIME 2 metadata file.\nThe contents of a metadata file following the ID column and header row (excluding comments and empty lines) are referred to as the metadata values. A single metadata value, defined by an (ID, column) pair, is referred to as a cell. The following rules apply to metadata values and cells:\n\nMay consist of any Unicode characters.\nEmpty cells represent missing data. Other values such as NA are not interpreted as missing data; only the empty cell is recognized as “missing”. Note that cells consisting solely of whitespace characters are also interpreted as missing data\nIf any cell in the metadata contains leading or trailing whitespace characters (e.g. spaces, tabs), those characters will be ignored when the file is loaded.\n\n\nRecommendations for identifiers:\n\nIdentifiers should be 36 characters long or less.\nIdentifiers should contain only ASCII alphanumeric characters (i.e. in the range of [a-z], [A-Z], or [0-9]), the period (.) character, or the dash (-) character.\nNote that some bioinformatics tools may have more restrictive requirements on identifiers than the recommendations that are outlined here. For example, Illumina sample sheet identifiers cannot have . characters, while we do include those in our set of recommended characters\n[cual-id] (https://github.com/johnchase/cual-id) can be used to help create identifiers and the associated paper also includes a discussion on how to choose identifiers [2].\n\nColumn types:\n\nQIIME 2 currently supports categorical and numeric metadata columns and will automatically attempt to infer the type of each metadata column\nQIIME 2 supports an optional comment directive to allow users to explicitly state a column’s type.\nThe comment directive must appear directly below the header row. The value in the ID column in this row must be #q2:types to indicate the row is a comment directive. Subsequent cells in this row may contain the values categorical or numeric (both case-insensitive). The empty cell is also supported if you do not wish to assign a type to a column\n\nMetadata validation:\n\nQIIME 2 will automatically validate a metadata file anytime it is used. Loading your metadata in QIIME 2 will typically present only a single error at a time, which can make identifying and resolving validation issues cumbersome, especially if there are many issues with the metadata.\nSample and feature metadata files stored in Google Sheets can additionally be validated using Keemei [3]\n\n\n\n\n\nThis tutorial focuses on data reused a Compilation of longitudinal microbiota data and hospitalome from hematopoietic cell transplantation patients [4]\n\n\n\n\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help\n\n\n\n\n\n\n\nRun this if needed. If you are not using mamba yet, replace mamba with conda to use this as an alternative installer.\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n#mamba activate qiime2-2023.7\n\n\n\n\n\n#set wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/QIIME/qiime_cmi_tutorial\"\ncd $wdir \n\n#activate QIIME environment \nmamba activate qiime2-2023.7\n\n\n\n\n\n\nmkdir data\nmkdir visualizations\n\n#download metadata table\nwget \\\n  -O 'sample-metadata.tsv' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/020-metadata/sample-metadata.tsv'\n\n#prepare metadata for qiime view \nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv \\\n  --o-visualization visualizations/metadata-summ-1.qzv\n\n#download sequencing data (already demultiplexed)\nwget \\\n  -O 'data_to_import.zip' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/030-importing/data_to_import.zip'\n\nunzip -d data_to_import data_to_import.zip\n\nrm data_to_import.zip\n\n\n\n\n\n#import data into qiime\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-format CasavaOneEightSingleLanePerSampleDirFmt \\\n  --input-path data_to_import \\\n  --output-path demultiplexed-sequences.qza\n\n#generate a summary of the imported data\nqiime demux summarize \\\n  --i-data demultiplexed-sequences.qza \\\n  --o-visualization visualizations/demultiplexed-sequences-summ.qzv\n\nqiime demux: supports demultiplexing of single-end and paired-end sequence reads and visualization of sequence quality information.\n\n\n\n\nqiime tools peek  demultiplexed-sequences.qza\n\n\n\n\nIf you have reads from multiple samples in the same file, you’ll need to demultiplex your sequences.\nIf your barcodes are still in your sequences, you can use functions from the cutadapt plugin. The cutadapt demux-single method looks for barcode sequences at the beginning of your reads (5’ end) with a certain error tolerance, removes them, and returns sequence data separated by each sample. The QIIME 2 forum has a tutorial on various functions available in cutadapt, including demultiplexing. You can learn more about how cutadapt works under the hood by reading their documentation.\nNote: Currently q2-demux and q2-cutadapt do not support demultiplexing dual-barcoded paired-end sequences, but only can demultiplex with barcodes in the forward reads. So for the time being, this type of demultiplexing needs to be done outside of QIIME 2 using other tools, for example bcl2fastq.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here.\nThere are two plugins to check out:\n\nq2-demux\ncutadapt\n\n\n\n\nWhether or not you need to merge reads depends on how you plan to cluster or denoise your sequences into amplicon sequence variants (ASVs) or operational taxonomic units (OTUs). If you plan to use deblur or OTU clustering methods next, join your sequences now. If you plan to use dada2 to denoise your sequences, do not merge — dada2 performs read merging automatically after denoising each sequence.\nIf you need to merge your reads, you can use the QIIME 2 q2-vsearch plugin with the merge-pairs method.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here.\n\n\n\nIf your data contains any non-biological sequences (e.g. primers, sequencing adapters, PCR spacers, etc), you should remove these.\nThe q2-cutadapt plugin has comprehensive methods for removing non-biological sequences from paired-end or single-end data.\nIf you’re going to use DADA2 to denoise your sequences, you can remove biological sequences at the same time as you call the denoising function. All of DADA2’s denoise fuctions have some sort of –p-trim parameter you can specify to remove base pairs from the 5’ end of your reads. (Deblur does not have this functionality yet.)\n\n\n\nThe names for these steps are very descriptive:\n\nWe denoise our sequences to remove and/or correct noisy reads.\nWe dereplicate our sequences to reduce repetition and file size/memory requirements in downstream steps.\nWe cluster sequences to collapse similar sequences (e.g., those that are ≥ 97% similar to each other) into single replicate sequences. This process, also known as OTU picking, was once a common procedure, used to simultaneously dereplicate but also perform a sort of quick-and-dirty denoising procedure (to capture stochastic sequencing and PCR errors, which should be rare and similar to more abundant centroid sequences). Use denoising methods instead if you can.\n\nThe denoising methods currently available in QIIME 2 include DADA2 and Deblur. Note that deblur (and also vsearch dereplicate-sequences) should be preceded by basic quality-score-based filtering, but this is unnecessary for dada2. Both Deblur and DADA2 contain internal chimera checking methods and abundance filtering, so additional filtering should not be necessary following these methods.\nTo put it simply, these methods filter out noisy sequences, correct errors in marginal sequences (in the case of DADA2), remove chimeric sequences, remove singletons, join denoised paired-end reads (in the case of DADA2), and then dereplicate those sequences.\n\n\nThe final products of all denoising and clustering methods/workflows are a FeatureTable[Frequency] (feature table) artifact and a FeatureData[Sequence] (representative sequences) artifact. These are two of the most important artifacts in an amplicon sequencing workflow, and are used for many downstream analyses.\nMany operations on these tables can be performed with q2-feature-table.\nWant to see which sequences are associated with each feature ID? Use qiime metadata tabulate with your FeatureData[Sequence] artifact as input.\n\n\n\nQuality control or denoising of the sequence data will be performed with DADA2 [5]. DADA2 is an model-based approach for correcting amplicon errors without constructing OTUs. Can be applied to every gene of interest and is not limited to the 16S.\nThe DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).\nAlso check out this tutorial for using DADA2. One important parameter to check is truncLen to trim low quality read ends. Also check out this paper discussing quality filtering [6].\nIn QIIME the denoise_paired action in the q2-dada2 plugin. This performs quality filtering, chimera checking, and paired- end read joining.\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots and identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. Your reads must still overlap after truncation in order to merge them later!\nSome thoughts on this:\n\nReviewing the data we notice that the twenty-fifth percentile quality score drops below 30 at position 204 in the forward reads and 205 in the reverse reads. We chose to use those values for the required truncation lengths. This truncates the 3’/5’ end of the of the input sequences. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads.\nSince the first base of the reverse reads is slightly lower than those that follow, I choose to trim that first base in the reverse reads, but apply no trimming to the forward reads. This trimming is probably unnecessary here, but is useful here for illustrating how this works.\nFigaro is a tool to automatically choose the trunc_len\nChimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2\n\nDesired overlap length\n\nOften between 10 and 20 for most applications.\nDADA2 often recommends 20\nRemember: longer overlaps means more 3’ end must be kept at a cost of decreased overall sequence quality\n\nOther parameters:\n\n--p-max-ee-f/r {number}: Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]. If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads (eg. maxEE=c(2,5))\n--p-trunc-q {integeer}: Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-for trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap {INTEGER}: The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method {TEXT Choices(‘independent’, ‘pseudo’)}: he method used to pool samples for denoising. By default, the dada function processes each sample independently. However, pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method {TEXT Choices(‘consensus’, ‘none’, ‘pooled’)}: The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-n-threads {INTEGER}: default 1\n\nOutputs:\n\nThe feature table describes which amplicon sequence variants (ASVs) were observed in which samples, and how many times each ASV was observed in each sample.\nThe feature data in this case is the sequence that defines each ASV. Generate and explore the summaries of each of these files.\nSanity check: in stats-dada2 check that outside of filtering, there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nSequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the asv table.\n\n\n#denoise data\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs demultiplexed-sequences.qza \\\n  --p-trunc-len-f 204 \\\n  --p-trim-left-r 1 \\\n  --p-trunc-len-r 205 \\\n  --o-representative-sequences asv-sequences-0.qza \\\n  --o-table feature-table-0.qza \\\n  --o-denoising-stats dada2-stats.qza\n\n#generate summaries\nqiime feature-table summarize \\\n  --i-table feature-table-0.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-0-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data asv-sequences-0.qza \\\n  --o-visualization visualizations/asv-sequences-0-summ.qzv\n\nqiime metadata tabulate \\\n  --m-input-file dada2-stats.qza \\\n  --o-visualization visualizations/stats-dada2.qzv\n\nOutputs:\n\nASV table,\nthe representative sequences,\nstatistics on the procedure\n\n\n\n\n\nWe’ll next obtain a much larger feature table representing all of the samples included in the study dataset. These would take too much time to denoise in this course, so we’ll start with the feature table, sequences, and metadata provided by the authors and filter to samples that we’ll use for our analyses.\nA full description can be foun on the QIIME website\n\n#cleanup first dataset\nmkdir upstream_tutorial\nmv *qza upstream_tutorial/\n\n#get the data\nwget \\\n  -O 'feature-table.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/feature-table.qza'\n\nwget \\\n  -O 'rep-seqs.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/rep-seqs.qza'\n\n#summarize table\nqiime feature-table summarize \\\n  --i-table feature-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data rep-seqs.qza \\\n  --o-visualization visualizations/rep-seqs-summ.qzv\n\nOutput explanation:\n\nA feature is essentially any unit of observation, e.g., an OTU, a sequence variant, a gene, a metabolite, etc, and a feature table is a matrix of sample X feature abundances (the number of times each feature was observed in each sample).\nminimum frequency is 5342 - if you click the “Interactive Sample Detail” tab and scroll to the bottom, you will see that the sample with the lowest feature frequency has 5342 observations. Similarly, the sample with the highest frequency of features has 193491 total observations - meaning that many/most features were seen more than once.\nmean frequency of features (881)? Does it mean that one feature is found in average 881 times throughout all the samples\n\n\n\n\n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table autofmt-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/autofmt-table-summ.qzv\n\n#filter time window\nqiime feature-table filter-samples \\\n  --i-table autofmt-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'DayRelativeToNearestHCT BETWEEN -10 AND 70' \\\n  --o-filtered-table filtered-table-1.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\n#done to reduce runtime (so optional)\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table filtered-table-2.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-2-summ.qzv\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\nWe can also also filter tables by lists:\n\n#create imaginary list of ids to keep\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\n#filter \nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nSome examples using where:\n\n–p-where “[subject]=‘subject-1’”\n\n–p-where “[body-site] IN (‘left palm’, ‘right palm’)”\n\n–p-where “[subject]=‘subject-1’ AND [body-site]=‘gut’”\n\n–p-where “[body-site]=‘gut’ OR [reported-antibiotic-usage]=‘Yes’”\n\n–p-where “[subject]=‘subject-1’ AND NOT [body-site]=‘gut’”\n\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza\n\n\n\n\n\nTo identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods: - classify-consensus-blast and classify-consensus-vsearch are both alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained - Machine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! - Most users do not even need to follow that tutorial and perform that training step, because the lovely QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods we’ve tested based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nIn the example below we use a pre-trained Naive Bayes taxonomic classifier. This particular classifier was trained on the Greengenes 13-8 database, where sequences were trimmed to represent only the region between the 515F / 806R primers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [7].\nCheck out this study discussing reproducible sequence taxonomy reference database management [8].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [9].\n\n\n\n\n#get classifier\nwget \\\n  -O 'gg-13-8-99-nb-classifier.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/020-taxonomy/gg-13-8-99-nb-classifier.qza'\n\n#assign taxonomic info to ASV sequences\nqiime feature-classifier classify-sklearn \\\n  --i-classifier gg-13-8-99-nb-classifier.qza \\\n  --i-reads filtered-sequences-1.qza \\\n  --o-classification taxonomy.qza\n\n#generate summary\nqiime metadata tabulate \\\n  --m-input-file taxonomy.qza \\\n  --o-visualization visualizations/taxonomy.qzv\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza\n\n\n\n\nYou may have noticed when looking at feature table summaries earlier that some of the samples contained very few ASV sequences. These often represent samples which didn’t amplify or sequence well, and when we start visualizing our data low numbers of sequences can cause misleading results, because the observed composition of the sample may not be reflective of the sample’s actual composition. For this reason it can be helpful to exclude samples with low ASV sequence counts from our samples. Here, we’ll filter out samples from which we have obtained fewer than 10,000 sequences.\n\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#summarize data\nqiime feature-table summarize \\\n  --i-table filtered-table-4.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-4-summ.qzv\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-4.qza \\\n  --o-filtered-data filtered-sequences-2.qza\n\n\n\n\n\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/taxa-bar-plots-1.qzv\n\nNotice: We can also generate heatmaps with feature-table heatmap\n\n\n\n\n\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose\n\n\n\n\n\n\n\nNotice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [10]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [11].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [12]\nNormalization and microbial differential abundance strategies depend upon data characteristics [11]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible.\n\n\n\n\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)\n\n\n\nIf continuous sample metadata columns (e.g., days-since-experiment-start) are correlated with alpha diversity, we can test for those associations here. If you’re interested in performing those tests (for this data set, or for others), you can use the qiime diversity alpha-correlation command.\nWe can also analyze sample composition in the context of categorical metadata using PERMANOVA using the beta-group-significance command. The code below was taken from another test but would allow to test whether distances between samples within a group, such as samples from the same body site (e.g., gut), are more similar to each other then they are to samples from the other groups (e.g., tongue, left palm, and right palm). If you call this command with the --p-pairwise parameter, as we’ll do here, it will also perform pairwise tests that will allow you to determine which specific pairs of groups (e.g., tongue and gut) differ from one another, if any. This command can be slow to run, especially when passing --p-pairwise, since it is based on permutation tests. So, unlike the previous commands, we’ll run beta-group-significance on specific columns of metadata that we’re interested in exploring, rather than all metadata columns to which it is applicable. Here we’ll apply this to our unweighted UniFrac distances, using two sample metadata columns, as follows.\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column body-site \\\n  --o-visualization core-metrics-results/unweighted-unifrac-body-site-significance.qzv \\\n  --p-pairwise\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column subject \\\n  --o-visualization core-metrics-results/unweighted-unifrac-subject-group-significance.qzv \\\n  --p-pairwise\n\nCheck this link for an example output.\nIf continuous sample metadata are correlated with sample composition, we can use the qiime metadata distance-matrix in combination with qiime diversity mantel and qiime diversity bioenvcommands.\n\n\n\nIn order to manage the repeated measures, we will use a linear mixed-effects model. In a mixed-effects model, we combine fixed-effects (your typical linear regression coefficients) with random-effects. These random effects are some (ostensibly random) per-group coefficient which minimizes the error within that group. In our situation, we would want our random effect to be the PatientID as we can see each subject has a different baseline for richness (and we have multiple measures for each patient). By making that a random effect, we can more accurately ascribe associations to the fixed effects as we treat each sample as a “draw” from a per-group distribution.\nIt is called a random effect because the cause of the per-subject deviation from the population intercept is not known. Its source is “random” in the statistical sense. That is randomness is not introduced to the model, but is instead tolerated by it.\nThere are several ways to create a linear model with random effects, but we will be using a random-intercept, which allows for the per-subject intercept to take on a different average from the population intercept (modeling what we saw in the group-significance plot above).\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column DayRelativeToNearestHCT \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-HCT.qzv\n\nHere we see a significant association between richness and the bone marrow transplant.\nOptions:\n\n–p-state-column TEXT Metadata column containing state (time) variable information.\n–p-individual-id-column TEXT Metadata column containing IDs for individual subjects.\n–p-metric TEXT Dependent variable column name. Must be a column name located in the metadata or feature table files.\n\nWe may also be interested in the effect of the auto fecal microbiota transplant. It should be known that these are generally correlated, so choosing one model over the other will require external knowledge.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-FMT.qzv\n\nWe also see a downward trend from the FMT. Since the goal of the FMT was to ameliorate the impact of the bone marrow transplant protocol (which involves an extreme course of antibiotics) on gut health, and the timing of the FMT is related to the timing of the marrow transplant, we might deduce that the negative coefficient is primarily related to the bone marrow transplant procedure. (We can’t prove this with statistics alone however, in this case, we are using abductive reasoning).\nLooking at the log-likelihood, we also note that the HCT result is slightly better than the FMT in accounting for the loss of richness. But only slightly, if we were to perform model testing it may not prove significant.\nIn any case, we can ask a more targeted question to identify if the FMT was useful in recovering richness.\nBy adding the autoFmtGroup to our linear model, we can see if there are different slopes for the two groups, based on an interaction term.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-treatmentVScontrol.qzv\n\nHere we see that the autoFmtGroup is not on its own a significant predictor of richness, but its interaction term with Q(‘day-relative-to-fmt’) is. This implies that there are different slopes between these groups, and we note that given the coding of Q(‘day-relative-to-fmt’):autoFmtGroup[T.treatment] we have a positive coefficient which counteracts (to a degree) the negative coefficient of Q(‘day-relative-to-fmt’).\nNotice: The slopes of the regression scatterplot in this visualization are not the same fit as our mixed-effects model in the table. They are a naive OLS fit to give a sense of the situation. A proper visualization would be a partial regression plot which can condition on other terms to show some effect in “isolation”. This is not currently implemented in QIIME 2.\n\n\n\n\n#test group significance \nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-faith-pd.qzv\n\n#lme\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric faith_pd \\\n  --o-visualization visualizations/lme-faith-pd-treatmentVScontrol.qzv\n\n\n\n\nBeta diversity is between sample diversity. This is useful for answering the question, how different are these microbial communities?\nList of available indices:\n\nJaccard distance (a qualitative measure of community dissimilarity)\nBray-Curtis distance (a quantitative measure of community dissimilarity)\nunweighted UniFrac distance (a qualitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\nweighted UniFrac distance (a quantitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\n\n\nqiime diversity beta-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metric braycurtis \\\n  --p-clustering-method nj \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/braycurtis-rarefaction-plot.qzv\n\n\numap is an ordination method that can be used in place of PCoA and has been shown to better resolve differences between microbiome samples in ordination plots [13].\nLike PCoA, umap operates on distance matrices. We’ll compute this on our weighted and unweighted UniFrac distance matrices.\n\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/unweighted_unifrac_distance_matrix.qza \\\n  --o-umap uu-umap.qza\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/weighted_unifrac_distance_matrix.qza \\\n  --o-umap wu-umap.qza\n\nIn the next few steps, we’ll integrate our unweighted UniFrac umap axis 1 values, and our Faith PD, evenness, and Shannon diversity values, as metadata in visualizations. This will provide a few different ways of interpreting these values.\n\nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization expanded-metadata-summ.qzv\n\nTo see how this information can be used, let’s generate another version of our taxonomy barplots that includes these new metadata values.\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization visualizations/taxa-bar-plots-2.qzv\n\nWe’ll start by integrating these values as metadata in our ordination plots. We’ll also customize these plots in another way: in addition to plotting the ordination axes, we’ll add an explicit time axis to these plots. This is often useful for visualization patterns in ordination plots in time series studies. We’ll add an axis for week-relative-to-hct.\n\nqiime emperor plot \\\n  --i-pcoa uu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa wu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/unweighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-pcoa-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/weighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-pcoa-emperor-w-time.qzv\n\nTip:\nQIIME 2’s q2-diversity plugin provides visualizations for assessing whether microbiome composition differs across groups of independent samples (for example, individuals with a certain disease state and healthy controls) and for assessing whether differences in microbiome composition are correlated with differences in a continuous variable (for example, subjects’ body mass index). These tools assume that all samples are independent of one another, and therefore aren’t applicable to the data used in this tutorial where multiple samples are obtained from the same individual. We therefore don’t illustrate the use of these visualizations on this data, but you can learn about these approaches and view examples in the Moving Pictures tutorial. The Moving Pictures tutorial contains example data and commands, like this tutorial does, so you can experiment with generating these visualizations on your own.\n\n\n\n\nANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [14].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use.\n\n\n\n\n\nBefore applying these analyses, we’re going to perform some additional operations on the feature table that will make these analyses run quicker and make the results more interpretable.\nFirst, we are going to use the taxonomic information that we generated earlier to redefine our features as microbial genera. To do this, we group (or collapse) ASV features based on their taxonomic assignments through the genus level. This is achieved using the q2-taxa plugin’s collapse action.\n\nqiime taxa collapse \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-level 6 \\\n  --o-collapsed-table genus-table.qza\n\nThen, to focus on the genera that are likely to display the most interesting patterns over time (and to reduce the runtime of the steps that come next), we will perform even more filtering. This time we’ll apply prevalence and abudnance based filtering. Specifically, we’ll require that a genus’s abundance is at least 1% in at least 10% of the samples.\nNote: The prevalence-based filtering applied here is fairly stringent. In your own analyses you may want to experiment with relaxed settings of these parameters. Because we want the commands below to run quickly, stringent filtering is helpful for the tutorial.\n\nqiime feature-table filter-features-conditionally \\\n  --i-table genus-table.qza \\\n  --p-prevalence 0.1 \\\n  --p-abundance 0.01 \\\n  --o-filtered-table filtered-genus-table.qza\n\nFinally, we’ll convert the counts in our feature table to relative frequencies. This is required for some of the analyses that we’re about to perform.\n\nqiime feature-table relative-frequency \\\n  --i-table filtered-genus-table.qza \\\n  --o-relative-frequency-table genus-rf-table.qza\n\n\n\n\nThe first plots we’ll generate are volatility plots. We’ll generate these using two different time variables. First, we’ll plot based on week-relative-to-hct.\nThe volatility visualizer generates interactive line plots that allow us to assess how volatile a dependent variable is over a continuous, independent variable (e.g., time) in one or more groups. See also the QIIME notes\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-hct \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-1.qzv\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-2.qzv\n\n\n\n\nThe last plots we’ll generate in this section will come from a QIIME 2 pipeline called feature-volatility. These use supervised regression to identify features that are most associated with changes over time, and add plotting of those features to a volatility control chart.\nAgain, we’ll generate the same plots but using two different time variables on the x-axes. First, we’ll plot based on week-relative-to-hct.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-hct \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility-2\n\nOutputs:\n\nvolatility-plot contains an interactive feature volatility plot. This is very similar to the plots produced by the volatility visualizer described above, with a couple key differences. First, only features are viewable as “metrics” (plotted on the y-axis). Second, feature metadata (feature importances and descriptive statistics) are plotted as bar charts below the volatility plot. The relative frequencies of different features can be plotted in the volatility chart by either selecting the “metric” selection tool, or by clicking on one of the bars in the bar plot. This makes it convenient to select features for viewing based on importance or other feature metadata. By default, the most important feature is plotted in the volatility plot when the visualization is viewed. Different feature metadata can be selected and sorted using the control panel to the right of the bar charts. Most of these should be self-explanatory, except for “cumulative average change” (the cumulative magnitude of change, both positive and negative, across states, and averaged across samples at each state), and “net average change” (positive and negative “cumulative average change” is summed to determine whether a feature increased or decreased in abundance between baseline and end of study).\naccuracy-results display the predictive accuracy of the regression model. This is important to view, as important features are meaningless if the model is inaccurate. See the sample classifier tutorial for more description of regressor accuracy results.\nfeature-importance contains the importance scores of all features. This is viewable in the feature volatility plot, but this artifact is nonetheless output for convenience. See the sample classifier tutorial for more description of feature importance scores.\nfiltered-table is a FeatureTable[RelativeFrequency] artifact containing only important features. This is output for convenience.\nsample-estimator contains the trained sample regressor. This is output for convenience, just in case you plan to regress additional samples. See the sample classifier tutorial for more description of the SampleEstimator type.\n\n\n\n\n\nIf you’re a veteran microbiome scientist and don’t want to use QIIME 2 for your analyses, you can extract your feature table and sequences from the artifact using the export tool. While export only outputs the data, the extract tool allows you to also extract other metadata such as the citations, provenance etc.\nNote that this places generically named files (e.g. feature-table.txt) into the output directory, so you may want to immediately rename the files to something more information (or somehow ensure that they stay in their original directory)!\nYou can also use the handy qiime2R package to import QIIME 2 artifacts directly into R."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#introductory-notes",
    "href": "source/Qiime/qiime_cmi_tutorial.html#introductory-notes",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "All files generated by QIIME 2 are either .qza or .qzv files, and these are simply zip files that store your data alongside some QIIME 2-specific metadata. You can unzip them with unzip xxx.qza\nThe .qza file extension is an abbreviation for QIIME Zipped Artifact, and the .qzv file extension is an abbreviation for QIIME Zipped Visualization. .qza files (which are often simply referred to as artifacts) are intermediary files in a QIIME 2 analysis, usually containing raw data of some sort.\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files (generally .qza and .qzv files) without requiring a QIIME installation.\nPlugins are software packages that can be developed by anyone and define actions, which are steps in an analysis workflow. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses.\nQIIME actions:\n\nA method, i.e.alpha-phylogenetic, accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts (qza file) as output.\nA visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input and generate qzv files. In contrast to a method, a visualizer produces exactly one visualization as output. An example of a QIIME 2 visualizer is the beta-group-significance action in the q2-diversity plugin.\nA pipeline can generate one or more .qza and/or .qzv as output. Pipelines are special in that they’re a type of action that can call other actions. They are often used by developers to define simplify common workflows so they can be run by users in a single step. For example, the core-metrics-phylogenetic action in the q2-diversity plugin is a Pipeline that runs both alpha-phylogenetic and beta-phylogenetic, as well as several other actions, in a single command.\n\nTypes used in QIIME 2:\n\nFile type: the format of a file used to store some data. For example, newick is a file type that is used for storing phylogenetic trees.\nData type: refer to how data is represented in a computer’s memory (i.e., RAM) while it’s actively in use.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. This is a representation of the meaning of the data. For example, two semantic types used in QIIME 2 are Phylogeny[Rooted] and Phylogeny[Unrooted], which are used to represent rooted and unrooted trees, respectively. QIIME 2 methods will describe what semantic types they take as input(s), and what semantic types they generate as output(s)."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#metadata",
    "href": "source/Qiime/qiime_cmi_tutorial.html#metadata",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Find out more also here.\n\nMetadata:\n\nSample metadata may include technical details, such as the DNA barcodes that were used for each sample in a multiplexed sequencing run, or descriptions of the samples, such as which subject, time point, and body site each sample came from\nFeature metadata is often a feature annotation, such as the taxonomy assigned to an amplicon sequence variant (ASV).\nQIIME 2 does not place restrictions on what types of metadata are expected to be present; there are no enforced “metadata standards”.\nthe MIxS and MIMARKS standards [1] provide recommendations for microbiome studies and may be helpful in determining what information to collect in your study. If you plan to deposit your data in a data archive (e.g. ENA or Qiita), it is also important to determine the types of metadata expected by that resource.\nIn QIIME 2, we always refer to these files as metadata files, but they are conceptually the same thing as QIIME 1 mapping files. QIIME 2 metadata files are backwards-compatible with QIIME 1 mapping files, meaning that you can use existing QIIME 1 mapping files in QIIME 2 without needing to make modifications to the file.\n\n\n\n\n\nUsually provided as TSV file (.tsv or .txt file extension) that provides data in form of rows and columns\nFirst row = non-comment, non-empty column headers containing a unique identifier for each metadata entry\nRows whose first cell begins with the pound sign (#) are interpreted as comments and may appear anywhere in the file. Comment rows are ignored by QIIME 2 and are for informational purposes only. Inline comments (i.e., comments that begin part-way through a row or at the end of a row) are not supported.\nEmpty rows (e.g. blank lines or rows consisting solely of empty cells) may appear anywhere in the file and are ignored.\nColumn 1: identifier (ID) column. This column defines the sample or feature IDs associated with your study. It is not recommended to mix sample and feature IDs in a single metadata file; keep sample and feature metadata stored in separate files. The ID column name (also referred to as the ID column header) must be:\n\nCase-insenitive (i.e., uppercase or lowercase, or a mixing of the two, is allowed): id, sampleid, sample id, sample-id, featureid feature id, feature-id\nIDs may consist of any Unicode characters, with the exception that IDs must not start with the pound sign (#), as those rows would be interpreted as comments and ignored.\nIDs cannot be empty (i.e. they must consist of at least one character).\nIDs must be unique (exact string matching is performed to detect duplicates).\nAt least one ID must be present in the file.\nIDs cannot be any of the reserved ID headers listed above.\n\nThe ID column is the first column in the metadata file, and can optionally be followed by additional columns defining metadata associated with each sample or feature ID. Metadata files are not required to have additional metadata columns, so a file containing only an ID column is a valid QIIME 2 metadata file.\nThe contents of a metadata file following the ID column and header row (excluding comments and empty lines) are referred to as the metadata values. A single metadata value, defined by an (ID, column) pair, is referred to as a cell. The following rules apply to metadata values and cells:\n\nMay consist of any Unicode characters.\nEmpty cells represent missing data. Other values such as NA are not interpreted as missing data; only the empty cell is recognized as “missing”. Note that cells consisting solely of whitespace characters are also interpreted as missing data\nIf any cell in the metadata contains leading or trailing whitespace characters (e.g. spaces, tabs), those characters will be ignored when the file is loaded.\n\n\nRecommendations for identifiers:\n\nIdentifiers should be 36 characters long or less.\nIdentifiers should contain only ASCII alphanumeric characters (i.e. in the range of [a-z], [A-Z], or [0-9]), the period (.) character, or the dash (-) character.\nNote that some bioinformatics tools may have more restrictive requirements on identifiers than the recommendations that are outlined here. For example, Illumina sample sheet identifiers cannot have . characters, while we do include those in our set of recommended characters\n[cual-id] (https://github.com/johnchase/cual-id) can be used to help create identifiers and the associated paper also includes a discussion on how to choose identifiers [2].\n\nColumn types:\n\nQIIME 2 currently supports categorical and numeric metadata columns and will automatically attempt to infer the type of each metadata column\nQIIME 2 supports an optional comment directive to allow users to explicitly state a column’s type.\nThe comment directive must appear directly below the header row. The value in the ID column in this row must be #q2:types to indicate the row is a comment directive. Subsequent cells in this row may contain the values categorical or numeric (both case-insensitive). The empty cell is also supported if you do not wish to assign a type to a column\n\nMetadata validation:\n\nQIIME 2 will automatically validate a metadata file anytime it is used. Loading your metadata in QIIME 2 will typically present only a single error at a time, which can make identifying and resolving validation issues cumbersome, especially if there are many issues with the metadata.\nSample and feature metadata files stored in Google Sheets can additionally be validated using Keemei [3]\n\n\n\n\n\nThis tutorial focuses on data reused a Compilation of longitudinal microbiota data and hospitalome from hematopoietic cell transplantation patients [4]\n\n\n\n\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#setup",
    "href": "source/Qiime/qiime_cmi_tutorial.html#setup",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Run this if needed. If you are not using mamba yet, replace mamba with conda to use this as an alternative installer.\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n#mamba activate qiime2-2023.7\n\n\n\n\n\n#set wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/QIIME/qiime_cmi_tutorial\"\ncd $wdir \n\n#activate QIIME environment \nmamba activate qiime2-2023.7"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#download-data",
    "href": "source/Qiime/qiime_cmi_tutorial.html#download-data",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "mkdir data\nmkdir visualizations\n\n#download metadata table\nwget \\\n  -O 'sample-metadata.tsv' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/020-metadata/sample-metadata.tsv'\n\n#prepare metadata for qiime view \nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv \\\n  --o-visualization visualizations/metadata-summ-1.qzv\n\n#download sequencing data (already demultiplexed)\nwget \\\n  -O 'data_to_import.zip' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/030-importing/data_to_import.zip'\n\nunzip -d data_to_import data_to_import.zip\n\nrm data_to_import.zip"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-import-import-data-into-qiime",
    "href": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-import-import-data-into-qiime",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "#import data into qiime\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-format CasavaOneEightSingleLanePerSampleDirFmt \\\n  --input-path data_to_import \\\n  --output-path demultiplexed-sequences.qza\n\n#generate a summary of the imported data\nqiime demux summarize \\\n  --i-data demultiplexed-sequences.qza \\\n  --o-visualization visualizations/demultiplexed-sequences-summ.qzv\n\nqiime demux: supports demultiplexing of single-end and paired-end sequence reads and visualization of sequence quality information."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-peek-check-artifact-format",
    "href": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-peek-check-artifact-format",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "qiime tools peek  demultiplexed-sequences.qza"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#demultiplexing",
    "href": "source/Qiime/qiime_cmi_tutorial.html#demultiplexing",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "If you have reads from multiple samples in the same file, you’ll need to demultiplex your sequences.\nIf your barcodes are still in your sequences, you can use functions from the cutadapt plugin. The cutadapt demux-single method looks for barcode sequences at the beginning of your reads (5’ end) with a certain error tolerance, removes them, and returns sequence data separated by each sample. The QIIME 2 forum has a tutorial on various functions available in cutadapt, including demultiplexing. You can learn more about how cutadapt works under the hood by reading their documentation.\nNote: Currently q2-demux and q2-cutadapt do not support demultiplexing dual-barcoded paired-end sequences, but only can demultiplex with barcodes in the forward reads. So for the time being, this type of demultiplexing needs to be done outside of QIIME 2 using other tools, for example bcl2fastq.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here.\nThere are two plugins to check out:\n\nq2-demux\ncutadapt"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#merging-reads",
    "href": "source/Qiime/qiime_cmi_tutorial.html#merging-reads",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Whether or not you need to merge reads depends on how you plan to cluster or denoise your sequences into amplicon sequence variants (ASVs) or operational taxonomic units (OTUs). If you plan to use deblur or OTU clustering methods next, join your sequences now. If you plan to use dada2 to denoise your sequences, do not merge — dada2 performs read merging automatically after denoising each sequence.\nIf you need to merge your reads, you can use the QIIME 2 q2-vsearch plugin with the merge-pairs method.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#removing-non-biological-sequences",
    "href": "source/Qiime/qiime_cmi_tutorial.html#removing-non-biological-sequences",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "If your data contains any non-biological sequences (e.g. primers, sequencing adapters, PCR spacers, etc), you should remove these.\nThe q2-cutadapt plugin has comprehensive methods for removing non-biological sequences from paired-end or single-end data.\nIf you’re going to use DADA2 to denoise your sequences, you can remove biological sequences at the same time as you call the denoising function. All of DADA2’s denoise fuctions have some sort of –p-trim parameter you can specify to remove base pairs from the 5’ end of your reads. (Deblur does not have this functionality yet.)"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#denoising-and-clustering",
    "href": "source/Qiime/qiime_cmi_tutorial.html#denoising-and-clustering",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "The names for these steps are very descriptive:\n\nWe denoise our sequences to remove and/or correct noisy reads.\nWe dereplicate our sequences to reduce repetition and file size/memory requirements in downstream steps.\nWe cluster sequences to collapse similar sequences (e.g., those that are ≥ 97% similar to each other) into single replicate sequences. This process, also known as OTU picking, was once a common procedure, used to simultaneously dereplicate but also perform a sort of quick-and-dirty denoising procedure (to capture stochastic sequencing and PCR errors, which should be rare and similar to more abundant centroid sequences). Use denoising methods instead if you can.\n\nThe denoising methods currently available in QIIME 2 include DADA2 and Deblur. Note that deblur (and also vsearch dereplicate-sequences) should be preceded by basic quality-score-based filtering, but this is unnecessary for dada2. Both Deblur and DADA2 contain internal chimera checking methods and abundance filtering, so additional filtering should not be necessary following these methods.\nTo put it simply, these methods filter out noisy sequences, correct errors in marginal sequences (in the case of DADA2), remove chimeric sequences, remove singletons, join denoised paired-end reads (in the case of DADA2), and then dereplicate those sequences.\n\n\nThe final products of all denoising and clustering methods/workflows are a FeatureTable[Frequency] (feature table) artifact and a FeatureData[Sequence] (representative sequences) artifact. These are two of the most important artifacts in an amplicon sequencing workflow, and are used for many downstream analyses.\nMany operations on these tables can be performed with q2-feature-table.\nWant to see which sequences are associated with each feature ID? Use qiime metadata tabulate with your FeatureData[Sequence] artifact as input.\n\n\n\nQuality control or denoising of the sequence data will be performed with DADA2 [5]. DADA2 is an model-based approach for correcting amplicon errors without constructing OTUs. Can be applied to every gene of interest and is not limited to the 16S.\nThe DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).\nAlso check out this tutorial for using DADA2. One important parameter to check is truncLen to trim low quality read ends. Also check out this paper discussing quality filtering [6].\nIn QIIME the denoise_paired action in the q2-dada2 plugin. This performs quality filtering, chimera checking, and paired- end read joining.\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots and identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. Your reads must still overlap after truncation in order to merge them later!\nSome thoughts on this:\n\nReviewing the data we notice that the twenty-fifth percentile quality score drops below 30 at position 204 in the forward reads and 205 in the reverse reads. We chose to use those values for the required truncation lengths. This truncates the 3’/5’ end of the of the input sequences. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads.\nSince the first base of the reverse reads is slightly lower than those that follow, I choose to trim that first base in the reverse reads, but apply no trimming to the forward reads. This trimming is probably unnecessary here, but is useful here for illustrating how this works.\nFigaro is a tool to automatically choose the trunc_len\nChimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2\n\nDesired overlap length\n\nOften between 10 and 20 for most applications.\nDADA2 often recommends 20\nRemember: longer overlaps means more 3’ end must be kept at a cost of decreased overall sequence quality\n\nOther parameters:\n\n--p-max-ee-f/r {number}: Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]. If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads (eg. maxEE=c(2,5))\n--p-trunc-q {integeer}: Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-for trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap {INTEGER}: The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method {TEXT Choices(‘independent’, ‘pseudo’)}: he method used to pool samples for denoising. By default, the dada function processes each sample independently. However, pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method {TEXT Choices(‘consensus’, ‘none’, ‘pooled’)}: The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-n-threads {INTEGER}: default 1\n\nOutputs:\n\nThe feature table describes which amplicon sequence variants (ASVs) were observed in which samples, and how many times each ASV was observed in each sample.\nThe feature data in this case is the sequence that defines each ASV. Generate and explore the summaries of each of these files.\nSanity check: in stats-dada2 check that outside of filtering, there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nSequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the asv table.\n\n\n#denoise data\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs demultiplexed-sequences.qza \\\n  --p-trunc-len-f 204 \\\n  --p-trim-left-r 1 \\\n  --p-trunc-len-r 205 \\\n  --o-representative-sequences asv-sequences-0.qza \\\n  --o-table feature-table-0.qza \\\n  --o-denoising-stats dada2-stats.qza\n\n#generate summaries\nqiime feature-table summarize \\\n  --i-table feature-table-0.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-0-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data asv-sequences-0.qza \\\n  --o-visualization visualizations/asv-sequences-0-summ.qzv\n\nqiime metadata tabulate \\\n  --m-input-file dada2-stats.qza \\\n  --o-visualization visualizations/stats-dada2.qzv\n\nOutputs:\n\nASV table,\nthe representative sequences,\nstatistics on the procedure"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#filtering-the-feature-table",
    "href": "source/Qiime/qiime_cmi_tutorial.html#filtering-the-feature-table",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "We’ll next obtain a much larger feature table representing all of the samples included in the study dataset. These would take too much time to denoise in this course, so we’ll start with the feature table, sequences, and metadata provided by the authors and filter to samples that we’ll use for our analyses.\nA full description can be foun on the QIIME website\n\n#cleanup first dataset\nmkdir upstream_tutorial\nmv *qza upstream_tutorial/\n\n#get the data\nwget \\\n  -O 'feature-table.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/feature-table.qza'\n\nwget \\\n  -O 'rep-seqs.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/rep-seqs.qza'\n\n#summarize table\nqiime feature-table summarize \\\n  --i-table feature-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data rep-seqs.qza \\\n  --o-visualization visualizations/rep-seqs-summ.qzv\n\nOutput explanation:\n\nA feature is essentially any unit of observation, e.g., an OTU, a sequence variant, a gene, a metabolite, etc, and a feature table is a matrix of sample X feature abundances (the number of times each feature was observed in each sample).\nminimum frequency is 5342 - if you click the “Interactive Sample Detail” tab and scroll to the bottom, you will see that the sample with the lowest feature frequency has 5342 observations. Similarly, the sample with the highest frequency of features has 193491 total observations - meaning that many/most features were seen more than once.\nmean frequency of features (881)? Does it mean that one feature is found in average 881 times throughout all the samples\n\n\n\n\n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table autofmt-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/autofmt-table-summ.qzv\n\n#filter time window\nqiime feature-table filter-samples \\\n  --i-table autofmt-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'DayRelativeToNearestHCT BETWEEN -10 AND 70' \\\n  --o-filtered-table filtered-table-1.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\n#done to reduce runtime (so optional)\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table filtered-table-2.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-2-summ.qzv\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\nWe can also also filter tables by lists:\n\n#create imaginary list of ids to keep\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\n#filter \nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nSome examples using where:\n\n–p-where “[subject]=‘subject-1’”\n\n–p-where “[body-site] IN (‘left palm’, ‘right palm’)”\n\n–p-where “[subject]=‘subject-1’ AND [body-site]=‘gut’”\n\n–p-where “[body-site]=‘gut’ OR [reported-antibiotic-usage]=‘Yes’”\n\n–p-where “[subject]=‘subject-1’ AND NOT [body-site]=‘gut’”\n\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#taxonomic-annotations",
    "href": "source/Qiime/qiime_cmi_tutorial.html#taxonomic-annotations",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "To identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods: - classify-consensus-blast and classify-consensus-vsearch are both alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained - Machine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! - Most users do not even need to follow that tutorial and perform that training step, because the lovely QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods we’ve tested based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nIn the example below we use a pre-trained Naive Bayes taxonomic classifier. This particular classifier was trained on the Greengenes 13-8 database, where sequences were trimmed to represent only the region between the 515F / 806R primers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [7].\nCheck out this study discussing reproducible sequence taxonomy reference database management [8].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [9].\n\n\n\n\n#get classifier\nwget \\\n  -O 'gg-13-8-99-nb-classifier.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/020-taxonomy/gg-13-8-99-nb-classifier.qza'\n\n#assign taxonomic info to ASV sequences\nqiime feature-classifier classify-sklearn \\\n  --i-classifier gg-13-8-99-nb-classifier.qza \\\n  --i-reads filtered-sequences-1.qza \\\n  --o-classification taxonomy.qza\n\n#generate summary\nqiime metadata tabulate \\\n  --m-input-file taxonomy.qza \\\n  --o-visualization visualizations/taxonomy.qzv\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza\n\n\n\n\nYou may have noticed when looking at feature table summaries earlier that some of the samples contained very few ASV sequences. These often represent samples which didn’t amplify or sequence well, and when we start visualizing our data low numbers of sequences can cause misleading results, because the observed composition of the sample may not be reflective of the sample’s actual composition. For this reason it can be helpful to exclude samples with low ASV sequence counts from our samples. Here, we’ll filter out samples from which we have obtained fewer than 10,000 sequences.\n\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#summarize data\nqiime feature-table summarize \\\n  --i-table filtered-table-4.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-4-summ.qzv\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-4.qza \\\n  --o-filtered-data filtered-sequences-2.qza"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#generate-taxonomic-composition-barplots",
    "href": "source/Qiime/qiime_cmi_tutorial.html#generate-taxonomic-composition-barplots",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "qiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/taxa-bar-plots-1.qzv\n\nNotice: We can also generate heatmaps with feature-table heatmap"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#phylogenetic-tree-construction",
    "href": "source/Qiime/qiime_cmi_tutorial.html#phylogenetic-tree-construction",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "One advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "href": "source/Qiime/qiime_cmi_tutorial.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Notice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [10]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [11].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [12]\nNormalization and microbial differential abundance strategies depend upon data characteristics [11]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#computing-diversity-metrics",
    "href": "source/Qiime/qiime_cmi_tutorial.html#computing-diversity-metrics",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "core-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)\n\n\n\nIf continuous sample metadata columns (e.g., days-since-experiment-start) are correlated with alpha diversity, we can test for those associations here. If you’re interested in performing those tests (for this data set, or for others), you can use the qiime diversity alpha-correlation command.\nWe can also analyze sample composition in the context of categorical metadata using PERMANOVA using the beta-group-significance command. The code below was taken from another test but would allow to test whether distances between samples within a group, such as samples from the same body site (e.g., gut), are more similar to each other then they are to samples from the other groups (e.g., tongue, left palm, and right palm). If you call this command with the --p-pairwise parameter, as we’ll do here, it will also perform pairwise tests that will allow you to determine which specific pairs of groups (e.g., tongue and gut) differ from one another, if any. This command can be slow to run, especially when passing --p-pairwise, since it is based on permutation tests. So, unlike the previous commands, we’ll run beta-group-significance on specific columns of metadata that we’re interested in exploring, rather than all metadata columns to which it is applicable. Here we’ll apply this to our unweighted UniFrac distances, using two sample metadata columns, as follows.\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column body-site \\\n  --o-visualization core-metrics-results/unweighted-unifrac-body-site-significance.qzv \\\n  --p-pairwise\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column subject \\\n  --o-visualization core-metrics-results/unweighted-unifrac-subject-group-significance.qzv \\\n  --p-pairwise\n\nCheck this link for an example output.\nIf continuous sample metadata are correlated with sample composition, we can use the qiime metadata distance-matrix in combination with qiime diversity mantel and qiime diversity bioenvcommands.\n\n\n\nIn order to manage the repeated measures, we will use a linear mixed-effects model. In a mixed-effects model, we combine fixed-effects (your typical linear regression coefficients) with random-effects. These random effects are some (ostensibly random) per-group coefficient which minimizes the error within that group. In our situation, we would want our random effect to be the PatientID as we can see each subject has a different baseline for richness (and we have multiple measures for each patient). By making that a random effect, we can more accurately ascribe associations to the fixed effects as we treat each sample as a “draw” from a per-group distribution.\nIt is called a random effect because the cause of the per-subject deviation from the population intercept is not known. Its source is “random” in the statistical sense. That is randomness is not introduced to the model, but is instead tolerated by it.\nThere are several ways to create a linear model with random effects, but we will be using a random-intercept, which allows for the per-subject intercept to take on a different average from the population intercept (modeling what we saw in the group-significance plot above).\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column DayRelativeToNearestHCT \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-HCT.qzv\n\nHere we see a significant association between richness and the bone marrow transplant.\nOptions:\n\n–p-state-column TEXT Metadata column containing state (time) variable information.\n–p-individual-id-column TEXT Metadata column containing IDs for individual subjects.\n–p-metric TEXT Dependent variable column name. Must be a column name located in the metadata or feature table files.\n\nWe may also be interested in the effect of the auto fecal microbiota transplant. It should be known that these are generally correlated, so choosing one model over the other will require external knowledge.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-FMT.qzv\n\nWe also see a downward trend from the FMT. Since the goal of the FMT was to ameliorate the impact of the bone marrow transplant protocol (which involves an extreme course of antibiotics) on gut health, and the timing of the FMT is related to the timing of the marrow transplant, we might deduce that the negative coefficient is primarily related to the bone marrow transplant procedure. (We can’t prove this with statistics alone however, in this case, we are using abductive reasoning).\nLooking at the log-likelihood, we also note that the HCT result is slightly better than the FMT in accounting for the loss of richness. But only slightly, if we were to perform model testing it may not prove significant.\nIn any case, we can ask a more targeted question to identify if the FMT was useful in recovering richness.\nBy adding the autoFmtGroup to our linear model, we can see if there are different slopes for the two groups, based on an interaction term.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-treatmentVScontrol.qzv\n\nHere we see that the autoFmtGroup is not on its own a significant predictor of richness, but its interaction term with Q(‘day-relative-to-fmt’) is. This implies that there are different slopes between these groups, and we note that given the coding of Q(‘day-relative-to-fmt’):autoFmtGroup[T.treatment] we have a positive coefficient which counteracts (to a degree) the negative coefficient of Q(‘day-relative-to-fmt’).\nNotice: The slopes of the regression scatterplot in this visualization are not the same fit as our mixed-effects model in the table. They are a naive OLS fit to give a sense of the situation. A proper visualization would be a partial regression plot which can condition on other terms to show some effect in “isolation”. This is not currently implemented in QIIME 2.\n\n\n\n\n#test group significance \nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-faith-pd.qzv\n\n#lme\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric faith_pd \\\n  --o-visualization visualizations/lme-faith-pd-treatmentVScontrol.qzv\n\n\n\n\nBeta diversity is between sample diversity. This is useful for answering the question, how different are these microbial communities?\nList of available indices:\n\nJaccard distance (a qualitative measure of community dissimilarity)\nBray-Curtis distance (a quantitative measure of community dissimilarity)\nunweighted UniFrac distance (a qualitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\nweighted UniFrac distance (a quantitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\n\n\nqiime diversity beta-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metric braycurtis \\\n  --p-clustering-method nj \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/braycurtis-rarefaction-plot.qzv\n\n\numap is an ordination method that can be used in place of PCoA and has been shown to better resolve differences between microbiome samples in ordination plots [13].\nLike PCoA, umap operates on distance matrices. We’ll compute this on our weighted and unweighted UniFrac distance matrices.\n\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/unweighted_unifrac_distance_matrix.qza \\\n  --o-umap uu-umap.qza\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/weighted_unifrac_distance_matrix.qza \\\n  --o-umap wu-umap.qza\n\nIn the next few steps, we’ll integrate our unweighted UniFrac umap axis 1 values, and our Faith PD, evenness, and Shannon diversity values, as metadata in visualizations. This will provide a few different ways of interpreting these values.\n\nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization expanded-metadata-summ.qzv\n\nTo see how this information can be used, let’s generate another version of our taxonomy barplots that includes these new metadata values.\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization visualizations/taxa-bar-plots-2.qzv\n\nWe’ll start by integrating these values as metadata in our ordination plots. We’ll also customize these plots in another way: in addition to plotting the ordination axes, we’ll add an explicit time axis to these plots. This is often useful for visualization patterns in ordination plots in time series studies. We’ll add an axis for week-relative-to-hct.\n\nqiime emperor plot \\\n  --i-pcoa uu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa wu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/unweighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-pcoa-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/weighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-pcoa-emperor-w-time.qzv\n\nTip:\nQIIME 2’s q2-diversity plugin provides visualizations for assessing whether microbiome composition differs across groups of independent samples (for example, individuals with a certain disease state and healthy controls) and for assessing whether differences in microbiome composition are correlated with differences in a continuous variable (for example, subjects’ body mass index). These tools assume that all samples are independent of one another, and therefore aren’t applicable to the data used in this tutorial where multiple samples are obtained from the same individual. We therefore don’t illustrate the use of these visualizations on this data, but you can learn about these approaches and view examples in the Moving Pictures tutorial. The Moving Pictures tutorial contains example data and commands, like this tutorial does, so you can experiment with generating these visualizations on your own."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#differential-abundance-testing",
    "href": "source/Qiime/qiime_cmi_tutorial.html#differential-abundance-testing",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "ANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [14].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#longitudinal-microbiome-analysis",
    "href": "source/Qiime/qiime_cmi_tutorial.html#longitudinal-microbiome-analysis",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "Before applying these analyses, we’re going to perform some additional operations on the feature table that will make these analyses run quicker and make the results more interpretable.\nFirst, we are going to use the taxonomic information that we generated earlier to redefine our features as microbial genera. To do this, we group (or collapse) ASV features based on their taxonomic assignments through the genus level. This is achieved using the q2-taxa plugin’s collapse action.\n\nqiime taxa collapse \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-level 6 \\\n  --o-collapsed-table genus-table.qza\n\nThen, to focus on the genera that are likely to display the most interesting patterns over time (and to reduce the runtime of the steps that come next), we will perform even more filtering. This time we’ll apply prevalence and abudnance based filtering. Specifically, we’ll require that a genus’s abundance is at least 1% in at least 10% of the samples.\nNote: The prevalence-based filtering applied here is fairly stringent. In your own analyses you may want to experiment with relaxed settings of these parameters. Because we want the commands below to run quickly, stringent filtering is helpful for the tutorial.\n\nqiime feature-table filter-features-conditionally \\\n  --i-table genus-table.qza \\\n  --p-prevalence 0.1 \\\n  --p-abundance 0.01 \\\n  --o-filtered-table filtered-genus-table.qza\n\nFinally, we’ll convert the counts in our feature table to relative frequencies. This is required for some of the analyses that we’re about to perform.\n\nqiime feature-table relative-frequency \\\n  --i-table filtered-genus-table.qza \\\n  --o-relative-frequency-table genus-rf-table.qza\n\n\n\n\nThe first plots we’ll generate are volatility plots. We’ll generate these using two different time variables. First, we’ll plot based on week-relative-to-hct.\nThe volatility visualizer generates interactive line plots that allow us to assess how volatile a dependent variable is over a continuous, independent variable (e.g., time) in one or more groups. See also the QIIME notes\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-hct \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-1.qzv\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-2.qzv\n\n\n\n\nThe last plots we’ll generate in this section will come from a QIIME 2 pipeline called feature-volatility. These use supervised regression to identify features that are most associated with changes over time, and add plotting of those features to a volatility control chart.\nAgain, we’ll generate the same plots but using two different time variables on the x-axes. First, we’ll plot based on week-relative-to-hct.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-hct \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility-2\n\nOutputs:\n\nvolatility-plot contains an interactive feature volatility plot. This is very similar to the plots produced by the volatility visualizer described above, with a couple key differences. First, only features are viewable as “metrics” (plotted on the y-axis). Second, feature metadata (feature importances and descriptive statistics) are plotted as bar charts below the volatility plot. The relative frequencies of different features can be plotted in the volatility chart by either selecting the “metric” selection tool, or by clicking on one of the bars in the bar plot. This makes it convenient to select features for viewing based on importance or other feature metadata. By default, the most important feature is plotted in the volatility plot when the visualization is viewed. Different feature metadata can be selected and sorted using the control panel to the right of the bar charts. Most of these should be self-explanatory, except for “cumulative average change” (the cumulative magnitude of change, both positive and negative, across states, and averaged across samples at each state), and “net average change” (positive and negative “cumulative average change” is summed to determine whether a feature increased or decreased in abundance between baseline and end of study).\naccuracy-results display the predictive accuracy of the regression model. This is important to view, as important features are meaningless if the model is inaccurate. See the sample classifier tutorial for more description of regressor accuracy results.\nfeature-importance contains the importance scores of all features. This is viewable in the feature volatility plot, but this artifact is nonetheless output for convenience. See the sample classifier tutorial for more description of feature importance scores.\nfiltered-table is a FeatureTable[RelativeFrequency] artifact containing only important features. This is output for convenience.\nsample-estimator contains the trained sample regressor. This is output for convenience, just in case you plan to regress additional samples. See the sample classifier tutorial for more description of the SampleEstimator type."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-export-export-data",
    "href": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-export-export-data",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "",
    "text": "If you’re a veteran microbiome scientist and don’t want to use QIIME 2 for your analyses, you can extract your feature table and sequences from the artifact using the export tool. While export only outputs the data, the extract tool allows you to also extract other metadata such as the citations, provenance etc.\nNote that this places generically named files (e.g. feature-table.txt) into the output directory, so you may want to immediately rename the files to something more information (or somehow ensure that they stay in their original directory)!\nYou can also use the handy qiime2R package to import QIIME 2 artifacts directly into R."
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "These tutorial closely follows the tutorial from IBEDs former bioinformatician, Evelyn Jongepier. A large part of the text found here was taken directly from this tutorial but extended as was seen fit and updated to work with a newer QIIME version.\nA test data set is provided with this tutorial and consists of already demultiplexed sequencing data. The files we are working with are:\n\nThe forward, R1, reads\nThe reverse, R2, reads\nThe metadata file describing what kind of data we work with (in our case that is simply a sample ID)\n\nOther comments on the files:\n\nThe barcodes used are ~20 bp long\nEach read is 250 bp long\nThe primers used are called 515F and 926R and target the V4 region of the 16S SSU rRNA resulting in a fragment length or amplicon size of 411 bp\nWhen removing the primer, we have a target region of around 371 bp\nHow to calculating whether or not there is an overlap between our forward and reverse read: (length of forward read) + (length of reverse read) − (length of amplicon) = length of overlap\n\n250 + 250 - 411 = ~90 bp overlap\n\n\n\n\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (i.e. provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. Semantic types enable QIIME 2 to identify artifacts that are suitable inputs to an analysis. For example, if an analysis expects a distance matrix as input, QIIME 2 can determine which artifacts have a distance matrix semantic type and prevent incompatible artifacts from being used in the analysis (e.g. an artifact representing a phylogenetic tree).\nPlugins are software packages that can be developed by anyone. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses. A method accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts as output. A visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input. In contrast to a method, a visualizer produces exactly one visualization as output.\n\n\n\n\n\nWe can install a QIIME 2 environment with with mamba:\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n\nIf you run into problem, have a look at QIIME 2 installation guide.\n\n\n\nNext, we download the example files for this tutorial. The folder contains:\n\nDe-multiplexed sequence fastq files in the data folder\nA manifest.csv file that lists the sampleID, sample location and read orientation in the data folder\nFiles generated during the workflow\n\n\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\ncd metabarcoding-qiime2-datapackage-v2021.06.2\n\n\n\n\nNotice:\n\nChange the path stored in wdir to wherever you downloaded the data\nSetting an environmental variable to your working directory is not needed but useful to have if you resume an analysis after closing the terminal or simply to remind yourself where you analysed your data\n\n\n#set working environment\nwdir=\"&lt;path_to_your_downloaded_folder&gt;/metabarcoding-qiime2-datapackage-v2021.06.2/\"\ncd $wdir\n\n#activate QIIME environment \nconda deactivate\nmamba activate qiime2-2023.7\n\n\n\n\nThe help argument allows you to get an explanation about what each plugin in QIIME 2 does:\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help\n\n\n\n\n\n\n\nThe manifest file includes the sample ids, the path to where each sequence read file, i.e. fastq.gz file, is stored, and the read orientation. Because we have paired-end data and thus two files for each sample, we will list each sample twice, once for the forward and once for the reverse orientation. This is what the first few lines of the manifest file look like if you open the file manually or run head data/MANIFEST.csv in the terminal:\nsample-id,absolute-filepath,direction\nDUMMY10,$PWD/data/Sample-DUMMY10_R1.fastq.gz,forward\nDUMMY10,$PWD/data/Sample-DUMMY10_R2.fastq.gz,reverse\nDUMMY11,$PWD/data/Sample-DUMMY11_R1.fastq.gz,forward\nDUMMY11,$PWD/data/Sample-DUMMY11_R2.fastq.gz,reverse\nDUMMY12,$PWD/data/Sample-DUMMY12_R1.fastq.gz,forward\nDUMMY12,$PWD/data/Sample-DUMMY12_R2.fastq.gz,reverse\nNotice:\n\nThe manifest file is a csv file, i.e. a comma-separated file\nWe use $PWD, standing for print working directory, to say that the input files are in the data folder which is found in our working directory.\n\n\n\n\nWe can import our data into QIIME as follows:\n\n#prepare a folder in which we store our imported data\nmkdir -p prep\n\n#import the fastq files into QIIME\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path data/MANIFEST.csv \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#check artifact type\nqiime tools peek prep/demux-seqs.qza \n\n#generate summary\nqiime demux summarize \\\n    --i-data prep/demux-seqs.qza \\\n    --o-visualization prep/demux-seqs.qzv\n\nIf we visualize prep/demux-seqs.qzv with QIIME view, we get an overview about our samples:\n\nSome additonal comments on importing data:\n\nA general tutorial on importing data can be found here\nYou can import data by:\n\nproviding the path to the sequences with --input-path\nOr you can import sequences using the maifest file, which maps the sample identifiers to a fastq.gz or fastq file. It requires a file path, which as to be an absolute filepath (but can use $PWD/) as well as the direction of the reads. The manifest file is compatible with the metadata format\n\nThere are different phred scores used in the input-format: Phred33 and Phred64. In Phred 64 files, you should not be seeing phred values &lt;64. But you can also look at upper case and lower case of quality representations (ascii letters). In general, lower case means 64.\n\nUseful Options:\n\n--type {TEXT} The semantic type of the artifact that will be created upon importing. Use –show-importable-types to see what importable semantic types are available in the current deployment. [required]\n--input-path {PATH} Path to file or directory that should be imported. [required]\n--output-path {ARTIFACT} Path where output artifact should be written. [required]\n--input-format {TEXT} The format of the data to be imported. If not provided, data must be in the format expected by the semantic type provided via –type.\n--show-importable-types Show the semantic types that can be supplied to –type to import data into an artifact.\n\n\n\n\n\n\n\nFastQC is a tool to judge the quality of sequencing data (in a visual way). It is installed on Crunchomics and if you want to install it on your own machine follow the instructions found here.\n\nmkdir prep/fastqc\nfastqc data/*gz -o prep/fastqc\n\n\n\n\nThe next step is to remove any primer sequences. We will use the cutadapt QIIME2 plugin for that. Because we have paired-end data, there is a forward and reverse primer, referenced by the parameters --p-front-f and --p-front-r.\n\nqiime cutadapt trim-paired \\\n    --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f GTGYCAGCMGCCGCGGTAA \\\n  --p-front-r CCGYCAATTYMTTTRAGTTT \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose\n\n#generate summary \nqiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#export data (not necessary but to give an example how to do this)\nqiime tools extract \\\n    --output-path prep/trimmed_remove_primers \\\n    --input-path prep/trimmed-seqs.qza\n\nUseful options (for the full list, check the help function):\n\nThere are many ways an adaptor can be ligated. Check the help function for all options and if you do not know where the adaptor is contact your sequencing center.\n--p-front-f TEXT… Sequence of an adapter ligated to the 5’ end. Searched in forward read\n--p-front-r TEXT… Sequence of an adapter ligated to the 5’ end. Searched in reverse read\n--p-error-rate PROPORTION Range(0, 1, inclusive_end=True) Maximum allowed error rate. [default: 0.1]\n--p-indels / --p-no-indels Allow insertions or deletions of bases when matching adapters. [default: True]\n--p-times INTEGER Remove multiple occurrences of an adapter if it is Range(1, None) repeated, up to times times. [default: 1]\n--p-match-adapter-wildcards / --p-no-match-adapter-wildcards Interpret IUPAC wildcards (e.g., N) in adapters. [default: True]\n--p-minimum-length INTEGER Range(1, None) Discard reads shorter than specified value. Note, the cutadapt default of 0 has been overridden, because that value produces empty sequence records. [default: 1]\n--p-max-expected-errors NUMBER Range(0, None) Discard reads that exceed maximum expected erroneous nucleotides. [optional] --p-max-n NUMBER Discard reads with more than COUNT N bases. If Range(0, None) COUNT_or_FRACTION is a number between 0 and 1, it is interpreted as a fraction of the read length. [optional] --p-quality-cutoff-5end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 5 prime end. [default: 0] --p-quality-cutoff-3end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 3 prime end. [default: 0] --p-quality-base INTEGER Range(0, None) How the Phred score is encoded (33 or 64). [default: 33]\n\n\n\n\n\nIn order to minimize the risks of sequencer error in targeted sequencing, clustering approaches were initially developed. Clustering approaches are based upon the idea that related/similar organisms will have similar gene sequences and that rare sequencing errors will have a trivial contribution, if any, to the consensus sequence for these clusters, or operating taxonomic units (OTUs).\nIn contrast, ASV methods generate an error model tailored to an individual sequencing run and employing algorithms that use the model to distinguish between true biological sequences and those generated by error. The two ASV methods we explore in this tutorial are Deblur and DADA2.\nDADA2 implements an algorithm that models the errors introduced during amplicon sequencing, and uses that error model to infer the true sample composition. DADA2 replaces the traditional OTU-picking step in amplicon sequencing workflows and instead produces tables of amplicon sequence variants (ASVs).\nDeblur uses pre-computed error profiles to obtain putative error-free sequences from Illumina MiSeq and HiSeq sequencing platforms. Unlike DADA2, Deblur operates on each sample independently. Additionally, reads are depleted from sequencing artifacts either using a set of known sequencing artifacts (such as PhiX) (negative filtering) or using a set of known 16S sequences (positive filtering).\n\n\n\n\nNotice:\n\nDepending on the QIIME 2 version you use, you might need to change qiime vsearch join-pairs to vsearch merge-pairs.\nSome more details on merging reads can be found here\nIf you plan to use DADA2 to join and denoise your paired end data, do not join your reads prior to denoising with DADA2; DADA2 expects reads that have not yet been joined, and will join the reads for you during the denoising process.\nSome notes on overlap calculation for dada2\nAnother option for joining is fastq-join\n\n\nmkdir -p deblur\n\n#merge reads\nqiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 2 \\\n  --verbose\n\n#summarize data\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nUseful options:\n\n--p-minlen {INTEGER} Sequences shorter than minlen after truncation are Range(0, None) discarded. [default: 1]\n--p-maxns {INTEGER} Sequences with more than maxns N characters are Range(0, None) discarded. [optional]\n--p-maxdiffs {INTEGER} Maximum number of mismatches in the area of overlap Range(0, None) during merging. [default: 10]\n--p-minmergelen {INTEGER Range(0, None) } Minimum length of the merged read to be retained. [optional]\n--p-maxee {NUMBER} Maximum number of expected errors in the merged read Range(0.0, None) to be retained. [optional]\n--p-threads {INTEGER Range(0, 8, inclusive_end=True)} The number of threads to use for computation. Does not scale much past 4 threads. [default: 1]\n\nWhen running this, we will get a lot of information printed to the screen. It is useful to look at this to know if the merge worked well. For example, we see below that ~90% of our sequences merged well, telling us that everything went fine. Much lower values might indicate some problems with the trimmed data.\nMerging reads 100%\n     45195  Pairs\n     40730  Merged (90.1%)\n      4465  Not merged (9.9%)\n\nPairs that failed merging due to various reasons:\n        50  too few kmers found on same diagonal\n        30  multiple potential alignments\n      1878  too many differences\n      1790  alignment score too low, or score drop too high\n       717  staggered read pairs\n\nStatistics of all reads:\n    233.02  Mean read length\n\nStatistics of merged reads:\n    374.39  Mean fragment length\n     14.67  Standard deviation of fragment length\n      0.32  Mean expected error in forward sequences\n      0.78  Mean expected error in reverse sequences\n      0.51  Mean expected error in merged sequences\n      0.21  Mean observed errors in merged region of forward sequences\n      0.71  Mean observed errors in merged region of reverse sequences\n      0.92  Mean observed errors in merged region\nNotice that if we look at deblur/joined-seqs.qzv with QIIME view we see an increased quality score in the middle of the region:\n\nWhen merging reads with tools like vsearch, the quality scores often increase, not decrease, in the region of overlap. The reason being, if the forward and reverse read call the same base at the same position (even if both are low quality), then the quality estimate for the base in that position goes up. That is, you have two independent observations of that base in that position. This is often the benefit of being able to merge paired reads, as you can recover / increase your confidence of the sequence in the region of overlap.\n\n\n\nNotice: Both DADA and deblur will do some quality filtering, so we do not need to be too strict here. One could consider whether to even include this step, however, running this might be useful to reduce the dataset site.\n\nqiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\n#summarize stats\nqiime metadata tabulate \\\n   --m-input-file deblur/filt-stats.qza \\\n   --o-visualization deblur/filt-stats.qzv\n\nDefault settings:\n\n--p-min-quality4 All PHRED scores less that this value are considered to be low PHRED scores.\n--p-quality-window 3: The maximum number of low PHRED scores that can be observed in direct succession before truncating a sequence read.\n--p-min-length-fraction 0.75: The minimum length that a sequence read can be following truncation and still be retained. This length should be provided as a fraction of the input sequence length. --p-max-ambiguous 0: The maximum number of ambiguous (i.e., N) base calls.\n\n\n\n\n\n#default settings\nqiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length 370 \\\n  --o-representative-sequences deblur/deblur-reprseqs2.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 2 \\\n  --verbose\n\nHow to set the parameters:\n\nWhen you use deblur-denoise, you need to truncate your fragments such that they are all of the same length. The position at which sequences are truncated is specified by the --p-trim-length parameter. Any sequence that is shorter than this value will be lost from your analyses. Any sequence that is longer will be truncated at this position.\nTo decide on what value to choose, inspect deblur/filt-seqs.qzv with QIIME view. Once you open the interactive quality plot and scroll down we see the following:\n\n\n\n\n\n\n\nWe can set --p-trim-length of 370, because that resulted in minimal data loss. That is, only &lt;9% of the reads were discarded for being too short, and only 4 bases were trimmed off from sequences of median length.\n\nUseful options:\n\n--p-trim-length INTEGER Sequence trim length, specify -1 to disable trimming. [required]\n--p-left-trim-len {INTEGER} Range(0, None) Sequence trimming from the 5’ end. A value of 0 will disable this trim. [default: 0]\n--p-mean-error NUMBER The mean per nucleotide error, used for original sequence estimate. [default: 0.005]\n--p-indel-probNUMBER Insertion/deletion (indel) probability (same for N indels). [default: 0.01]\n--p-indel-max INTEGER Maximum number of insertion/deletions. [default: 3]\n--p-min-reads INTEGER Retain only features appearing at least min-reads times across all samples in the resulting feature table. [default: 10]\n--p-min-size INTEGER In each sample, discard all features with an abundance less than min-size. [default: 2]\n--p-jobs-to-start INTEGER Number of jobs to start (if to run in parallel). [default: 1]\n\nGenerated outputs:\n\nA feature table artifact with the frequencies per sample and feature.\nA representative sequences artifact with one single fasta sequence for each of the features in the feature table.\nA stats artifact with details of how many reads passed each filtering step of the deblur procedure.\n\nWe can create visualizations from the different outputs as follows:\n\n#look at stats\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats_noremoval.qza \\\n  --o-visualization deblur/deblur-stats_noremova.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table_noremoval.qza \\\n  --o-visualization deblur/deblur-table_noremova.qzv\n\nIf we check the visualization with QIIME 2 view (or qiime tools view deblur/deblur-table.qzv) we get some feelings about how many ASVs were generated and where we lost things. Often times the Interactive sample detail page can be particularily interesting. This shows you the frequency per sample. If there is large variation in this number between samples, it means it is difficult to directly compare samples. In that case it is often recommended to standardize your data by for instance rarefaction. Rarefaction will not be treated in detail in the tutorial, but the idea is laid out below.\n\nOut of the 1,346,802 filtered and merged reads we get 3,636 features with a total frequency of 290,671. So we only retained ~21%. If we compare this to the QIIME CMI tutorial we have 662 features with a total frequency of 1,535,691 (~84) so this is definitely something to watch out for.\nWith this dataset we loose a lot to “fraction-artifact-with-minsize” . This value is related to the --p-min-size parameter which sets the min abundance of a read to be retained (default 2). So we simply might loose things because it is a subsetted dataset and it means that most of your reads are singletons and so are discarded. For a real dataset this could be due to improper merging, not having removed primer/barcodes from your reads, or that there just is that many singletons (though unlikely imo)\nloss of sequences with merged reads in deblur compared to just the forward reads has been observed and discussed before see here\n\n\n\n\n\nThe code below is not required to be run. However, the code is useful in case something goes wrong and you need to check if the file you work with is corrupted.\n\n#check file: Result deblur/filt-seqs.qza appears to be valid at level=max.\nqiime tools validate deblur/filt-seqs.qza\n\n\n\n\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots to: - identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. - If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. - If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads - Figaro is a tool to automatically choose the trunc_len - Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2 - Your reads must still overlap after truncation in order to merge them later!\n\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trim-left-f 0 \\\n  --p-trim-left-r 0 \\\n  --p-trunc-len-f 230 \\\n  --p-trunc-len-r 220 \\\n  --p-n-threads 4 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose\n\nUseful options:\n\n--p-trunc-len-f INTEGER Position at which forward read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trunc-len-r INTEGER Position at which reverse read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trim-left-f INTEGER Position at which forward read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-trim-left-r INTEGER Position at which reverse read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-max-ee-f NUMBER Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-max-ee-r NUMBER Reverse reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-trunc-q INTEGER Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-f or trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap INTEGER Range(4, None) The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method TEXT Choices(‘independent’, ‘pseudo’) The method used to pool samples for denoising. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method TEXT Choices(‘consensus’, ‘none’, ‘pooled’) The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-min-fold-parent-over-abundance NUMBER The minimum abundance of potential parents of a sequence being tested as chimeric, expressed as a fold-change versus the abundance of the sequence being tested. Values should be greater than or equal to 1 (i.e. parents should be more abundant than the sequence being tested). This parameter has no effect if chimera-method is “none”. [default: 1.0]\n--p-n-threads INTEGER The number of threads to use for multithreaded processing. If 0 is provided, all available cores will be used. [default: 1]\n--p-n-reads-learn INTEGER The number of reads to use when training the error model. Smaller numbers will result in a shorter run time but a less reliable error model. [default: 1000000]\n\nLet’s now summarize the DADA2 output:\n\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nNotes:\n\nSanity check: in the stats visualizations for both DADA2 and deblur check that outside of filtering, there should be no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nsequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the ASV table.\n\nIf we compare the two tools, we should see something like this:\n\nDeblur:\n\n3,636 features\n290,671 total frequency (out of 1,346,802 filtered and merged reads, ca 21%)\nnothing at frequencies below 2500, most samples with frequencies between 2500-7500, 1 sample with 20 000\n\nDada2:\n\n13,934 features\n648,666 total frequency (out of 1554929 trimmed reads, ~41%)\nmost samples at 10,000 frequency/sample peak, histrogram looks closer to normal distribution, 1 sample with ~37 000 frequencies\n\n\nOveral Dada2 seems beneficial in that less reads are lost but without some deeper dive into the data the qzv files are not informative enough to decide what happened to the sequences. The reason why more DADA2 retains more reads is related to the quality filtering steps. To better compare sequences, we could compare them at a sampling depth of 2000 and should see: - Retained 94,000 (32.34%) features in 47 (100.00%) samples (deblur) instead of 290,671 (100.00%) features in 47 (100.00%) - Retained 94,000 (14.49%) features in 47 (95.92%) samples (dada2) instead of 648,666 (100.00%) features in 49 (100.00%)\nBased on that both methods produce very similar results.\n\n\n\n\nTo identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods:\n\nclassify-consensus-blast\nclassify-consensus-vsearch\nMachine-learning-based classification methods\n\nThe first two are alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained\nMachine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [@bokulich2018].\nCheck out this study discussing reproducible sequence taxonomy reference database management [@ii2021].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [@kaehler2019].\n\nFor the steps below a classifier using SILVA_138_99 is pre-computed the classifier. Steps outlining how to do this can be found here.\nRequirements:\n\nThe reference sequences\nThe reference taxonomy\n\n\n\nFirst, we import the SILVA database (consisting of fasta sequences and ta taxonomy table) into QIIME:\n\nqiime tools import \\\n  --type \"FeatureData[Sequence]\" \\\n  --input-path db/SILVA_138_99_16S-ref-seqs.fna \\\n  --output-path db/SILVA_138_99_16S-ref-seqs.qza\n\nqiime tools import \\\n  --type \"FeatureData[Taxonomy]\" \\\n  --input-format HeaderlessTSVTaxonomyFormat \\\n  --input-path db/SILVA_138_99_16S-ref-taxonomy.txt \\\n  --output-path db/SILVA_138_99_16S-ref-taxonomy.qza\n\n\n\n\nWe can now use these same sequences we used to amplify our 16S sequences to extract the corresponding region of the 16S rRNA sequences from the SILVA database, like so:\n\nqiime feature-classifier extract-reads \\\n  --i-sequences db/SILVA_138_99_16S-ref-seqs.qza \\\n  --p-f-primer GTGYCAGCMGCCGCGGTAA \\\n  --p-r-primer CCGYCAATTYMTTTRAGTTT \\\n  --o-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --p-n-jobs 2\n\nOptions:\n\n--p-trim-right 0\n--p-trunc-len 0\n--p-identity {NUMBER} minimum combined primer match identity threshold. [default: 0.8] –p-min-length {INTEGER} Minimum amplicon length. Shorter amplicons are Range(0, None) discarded. Applied after trimming and truncation, so be aware that trimming may impact sequence retention. Set to zero to disable min length filtering. [default: 50]\n--p-max-length 0\n\nThis results in :\n\nMin length: 56 (we have partial sequences)\nMax length: 2316 (indicates we might have some non-16S seqs or some 16S sequences have introns)\nMean length: 394 (What we would expect based on our 16S data)\n\n\n\n\nNext, we use the reference fragments you just created to train your classifier specifically on your region of interest.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have &gt;32GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the db folder.\n\n\n\nqiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --i-reference-taxonomy db/SILVA_138_99_16S-ref-taxonomy.qza \\\n  --o-classifier db/SILVA_138_99_16S-ref-classifier.qza\n\n\n\n\nNow that we have a trained classifier and a set of representative sequences from our DADA2-denoise analyses we can finally classify the sequences in our dataset.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have ~50 GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the taxonomy folder.\n\n\n\nqiime feature-classifier classify-sklearn \\\n  --i-classifier db/SILVA_138_99_16S-ref-classifier.qza \\\n  --p-n-jobs 16 \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza\n\nArguments:\n\n--p-confidence {VALUE Float % Range(0, 1, inclusive_end=True) | Str % Choices(‘disable’)} Confidence threshold for limiting taxonomic depth. Set to “disable” to disable confidence calculation, or 0 to calculate confidence but not apply it to limit the taxonomic depth of the assignments. [default: 0.7]\n--p-read-orientation {TEXT Choices(‘same’, ‘reverse-complement’, ‘auto’)} Direction of reads with respect to reference sequences. same will cause reads to be classified unchanged; reverse-complement will cause reads to be reversed and complemented prior to classification. “auto” will autodetect orientation based on the confidence estimates for the first 100 reads. [default: ‘auto’]\n\nNext, view the data with:\n\nqiime metadata tabulate \\\n --m-input-file taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nqiime2 tools view taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nWe see:\n\nThat this also reports a confidence score ranging between 0.7 and 1. The lower limit of 0.7 is the default value (see also the qiime feature-classifier classify-sklearn help function). You can opt for a lower value to increase the number of features with a classification, but beware that that will also increase the risk of spurious classifcations!\n\n\n\n\nBefore running the command, we will need to prepare a metadata file. This metadata file should contain information on the samples. For instance, at which depth was the sample taken, from which location does it come, was it subjected to experimental or control treatment etc. etc. This information is of course very specific to the study design but at the very least it should look like this (see also data/META.tsv):\n#SampleID\n#q2:types\nDUMMY1\nDUMMY10\n...\nbut we can add any variables, like so:\n#SampleID    BarcodeSequence Location        depth   location        treatment       grainsize       flowrate        age\n#q2:types    categorical     categorical     categorical     catechorical    categorical     numeric numeric numeric\n&lt;your data&gt;\nNext, we can generate a barplot like this:\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n  --m-metadata-file data/META.tsv \\\n  --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv\n\nqiime tools view taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv\n\n\n\n\n\n\n\nThese are just some general comments on filtering ASV tables and are not yet implemented in this specific tutorial. A full description of filtering methods can be found on the QIIME website\n\n#filter using the metadata info \n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#filter out samples from which we have obtained fewer than 10,000 sequences\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#create imaginary list of ids to keep and use this to filter\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\n\n\n\n\n#filter the representative sequences based on a filtered ASV table\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza\n\n\n\n\n\nNote: This are general notes and not yet implemented in this tutorial\n\n\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose\n\n\n\n\n\n\n\nNotice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [@gloor2017]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [@weiss2017].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [@mcmurdie2014]\nNormalization and microbial differential abundance strategies depend upon data characteristics [@weiss2017]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible.\n\n\n\n\nNote: These are general notes and not yet implemented in this tutorial\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)\n\n\n\n\nANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [@mandal2015].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use.\n\n\n\nIf submitting jobs via the srun command it can be useful to submit them via a screen for long running jobs. Some basics on using a screen can be found here.\n\n\nGeneral notice: In Evelyn’s tutorial qiime2-2021.2 was used. For this run, qiime2 was updated to a newer version and the tutorial code adjusted if needed.\nTo be able to access the amplicomics share, and the QIIME 2 installation and necessary databases, please contact Nina Dombrowski with your UvAnetID.\n\n#check what environments we already have\nconda env list\n\n#add amplicomics environment to be able to use the QIIME conda install\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\n\n\n\nIf you run this the first time, start by downloading the tutorial data.\n\n#download data data\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\n\n\n\n\nBelow, we set some environmental variables describing the location for key files we want to use. The only thing that you need to change yourself is the path to the working directory, ie. wdir, and the path_to_where_you_downloaded_the_data.\n\nwdir=\"/home/ndombro/tutorials/evelyn_qiime\"\ncd $wdir\n\n#make symbolic link to have the structures in the tutorial work\nln -s path_to_where_you_downloaded_the_data/metabarcoding-qiime2-datapackage-v2021.06.2/data ./\n\n#set environmental variables\nexport MANIFEST=\"data/MANIFEST.csv\"\nexport META=\"data/META.tsv\"\nexport PRIMFWD=\"GTGYCAGCMGCCGCGGTAA\"\nexport PRIMREV=\"CCGYCAATTYMTTTRAGTTT\"\nexport TRIMLENG=370\nexport TRUNKFWD=230\nexport TRUNKREV=220\n\nexport DBSEQ=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-seqs.qza\"\nexport DBTAX=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-tax.qza\"\nexport DBPREFIX=\"SILVA_138_99_16S\"\n\n#activate qiime environment\nmamba activate qiime2-2023.7\n\n\n\n\n\nmkdir -p logs\nmkdir -p prep\nmkdir -p deblur\nmkdir -p dada2\nmkdir -p db\nmkdir -p taxonomy\n\n\n\n\nWhen using Slurm we use the following settings to say that:\n\nthe job consists of 1 task (-n 1)\nthe job needs to be run on 1 cpu (–cpus-per-task 1)\n\n\nsrun -n 1 --cpus-per-task 1 qiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path $MANIFEST \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#create visualization\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/demux-seqs.qza \\\n  --o-visualization prep/demux-seqs.qzv\n\n#instructions to view qvz (tool not available, need to check if I can find it) \n#how-to-view-this-qzv prep/demux-seqs.qzv\n\nSummary of statistics:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\nsrun -n 1 --cpus-per-task 4 qiime cutadapt trim-paired \\\n  --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f $PRIMFWD \\\n  --p-front-r $PRIMREV \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-cutadapt-trim-paired.log\n\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#make a table out of this (quick, so ok to run on headnode)\nsrun -n 1 --cpus-per-task 1 qiime tools extract \\\n   --input-path prep/trimmed-seqs.qzv \\\n   --output-path visualizations/trimmed-seqs\n\n\nInformation about the 2&gt;&1 is a redirection operator :\n\nIt says to “redirect stderr (file descriptor 2) to the same location as stdout (file descriptor 1).” This means that both stdout and stderr will be combined and sent to the same destination\n\nThe tee command is used to:\n\nLogging: It captures the standard output (stdout) and standard error (stderr) of the command that precedes it and saves it to a file specified as its argument. In this case, it saves the output and errors to the file logs/qiime-cutadapt-trim-paired.log.\nDisplaying Output: In addition to saving the output to a file, tee also displays the captured output and errors on the terminal in real-time. This allows you to see the progress and any error messages while the command is running.\n\nIf we would want to run the command without tee we could do 2&gt;&1 &gt; logs/qiime-cutadapt-trim-paired.log but this won’t print output to the screen\n\nSummary of run:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\n\nNote that for the qiime cutadapt trim-paired-command you used p-cores while here you need p-threads\n\nsrun -n 1 --cpus-per-task 4 qiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 4 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-vsearch-join-pairs.log\n\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nNotice: This step cannot be sped up because the qiime quality-filter q-score command does not have an option to use more cpus or threads.\n\nsrun -n 1 --cpus-per-task 1 qiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-quality-filter-q-score.log\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\nqiime metadata tabulate \\\n  --m-input-file deblur/filt-stats.qza \\\n  --o-visualization deblur/filt-stats.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nBelow we:\n\nWe defined a memory allocation of 16GB. This is the total amount of RAM you expect to need for this job (+ a bit more to be on the safe side). How much you need is not always easy to predict and requires some experience/trial and error. As a rule of thumb: if you get an Out Of Memory error, double it and try again. Note that you can also define mem-per-cpu, which may be easier to work with if you often change the number of cpus between analyses.\nWe get a warning warn' method is deprecated, use 'warning' instead, but we can ignore that\n\n\n#job id: 398288\nsrun -n 1 --cpus-per-task 10 --mem=16GB qiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length $TRIMLENG \\\n  --o-representative-sequences deblur/deblur-reprseqs.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --p-min-size 2 \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 10 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-deblur-denoise-16S.log\n\n#check memory requirements: 1.6GB\n#maxrss = maximum amount of memory used at any time by any process in that job\nsacct -j 398288 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\ncat deblur.log &gt;&gt; logs/qiime_deblur_denoise-16S.log && rm deblur.log\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data deblur/deblur-reprseqs.qza \\\n  --o-visualization deblur/deblur-reprseqs.qzv\n\n\n\n\n\nDADA2 performs filtering, joining and denoising all with one single command, and therefor takes longer to run. Here, it takes ca. 30m5.334s using 8 cpus. It is tempting to just increase the number of cpus when impatient, but please note that not all commands are very efficient at running more jobs in parallel (i.e. on multiple cpus). The reason often is that the rate-limiting step in the workflow is unable to use multiple cpus, so all but one are idle.\n\n\n#job id: 398291\ntime srun -n 1 --cpus-per-task 8 --mem=32GB qiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trunc-len-f $TRUNKFWD \\\n  --p-trunc-len-r $TRUNKREV \\\n  --p-n-threads 8 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-dada2-denoise-paired.log\n\n#get mem usage\nsacct -j 398291 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#get summaries\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nUsage info –&gt; 7.64878 GB needed and 50 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398291            qiime          8              00:49:55\n398291.exte+     extern          8          0   00:49:55\n398291.0          qiime          8   7648780K   00:49:55\n\n\n\n\n\n\n\n#job id: 398333\ntime srun -n 1 --cpus-per-task 8 --mem=4GB qiime feature-classifier extract-reads \\\n  --i-sequences $DBSEQ \\\n  --p-f-primer $PRIMFWD \\\n  --p-r-primer $PRIMREV \\\n  --o-reads db/$DBPREFIX-ref-frags.qza \\\n  --p-n-jobs 8 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398333 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nUsage info –&gt; 1.43846 GB needed in 4 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398333            qiime          8              00:04:33\n398333.exte+     extern          8          0   00:04:33\n398333.0          qiime          8   1438460K   00:04:33\n\n\n\nTraining a classifier takes quite some time, especially because the qiime feature-classifier fit-classifier-naive-bayes- command cannot use multiple cpus in parallel (see qiime feature-classifier fit-classifier-naive-bayes –help). Here, it took ca. 145m56.836s. You can however often re-use your classifier, provided you used the same primers and db.\n\n#job id: 398335\ntime srun -n 1 --cpus-per-task 1 --mem=32GB qiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/$DBPREFIX-ref-frags.qza \\\n  --i-reference-taxonomy $DBTAX \\\n  --o-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --p-verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398335 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nResources needed –&gt; 33.4GB\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398335            qiime          2              02:19:24\n398335.exte+     extern          2          0   02:19:24\n398335.0          qiime          1  33469976K   02:19:24\n\n\n\nImportant\nThe classifier takes up quite some disc space. If you ‘just’ run it you are likely to get a No space left on device-error. You can avoid this by changing the directory where temporary files are written to, but make sure you have sufficient space there as well of course.\n\n#prepare tmp dir \nmkdir -p tmptmp\nexport TMPDIR=$PWD/tmptmp/\n\n#job id: 398339\ntime srun -n 1 --cpus-per-task 32 --mem=160GB qiime feature-classifier classify-sklearn \\\n  --i-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --p-n-jobs 32 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-classify-sklearn.log\n\n#get mem usage\nsacct -j 398339 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#cleanup\nrm -fr tmptmp\n\nqiime metadata tabulate \\\n  --m-input-file taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxonomy.qzv\n\nResource need –&gt; 158.7 GB:\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398339            qiime         32              00:13:57\n398339.exte+     extern         32          0   00:13:57\n398339.0          qiime         32 158749076K   00:13:57\n\n\n\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --m-metadata-file $META \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxplot.qzv\n\n\n\n\n\n\nscp crunchomics:'/home/ndombro/tutorials/evelyn_qiime/*/*.qzv' crunchomics_files/",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#qiime2-terminology",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#qiime2-terminology",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "Data produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (i.e. provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. Semantic types enable QIIME 2 to identify artifacts that are suitable inputs to an analysis. For example, if an analysis expects a distance matrix as input, QIIME 2 can determine which artifacts have a distance matrix semantic type and prevent incompatible artifacts from being used in the analysis (e.g. an artifact representing a phylogenetic tree).\nPlugins are software packages that can be developed by anyone. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses. A method accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts as output. A visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input. In contrast to a method, a visualizer produces exactly one visualization as output.",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#setup",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#setup",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "We can install a QIIME 2 environment with with mamba:\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n\nIf you run into problem, have a look at QIIME 2 installation guide.\n\n\n\nNext, we download the example files for this tutorial. The folder contains:\n\nDe-multiplexed sequence fastq files in the data folder\nA manifest.csv file that lists the sampleID, sample location and read orientation in the data folder\nFiles generated during the workflow\n\n\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\ncd metabarcoding-qiime2-datapackage-v2021.06.2\n\n\n\n\nNotice:\n\nChange the path stored in wdir to wherever you downloaded the data\nSetting an environmental variable to your working directory is not needed but useful to have if you resume an analysis after closing the terminal or simply to remind yourself where you analysed your data\n\n\n#set working environment\nwdir=\"&lt;path_to_your_downloaded_folder&gt;/metabarcoding-qiime2-datapackage-v2021.06.2/\"\ncd $wdir\n\n#activate QIIME environment \nconda deactivate\nmamba activate qiime2-2023.7\n\n\n\n\nThe help argument allows you to get an explanation about what each plugin in QIIME 2 does:\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#import-data",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#import-data",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "The manifest file includes the sample ids, the path to where each sequence read file, i.e. fastq.gz file, is stored, and the read orientation. Because we have paired-end data and thus two files for each sample, we will list each sample twice, once for the forward and once for the reverse orientation. This is what the first few lines of the manifest file look like if you open the file manually or run head data/MANIFEST.csv in the terminal:\nsample-id,absolute-filepath,direction\nDUMMY10,$PWD/data/Sample-DUMMY10_R1.fastq.gz,forward\nDUMMY10,$PWD/data/Sample-DUMMY10_R2.fastq.gz,reverse\nDUMMY11,$PWD/data/Sample-DUMMY11_R1.fastq.gz,forward\nDUMMY11,$PWD/data/Sample-DUMMY11_R2.fastq.gz,reverse\nDUMMY12,$PWD/data/Sample-DUMMY12_R1.fastq.gz,forward\nDUMMY12,$PWD/data/Sample-DUMMY12_R2.fastq.gz,reverse\nNotice:\n\nThe manifest file is a csv file, i.e. a comma-separated file\nWe use $PWD, standing for print working directory, to say that the input files are in the data folder which is found in our working directory.\n\n\n\n\nWe can import our data into QIIME as follows:\n\n#prepare a folder in which we store our imported data\nmkdir -p prep\n\n#import the fastq files into QIIME\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path data/MANIFEST.csv \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#check artifact type\nqiime tools peek prep/demux-seqs.qza \n\n#generate summary\nqiime demux summarize \\\n    --i-data prep/demux-seqs.qza \\\n    --o-visualization prep/demux-seqs.qzv\n\nIf we visualize prep/demux-seqs.qzv with QIIME view, we get an overview about our samples:\n\nSome additonal comments on importing data:\n\nA general tutorial on importing data can be found here\nYou can import data by:\n\nproviding the path to the sequences with --input-path\nOr you can import sequences using the maifest file, which maps the sample identifiers to a fastq.gz or fastq file. It requires a file path, which as to be an absolute filepath (but can use $PWD/) as well as the direction of the reads. The manifest file is compatible with the metadata format\n\nThere are different phred scores used in the input-format: Phred33 and Phred64. In Phred 64 files, you should not be seeing phred values &lt;64. But you can also look at upper case and lower case of quality representations (ascii letters). In general, lower case means 64.\n\nUseful Options:\n\n--type {TEXT} The semantic type of the artifact that will be created upon importing. Use –show-importable-types to see what importable semantic types are available in the current deployment. [required]\n--input-path {PATH} Path to file or directory that should be imported. [required]\n--output-path {ARTIFACT} Path where output artifact should be written. [required]\n--input-format {TEXT} The format of the data to be imported. If not provided, data must be in the format expected by the semantic type provided via –type.\n--show-importable-types Show the semantic types that can be supplied to –type to import data into an artifact.",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#quality-controls",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#quality-controls",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "FastQC is a tool to judge the quality of sequencing data (in a visual way). It is installed on Crunchomics and if you want to install it on your own machine follow the instructions found here.\n\nmkdir prep/fastqc\nfastqc data/*gz -o prep/fastqc\n\n\n\n\nThe next step is to remove any primer sequences. We will use the cutadapt QIIME2 plugin for that. Because we have paired-end data, there is a forward and reverse primer, referenced by the parameters --p-front-f and --p-front-r.\n\nqiime cutadapt trim-paired \\\n    --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f GTGYCAGCMGCCGCGGTAA \\\n  --p-front-r CCGYCAATTYMTTTRAGTTT \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose\n\n#generate summary \nqiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#export data (not necessary but to give an example how to do this)\nqiime tools extract \\\n    --output-path prep/trimmed_remove_primers \\\n    --input-path prep/trimmed-seqs.qza\n\nUseful options (for the full list, check the help function):\n\nThere are many ways an adaptor can be ligated. Check the help function for all options and if you do not know where the adaptor is contact your sequencing center.\n--p-front-f TEXT… Sequence of an adapter ligated to the 5’ end. Searched in forward read\n--p-front-r TEXT… Sequence of an adapter ligated to the 5’ end. Searched in reverse read\n--p-error-rate PROPORTION Range(0, 1, inclusive_end=True) Maximum allowed error rate. [default: 0.1]\n--p-indels / --p-no-indels Allow insertions or deletions of bases when matching adapters. [default: True]\n--p-times INTEGER Remove multiple occurrences of an adapter if it is Range(1, None) repeated, up to times times. [default: 1]\n--p-match-adapter-wildcards / --p-no-match-adapter-wildcards Interpret IUPAC wildcards (e.g., N) in adapters. [default: True]\n--p-minimum-length INTEGER Range(1, None) Discard reads shorter than specified value. Note, the cutadapt default of 0 has been overridden, because that value produces empty sequence records. [default: 1]\n--p-max-expected-errors NUMBER Range(0, None) Discard reads that exceed maximum expected erroneous nucleotides. [optional] --p-max-n NUMBER Discard reads with more than COUNT N bases. If Range(0, None) COUNT_or_FRACTION is a number between 0 and 1, it is interpreted as a fraction of the read length. [optional] --p-quality-cutoff-5end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 5 prime end. [default: 0] --p-quality-cutoff-3end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 3 prime end. [default: 0] --p-quality-base INTEGER Range(0, None) How the Phred score is encoded (33 or 64). [default: 33]",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#generate-asvs-using-deblur-or-dada2",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#generate-asvs-using-deblur-or-dada2",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "In order to minimize the risks of sequencer error in targeted sequencing, clustering approaches were initially developed. Clustering approaches are based upon the idea that related/similar organisms will have similar gene sequences and that rare sequencing errors will have a trivial contribution, if any, to the consensus sequence for these clusters, or operating taxonomic units (OTUs).\nIn contrast, ASV methods generate an error model tailored to an individual sequencing run and employing algorithms that use the model to distinguish between true biological sequences and those generated by error. The two ASV methods we explore in this tutorial are Deblur and DADA2.\nDADA2 implements an algorithm that models the errors introduced during amplicon sequencing, and uses that error model to infer the true sample composition. DADA2 replaces the traditional OTU-picking step in amplicon sequencing workflows and instead produces tables of amplicon sequence variants (ASVs).\nDeblur uses pre-computed error profiles to obtain putative error-free sequences from Illumina MiSeq and HiSeq sequencing platforms. Unlike DADA2, Deblur operates on each sample independently. Additionally, reads are depleted from sequencing artifacts either using a set of known sequencing artifacts (such as PhiX) (negative filtering) or using a set of known 16S sequences (positive filtering).\n\n\n\n\nNotice:\n\nDepending on the QIIME 2 version you use, you might need to change qiime vsearch join-pairs to vsearch merge-pairs.\nSome more details on merging reads can be found here\nIf you plan to use DADA2 to join and denoise your paired end data, do not join your reads prior to denoising with DADA2; DADA2 expects reads that have not yet been joined, and will join the reads for you during the denoising process.\nSome notes on overlap calculation for dada2\nAnother option for joining is fastq-join\n\n\nmkdir -p deblur\n\n#merge reads\nqiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 2 \\\n  --verbose\n\n#summarize data\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nUseful options:\n\n--p-minlen {INTEGER} Sequences shorter than minlen after truncation are Range(0, None) discarded. [default: 1]\n--p-maxns {INTEGER} Sequences with more than maxns N characters are Range(0, None) discarded. [optional]\n--p-maxdiffs {INTEGER} Maximum number of mismatches in the area of overlap Range(0, None) during merging. [default: 10]\n--p-minmergelen {INTEGER Range(0, None) } Minimum length of the merged read to be retained. [optional]\n--p-maxee {NUMBER} Maximum number of expected errors in the merged read Range(0.0, None) to be retained. [optional]\n--p-threads {INTEGER Range(0, 8, inclusive_end=True)} The number of threads to use for computation. Does not scale much past 4 threads. [default: 1]\n\nWhen running this, we will get a lot of information printed to the screen. It is useful to look at this to know if the merge worked well. For example, we see below that ~90% of our sequences merged well, telling us that everything went fine. Much lower values might indicate some problems with the trimmed data.\nMerging reads 100%\n     45195  Pairs\n     40730  Merged (90.1%)\n      4465  Not merged (9.9%)\n\nPairs that failed merging due to various reasons:\n        50  too few kmers found on same diagonal\n        30  multiple potential alignments\n      1878  too many differences\n      1790  alignment score too low, or score drop too high\n       717  staggered read pairs\n\nStatistics of all reads:\n    233.02  Mean read length\n\nStatistics of merged reads:\n    374.39  Mean fragment length\n     14.67  Standard deviation of fragment length\n      0.32  Mean expected error in forward sequences\n      0.78  Mean expected error in reverse sequences\n      0.51  Mean expected error in merged sequences\n      0.21  Mean observed errors in merged region of forward sequences\n      0.71  Mean observed errors in merged region of reverse sequences\n      0.92  Mean observed errors in merged region\nNotice that if we look at deblur/joined-seqs.qzv with QIIME view we see an increased quality score in the middle of the region:\n\nWhen merging reads with tools like vsearch, the quality scores often increase, not decrease, in the region of overlap. The reason being, if the forward and reverse read call the same base at the same position (even if both are low quality), then the quality estimate for the base in that position goes up. That is, you have two independent observations of that base in that position. This is often the benefit of being able to merge paired reads, as you can recover / increase your confidence of the sequence in the region of overlap.\n\n\n\nNotice: Both DADA and deblur will do some quality filtering, so we do not need to be too strict here. One could consider whether to even include this step, however, running this might be useful to reduce the dataset site.\n\nqiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\n#summarize stats\nqiime metadata tabulate \\\n   --m-input-file deblur/filt-stats.qza \\\n   --o-visualization deblur/filt-stats.qzv\n\nDefault settings:\n\n--p-min-quality4 All PHRED scores less that this value are considered to be low PHRED scores.\n--p-quality-window 3: The maximum number of low PHRED scores that can be observed in direct succession before truncating a sequence read.\n--p-min-length-fraction 0.75: The minimum length that a sequence read can be following truncation and still be retained. This length should be provided as a fraction of the input sequence length. --p-max-ambiguous 0: The maximum number of ambiguous (i.e., N) base calls.\n\n\n\n\n\n#default settings\nqiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length 370 \\\n  --o-representative-sequences deblur/deblur-reprseqs2.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 2 \\\n  --verbose\n\nHow to set the parameters:\n\nWhen you use deblur-denoise, you need to truncate your fragments such that they are all of the same length. The position at which sequences are truncated is specified by the --p-trim-length parameter. Any sequence that is shorter than this value will be lost from your analyses. Any sequence that is longer will be truncated at this position.\nTo decide on what value to choose, inspect deblur/filt-seqs.qzv with QIIME view. Once you open the interactive quality plot and scroll down we see the following:\n\n\n\n\n\n\n\nWe can set --p-trim-length of 370, because that resulted in minimal data loss. That is, only &lt;9% of the reads were discarded for being too short, and only 4 bases were trimmed off from sequences of median length.\n\nUseful options:\n\n--p-trim-length INTEGER Sequence trim length, specify -1 to disable trimming. [required]\n--p-left-trim-len {INTEGER} Range(0, None) Sequence trimming from the 5’ end. A value of 0 will disable this trim. [default: 0]\n--p-mean-error NUMBER The mean per nucleotide error, used for original sequence estimate. [default: 0.005]\n--p-indel-probNUMBER Insertion/deletion (indel) probability (same for N indels). [default: 0.01]\n--p-indel-max INTEGER Maximum number of insertion/deletions. [default: 3]\n--p-min-reads INTEGER Retain only features appearing at least min-reads times across all samples in the resulting feature table. [default: 10]\n--p-min-size INTEGER In each sample, discard all features with an abundance less than min-size. [default: 2]\n--p-jobs-to-start INTEGER Number of jobs to start (if to run in parallel). [default: 1]\n\nGenerated outputs:\n\nA feature table artifact with the frequencies per sample and feature.\nA representative sequences artifact with one single fasta sequence for each of the features in the feature table.\nA stats artifact with details of how many reads passed each filtering step of the deblur procedure.\n\nWe can create visualizations from the different outputs as follows:\n\n#look at stats\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats_noremoval.qza \\\n  --o-visualization deblur/deblur-stats_noremova.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table_noremoval.qza \\\n  --o-visualization deblur/deblur-table_noremova.qzv\n\nIf we check the visualization with QIIME 2 view (or qiime tools view deblur/deblur-table.qzv) we get some feelings about how many ASVs were generated and where we lost things. Often times the Interactive sample detail page can be particularily interesting. This shows you the frequency per sample. If there is large variation in this number between samples, it means it is difficult to directly compare samples. In that case it is often recommended to standardize your data by for instance rarefaction. Rarefaction will not be treated in detail in the tutorial, but the idea is laid out below.\n\nOut of the 1,346,802 filtered and merged reads we get 3,636 features with a total frequency of 290,671. So we only retained ~21%. If we compare this to the QIIME CMI tutorial we have 662 features with a total frequency of 1,535,691 (~84) so this is definitely something to watch out for.\nWith this dataset we loose a lot to “fraction-artifact-with-minsize” . This value is related to the --p-min-size parameter which sets the min abundance of a read to be retained (default 2). So we simply might loose things because it is a subsetted dataset and it means that most of your reads are singletons and so are discarded. For a real dataset this could be due to improper merging, not having removed primer/barcodes from your reads, or that there just is that many singletons (though unlikely imo)\nloss of sequences with merged reads in deblur compared to just the forward reads has been observed and discussed before see here\n\n\n\n\n\nThe code below is not required to be run. However, the code is useful in case something goes wrong and you need to check if the file you work with is corrupted.\n\n#check file: Result deblur/filt-seqs.qza appears to be valid at level=max.\nqiime tools validate deblur/filt-seqs.qza\n\n\n\n\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots to: - identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. - If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. - If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads - Figaro is a tool to automatically choose the trunc_len - Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2 - Your reads must still overlap after truncation in order to merge them later!\n\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trim-left-f 0 \\\n  --p-trim-left-r 0 \\\n  --p-trunc-len-f 230 \\\n  --p-trunc-len-r 220 \\\n  --p-n-threads 4 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose\n\nUseful options:\n\n--p-trunc-len-f INTEGER Position at which forward read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trunc-len-r INTEGER Position at which reverse read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trim-left-f INTEGER Position at which forward read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-trim-left-r INTEGER Position at which reverse read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-max-ee-f NUMBER Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-max-ee-r NUMBER Reverse reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-trunc-q INTEGER Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-f or trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap INTEGER Range(4, None) The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method TEXT Choices(‘independent’, ‘pseudo’) The method used to pool samples for denoising. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method TEXT Choices(‘consensus’, ‘none’, ‘pooled’) The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-min-fold-parent-over-abundance NUMBER The minimum abundance of potential parents of a sequence being tested as chimeric, expressed as a fold-change versus the abundance of the sequence being tested. Values should be greater than or equal to 1 (i.e. parents should be more abundant than the sequence being tested). This parameter has no effect if chimera-method is “none”. [default: 1.0]\n--p-n-threads INTEGER The number of threads to use for multithreaded processing. If 0 is provided, all available cores will be used. [default: 1]\n--p-n-reads-learn INTEGER The number of reads to use when training the error model. Smaller numbers will result in a shorter run time but a less reliable error model. [default: 1000000]\n\nLet’s now summarize the DADA2 output:\n\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nNotes:\n\nSanity check: in the stats visualizations for both DADA2 and deblur check that outside of filtering, there should be no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nsequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the ASV table.\n\nIf we compare the two tools, we should see something like this:\n\nDeblur:\n\n3,636 features\n290,671 total frequency (out of 1,346,802 filtered and merged reads, ca 21%)\nnothing at frequencies below 2500, most samples with frequencies between 2500-7500, 1 sample with 20 000\n\nDada2:\n\n13,934 features\n648,666 total frequency (out of 1554929 trimmed reads, ~41%)\nmost samples at 10,000 frequency/sample peak, histrogram looks closer to normal distribution, 1 sample with ~37 000 frequencies\n\n\nOveral Dada2 seems beneficial in that less reads are lost but without some deeper dive into the data the qzv files are not informative enough to decide what happened to the sequences. The reason why more DADA2 retains more reads is related to the quality filtering steps. To better compare sequences, we could compare them at a sampling depth of 2000 and should see: - Retained 94,000 (32.34%) features in 47 (100.00%) samples (deblur) instead of 290,671 (100.00%) features in 47 (100.00%) - Retained 94,000 (14.49%) features in 47 (95.92%) samples (dada2) instead of 648,666 (100.00%) features in 49 (100.00%)\nBased on that both methods produce very similar results.",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#taxonomic-classification-naive-bayes",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#taxonomic-classification-naive-bayes",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "To identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods:\n\nclassify-consensus-blast\nclassify-consensus-vsearch\nMachine-learning-based classification methods\n\nThe first two are alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained\nMachine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [@bokulich2018].\nCheck out this study discussing reproducible sequence taxonomy reference database management [@ii2021].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [@kaehler2019].\n\nFor the steps below a classifier using SILVA_138_99 is pre-computed the classifier. Steps outlining how to do this can be found here.\nRequirements:\n\nThe reference sequences\nThe reference taxonomy\n\n\n\nFirst, we import the SILVA database (consisting of fasta sequences and ta taxonomy table) into QIIME:\n\nqiime tools import \\\n  --type \"FeatureData[Sequence]\" \\\n  --input-path db/SILVA_138_99_16S-ref-seqs.fna \\\n  --output-path db/SILVA_138_99_16S-ref-seqs.qza\n\nqiime tools import \\\n  --type \"FeatureData[Taxonomy]\" \\\n  --input-format HeaderlessTSVTaxonomyFormat \\\n  --input-path db/SILVA_138_99_16S-ref-taxonomy.txt \\\n  --output-path db/SILVA_138_99_16S-ref-taxonomy.qza\n\n\n\n\nWe can now use these same sequences we used to amplify our 16S sequences to extract the corresponding region of the 16S rRNA sequences from the SILVA database, like so:\n\nqiime feature-classifier extract-reads \\\n  --i-sequences db/SILVA_138_99_16S-ref-seqs.qza \\\n  --p-f-primer GTGYCAGCMGCCGCGGTAA \\\n  --p-r-primer CCGYCAATTYMTTTRAGTTT \\\n  --o-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --p-n-jobs 2\n\nOptions:\n\n--p-trim-right 0\n--p-trunc-len 0\n--p-identity {NUMBER} minimum combined primer match identity threshold. [default: 0.8] –p-min-length {INTEGER} Minimum amplicon length. Shorter amplicons are Range(0, None) discarded. Applied after trimming and truncation, so be aware that trimming may impact sequence retention. Set to zero to disable min length filtering. [default: 50]\n--p-max-length 0\n\nThis results in :\n\nMin length: 56 (we have partial sequences)\nMax length: 2316 (indicates we might have some non-16S seqs or some 16S sequences have introns)\nMean length: 394 (What we would expect based on our 16S data)\n\n\n\n\nNext, we use the reference fragments you just created to train your classifier specifically on your region of interest.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have &gt;32GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the db folder.\n\n\n\nqiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --i-reference-taxonomy db/SILVA_138_99_16S-ref-taxonomy.qza \\\n  --o-classifier db/SILVA_138_99_16S-ref-classifier.qza\n\n\n\n\nNow that we have a trained classifier and a set of representative sequences from our DADA2-denoise analyses we can finally classify the sequences in our dataset.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have ~50 GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the taxonomy folder.\n\n\n\nqiime feature-classifier classify-sklearn \\\n  --i-classifier db/SILVA_138_99_16S-ref-classifier.qza \\\n  --p-n-jobs 16 \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza\n\nArguments:\n\n--p-confidence {VALUE Float % Range(0, 1, inclusive_end=True) | Str % Choices(‘disable’)} Confidence threshold for limiting taxonomic depth. Set to “disable” to disable confidence calculation, or 0 to calculate confidence but not apply it to limit the taxonomic depth of the assignments. [default: 0.7]\n--p-read-orientation {TEXT Choices(‘same’, ‘reverse-complement’, ‘auto’)} Direction of reads with respect to reference sequences. same will cause reads to be classified unchanged; reverse-complement will cause reads to be reversed and complemented prior to classification. “auto” will autodetect orientation based on the confidence estimates for the first 100 reads. [default: ‘auto’]\n\nNext, view the data with:\n\nqiime metadata tabulate \\\n --m-input-file taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nqiime2 tools view taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nWe see:\n\nThat this also reports a confidence score ranging between 0.7 and 1. The lower limit of 0.7 is the default value (see also the qiime feature-classifier classify-sklearn help function). You can opt for a lower value to increase the number of features with a classification, but beware that that will also increase the risk of spurious classifcations!\n\n\n\n\nBefore running the command, we will need to prepare a metadata file. This metadata file should contain information on the samples. For instance, at which depth was the sample taken, from which location does it come, was it subjected to experimental or control treatment etc. etc. This information is of course very specific to the study design but at the very least it should look like this (see also data/META.tsv):\n#SampleID\n#q2:types\nDUMMY1\nDUMMY10\n...\nbut we can add any variables, like so:\n#SampleID    BarcodeSequence Location        depth   location        treatment       grainsize       flowrate        age\n#q2:types    categorical     categorical     categorical     catechorical    categorical     numeric numeric numeric\n&lt;your data&gt;\nNext, we can generate a barplot like this:\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n  --m-metadata-file data/META.tsv \\\n  --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv\n\nqiime tools view taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#filtering",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#filtering",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "These are just some general comments on filtering ASV tables and are not yet implemented in this specific tutorial. A full description of filtering methods can be found on the QIIME website\n\n#filter using the metadata info \n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#filter out samples from which we have obtained fewer than 10,000 sequences\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#create imaginary list of ids to keep and use this to filter\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\n\n\n\n\n#filter the representative sequences based on a filtered ASV table\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#phylogenetic-tree-construction",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#phylogenetic-tree-construction",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "Note: This are general notes and not yet implemented in this tutorial\n\n\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "Notice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [@gloor2017]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [@weiss2017].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [@mcmurdie2014]\nNormalization and microbial differential abundance strategies depend upon data characteristics [@weiss2017]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible.",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#computing-diversity-metrics",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#computing-diversity-metrics",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "Note: These are general notes and not yet implemented in this tutorial\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#differential-abundance-testing",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#differential-abundance-testing",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "ANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [@mandal2015].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use.",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#sec-crunchomics",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#sec-crunchomics",
    "title": "A short QIIME tutorial",
    "section": "",
    "text": "If submitting jobs via the srun command it can be useful to submit them via a screen for long running jobs. Some basics on using a screen can be found here.\n\n\nGeneral notice: In Evelyn’s tutorial qiime2-2021.2 was used. For this run, qiime2 was updated to a newer version and the tutorial code adjusted if needed.\nTo be able to access the amplicomics share, and the QIIME 2 installation and necessary databases, please contact Nina Dombrowski with your UvAnetID.\n\n#check what environments we already have\nconda env list\n\n#add amplicomics environment to be able to use the QIIME conda install\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\n\n\n\nIf you run this the first time, start by downloading the tutorial data.\n\n#download data data\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\n\n\n\n\nBelow, we set some environmental variables describing the location for key files we want to use. The only thing that you need to change yourself is the path to the working directory, ie. wdir, and the path_to_where_you_downloaded_the_data.\n\nwdir=\"/home/ndombro/tutorials/evelyn_qiime\"\ncd $wdir\n\n#make symbolic link to have the structures in the tutorial work\nln -s path_to_where_you_downloaded_the_data/metabarcoding-qiime2-datapackage-v2021.06.2/data ./\n\n#set environmental variables\nexport MANIFEST=\"data/MANIFEST.csv\"\nexport META=\"data/META.tsv\"\nexport PRIMFWD=\"GTGYCAGCMGCCGCGGTAA\"\nexport PRIMREV=\"CCGYCAATTYMTTTRAGTTT\"\nexport TRIMLENG=370\nexport TRUNKFWD=230\nexport TRUNKREV=220\n\nexport DBSEQ=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-seqs.qza\"\nexport DBTAX=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-tax.qza\"\nexport DBPREFIX=\"SILVA_138_99_16S\"\n\n#activate qiime environment\nmamba activate qiime2-2023.7\n\n\n\n\n\nmkdir -p logs\nmkdir -p prep\nmkdir -p deblur\nmkdir -p dada2\nmkdir -p db\nmkdir -p taxonomy\n\n\n\n\nWhen using Slurm we use the following settings to say that:\n\nthe job consists of 1 task (-n 1)\nthe job needs to be run on 1 cpu (–cpus-per-task 1)\n\n\nsrun -n 1 --cpus-per-task 1 qiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path $MANIFEST \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#create visualization\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/demux-seqs.qza \\\n  --o-visualization prep/demux-seqs.qzv\n\n#instructions to view qvz (tool not available, need to check if I can find it) \n#how-to-view-this-qzv prep/demux-seqs.qzv\n\nSummary of statistics:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\nsrun -n 1 --cpus-per-task 4 qiime cutadapt trim-paired \\\n  --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f $PRIMFWD \\\n  --p-front-r $PRIMREV \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-cutadapt-trim-paired.log\n\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#make a table out of this (quick, so ok to run on headnode)\nsrun -n 1 --cpus-per-task 1 qiime tools extract \\\n   --input-path prep/trimmed-seqs.qzv \\\n   --output-path visualizations/trimmed-seqs\n\n\nInformation about the 2&gt;&1 is a redirection operator :\n\nIt says to “redirect stderr (file descriptor 2) to the same location as stdout (file descriptor 1).” This means that both stdout and stderr will be combined and sent to the same destination\n\nThe tee command is used to:\n\nLogging: It captures the standard output (stdout) and standard error (stderr) of the command that precedes it and saves it to a file specified as its argument. In this case, it saves the output and errors to the file logs/qiime-cutadapt-trim-paired.log.\nDisplaying Output: In addition to saving the output to a file, tee also displays the captured output and errors on the terminal in real-time. This allows you to see the progress and any error messages while the command is running.\n\nIf we would want to run the command without tee we could do 2&gt;&1 &gt; logs/qiime-cutadapt-trim-paired.log but this won’t print output to the screen\n\nSummary of run:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\n\nNote that for the qiime cutadapt trim-paired-command you used p-cores while here you need p-threads\n\nsrun -n 1 --cpus-per-task 4 qiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 4 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-vsearch-join-pairs.log\n\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nNotice: This step cannot be sped up because the qiime quality-filter q-score command does not have an option to use more cpus or threads.\n\nsrun -n 1 --cpus-per-task 1 qiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-quality-filter-q-score.log\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\nqiime metadata tabulate \\\n  --m-input-file deblur/filt-stats.qza \\\n  --o-visualization deblur/filt-stats.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nBelow we:\n\nWe defined a memory allocation of 16GB. This is the total amount of RAM you expect to need for this job (+ a bit more to be on the safe side). How much you need is not always easy to predict and requires some experience/trial and error. As a rule of thumb: if you get an Out Of Memory error, double it and try again. Note that you can also define mem-per-cpu, which may be easier to work with if you often change the number of cpus between analyses.\nWe get a warning warn' method is deprecated, use 'warning' instead, but we can ignore that\n\n\n#job id: 398288\nsrun -n 1 --cpus-per-task 10 --mem=16GB qiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length $TRIMLENG \\\n  --o-representative-sequences deblur/deblur-reprseqs.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --p-min-size 2 \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 10 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-deblur-denoise-16S.log\n\n#check memory requirements: 1.6GB\n#maxrss = maximum amount of memory used at any time by any process in that job\nsacct -j 398288 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\ncat deblur.log &gt;&gt; logs/qiime_deblur_denoise-16S.log && rm deblur.log\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data deblur/deblur-reprseqs.qza \\\n  --o-visualization deblur/deblur-reprseqs.qzv\n\n\n\n\n\nDADA2 performs filtering, joining and denoising all with one single command, and therefor takes longer to run. Here, it takes ca. 30m5.334s using 8 cpus. It is tempting to just increase the number of cpus when impatient, but please note that not all commands are very efficient at running more jobs in parallel (i.e. on multiple cpus). The reason often is that the rate-limiting step in the workflow is unable to use multiple cpus, so all but one are idle.\n\n\n#job id: 398291\ntime srun -n 1 --cpus-per-task 8 --mem=32GB qiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trunc-len-f $TRUNKFWD \\\n  --p-trunc-len-r $TRUNKREV \\\n  --p-n-threads 8 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-dada2-denoise-paired.log\n\n#get mem usage\nsacct -j 398291 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#get summaries\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nUsage info –&gt; 7.64878 GB needed and 50 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398291            qiime          8              00:49:55\n398291.exte+     extern          8          0   00:49:55\n398291.0          qiime          8   7648780K   00:49:55\n\n\n\n\n\n\n\n#job id: 398333\ntime srun -n 1 --cpus-per-task 8 --mem=4GB qiime feature-classifier extract-reads \\\n  --i-sequences $DBSEQ \\\n  --p-f-primer $PRIMFWD \\\n  --p-r-primer $PRIMREV \\\n  --o-reads db/$DBPREFIX-ref-frags.qza \\\n  --p-n-jobs 8 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398333 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nUsage info –&gt; 1.43846 GB needed in 4 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398333            qiime          8              00:04:33\n398333.exte+     extern          8          0   00:04:33\n398333.0          qiime          8   1438460K   00:04:33\n\n\n\nTraining a classifier takes quite some time, especially because the qiime feature-classifier fit-classifier-naive-bayes- command cannot use multiple cpus in parallel (see qiime feature-classifier fit-classifier-naive-bayes –help). Here, it took ca. 145m56.836s. You can however often re-use your classifier, provided you used the same primers and db.\n\n#job id: 398335\ntime srun -n 1 --cpus-per-task 1 --mem=32GB qiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/$DBPREFIX-ref-frags.qza \\\n  --i-reference-taxonomy $DBTAX \\\n  --o-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --p-verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398335 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nResources needed –&gt; 33.4GB\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398335            qiime          2              02:19:24\n398335.exte+     extern          2          0   02:19:24\n398335.0          qiime          1  33469976K   02:19:24\n\n\n\nImportant\nThe classifier takes up quite some disc space. If you ‘just’ run it you are likely to get a No space left on device-error. You can avoid this by changing the directory where temporary files are written to, but make sure you have sufficient space there as well of course.\n\n#prepare tmp dir \nmkdir -p tmptmp\nexport TMPDIR=$PWD/tmptmp/\n\n#job id: 398339\ntime srun -n 1 --cpus-per-task 32 --mem=160GB qiime feature-classifier classify-sklearn \\\n  --i-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --p-n-jobs 32 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-classify-sklearn.log\n\n#get mem usage\nsacct -j 398339 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#cleanup\nrm -fr tmptmp\n\nqiime metadata tabulate \\\n  --m-input-file taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxonomy.qzv\n\nResource need –&gt; 158.7 GB:\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398339            qiime         32              00:13:57\n398339.exte+     extern         32          0   00:13:57\n398339.0          qiime         32 158749076K   00:13:57\n\n\n\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --m-metadata-file $META \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxplot.qzv\n\n\n\n\n\n\nscp crunchomics:'/home/ndombro/tutorials/evelyn_qiime/*/*.qzv' crunchomics_files/",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "index.html#useful-tutorials",
    "href": "index.html#useful-tutorials",
    "title": "Bioinformatics guidance page",
    "section": "Useful tutorials",
    "text": "Useful tutorials\n\nGetting started with bash\n\nA tutorial on using bash and an HPC\nVersion control with git\nA tutorial on using AWK, a command line tool for filtering tables, extracting patterns, etc… If you want to follow this tutorial then you can download the required input files from here\n\n\n\nUsing R\n\nAn R cookbook including some example files if you want to code along\nTutorial on data manipulation with dplyr\nTutorial on data visualization with ggplot2\n\n\n\nBioinformatic workflows\n\nFrom sequence file to OTU table with Qiime\nAnalysing an OTU table with R\nAssembling a metagenome\nMetagenomic binning\nAnnotating microbial genomes\nHow to do generate a species tree",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "index.html#bioinformatic-tools-a-z",
    "href": "index.html#bioinformatic-tools-a-z",
    "title": "Bioinformatics guidance page",
    "section": "Bioinformatic tools A-Z",
    "text": "Bioinformatic tools A-Z\n\nChopper: A tool for quality filtering of long read data\nFAMA: A fast pipeline for functional and taxonomic analysis of metagenomic sequences\nFastP: A tool for fast all-in-one preprocessing of FastQ files\nFastQC: A quality control tool for read sequencing data\nInterproscan: A tool to scan protein and nucleic sequences against InterPro signatures\nITSx: A tool to extract ITS1 and ITS2 subregions from ITS sequences\nKraken2: A taxonomic sequence classifier using kmers\nMETABOLIC: A tool to predict functional trait profiles in genome datasets\nMinimap2: A program to align DNA or mRNA sequences against a reference database\nNanoClass2: A taxonomic meta-classifier for long-read 16S/18S rRNA gene sequencing data\nNanoITS: A taxonomic meta-classifier for long-read ITS operon sequencing data\nNanoPlot: Plotting tool for long read sequencing data\nNanoQC: A quality control tool for long read sequencing data\nPorechop: A tool for finding and removing adapters from Nanopore reads\nSortMerNa: A tool to filter ribosomal RNAs in metatranscriptomic data\nTrinity: A tool to assemble transcript sequences from Illumina RNA-Seq data",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "source/ITSx/itsx_readme.html",
    "href": "source/ITSx/itsx_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "“ITSx is an open source software utility to extract the highly variable ITS1 and ITS2 subregions from ITS sequences, which is commonly used as a molecular barcode for e.g. fungi.”\n\nWebsite\nManual\nPaper. Please do not forget to cite the ITSx paper whenever you use the software (Bengtsson-Palme et al. 2013).\n\nAvailable on Crunchomics: No\n\n\n\nITSx can be installed from scratch but below is the code needed to install the software with mamba.\nIf there is an issue with the mamba/conda installation, the software can also be downloaded with wget https://microbiology.se/sw/ITSx_1.1.3.tar.gz. After the download, decompress the folder and follow the information in the readme.txt. The download also comes with a test.fasta which can be used to test either installation.\nData for testing can also be found here.\n\nmamba create -n fungi_its\nmamba install -n fungi_its -c bioconda itsx\n\n\n\n\n\nRequired input: FASTA format (aligned or unaligned, DNA or RNA)\nGenerated output:\n\none summary file of the entire run\none more detailed table containing the positions in the respective sequences where the ITS subregions were found\none “semi-graphical” representation of hits\none FASTA file of all identified ITS sequences\none FASTA file for the ITS1 and ITS2 regions\nif entries that did not contain any ITS region are found, a list of sequence IDs representing those entries (optional)\n\nUseful arguments (not extensive, check manual for all arguments):\n\n--save_regions: Get all regions of interest, not only ITS1/2\n-E {value}: E-value cutoff (default 1e-5)\n-S {value}: Domain score cutoff (default 0)\n--cpu {value }: Number of cpus to use (default 1)\n--multi_thread {T/F}: Multi-thread the HMMER-search. On (T) by default if the number of CPUs/cores is larger than one (–cpu option &gt; 1), else off (F)\n--preserve {T/F}: If on, ITSx will preserve the sequence headers from the input file instead of replacing them with ITSx headers in the output. Off (F) by default.\n--only_full {T/F}: If true, the output is limited to full-length ITS1 and ITS2 regions only. Off (F) by default.\n--minlen {value} Minimum length the ITS regions must be to be outputted in the concatenated file (see –concat above). Default is zero (0).\n\n\n\n\n\n#activate the right environment\nmamba activate fungi_its\n\n#download data for testing the installation (optional)\nmkdir testing\nwget -P testing https://raw.githubusercontent.com/ScienceParkStudyGroup/software_information/main/data/itsx/test.fasta\n\n#testrun (adjust path of test.fasta to where ever you downloaded the software)\nITSx -i testing/test.fasta --save_regions all -o testing/ITS_test_v1\n\n\n#deactivate environment (if using environment)\nmamba deactivate\n\nRegions extracted from test file (notice how the full fasta ONLY contains sequences with all regions):\n\ntesting/ITS_test_v1.5_8S.fasta:50\ntesting/ITS_test_v1.ITS1.fasta:50\ntesting/ITS_test_v1.ITS2.fasta:50\ntesting/ITS_test_v1.LSU.fasta:32\ntesting/ITS_test_v1.SSU.fasta:31\ntesting/ITS_test_v1.full.fasta:19\ntesting/ITS_test_v1_no_detections.fasta:0\ntesting/test.fasta:50",
    "crumbs": [
      "Welcome page",
      "Fungal genomics",
      "ITSx"
    ]
  },
  {
    "objectID": "source/ITSx/itsx_readme.html#itsx",
    "href": "source/ITSx/itsx_readme.html#itsx",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "“ITSx is an open source software utility to extract the highly variable ITS1 and ITS2 subregions from ITS sequences, which is commonly used as a molecular barcode for e.g. fungi.”\n\nWebsite\nManual\nPaper. Please do not forget to cite the ITSx paper whenever you use the software (Bengtsson-Palme et al. 2013).\n\nAvailable on Crunchomics: No\n\n\n\nITSx can be installed from scratch but below is the code needed to install the software with mamba.\nIf there is an issue with the mamba/conda installation, the software can also be downloaded with wget https://microbiology.se/sw/ITSx_1.1.3.tar.gz. After the download, decompress the folder and follow the information in the readme.txt. The download also comes with a test.fasta which can be used to test either installation.\nData for testing can also be found here.\n\nmamba create -n fungi_its\nmamba install -n fungi_its -c bioconda itsx\n\n\n\n\n\nRequired input: FASTA format (aligned or unaligned, DNA or RNA)\nGenerated output:\n\none summary file of the entire run\none more detailed table containing the positions in the respective sequences where the ITS subregions were found\none “semi-graphical” representation of hits\none FASTA file of all identified ITS sequences\none FASTA file for the ITS1 and ITS2 regions\nif entries that did not contain any ITS region are found, a list of sequence IDs representing those entries (optional)\n\nUseful arguments (not extensive, check manual for all arguments):\n\n--save_regions: Get all regions of interest, not only ITS1/2\n-E {value}: E-value cutoff (default 1e-5)\n-S {value}: Domain score cutoff (default 0)\n--cpu {value }: Number of cpus to use (default 1)\n--multi_thread {T/F}: Multi-thread the HMMER-search. On (T) by default if the number of CPUs/cores is larger than one (–cpu option &gt; 1), else off (F)\n--preserve {T/F}: If on, ITSx will preserve the sequence headers from the input file instead of replacing them with ITSx headers in the output. Off (F) by default.\n--only_full {T/F}: If true, the output is limited to full-length ITS1 and ITS2 regions only. Off (F) by default.\n--minlen {value} Minimum length the ITS regions must be to be outputted in the concatenated file (see –concat above). Default is zero (0).\n\n\n\n\n\n#activate the right environment\nmamba activate fungi_its\n\n#download data for testing the installation (optional)\nmkdir testing\nwget -P testing https://raw.githubusercontent.com/ScienceParkStudyGroup/software_information/main/data/itsx/test.fasta\n\n#testrun (adjust path of test.fasta to where ever you downloaded the software)\nITSx -i testing/test.fasta --save_regions all -o testing/ITS_test_v1\n\n\n#deactivate environment (if using environment)\nmamba deactivate\n\nRegions extracted from test file (notice how the full fasta ONLY contains sequences with all regions):\n\ntesting/ITS_test_v1.5_8S.fasta:50\ntesting/ITS_test_v1.ITS1.fasta:50\ntesting/ITS_test_v1.ITS2.fasta:50\ntesting/ITS_test_v1.LSU.fasta:32\ntesting/ITS_test_v1.SSU.fasta:31\ntesting/ITS_test_v1.full.fasta:19\ntesting/ITS_test_v1_no_detections.fasta:0\ntesting/test.fasta:50",
    "crumbs": [
      "Welcome page",
      "Fungal genomics",
      "ITSx"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html",
    "href": "source/Qiime/OTU_table_analysis.html",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "The goal of this tutorial is to analyse 16S rRNA gene data. Our input file is an OTU/ASV table that already contains some taxa information and will go through the following steps:\n\nReading the data into R\nFiltering the data\nNormalizing the data\nVisualizing the data (i.e. alpha/beta diversity, barplots, …)\n\nIn the example we look at an OTU table of 28 samples. These 28 samples represent three distinct microbiomes from three Winogradsky column experiments in which columns were created using wood, paper and a wood/paper mix as substrate. DNA was collected on two separate dates, so another category we can compare is the sampling date.\nIf you want to follow this tutorial, then you can find the required input files here.\n\n\nWe start with setting a seed seed for normalization protocol.\nSetting a set is not essential but this way we make sure that we get the same results when normalizing our OTU table. If we randomly select some observations for any task in R or in any statistical software it results in different values all the time and this happens because of randomization. If we want to keep the values that are produced at first random selection then we can do this by storing them in an object after randomization or we can fix the randomization procedure so that we get the same results all the time.\n\n#check if wdir is correct\n#getwd()\n\n#set seed\nset.seed(1) \n\n\n\n\nSome packages required for this workflow are installed with BiocManager or devtools, if you need to install any of these tools, remove the # from the code and run it.\n\n#if (!require(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n\n#BiocManager::install(\"phyloseq\")\n#BiocManager::install(\"microbiome\")\n#BiocManager::install(\"ALDEx2\")\n#BiocManager::install(\"DESeq2\")\n\n\n\n\n\nlibrary(tidyverse) #general parsing\nlibrary(data.table) #general parsing\nlibrary(phyloseq) #phyloseq object loading\nlibrary(vegan) #rarefaction\nlibrary(microbiome) #normalization\nlibrary(ALDEx2) #stats\nlibrary(DESeq2) #stats\nlibrary(grid) #organizing multiple plots\nlibrary(gridExtra) #organizing multiple plots\nlibrary(scales) #plot aesthetic, comma setting\n#library(ANCOMBC) #stats, tba\n\n\n\n\nNext, we read in some custom things such as:\n\na custom theme that we will use for plotting our graphs\n\na custom function that we use to calculate some summary statistics for our otu table\n\na color vector to have more control over what colors are used in our graphs\n\nDefining custom themes for our plots and custom functions is useful because it means that instead of re-writing the commands for our analyses over and over again if we want to use them more than once, we can just use these functions instead.\nYou can add custom functions closer to where you use them in the code, but I prefer to have them organized in one section of the workflow to keep my code organized.\n\n#define custom theme for generating figures\ncustom_theme &lt;- function() {\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(), \n    panel.border =element_blank(),\n    axis.line.x = element_line(color=\"black\", size = 0.5),\n    axis.line.y = element_line(color=\"black\", size = 0.5),\n    strip.text.x = element_text(size = 7),\n    strip.text.y = element_text(size = 7),\n    strip.background = element_rect(fil=\"#FFFFFF\", color = \"black\", linewidth = 0.5),\n    axis.text.x = element_text(size = 7),\n    axis.text.y = element_text(size = 7),\n    legend.text = element_text(size = 8), legend.title = element_text(size = 10)\n  )\n}\n\n#generate color scheme \nc25 &lt;- c(\"dodgerblue2\", \"#E31A1C\", \"green4\", \"#6A3D9A\", \"#FF7F00\", \"black\", \"gold1\", \"skyblue2\", \"#FB9A99\", \n        \"palegreen2\", \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\", \"deeppink1\", \"blue1\", \n        \"steelblue4\", \"darkturquoise\", \"green1\", \"yellow4\", \"yellow3\",\"darkorange4\", \"brown\")\n\n\n#define a custom function to summarize several aspects of our otu table\nsummarize_table &lt;- function(df) {\n  total_reads &lt;- sum(df)\n  otu_number &lt;- length(df)\n  num_singletons &lt;- length(df[df == 1])\n  num_doubletons &lt;- length(df[df == 2])\n  num_less_than_10 &lt;- length(df[df &lt; 10])\n  total_reads_less_than_10 &lt;- sum(df[df &lt; 10])\n  perc_reads_less_than_10 &lt;- (total_reads_less_than_10 / sum(df)) * 100\n  \n  cat(\"Total number of reads:\", format(total_reads, big.mark = \",\"), \"\\n\")\n  cat(\"Number of OTUs\",  format(otu_number, big.mark = \",\"), \"\\n\")\n  cat(\"Number of singleton OTUs:\",  format(num_singletons, big.mark = \",\"), \"\\n\")\n  cat(\"Number of doubleton OTUs:\",  format(num_doubletons, big.mark = \",\"), \"\\n\")\n  cat(\"Number of OTUs with less than 10 seqs:\",  format(num_less_than_10, big.mark = \",\"), \"\\n\")\n  cat(\"Total reads for OTUs with less than 10 seqs:\",  format(total_reads_less_than_10, big.mark = \",\"), \"\\n\")\n  cat(\"Percentage of reads for OTUs with less than 10 seqs:\",  sprintf(\"%.2f%%\", perc_reads_less_than_10), \"\\n\")\n  \n}\n\n\n\n\n\n\nAn OTU table contains a column with the OTUs (taxonomic ranks in our case) and one column per sample with the counts how often OTU is found in the sample. It might look something like this:\n\n\n\n\n\n\n\n\n\n\n#NAME\nEP1910\nRMT\nKJB3\nTJR\n\n\n\n\nBacteria;Acidobacteria;Acidobacteriia;Acidobacteriales;Acidobacteriaceae (Subgroup 1);NA\n0\n0\n0\n0\n\n\nBacteria;Acidobacteria;Acidobacteriia;Solibacterales;Solibacteraceae (Subgroup 3);PAUC26f\n0\n5\n3\n1\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you follow this tutorial, ensure that your otu table is in a folder called input and that this folder is located where your R script is. If your otu table is somewhere else, change the path stored in filepaths accordingly.\n\n\n\n#provide the path to the otu table\nfile_paths &lt;- c(\"input/otu_table.txt\")\n\n#read in otu table\nmerged_otu_table &lt;- read.table(file_paths, header = T, sep = '\\t', comment = \"\")\ncolnames(merged_otu_table)[1] &lt;- \"taxid\"\n\n#replace NA with 0\nmerged_otu_table[is.na(merged_otu_table)] &lt;- 0\n\n#use the taxon as rownames\nrownames(merged_otu_table) &lt;- merged_otu_table$taxid\nmerged_otu_table$taxid &lt;- NULL\n\n#check how many otus and samples we have\ndim(merged_otu_table)\n\n[1] 1530   28\n\n\nWith this example OTU table, we work with 28 samples and 1530 OTUs.\n\n\n\nIf you have more than one table, for example if you generated an OTU table using different classifiers, you could read in the data as follows:\n\n\nShow the code\nfile_paths &lt;- c(\"results/classification/minimap2/ITS1.merged.outmat.tsv\",\n                \"results/classification/kraken2/ITS1.merged.outmat.tsv\")\n\nmethods &lt;- character(0)\n\nfor (i in 1:length(file_paths)){\n  table &lt;- read.table(file_paths[i], header = T, sep = '\\t', comment = \"\")\n  method &lt;- gsub(\".*/([^/]+)/.*\", \"\\\\1\", file_paths[i])\n  names(table)[1] &lt;- \"taxid\"\n  methods &lt;- c(methods, method)\n  colnames(table)[-1] &lt;- paste0(colnames(table)[-1], \"_\", method)\n  ifelse (i == 1, merged_otu_table &lt;- table, \n          merged_otu_table &lt;- merge(merged_otu_table, table, by = \"taxid\",  all=TRUE))\n} \n\n# Replace NA with 0\nmerged_otu_table[is.na(merged_otu_table)] &lt;- 0\n\n# Restore row names\nrownames(merged_otu_table) &lt;- merged_otu_table$taxid\nmerged_otu_table$taxid &lt;- NULL\n\ndim(merged_otu_table)\n\n\n\n\n\nNext, we read in another table that contains more information about our samples. Such a table could look something like this:\n\n\n\n#NAME\ntreatment\nDate\n\n\n\n\nEP1910\nwood\n2023_1\n\n\nRMT\npaper\n2023_1\n\n\nKJB3\nmix\n2023_1\n\n\nTJR\npaper\n2023_1\n\n\nIB5\nwood\n2023_1\n\n\nALIDNA\nwood\n2023_1\n\n\nIG7\npaper\n2023_1\n\n\nB314\nmix\n2023_1\n\n\n\n\n#read in metadata file\nmetadata_combined &lt;- read.table(\"input/sample_table.txt\", header = TRUE, row.names = 1, sep = \"\\t\", comment.char = \"\")\n\n#add extra column for sample names\nmetadata_combined$name &lt;- paste0(metadata_combined$treatment, \"_\", rownames(metadata_combined))\nmetadata_combined$sample_id &lt;- rownames(metadata_combined)\n\n#order the factors for our names column\nmetadata_combined &lt;- metadata_combined |&gt; \n  arrange(desc(treatment))\n\n#view output\nhead(metadata_combined)\n\n\n\n\n\n\ntreatment\nDate\nname\nsample_id\n\n\n\n\nEP1910\nwood\n2023_1\nwood_EP1910\nEP1910\n\n\nIB5\nwood\n2023_1\nwood_IB5\nIB5\n\n\nALIDNA\nwood\n2023_1\nwood_ALIDNA\nALIDNA\n\n\nVS15G\nwood\n2023_1\nwood_VS15G\nVS15G\n\n\nNZC319\nwood\n2023_1\nwood_NZC319\nNZC319\n\n\nE321\nwood\n2023_1\nwood_E321\nE321\n\n\n\n\n\n\n\n\n\n\nNext, we generate a table that list the taxonomy information for each taxonomic rank. We do this by taking the information from our OTU table. Depending on how you analysed your 16S rRNA gene sequences, you might have:\n\nan OTU table with the taxonomy information as row names instead of IDs. That is what we are working with for this tutorial\nan OTU table with IDs (ASV1, ASV2, … or OTU1, OTU2, …) and a separate table with the taxonomy information. If that is the case, you can read in the taxonomy information separate\n\n\n#extract taxonomy information and convert to separate df\ndf &lt;- as.data.frame(rownames(merged_otu_table))\ncolnames(df) &lt;- \"OTU\"\n\n#separate the taxonomic headers into separate rows                      \ntaxonomy_file &lt;- df |&gt; \n  distinct(OTU) |&gt; \n  separate(OTU,\n           c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\"), \n           sep = \";\", remove = FALSE) |&gt; \n  column_to_rownames(var = \"OTU\")\n\n#view file\nhead(taxonomy_file)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKingdom\nPhylum\nClass\nOrder\nFamily\nGenus\n\n\n\n\nBacteria;Acetothermia;Acetothermiia;bacterium enrichment culture clone 73(2013);bacterium enrichment culture clone 73(2013);bacterium enrichment culture clone 73(2013)\nBacteria\nAcetothermia\nAcetothermiia\nbacterium enrichment culture clone 73(2013)\nbacterium enrichment culture clone 73(2013)\nbacterium enrichment culture clone 73(2013)\n\n\nBacteria;Acetothermia;Acetothermiia;uncultured bacterium;uncultured bacterium;uncultured bacterium\nBacteria\nAcetothermia\nAcetothermiia\nuncultured bacterium\nuncultured bacterium\nuncultured bacterium\n\n\nBacteria;Acidobacteria;Acidobacteriia;Acidobacteriales;Acidobacteriaceae (Subgroup 1);NA\nBacteria\nAcidobacteria\nAcidobacteriia\nAcidobacteriales\nAcidobacteriaceae (Subgroup 1)\nNA\n\n\nBacteria;Acidobacteria;Acidobacteriia;Solibacterales;Solibacteraceae (Subgroup 3);PAUC26f\nBacteria\nAcidobacteria\nAcidobacteriia\nSolibacterales\nSolibacteraceae (Subgroup 3)\nPAUC26f\n\n\nBacteria;Acidobacteria;Aminicenantia;Aminicenantales;NA;NA\nBacteria\nAcidobacteria\nAminicenantia\nAminicenantales\nNA\nNA\n\n\nBacteria;Acidobacteria;Aminicenantia;Aminicenantales;uncultured Aminicenantes bacterium;uncultured Aminicenantes bacterium\nBacteria\nAcidobacteria\nAminicenantia\nAminicenantales\nuncultured Aminicenantes bacterium\nuncultured Aminicenantes bacterium\n\n\n\n\n\n\n\n\n\nA phyloseq object combines different elements of an analysis (i.e. the OTU table, the list of taxa and the mapping file) into one single object. We can easily generate such an object with the three dataframes we have generated above:\n\n#combine data\nOTU = otu_table(merged_otu_table, taxa_are_rows = TRUE)\nTAX = tax_table(as.matrix(taxonomy_file))\nphyseq = phyloseq(OTU, TAX, sample_data(metadata_combined))\n\n#view structure\nphyseq\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1530 taxa and 28 samples ]\nsample_data() Sample Data:       [ 28 samples by 4 sample variables ]\ntax_table()   Taxonomy Table:    [ 1530 taxa by 6 taxonomic ranks ]\n\n\n\n\n\n\n\nBelow, we use our custom function summarize_table calculate some summary statistics. As said above, we could easily do this without a function, however, since we want to compare the statistics before and after filtering the OTU table, the function is useful to have, since we do not need to copy-paste the exact same code in two spots of the workflow:\n\n#calculate the number of reads found per otu\nreads_per_OTU &lt;- taxa_sums(physeq)\n\n#summarize the data\nsummarize_table(reads_per_OTU)\n\nTotal number of reads: 191,661 \nNumber of OTUs 1,530 \nNumber of singleton OTUs: 359 \nNumber of doubleton OTUs: 148 \nNumber of OTUs with less than 10 seqs: 867 \nTotal reads for OTUs with less than 10 seqs: 2,483 \nPercentage of reads for OTUs with less than 10 seqs: 1.30% \n\n\nFor this workflow, we define singletons as reads/OTUs with a sequence that is present exactly once in the dataset.\nNotice that another definition of singletons can be as taxa/OTU present in a single sample.\nIn amplicon data analysis it is useful to remove reads with low counts because they are very likely due to sequencing errors. Here, we make the assumption that sequencing errors are independent and randomly distributed and that erroneous sequences will occur much less often than a biologically true sequence. For our analyses not to be affected by potentially erroneous sequences we will remove them during the data filtering step.\n\n\n\nNext, we calculate the cumulative counts of our reads to get a feeling for how much data we would loose if we remove singletons, doubletons, etc. To do this, we calculate something like this:\nFor OTUs with an read count/abundance of 1, there are 359 occurrences (N), so the cumulative sum is 359.\nFor OTUs with an read count/abundance of 2, there are 148 occurrences (N), so the cumulative sum becomes 359 + 148 = 507.\nThis would tell us that if we would remove doubletons, 507 OTUs total to be removed, and so on … We do this, to get an idea about how many OTUs would be removed in total if we want to remove singletons, doubletons, 5-tons, etc…\n\n#extract the relevant data into a data table\ndt_taxa = data.table(tax_table(physeq), OTUabundance = taxa_sums(physeq), OTU = taxa_names(physeq))\n\n#create a new df with the abundances and the occurrences (N)\ndt_cumsum = dt_taxa[, .N, by = OTUabundance]\n\n#sort the df\nsetkey(dt_cumsum, OTUabundance)\n\n#add a new column with the cumulative sum for the otus\ndt_cumsum[, CumSumOTUs := cumsum(N)]\n\n#add a new column with the cumulative sum for the reads\ndt_cumsum[, CumSumReads := cumsum(dt_cumsum$OTUabundance * dt_cumsum$N )]\n\n#get the total read nr\ntotal_abundance_cumsum &lt;- sum(dt_cumsum$OTUabundance * dt_cumsum$N)\n\n#calc perc of reads removed\ndt_cumsum$perc_read_removed &lt;- dt_cumsum$CumSumReads/total_abundance_cumsum*100\n\n#view the df\nhead(dt_cumsum)\n\n\n\n\n\nOTUabundance\nN\nCumSumOTUs\nCumSumReads\nperc_read_removed\n\n\n\n\n1\n359\n359\n359\n0.1873099\n\n\n2\n148\n507\n655\n0.3417492\n\n\n3\n100\n607\n955\n0.4982756\n\n\n4\n65\n672\n1215\n0.6339318\n\n\n5\n64\n736\n1535\n0.8008932\n\n\n6\n47\n783\n1817\n0.9480280\n\n\n\n\n\n\nNext, we can plot this and zoom into how many OTUs we would remove if we remove singletons, doubletons, 5-tons, … :\n\n# Define the plot\np1 &lt;- ggplot(dt_cumsum, aes(OTUabundance, CumSumOTUs)) + \n  geom_point() +\n  xlab(\"Filtering threshold, minimum total counts\") +\n  ylab(\"OTUs Filtered\") +\n  ggtitle(\"OTUs that would be filtered vs. the minimum count threshold\" ) + \n  custom_theme() +\n  theme(plot.title = element_text(size = 8, face = \"bold\"))\n\n#find the max value to scale our plot\nmax &lt;- max(dt_cumsum$OTUabundance)\n\n# Specify zoom levels\nzoom_levels &lt;- c(1, 2, 5, 10, 25, 50,  100, max)\n\n# Create and arrange plots in a 2x2 grid\nplot_list &lt;- list()\nfor (zoom in zoom_levels) {\n  #subset data to our zoom level\n  subset_data &lt;- dt_cumsum[OTUabundance &lt;= zoom]\n  \n  #define the max value to scale each plot\n  max_cumsum &lt;- max(subset_data$CumSumOTUs)\n  \n  #generate a plot\n  p2 &lt;- p1 + \n    xlim(0, zoom) +\n    ggtitle(\"\") +\n    custom_theme() +\n    scale_y_continuous(limits = c(0, max_cumsum), \n                       breaks = seq(0, max_cumsum, \n                       by = max_cumsum / 4), \n                       labels = label_number())\n  \n  plot_list[[length(plot_list) + 1]] &lt;- p2\n}\n\n# Arrange plots in a 2x2 grid\ngrid.arrange(grobs = plot_list, ncol = 2,\n             top = textGrob(\"OTUs that would be filtered vs. the minimum count threshold\",gp=gpar(fontsize=12,font=3)))\n\n\n\n\n\n\n\n\nWe can also check the percentage of reads filtered:\n\n# Define the plot\np1 &lt;- ggplot(dt_cumsum, aes(OTUabundance, perc_read_removed)) + \n  geom_point() +\n  xlab(\"Filtering threshold, minimum total counts\") +\n  ylab(\"% reads filtered\") +\n  ggtitle(\"OTUs that would be filtered vs. the minimum count threshold\" ) + \n  custom_theme() +\n  theme(plot.title = element_text(size = 8, face = \"bold\"))\n\nmax &lt;- max(dt_cumsum$OTUabundance)\n\n# Specify zoom levels\nzoom_levels &lt;- c(1, 2, 5, 10, 25, 50,  100, max)\n\n# Create and arrange plots in a 2x2 grid\nplot_list &lt;- list()\nfor (zoom in zoom_levels) {\n  subset_data &lt;- dt_cumsum[OTUabundance &lt;= zoom]\n  max_perc &lt;- max(subset_data$perc_read_removed)\n  \n  p2 &lt;- p1 + \n    xlim(0, zoom) +\n    ggtitle(\"\") +\n    custom_theme() +\n    scale_y_continuous(limits = c(0, max_perc), \n                       breaks = seq(0, max_perc, \n                       by = max_perc / 4),\n                       labels = label_comma())\n\n  plot_list[[length(plot_list) + 1]] &lt;- p2\n}\n\n# Arrange plots in a 2x2 grid\ngrid.arrange(grobs = plot_list, ncol = 2,\n             top = textGrob(\"% of reads that would be filtered vs. the minimum count threshold\",gp=gpar(fontsize=12,font=3)))\n\n\n\n\n\n\n\n\nOverall, we can see that even if we are conservative and for example remove 5-tons that while we remove quite a high number of OTUs (~740), however, we remove less than 1% of the reads.\nFinding a good cut-off can be tricky and if you are unsure and do not want to remove rare taxa you might only want to remove singletons or play around with different thresholds.\n\n\n\nNext, let’s explore a bit closer how our reads are distributed among our different samples:\n\n#count the number of reads per sample\nsample_counts &lt;- as.data.frame(colSums(merged_otu_table))\n                  \n#clean up the dataframe\nnames(sample_counts)[1] &lt;- \"counts\"\nsample_counts$sampleID &lt;- rownames(sample_counts)\n\n#plot counts\np_counts &lt;-\n  ggplot(data = sample_counts, aes(x = reorder(sampleID, counts, FUN=sum, decreasing = TRUE), y = counts)) +\n  geom_point() +\n  geom_text(aes(x = , sampleID, y = counts, label = counts),  hjust = 0, nudge_y = 200 , size = 2.5) +\n  coord_flip() +\n  xlab(\"\") + \n  ylab(\"Read counts\") +\n  custom_theme()\n\np_counts\n\n\n\n\n\n\n\n\nIn this example, we see two samples with almost no reads and we want to make sure to remove these samples in our filtering step.\nWe also see that we have a large difference between different samples and that our samples have read depths ranging from 2 to 24,368. To be able to compare for example sample IV (~25,000 reads) with sample MN (~1,000 reads) we need to normalize our data after the data filtering step.\n\n\n\n\nNext, we filter the data. Specifically, we:\n\nRemove OTUs that are not assigned to anything at Phylum rank. The subset_taxa function can be used to remove any taxa you want, i.e. if you have plant DNA in your sample, you could use this to remove chloroplast sequences as well.\n\nRemove samples with total read counts less than 20. This cutoff is arbitrary and depends a bit on your data. To choose a good value, explore the read counts you have per sample and define a cutoff based on that. In this example, we mainly want to remove the two low read samples we have seen in our plot.\n\nRemove low count OTUs: The threshold is up to you; removing singletons or doubletons is common, but you can be more conservative and remove any counts less than 5 or 10, … . To decide on a cutoff you can look at the plots we generated before to get a feeling for how many OTUs/reads will be removed.\n\nIn our example, we want to remove samples with 20 or less reads, remove singletons only and remove OTUs that occur in less than 10% of our samples (v1). Since there are many different thoughts about OTU table filtering, you can also find two other options (v2 and v3) on how to filter a table.\nFor most analyses we will work with the filtered data but there are some diversity metrics which rely on the presence of singletons within a sample (richness estimates, i.e. Chao), so you might choose to leave them in for those sorts of analyses only.\n\n#define cutoffs\ncounts_per_sample &lt;- 20\notu_nr_cutoff &lt;- 1\nmin_percentage_samples &lt;- 10\n\n#remove taxa without tax assignment at Phylum rank\nphyseq_filt &lt;- subset_taxa(physeq, Phylum != \"NA\")\n\n#remove samples with less than 20 reads\nphyseq_filt &lt;- prune_samples(sample_sums(physeq)&gt;= counts_per_sample, physeq)\n\n#calculate the minimum number of samples an otu should be present in\nmin_samples &lt;- ceiling((min_percentage_samples / 100) * nsamples(physeq_filt))\n\n#remove otus that occur only rarely (v1)\n#here, we calculate the total abundance of each otu across all samples and checks if it's greater than the specified otu_nr_cutoff AND we check if the otu occurs in at least &lt;min_percentage_samples&gt;% of samples\n#we only retain OTUs that satisfy this condition \nphyseq_filt &lt;- prune_taxa(taxa_sums(physeq_filt) &gt; otu_nr_cutoff & taxa_sums(physeq_filt) &gt;= min_samples, physeq_filt)\nphyseq_filt\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1023 taxa and 26 samples ]\nsample_data() Sample Data:       [ 26 samples by 4 sample variables ]\ntax_table()   Taxonomy Table:    [ 1023 taxa by 6 taxonomic ranks ]\n\n\nHere, you find some alternative ideas on how to filter your data:\n\n#remove otus that occur only rarely (v2)\n#here, we count the number of samples where the abundance of an otu is &gt; 0. \n#If this count is greater than the specified otu_nr_cutoff, the taxon is retained.\n#physeq_filt &lt;- filter_taxa(physeq, function (x) {sum(x &gt; 0) &gt; otu_nr_cutoff}, prune=TRUE)\n#physeq_filt\n\n#remove otus that occur only rarely (v3)\n#here, we remove taxa not seen more than 1 times in at least 10% of the samples\n#physeq_filt = filter_taxa(physeq, function(x) sum(x &gt; 1) &gt; (0.1*length(x)), TRUE)\n#physeq_filt\n\nNext, we can calculate the summary statistics with the custom summarize_table function we have defined before:\n\n#calculate the number of reads found per otu\nreads_per_OTU_filt &lt;- taxa_sums(physeq_filt)\n\n#summarize the data\nsummarize_table(reads_per_OTU_filt)\n\nTotal number of reads: 190,995 \nNumber of OTUs 1,023 \nNumber of singleton OTUs: 0 \nNumber of doubleton OTUs: 0 \nNumber of OTUs with less than 10 seqs: 360 \nTotal reads for OTUs with less than 10 seqs: 1,827 \nPercentage of reads for OTUs with less than 10 seqs: 0.96% \n\n\n\n\n\nBelow, we generate three new phyloseq objects using three different normalization approaches:\n1. *Compositional**: transforms the data into relative abundance\n2. *CLR**: stands for centered log-ratio transform and allows us to compare proportions of OTUs within each sample. After this transformation the values will no longer be counts, but rather the dominance (or lack thereof) for each taxa relative to the geometric mean of all taxa on the logarithmic scale\n3. *Rarefaction**: scales all of your reads down to the lowest total sequencing depth. Notice, that this might drop many OTUs in higher read samples and might lead to under-estimation of low-abundant OTUs. Here, we use the non-filtered data as rarefaction expects singletons, and for that purpose they should be retained, unless you have reason to think they are wrong (and which of them are wrong). Depending on whether you want to filter taxa (such as chloroplast), you might want to filter those before doing rarefaction.\nGenerating different phyloseq objects for different normalization approaches allows us to easily compare analysis steps with different inputs.\nIf you want to familiarize yourself more about why and how to normalize data, feel free to check out the following resources that give some more insights into the topic. As you will see, there is a lot of debate about data treatment and you might want to test and compare different approaches with your data.\n\nMicrobiome Datasets Are Compositional: And This Is Not Optional\nUnderstanding sequencing data as compositions: an outlook and review\nWaste Not, Want Not: Why Rarefying Microbiome Data is Inadmissible\nTo rarefy or not to rarefy: robustness and efficiency trade-offs of rarefying microbiome data\nNormalization and microbial differential abundance strategies depend upon data characteristics\n\n\nphyseq_rel_abundance &lt;- microbiome::transform(physeq_filt, \"compositional\")\nphyseq_clr &lt;- microbiome::transform(physeq_filt, \"clr\")\nphyseq_rarified &lt;- rarefy_even_depth(physeq)\n\n\n\n\n\n\nRarefaction curves illustrate how well your sample was sampled. The rarefaction function takes a random subsample of each column in your OTU table of a given size, starting with a very small subsample, and counts how many unique OTUs were observed in that subsample. The analysis is repeated with increasing subsample sizes until the maximum actual read depth for your table is reached.\nIn an ideal dataset, each curve should reach a plateau, i.e. horizontal asymptote, ideally around the same depth. While this almost never is the case, exploring these graphs gives us an idea how well each sample was sampled.\nIn our dataset below, we will use the unfiltered data, since rarefaction is highly dependent on the number of singletons in a sample – the rarer the sequence, the more likely it is that it will be observed only once in a sample.\n\n#create a df from our otu table\ndf &lt;- as.data.frame(otu_table(physeq))\n\n#rarefy data with a step size of 50, using tidy = TRUE will return a dataframe instead of a plot\ndf_rare &lt;- rarecurve(t(df), step=50, cex=0.5, label = FALSE, tidy = TRUE)\n\n#add metadata\ndf_rare &lt;- merge(df_rare, metadata_combined, by.x = \"Site\", by.y = \"sample_id\")\n\n#transform df and plot\nplot_rare &lt;-\n  df_rare |&gt;\n    group_by(Site) |&gt; \n    #find the max value per site\n    mutate(max_sample = max(Sample)) |&gt;\n    #for each site add a label for the max value only (we do this to create a single label to add to the plot)\n    mutate(label = if_else(Sample == max_sample, as.character(Site), NA_character_)) |&gt;\n    #plot\n    ggplot(aes(x = Sample, y = Species, color = treatment, group = interaction(Site, treatment))) + \n    geom_line() + \n    facet_grid(cols = vars(treatment)) +\n    geom_text(aes(label = label),\n              position = position_nudge(x = 2000),\n              na.rm = TRUE, size = 3) +\n    custom_theme()\n\nplot_rare\n\n\n\n\n\n\n\n\nIn this example we can see that almost none of the sample reach a plateau, RH is the closest but not there yet. This suggests that we mainly sampled the abundant members of our community and might miss many rare taxa. This is something to keep in mind for other analyses, such as alpha diversity analyses.\n\n\n\nAlpha diversity measures the diversity within our sample and we distinguish between species richness (i.e. the number of species) and species richness (i.e. how relatively abundant each of the species are).\nSomething to keep in mind: Some richness-based alpha diversity metrics require singletons to estimate OTUs/ASVs observed zero times in the data. These methods try to to estimate how many rare things are around that you didn’t see, and the rare things you did see (that inform you about the unseen others) will be seen 1 or 2 times. Different methods (e.g. rarefaction or Chao’s S1) all have that basic dependence.\nHowever, working with sequencing data we have one caveat: Some sequences are being miss classified as new OTUs/ASVs that simply don’t really exist. More specifically, errors/chimeras/artefacts/contaminants get interpreted as real OTUs (because they are different enough) and those show up largely in the singleton/doubletons and can almost entirely drive richness estimates to nonsensical values.\n\n#calculate different alpha diversity indicators\nrichness_meta &lt;-microbiome::alpha(physeq_filt, index = \"all\")\n\n#add the sample id to table\nrichness_meta$sample_id &lt;- rownames(richness_meta)\n\n#add other metadata data\nrichness_meta  &lt;- merge(richness_meta, metadata_combined, by = \"sample_id\")\n\n#check what parameters are calculated\ncolnames(richness_meta)\n\n [1] \"sample_id\"                  \"observed\"                  \n [3] \"chao1\"                      \"diversity_inverse_simpson\" \n [5] \"diversity_gini_simpson\"     \"diversity_shannon\"         \n [7] \"diversity_fisher\"           \"diversity_coverage\"        \n [9] \"evenness_camargo\"           \"evenness_pielou\"           \n[11] \"evenness_simpson\"           \"evenness_evar\"             \n[13] \"evenness_bulla\"             \"dominance_dbp\"             \n[15] \"dominance_dmn\"              \"dominance_absolute\"        \n[17] \"dominance_relative\"         \"dominance_simpson\"         \n[19] \"dominance_core_abundance\"   \"dominance_gini\"            \n[21] \"rarity_log_modulo_skewness\" \"rarity_low_abundance\"      \n[23] \"rarity_rare_abundance\"      \"treatment\"                 \n[25] \"Date\"                       \"name\"                      \n\n\nNext, we can generate a plot. Based on our discussion on singletons and since we calculated the diversity estimates after removing singletons, we will look at evenness indices, such as the Shannon index.\n\n#generate figure\nalpha_plot &lt;-\n  ggplot(richness_meta, aes(x = treatment, y = diversity_shannon)) +\n    geom_boxplot() +\n    geom_jitter(aes(color = Date), width = 0.2, size = 4) +\n    labs(x = \"\", y = \"Shannon index\") +\n    custom_theme() +\n    theme(legend.key=element_blank())\n\nalpha_plot\n\n\n\n\n\n\n\n#save the figure to our computer\n#ggsave(paste0(\"results/plots/alpha-div.png\"), plot=alpha_plot, device=\"png\")\n\nWe see that there is not a difference in terms of species evenness when comparing our different samples.\n\n\n\nIn contrast to alpha diversity, beta diversity quantifies the dissimilarity between communities (multiple samples).\nCommonly used beta-diversity measures include the:\n\nBray-Curtis index (for compositional/abundance data)\n\nJaccard index (for presence/absence data)\n\nAitchison distance (use Euclidean distance for clr transformed abundances, aiming to avoid the compositionality bias)\n\nUnifrac distance (that takes into account the phylogenetic tree information)\n\n…\n\nIn general it is best to avoid methods that use Euclidean distance as microbiome data are compositional, sparse datasets and better suited for the above mentioned methods. Aitchison is an exception as it calculates an Euclidean distance only after clr transformation (after which our data is suited for an Euclidean space).\nThe methods above generate distance matrices, which can then be used for ordination,which we use to reduce the dimensionality of our data for a more efficient analysis and visualization. Based on the type of algorithm, ordination methods can be divided in two categories:\n\nunsupervised, which measure variation in the data without additional information on covariates or other supervision of the model, including:\n\nPrincipal Coordinate Analysis (PCoA)\n\nPrincipal Component Analysis (PCA)\n\nUniform Manifold Approximation and Projection for Dimension Reduction (UMAP)\n\n\nsupervised ordination:\n\ndistance-based Redundancy Analysis (dbRDA)\n\n\n\n\nFor Bray-Curtis our data should not have negative entries, so we will use the relative abundance (alternatively, you could also look at the rarefied) data:\n\n#calculate PCOA using Phyloseq package\npcoa_bc &lt;- ordinate(physeq_rarified, \"PCoA\", \"bray\") \n\nplot_beta &lt;-\n  plot_ordination(physeq_rarified, pcoa_bc, color = \"treatment\") + \n    geom_point(size = 3) +\n    #add a 95% confidence level for a multivariate t-distribution\n    stat_ellipse(aes(group = treatment), linetype = 2) +\n    custom_theme()\n\nplot_beta\n\nWarning in MASS::cov.trob(data[, vars]): Probable convergence failure\n\n\n\n\n\n\n\n\n\nThis two-dimensions PCOA plot shows 34% of the total variance between the samples. We can also see that our samples are not forming distinct clusters, i.e. microbiomes from the mixed, paper and wood communities appear very similar.\nWe can also perform our statistics to confirm this visual finding:\n\n#Generate distance matrix\nrare_dist_matrix &lt;- phyloseq::distance(physeq_rarified, method = \"bray\") \n\n#ADONIS test\nadonis2(rare_dist_matrix ~ phyloseq::sample_data(physeq_rarified)$treatment, permutations=9999, method=\"bray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSumOfSqs\nR2\nF\nPr(&gt;F)\n\n\n\n\nphyloseq::sample_data(physeq_rarified)$treatment\n2\n0.9508929\n0.0750529\n1.014286\n0.4187\n\n\nResidual\n25\n11.7187500\n0.9249471\nNA\nNA\n\n\nTotal\n27\n12.6696429\n1.0000000\nNA\nNA\n\n\n\n\n\n\nThis confirms that there do not seem to be any statistically significant differences between our samples.\n\n\n\nNext, we generate a beta-diversity ordination using the Aitchison distance as an alternative way to look at beta diversity (and well suited for investigating compositional data). We do this by applying PCA to our centered log-ratio (clr) transformed counts.\n\n#PCA via phyloseq\n#RDA without constraints is a PCA\nord_clr &lt;- phyloseq::ordinate(physeq_clr, \"RDA\")\n\n#Plot scree plot to plot eigenvalues, i.e.the total amount of variance that can be explained by a given principal component\nphyloseq::plot_scree(ord_clr) + \n  geom_bar(stat=\"identity\") +\n  custom_theme() +\n  labs(x = \"\", y = \"Proportion of Variance\\n\") \n\n\n\n\n\n\n\n\n\n#scale axes based on the eigenvalues\nclr1 &lt;- ord_clr$CA$eig[1] / sum(ord_clr$CA$eig)\nclr2 &lt;- ord_clr$CA$eig[2] / sum(ord_clr$CA$eig)\n\n#and plot\nphyloseq::plot_ordination(physeq_filt, ord_clr, color = \"treatment\") + \n  geom_point(size = 2) +\n  coord_fixed(clr2 / clr1) +\n  stat_ellipse(aes(group = treatment), linetype = 2) +\n  custom_theme() \n\n\n\n\n\n\n\n\nWhile PCA is an exploratory data visualization tool, we can test whether the samples cluster beyond that expected by sampling variability using permutational multivariate analysis of variance (PERMANOVA) with the adonis package.\n\n#Generate distance matrix\nclr_dist_matrix &lt;- phyloseq::distance(physeq_clr, method = \"euclidean\") \n\n#ADONIS test\nadonis2(clr_dist_matrix ~ phyloseq::sample_data(physeq_clr)$treatment, permutations=9999, method=\"bray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSumOfSqs\nR2\nF\nPr(&gt;F)\n\n\n\n\nphyloseq::sample_data(physeq_clr)$treatment\n2\n4301.017\n0.0910778\n1.152348\n0.2031\n\n\nResidual\n23\n42922.544\n0.9089222\nNA\nNA\n\n\nTotal\n25\n47223.561\n1.0000000\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nNext, we want to plot the taxa distribution. Let us first look at the most abundant phyla and check how similar our different samples are:\n\n#condense data at specific tax rank, i.e. on phylum level\ngrouped_taxa &lt;- tax_glom(physeq_rel_abundance, \"Phylum\", NArm=FALSE)\n  \n#find top19 most abundant taxa \ntop_taxa &lt;- names(sort(taxa_sums(grouped_taxa), TRUE)[1:19])\n\n#make a list for the remaining taxa\nother_taxa &lt;- names(taxa_sums(grouped_taxa))[which(!names(taxa_sums(grouped_taxa)) %in% top_taxa)]\n\n#group the low abundant taxa into one group\nmerged_physeq = merge_taxa(grouped_taxa, other_taxa, 2)\n  \n#transform phyloseq object into dataframe\ndf &lt;- psmelt(merged_physeq)\n\n#cleanup the names in the df\nnames(df)[names(df) == \"Phylum\"] &lt;- \"tax_level\"\n\n#replace NAs, with other (NAs are generated for the other category)\ndf$tax_level[which(is.na(df$tax_level))] &lt;- \"Other\"\n  \n#create a df to sort taxa labels by abundance\nsorted_labels &lt;- df |&gt; \n  group_by(tax_level) |&gt; \n  summarise(sum = sum(Abundance)) |&gt; \n  arrange(desc(sum))\n  \n#Get list of sorted levels excluding \"Other\"\ndesired_levels &lt;- setdiff(sorted_labels$tax_level, \"Other\")\n  \n#sort df using the sorted levels and ensure that \"Other\" is the last category\ndf$tax_level2 &lt;- factor(df$tax_level, levels = c(desired_levels, \"Other\"))\n  \n#generate color scheme\ncols &lt;- c25[1:length(unique(df$tax_level2))]\ncols[levels(df$tax_level2) == \"Other\"] &lt;- \"#CCCCCC\"\n  \n#plot\nfig &lt;-\n  ggplot(df, aes(x = Sample, y = Abundance, fill = tax_level2) ) +\n    geom_bar(position = \"stack\", stat = \"identity\", width = 0.9) +\n    labs(y = \"Relative abundance\", x = \"\", title = paste0(\"Relative abundance at \", \"Phylum\", \" rank\")) +\n    scale_fill_manual(name = paste0(\"Phylum\",\"_rank\"), labels = levels(df$tax_level2), values = cols) +\n    facet_grid(cols =  vars(treatment), scales = \"free\", space = \"free\") +\n    #scale_y_continuous(expand = c(0, 0), limits = c(0, 1.01)) +\n    custom_theme() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1 ) ) +\n    guides(fill=guide_legend(title = \"Phylum\"))\n  \nfig\n\n\n\n\n\n\n\n\nSince we want to generate one plot for each taxonomic rank, i.e. Phylum, Class, Order,…, we can do this in a loop. The figures will be generated in the folder results/plots/.\n\n#if not there already, create output folder\nimg_path=\"results/plots/\"\ndir.create(img_path, recursive = TRUE)\n\n#generate one barplot for each taxonomic level\nfor (level in colnames(taxonomy_file)){\n  \n  #condense data at specific tax rank, i.e. on phylum level\n  grouped_taxa &lt;- tax_glom(physeq_rel_abundance, level)\n  \n  #find top19 most abundant taxa \n  top_taxa &lt;- names(sort(taxa_sums(grouped_taxa), TRUE)[1:19])\n  \n  #make a list for the remaining taxa\n  other_taxa &lt;- names(taxa_sums(grouped_taxa))[which(!names(taxa_sums(grouped_taxa)) %in% top_taxa)]\n  \n  #group the low abundant taxa into one group\n  merged_physeq = merge_taxa(grouped_taxa, other_taxa, 2)\n  \n  #transform phyloseq object into dataframe\n  df &lt;- psmelt(merged_physeq) \n  \n  #cleanup the dataframe\n  names(df)[names(df) == level] &lt;- \"tax_level\"\n  df$tax_level[which(is.na(df$tax_level))] &lt;- \"Other\"\n  \n  #create a df to sort taxa labels by abundance\n  sorted_labels &lt;- df |&gt; \n    group_by(tax_level) |&gt; \n    summarise(sum = sum(Abundance)) |&gt; \n    arrange(desc(sum))\n  \n  #Get list of sorted levels excluding \"Other\"\n  desired_levels &lt;- setdiff(sorted_labels$tax_level, \"Other\")\n  \n  #sort df using the sorted levels and ensure that \"Other\" is the last category\n  df$tax_level2 &lt;- factor(df$tax_level, levels = c(desired_levels, \"Other\"))\n  \n  #generate color scheme\n  cols &lt;- c25[1:length(unique(df$tax_level2))]\n  cols[levels(df$tax_level2) == \"Other\"] &lt;- \"#CCCCCC\"\n  \n  #plot\n  fig &lt;-\n    ggplot(df, aes(x = Sample, y = Abundance, fill = tax_level2) ) +\n      geom_bar(position = \"stack\", stat = \"identity\", width = 0.9) +\n      labs(y = \"Relative abundance\", x = \"\", title = paste0(\"Relative abundance at \", level, \" rank\")) +\n      scale_fill_manual(name = paste0(level,\"_rank\"), labels = levels(df$tax_level2), values = cols) +\n      facet_grid(cols =  vars(treatment), scales = \"free\", space = \"free\") +\n      scale_y_continuous(expand = c(0, 0), limits = c(0, 1.01)) +\n      custom_theme() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1 ) ) +\n      guides(fill=guide_legend(title=level))\n  \n  ggsave(paste0(img_path, level, \"_barplot_ra.png\"), plot = fig, device=\"png\")\n  }\n\nGenerated plots:\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\nNext, we want to test, whether any OTUs differ among our different conditions, i.e. sampling dates or treatment.\n\n\n\n\n\n\nCaution\n\n\n\nNotice: This is still in development and needs to be optimized, use with care!\n\n\n\n\nFirst, let us compare for differences for a category where we compare two factors. For example, we might want to ask whether there are any significant differences when comparing our samples isolated at different dates.\nLet’s first test this using the non-parametric Wilcoxon rank-sum test.\n\n#Generate data.frame with OTUs and metadata\nps_wilcox &lt;- as.data.frame(t(as.data.frame(otu_table(physeq_clr))))\nps_wilcox$Date &lt;- sample_data(physeq_clr)$Date\n\n#Define functions to pass to map\nwilcox_model &lt;- function(df){\n  wilcox.test(abund ~ Date, data = df)\n}\n\nwilcox_pval &lt;- function(df){\n  wilcox.test(abund ~ Date, data = df)$p.value\n}\n\n#Create nested data frames by OTU and loop over each using map \nwilcox_results &lt;- ps_wilcox |&gt;\n  gather(key = OTU, value = abund, -Date) |&gt;\n  group_by(OTU) |&gt;\n  nest() |&gt;\n  mutate(wilcox_test = map(data, wilcox_model),\n         p_value = map(data, wilcox_pval))  \n\n#Show results\nwilcox_results$wilcox_test[[1]]\n\n\n    Wilcoxon rank sum exact test\n\ndata:  abund by Date\nW = 124, p-value = 0.04412\nalternative hypothesis: true location shift is not equal to 0\n\n\nOk, there seems to be something going on, let’s zoom in and look for differences per OTU level (genus rank in our case). To do this we first will correct for multiple comparison testing via false discovery rate (FDR). We also will filter the table to only include values with an FDR &lt; 0.001.\n\n#unnest df\nwilcox_results &lt;- wilcox_results |&gt;\n  dplyr::select(OTU, p_value) |&gt;\n  unnest(cols = c(p_value))\n\n#Computing FDR corrected p-values (since we do multiple statistical comparisons)\nwilcox_results &lt;- wilcox_results |&gt;\n  arrange(p_value) |&gt;\n  mutate(BH_FDR = p.adjust(p_value, \"BH\")) |&gt;\n  filter(BH_FDR &lt; 0.001) |&gt;\n  dplyr::select(OTU, p_value, BH_FDR, everything())\n\nhead(wilcox_results)  \n\n\n\n\n\n\n\n\n\n\nOTU\np_value\nBH_FDR\n\n\n\n\nBacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Ika33;uncultured bacterium\n0.0000523\n0.0000523\n\n\nBacteria;Bacteroidetes;Bacteroidia;Bacteroidales;NA;uncultured bacterium\n0.0001304\n0.0001304\n\n\nBacteria;Bacteroidetes;Chlorobia;Chlorobiales;Chlorobiaceae;Prosthecochloris\n0.0003840\n0.0003840\n\n\nBacteria;Firmicutes;Clostridia;Clostridiales;Clostridiaceae 1;Clostridium sensu stricto 1\n0.0004930\n0.0004930\n\n\nBacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Marinilabiliaceae;Marinilabilia\n0.0006280\n0.0006280\n\n\nBacteria;Proteobacteria;Deltaproteobacteria;Bdellovibrionales;Bacteriovoracaceae;uncultured\n0.0006280\n0.0006280\n\n\n\n\n\n\nThere, seem to be a few OTUs that are distinct between sampling dates, lets explore them further by generating some plots:\n\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_clr)[as.character(wilcox_results$OTU), ], \"matrix\")),as.data.frame(as(otu_table(physeq_clr)[as.character(wilcox_results$OTU), ], \"matrix\")))\n\ndf_long &lt;- reshape2::melt(df)\ndf_long &lt;- merge(df_long, metadata_combined, by.x = \"variable\", by.y = \"sample_id\")\n\nggplot(df_long, aes(x=Genus, y=value, color = Date)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n  facet_wrap(vars(Genus), scales = \"free\") +\n  scale_color_manual(values = c(\"orange\", \"purple\")) +\n  xlab(\"\") +\n  ylab(\"CLR abundance\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nWe can see that all of the OTUs that showed differences between sampling dates are very low abundant OTUs.\nWilcoxon has some down-sites when it comes to treating zero values, an alternative approach is to use the ANOVA-like differential expression (ALDEx2) method. The aldex function is a wrapper that performs log-ratio transformation and statistical testing in a single line of code, which is why we feed the non-normalized data into it:\n\naldex2_da &lt;- ALDEx2::aldex(data.frame(phyloseq::otu_table(physeq_filt)), phyloseq::sample_data(physeq_filt)$Date, test=\"t\", effect = TRUE, denom=\"iqlr\")\n\nSpecifically, this function:\n\ngenerates Monte Carlo samples of the Dirichlet distribution for each sample,\nconverts each instance using a log-ratio transform,\n\nreturns test results for two sample (Welch’s t, Wilcoxon) or multi-sample (glm, Kruskal-Wallace) tests.\n\nBu using iqlr the function accounts for data with systematic variation and centers the features on the set features that have variance that is between the lower and upper quartile of variance. This provides results that are more robust to asymmetric features between groups.\nNext, we can plot the effect size. This plot shows the median log2 fold difference by the median log2 dispersion (a measure of the effect size by the variability). Differentially abundant taxa will be those where the difference exceeds the dispersion. Points toward the top of the figure are more abundant in our samples from sampling date 2 while those towards the bottom are more abundant in sampling date 1. Taxa with BH-FDR corrected p-values would be shown in red.\n\n#plot effect sizes\nALDEx2::aldex.plot(aldex2_da, type=\"MW\", test=\"wilcox\", called.cex = 1, cutoff.pval = 0.001)\n\n\n\n\n\n\n\n\nFinally, we can print the output with the taxa information added.\n\n#Clean up presentation\nsig_aldex2 &lt;- aldex2_da |&gt;\n  rownames_to_column(var = \"OTU\") |&gt;\n  filter(wi.eBH &lt; 0.001) |&gt;\n  arrange(effect, wi.eBH) |&gt;\n  dplyr::select(OTU, diff.btw, diff.win, effect, wi.ep, wi.eBH)\n\nhead(sig_aldex2)\n\n\n\n\n\nOTU\ndiff.btw\ndiff.win\neffect\nwi.ep\nwi.eBH\n\n\n\n\n\n\n\n\nIn our case an empty dataframe is returned, no significant differences were detected when using this statistical approach.\n\n\n\nNext, we want to test, if there are any OTUs that differ between our three distinct growth conditions, i.e. treatments. One way to do this is to use phyloseqs mt function.\nThe default test applied to each taxa is a two-sample two-sided t.test. In our case this would fail with an error since we want to look at a data variable that contains three classes. To run this, we add test = \"f\" to run a F-test. Additionally, we use FDR as multiple-hypthesis correction. B specifies the number of permutations.\n\n#F-test, by specifying test=\"f\" \nres &lt;- mt(physeq_clr, \"treatment\", method = \"fdr\", test = \"f\", B = 300)\n\nB=300\nb=3 b=6 b=9 b=12    b=15    b=18    b=21    b=24    b=27    b=30    \nb=33    b=36    b=39    b=42    b=45    b=48    b=51    b=54    b=57    b=60    \nb=63    b=66    b=69    b=72    b=75    b=78    b=81    b=84    b=87    b=90    \nb=93    b=96    b=99    b=102   b=105   b=108   b=111   b=114   b=117   b=120   \nb=123   b=126   b=129   b=132   b=135   b=138   b=141   b=144   b=147   b=150   \nb=153   b=156   b=159   b=162   b=165   b=168   b=171   b=174   b=177   b=180   \nb=183   b=186   b=189   b=192   b=195   b=198   b=201   b=204   b=207   b=210   \nb=213   b=216   b=219   b=222   b=225   b=228   b=231   b=234   b=237   b=240   \nb=243   b=246   b=249   b=252   b=255   b=258   b=261   b=264   b=267   b=270   \nb=273   b=276   b=279   b=282   b=285   b=288   b=291   b=294   b=297   b=300   \nr=10    r=20    r=30    r=40    r=50    r=60    r=70    r=80    r=90    r=100   \nr=110   r=120   r=130   r=140   r=150   r=160   r=170   r=180   r=190   r=200   \nr=210   r=220   r=230   r=240   r=250   r=260   r=270   r=280   r=290   r=300   \nr=310   r=320   r=330   r=340   r=350   r=360   r=370   r=380   r=390   r=400   \nr=410   r=420   r=430   r=440   r=450   r=460   r=470   r=480   r=490   r=500   \nr=510   r=520   r=530   r=540   r=550   r=560   r=570   r=580   r=590   r=600   \nr=610   r=620   r=630   r=640   r=650   r=660   r=670   r=680   r=690   r=700   \nr=710   r=720   r=730   r=740   r=750   r=760   r=770   r=780   r=790   r=800   \nr=810   r=820   r=830   r=840   r=850   r=860   r=870   r=880   r=890   r=900   \nr=910   r=920   r=930   r=940   r=950   r=960   r=970   r=980   r=990   r=1000  \nr=1010  r=1020  \n\n#identify significant otus\nres_sig &lt;- res |&gt; \n  filter(adjp &lt; 0.001)\n  \nhead(res_sig)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nindex\nteststat\nrawp\nadjp\nplower\nKingdom\nPhylum\nClass\nOrder\nFamily\nGenus\nfdr\n\n\n\n\n\n\n\n\nWe get an empty dataframe, since no significant differences were detected.\n\n\n\nDESeq2 performs a differential expression analysis based on the Negative Binomial (a.k.a. Gamma-Poisson) distribution. DeSeq normalizes the data throughout its analysis, so we input only the filtered data. For more theory, visit this page.\n\n#convert treatment column to a factor\nsample_data(physeq_filt)$treatment &lt;- as.factor(sample_data(physeq_filt)$treatment)\n\n#Convert the phyloseq object to a DESeqDataSet and run DESeq2:\nds &lt;- phyloseq_to_deseq2(physeq_filt, ~ treatment)\n\n#since our samples contain a lot of 0s (something DeSeq is NOT designed for) we use some alternative means to estimate the size factor\nds &lt;-estimateSizeFactors(ds, type = 'poscounts')\nds &lt;- DESeq(ds)\n\n#extract the results and filter by a FDR cutoff of 0.001 and\n#find significantly differentially abundant OTU between the seasons “paper” and “wood”\nalpha = 0.001\nres = results(ds, contrast=c(\"treatment\", \"paper\", \"wood\"), alpha=alpha)\nres = res[order(res$padj, na.last=NA), ]\nres_sig = res[(res$padj &lt; alpha), ]\n\n#print number of significant results\ndim(res_sig)\n\n[1] 2 6\n\n\nPlot significant OTUs (log2fold change):\n\n#extract relevant data for significant otus\nres_sig = cbind(as(res_sig, \"data.frame\"), as(tax_table(physeq_filt)[rownames(res_sig), ], \"matrix\"))\n\n#plot\nggplot(res_sig, aes(x=Genus, y=log2FoldChange, color=Phylum)) +\n  geom_jitter(size=3, width = 0.2) +\n  custom_theme() +\n  theme(axis.text.x = element_text(angle = -90, hjust = 0, vjust=0.5))\n\n\n\n\n\n\n\n\nPlot significant OTUs (relative abundance):\n\n#extract relevant data for significant otus\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_clr)[rownames(res_sig), ], \"matrix\")),as.data.frame(as(otu_table(physeq_clr)[rownames(res_sig), ], \"matrix\")))\n\n#convert to long df\ndf_long &lt;- reshape2::melt(df)\n\nUsing Kingdom, Phylum, Class, Order, Family, Genus as id variables\n\n#add metadata\ndf_long &lt;- merge(df_long, metadata_combined, by.x = \"variable\", by.y = \"sample_id\")\n\n#plot\nggplot(df_long, aes(x=Genus, y=value, color = treatment)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n  facet_wrap(vars(Genus), scales = \"free\") +\n  scale_color_manual(values = c(\"grey\", \"orange\", \"purple\")) +\n  xlab(\"\") +\n  ylab(\"CLR abundance\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nNotice, how in our plots there seems to be no major difference between the wood and paper samples for these OTUs. Considering that both OTUs have marked outliers, it is likely that our statistics are driven a lot by these outliers and we should interpret our results with care.\nAlso, remember that these two genera are also not returned with the other statistical test we did. If we stay conservative we would say that we see no differences between the different sample types.\n\n\n\ntba\n\n#out = ancom(data = NULL, assay_name = NULL,\n#           tax_level = \"Genus\", phyloseq = physeq_filt,\n#           p_adj_method = \"holm\", prv_cut = 0.10, lib_cut = 1000,\n#           main_var = \"treatment\",\n#           rand_formula = NULL, lme_control = NULL, struc_zero = TRUE,\n#           neg_lb = TRUE, alpha = 0.05, n_cl = 2)\n\n\n#res = out$res\n#res",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#setup",
    "href": "source/Qiime/OTU_table_analysis.html#setup",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "We start with setting a seed seed for normalization protocol.\nSetting a set is not essential but this way we make sure that we get the same results when normalizing our OTU table. If we randomly select some observations for any task in R or in any statistical software it results in different values all the time and this happens because of randomization. If we want to keep the values that are produced at first random selection then we can do this by storing them in an object after randomization or we can fix the randomization procedure so that we get the same results all the time.\n\n#check if wdir is correct\n#getwd()\n\n#set seed\nset.seed(1)",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#installation-notes",
    "href": "source/Qiime/OTU_table_analysis.html#installation-notes",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "Some packages required for this workflow are installed with BiocManager or devtools, if you need to install any of these tools, remove the # from the code and run it.\n\n#if (!require(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n\n#BiocManager::install(\"phyloseq\")\n#BiocManager::install(\"microbiome\")\n#BiocManager::install(\"ALDEx2\")\n#BiocManager::install(\"DESeq2\")",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#load-packages",
    "href": "source/Qiime/OTU_table_analysis.html#load-packages",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "library(tidyverse) #general parsing\nlibrary(data.table) #general parsing\nlibrary(phyloseq) #phyloseq object loading\nlibrary(vegan) #rarefaction\nlibrary(microbiome) #normalization\nlibrary(ALDEx2) #stats\nlibrary(DESeq2) #stats\nlibrary(grid) #organizing multiple plots\nlibrary(gridExtra) #organizing multiple plots\nlibrary(scales) #plot aesthetic, comma setting\n#library(ANCOMBC) #stats, tba",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#define-custom-functions",
    "href": "source/Qiime/OTU_table_analysis.html#define-custom-functions",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "Next, we read in some custom things such as:\n\na custom theme that we will use for plotting our graphs\n\na custom function that we use to calculate some summary statistics for our otu table\n\na color vector to have more control over what colors are used in our graphs\n\nDefining custom themes for our plots and custom functions is useful because it means that instead of re-writing the commands for our analyses over and over again if we want to use them more than once, we can just use these functions instead.\nYou can add custom functions closer to where you use them in the code, but I prefer to have them organized in one section of the workflow to keep my code organized.\n\n#define custom theme for generating figures\ncustom_theme &lt;- function() {\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(), \n    panel.border =element_blank(),\n    axis.line.x = element_line(color=\"black\", size = 0.5),\n    axis.line.y = element_line(color=\"black\", size = 0.5),\n    strip.text.x = element_text(size = 7),\n    strip.text.y = element_text(size = 7),\n    strip.background = element_rect(fil=\"#FFFFFF\", color = \"black\", linewidth = 0.5),\n    axis.text.x = element_text(size = 7),\n    axis.text.y = element_text(size = 7),\n    legend.text = element_text(size = 8), legend.title = element_text(size = 10)\n  )\n}\n\n#generate color scheme \nc25 &lt;- c(\"dodgerblue2\", \"#E31A1C\", \"green4\", \"#6A3D9A\", \"#FF7F00\", \"black\", \"gold1\", \"skyblue2\", \"#FB9A99\", \n        \"palegreen2\", \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\", \"deeppink1\", \"blue1\", \n        \"steelblue4\", \"darkturquoise\", \"green1\", \"yellow4\", \"yellow3\",\"darkorange4\", \"brown\")\n\n\n#define a custom function to summarize several aspects of our otu table\nsummarize_table &lt;- function(df) {\n  total_reads &lt;- sum(df)\n  otu_number &lt;- length(df)\n  num_singletons &lt;- length(df[df == 1])\n  num_doubletons &lt;- length(df[df == 2])\n  num_less_than_10 &lt;- length(df[df &lt; 10])\n  total_reads_less_than_10 &lt;- sum(df[df &lt; 10])\n  perc_reads_less_than_10 &lt;- (total_reads_less_than_10 / sum(df)) * 100\n  \n  cat(\"Total number of reads:\", format(total_reads, big.mark = \",\"), \"\\n\")\n  cat(\"Number of OTUs\",  format(otu_number, big.mark = \",\"), \"\\n\")\n  cat(\"Number of singleton OTUs:\",  format(num_singletons, big.mark = \",\"), \"\\n\")\n  cat(\"Number of doubleton OTUs:\",  format(num_doubletons, big.mark = \",\"), \"\\n\")\n  cat(\"Number of OTUs with less than 10 seqs:\",  format(num_less_than_10, big.mark = \",\"), \"\\n\")\n  cat(\"Total reads for OTUs with less than 10 seqs:\",  format(total_reads_less_than_10, big.mark = \",\"), \"\\n\")\n  cat(\"Percentage of reads for OTUs with less than 10 seqs:\",  sprintf(\"%.2f%%\", perc_reads_less_than_10), \"\\n\")\n  \n}",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#read-in-the-data",
    "href": "source/Qiime/OTU_table_analysis.html#read-in-the-data",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "An OTU table contains a column with the OTUs (taxonomic ranks in our case) and one column per sample with the counts how often OTU is found in the sample. It might look something like this:\n\n\n\n\n\n\n\n\n\n\n#NAME\nEP1910\nRMT\nKJB3\nTJR\n\n\n\n\nBacteria;Acidobacteria;Acidobacteriia;Acidobacteriales;Acidobacteriaceae (Subgroup 1);NA\n0\n0\n0\n0\n\n\nBacteria;Acidobacteria;Acidobacteriia;Solibacterales;Solibacteraceae (Subgroup 3);PAUC26f\n0\n5\n3\n1\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you follow this tutorial, ensure that your otu table is in a folder called input and that this folder is located where your R script is. If your otu table is somewhere else, change the path stored in filepaths accordingly.\n\n\n\n#provide the path to the otu table\nfile_paths &lt;- c(\"input/otu_table.txt\")\n\n#read in otu table\nmerged_otu_table &lt;- read.table(file_paths, header = T, sep = '\\t', comment = \"\")\ncolnames(merged_otu_table)[1] &lt;- \"taxid\"\n\n#replace NA with 0\nmerged_otu_table[is.na(merged_otu_table)] &lt;- 0\n\n#use the taxon as rownames\nrownames(merged_otu_table) &lt;- merged_otu_table$taxid\nmerged_otu_table$taxid &lt;- NULL\n\n#check how many otus and samples we have\ndim(merged_otu_table)\n\n[1] 1530   28\n\n\nWith this example OTU table, we work with 28 samples and 1530 OTUs.\n\n\n\nIf you have more than one table, for example if you generated an OTU table using different classifiers, you could read in the data as follows:\n\n\nShow the code\nfile_paths &lt;- c(\"results/classification/minimap2/ITS1.merged.outmat.tsv\",\n                \"results/classification/kraken2/ITS1.merged.outmat.tsv\")\n\nmethods &lt;- character(0)\n\nfor (i in 1:length(file_paths)){\n  table &lt;- read.table(file_paths[i], header = T, sep = '\\t', comment = \"\")\n  method &lt;- gsub(\".*/([^/]+)/.*\", \"\\\\1\", file_paths[i])\n  names(table)[1] &lt;- \"taxid\"\n  methods &lt;- c(methods, method)\n  colnames(table)[-1] &lt;- paste0(colnames(table)[-1], \"_\", method)\n  ifelse (i == 1, merged_otu_table &lt;- table, \n          merged_otu_table &lt;- merge(merged_otu_table, table, by = \"taxid\",  all=TRUE))\n} \n\n# Replace NA with 0\nmerged_otu_table[is.na(merged_otu_table)] &lt;- 0\n\n# Restore row names\nrownames(merged_otu_table) &lt;- merged_otu_table$taxid\nmerged_otu_table$taxid &lt;- NULL\n\ndim(merged_otu_table)\n\n\n\n\n\nNext, we read in another table that contains more information about our samples. Such a table could look something like this:\n\n\n\n#NAME\ntreatment\nDate\n\n\n\n\nEP1910\nwood\n2023_1\n\n\nRMT\npaper\n2023_1\n\n\nKJB3\nmix\n2023_1\n\n\nTJR\npaper\n2023_1\n\n\nIB5\nwood\n2023_1\n\n\nALIDNA\nwood\n2023_1\n\n\nIG7\npaper\n2023_1\n\n\nB314\nmix\n2023_1\n\n\n\n\n#read in metadata file\nmetadata_combined &lt;- read.table(\"input/sample_table.txt\", header = TRUE, row.names = 1, sep = \"\\t\", comment.char = \"\")\n\n#add extra column for sample names\nmetadata_combined$name &lt;- paste0(metadata_combined$treatment, \"_\", rownames(metadata_combined))\nmetadata_combined$sample_id &lt;- rownames(metadata_combined)\n\n#order the factors for our names column\nmetadata_combined &lt;- metadata_combined |&gt; \n  arrange(desc(treatment))\n\n#view output\nhead(metadata_combined)\n\n\n\n\n\n\ntreatment\nDate\nname\nsample_id\n\n\n\n\nEP1910\nwood\n2023_1\nwood_EP1910\nEP1910\n\n\nIB5\nwood\n2023_1\nwood_IB5\nIB5\n\n\nALIDNA\nwood\n2023_1\nwood_ALIDNA\nALIDNA\n\n\nVS15G\nwood\n2023_1\nwood_VS15G\nVS15G\n\n\nNZC319\nwood\n2023_1\nwood_NZC319\nNZC319\n\n\nE321\nwood\n2023_1\nwood_E321\nE321",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#generate-taxonomy-file",
    "href": "source/Qiime/OTU_table_analysis.html#generate-taxonomy-file",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "Next, we generate a table that list the taxonomy information for each taxonomic rank. We do this by taking the information from our OTU table. Depending on how you analysed your 16S rRNA gene sequences, you might have:\n\nan OTU table with the taxonomy information as row names instead of IDs. That is what we are working with for this tutorial\nan OTU table with IDs (ASV1, ASV2, … or OTU1, OTU2, …) and a separate table with the taxonomy information. If that is the case, you can read in the taxonomy information separate\n\n\n#extract taxonomy information and convert to separate df\ndf &lt;- as.data.frame(rownames(merged_otu_table))\ncolnames(df) &lt;- \"OTU\"\n\n#separate the taxonomic headers into separate rows                      \ntaxonomy_file &lt;- df |&gt; \n  distinct(OTU) |&gt; \n  separate(OTU,\n           c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\"), \n           sep = \";\", remove = FALSE) |&gt; \n  column_to_rownames(var = \"OTU\")\n\n#view file\nhead(taxonomy_file)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKingdom\nPhylum\nClass\nOrder\nFamily\nGenus\n\n\n\n\nBacteria;Acetothermia;Acetothermiia;bacterium enrichment culture clone 73(2013);bacterium enrichment culture clone 73(2013);bacterium enrichment culture clone 73(2013)\nBacteria\nAcetothermia\nAcetothermiia\nbacterium enrichment culture clone 73(2013)\nbacterium enrichment culture clone 73(2013)\nbacterium enrichment culture clone 73(2013)\n\n\nBacteria;Acetothermia;Acetothermiia;uncultured bacterium;uncultured bacterium;uncultured bacterium\nBacteria\nAcetothermia\nAcetothermiia\nuncultured bacterium\nuncultured bacterium\nuncultured bacterium\n\n\nBacteria;Acidobacteria;Acidobacteriia;Acidobacteriales;Acidobacteriaceae (Subgroup 1);NA\nBacteria\nAcidobacteria\nAcidobacteriia\nAcidobacteriales\nAcidobacteriaceae (Subgroup 1)\nNA\n\n\nBacteria;Acidobacteria;Acidobacteriia;Solibacterales;Solibacteraceae (Subgroup 3);PAUC26f\nBacteria\nAcidobacteria\nAcidobacteriia\nSolibacterales\nSolibacteraceae (Subgroup 3)\nPAUC26f\n\n\nBacteria;Acidobacteria;Aminicenantia;Aminicenantales;NA;NA\nBacteria\nAcidobacteria\nAminicenantia\nAminicenantales\nNA\nNA\n\n\nBacteria;Acidobacteria;Aminicenantia;Aminicenantales;uncultured Aminicenantes bacterium;uncultured Aminicenantes bacterium\nBacteria\nAcidobacteria\nAminicenantia\nAminicenantales\nuncultured Aminicenantes bacterium\nuncultured Aminicenantes bacterium",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#generate-phyloseq-object",
    "href": "source/Qiime/OTU_table_analysis.html#generate-phyloseq-object",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "A phyloseq object combines different elements of an analysis (i.e. the OTU table, the list of taxa and the mapping file) into one single object. We can easily generate such an object with the three dataframes we have generated above:\n\n#combine data\nOTU = otu_table(merged_otu_table, taxa_are_rows = TRUE)\nTAX = tax_table(as.matrix(taxonomy_file))\nphyseq = phyloseq(OTU, TAX, sample_data(metadata_combined))\n\n#view structure\nphyseq\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1530 taxa and 28 samples ]\nsample_data() Sample Data:       [ 28 samples by 4 sample variables ]\ntax_table()   Taxonomy Table:    [ 1530 taxa by 6 taxonomic ranks ]",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#explore-the-raw-data",
    "href": "source/Qiime/OTU_table_analysis.html#explore-the-raw-data",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "Below, we use our custom function summarize_table calculate some summary statistics. As said above, we could easily do this without a function, however, since we want to compare the statistics before and after filtering the OTU table, the function is useful to have, since we do not need to copy-paste the exact same code in two spots of the workflow:\n\n#calculate the number of reads found per otu\nreads_per_OTU &lt;- taxa_sums(physeq)\n\n#summarize the data\nsummarize_table(reads_per_OTU)\n\nTotal number of reads: 191,661 \nNumber of OTUs 1,530 \nNumber of singleton OTUs: 359 \nNumber of doubleton OTUs: 148 \nNumber of OTUs with less than 10 seqs: 867 \nTotal reads for OTUs with less than 10 seqs: 2,483 \nPercentage of reads for OTUs with less than 10 seqs: 1.30% \n\n\nFor this workflow, we define singletons as reads/OTUs with a sequence that is present exactly once in the dataset.\nNotice that another definition of singletons can be as taxa/OTU present in a single sample.\nIn amplicon data analysis it is useful to remove reads with low counts because they are very likely due to sequencing errors. Here, we make the assumption that sequencing errors are independent and randomly distributed and that erroneous sequences will occur much less often than a biologically true sequence. For our analyses not to be affected by potentially erroneous sequences we will remove them during the data filtering step.\n\n\n\nNext, we calculate the cumulative counts of our reads to get a feeling for how much data we would loose if we remove singletons, doubletons, etc. To do this, we calculate something like this:\nFor OTUs with an read count/abundance of 1, there are 359 occurrences (N), so the cumulative sum is 359.\nFor OTUs with an read count/abundance of 2, there are 148 occurrences (N), so the cumulative sum becomes 359 + 148 = 507.\nThis would tell us that if we would remove doubletons, 507 OTUs total to be removed, and so on … We do this, to get an idea about how many OTUs would be removed in total if we want to remove singletons, doubletons, 5-tons, etc…\n\n#extract the relevant data into a data table\ndt_taxa = data.table(tax_table(physeq), OTUabundance = taxa_sums(physeq), OTU = taxa_names(physeq))\n\n#create a new df with the abundances and the occurrences (N)\ndt_cumsum = dt_taxa[, .N, by = OTUabundance]\n\n#sort the df\nsetkey(dt_cumsum, OTUabundance)\n\n#add a new column with the cumulative sum for the otus\ndt_cumsum[, CumSumOTUs := cumsum(N)]\n\n#add a new column with the cumulative sum for the reads\ndt_cumsum[, CumSumReads := cumsum(dt_cumsum$OTUabundance * dt_cumsum$N )]\n\n#get the total read nr\ntotal_abundance_cumsum &lt;- sum(dt_cumsum$OTUabundance * dt_cumsum$N)\n\n#calc perc of reads removed\ndt_cumsum$perc_read_removed &lt;- dt_cumsum$CumSumReads/total_abundance_cumsum*100\n\n#view the df\nhead(dt_cumsum)\n\n\n\n\n\nOTUabundance\nN\nCumSumOTUs\nCumSumReads\nperc_read_removed\n\n\n\n\n1\n359\n359\n359\n0.1873099\n\n\n2\n148\n507\n655\n0.3417492\n\n\n3\n100\n607\n955\n0.4982756\n\n\n4\n65\n672\n1215\n0.6339318\n\n\n5\n64\n736\n1535\n0.8008932\n\n\n6\n47\n783\n1817\n0.9480280\n\n\n\n\n\n\nNext, we can plot this and zoom into how many OTUs we would remove if we remove singletons, doubletons, 5-tons, … :\n\n# Define the plot\np1 &lt;- ggplot(dt_cumsum, aes(OTUabundance, CumSumOTUs)) + \n  geom_point() +\n  xlab(\"Filtering threshold, minimum total counts\") +\n  ylab(\"OTUs Filtered\") +\n  ggtitle(\"OTUs that would be filtered vs. the minimum count threshold\" ) + \n  custom_theme() +\n  theme(plot.title = element_text(size = 8, face = \"bold\"))\n\n#find the max value to scale our plot\nmax &lt;- max(dt_cumsum$OTUabundance)\n\n# Specify zoom levels\nzoom_levels &lt;- c(1, 2, 5, 10, 25, 50,  100, max)\n\n# Create and arrange plots in a 2x2 grid\nplot_list &lt;- list()\nfor (zoom in zoom_levels) {\n  #subset data to our zoom level\n  subset_data &lt;- dt_cumsum[OTUabundance &lt;= zoom]\n  \n  #define the max value to scale each plot\n  max_cumsum &lt;- max(subset_data$CumSumOTUs)\n  \n  #generate a plot\n  p2 &lt;- p1 + \n    xlim(0, zoom) +\n    ggtitle(\"\") +\n    custom_theme() +\n    scale_y_continuous(limits = c(0, max_cumsum), \n                       breaks = seq(0, max_cumsum, \n                       by = max_cumsum / 4), \n                       labels = label_number())\n  \n  plot_list[[length(plot_list) + 1]] &lt;- p2\n}\n\n# Arrange plots in a 2x2 grid\ngrid.arrange(grobs = plot_list, ncol = 2,\n             top = textGrob(\"OTUs that would be filtered vs. the minimum count threshold\",gp=gpar(fontsize=12,font=3)))\n\n\n\n\n\n\n\n\nWe can also check the percentage of reads filtered:\n\n# Define the plot\np1 &lt;- ggplot(dt_cumsum, aes(OTUabundance, perc_read_removed)) + \n  geom_point() +\n  xlab(\"Filtering threshold, minimum total counts\") +\n  ylab(\"% reads filtered\") +\n  ggtitle(\"OTUs that would be filtered vs. the minimum count threshold\" ) + \n  custom_theme() +\n  theme(plot.title = element_text(size = 8, face = \"bold\"))\n\nmax &lt;- max(dt_cumsum$OTUabundance)\n\n# Specify zoom levels\nzoom_levels &lt;- c(1, 2, 5, 10, 25, 50,  100, max)\n\n# Create and arrange plots in a 2x2 grid\nplot_list &lt;- list()\nfor (zoom in zoom_levels) {\n  subset_data &lt;- dt_cumsum[OTUabundance &lt;= zoom]\n  max_perc &lt;- max(subset_data$perc_read_removed)\n  \n  p2 &lt;- p1 + \n    xlim(0, zoom) +\n    ggtitle(\"\") +\n    custom_theme() +\n    scale_y_continuous(limits = c(0, max_perc), \n                       breaks = seq(0, max_perc, \n                       by = max_perc / 4),\n                       labels = label_comma())\n\n  plot_list[[length(plot_list) + 1]] &lt;- p2\n}\n\n# Arrange plots in a 2x2 grid\ngrid.arrange(grobs = plot_list, ncol = 2,\n             top = textGrob(\"% of reads that would be filtered vs. the minimum count threshold\",gp=gpar(fontsize=12,font=3)))\n\n\n\n\n\n\n\n\nOverall, we can see that even if we are conservative and for example remove 5-tons that while we remove quite a high number of OTUs (~740), however, we remove less than 1% of the reads.\nFinding a good cut-off can be tricky and if you are unsure and do not want to remove rare taxa you might only want to remove singletons or play around with different thresholds.\n\n\n\nNext, let’s explore a bit closer how our reads are distributed among our different samples:\n\n#count the number of reads per sample\nsample_counts &lt;- as.data.frame(colSums(merged_otu_table))\n                  \n#clean up the dataframe\nnames(sample_counts)[1] &lt;- \"counts\"\nsample_counts$sampleID &lt;- rownames(sample_counts)\n\n#plot counts\np_counts &lt;-\n  ggplot(data = sample_counts, aes(x = reorder(sampleID, counts, FUN=sum, decreasing = TRUE), y = counts)) +\n  geom_point() +\n  geom_text(aes(x = , sampleID, y = counts, label = counts),  hjust = 0, nudge_y = 200 , size = 2.5) +\n  coord_flip() +\n  xlab(\"\") + \n  ylab(\"Read counts\") +\n  custom_theme()\n\np_counts\n\n\n\n\n\n\n\n\nIn this example, we see two samples with almost no reads and we want to make sure to remove these samples in our filtering step.\nWe also see that we have a large difference between different samples and that our samples have read depths ranging from 2 to 24,368. To be able to compare for example sample IV (~25,000 reads) with sample MN (~1,000 reads) we need to normalize our data after the data filtering step.",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#filter-data",
    "href": "source/Qiime/OTU_table_analysis.html#filter-data",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "Next, we filter the data. Specifically, we:\n\nRemove OTUs that are not assigned to anything at Phylum rank. The subset_taxa function can be used to remove any taxa you want, i.e. if you have plant DNA in your sample, you could use this to remove chloroplast sequences as well.\n\nRemove samples with total read counts less than 20. This cutoff is arbitrary and depends a bit on your data. To choose a good value, explore the read counts you have per sample and define a cutoff based on that. In this example, we mainly want to remove the two low read samples we have seen in our plot.\n\nRemove low count OTUs: The threshold is up to you; removing singletons or doubletons is common, but you can be more conservative and remove any counts less than 5 or 10, … . To decide on a cutoff you can look at the plots we generated before to get a feeling for how many OTUs/reads will be removed.\n\nIn our example, we want to remove samples with 20 or less reads, remove singletons only and remove OTUs that occur in less than 10% of our samples (v1). Since there are many different thoughts about OTU table filtering, you can also find two other options (v2 and v3) on how to filter a table.\nFor most analyses we will work with the filtered data but there are some diversity metrics which rely on the presence of singletons within a sample (richness estimates, i.e. Chao), so you might choose to leave them in for those sorts of analyses only.\n\n#define cutoffs\ncounts_per_sample &lt;- 20\notu_nr_cutoff &lt;- 1\nmin_percentage_samples &lt;- 10\n\n#remove taxa without tax assignment at Phylum rank\nphyseq_filt &lt;- subset_taxa(physeq, Phylum != \"NA\")\n\n#remove samples with less than 20 reads\nphyseq_filt &lt;- prune_samples(sample_sums(physeq)&gt;= counts_per_sample, physeq)\n\n#calculate the minimum number of samples an otu should be present in\nmin_samples &lt;- ceiling((min_percentage_samples / 100) * nsamples(physeq_filt))\n\n#remove otus that occur only rarely (v1)\n#here, we calculate the total abundance of each otu across all samples and checks if it's greater than the specified otu_nr_cutoff AND we check if the otu occurs in at least &lt;min_percentage_samples&gt;% of samples\n#we only retain OTUs that satisfy this condition \nphyseq_filt &lt;- prune_taxa(taxa_sums(physeq_filt) &gt; otu_nr_cutoff & taxa_sums(physeq_filt) &gt;= min_samples, physeq_filt)\nphyseq_filt\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1023 taxa and 26 samples ]\nsample_data() Sample Data:       [ 26 samples by 4 sample variables ]\ntax_table()   Taxonomy Table:    [ 1023 taxa by 6 taxonomic ranks ]\n\n\nHere, you find some alternative ideas on how to filter your data:\n\n#remove otus that occur only rarely (v2)\n#here, we count the number of samples where the abundance of an otu is &gt; 0. \n#If this count is greater than the specified otu_nr_cutoff, the taxon is retained.\n#physeq_filt &lt;- filter_taxa(physeq, function (x) {sum(x &gt; 0) &gt; otu_nr_cutoff}, prune=TRUE)\n#physeq_filt\n\n#remove otus that occur only rarely (v3)\n#here, we remove taxa not seen more than 1 times in at least 10% of the samples\n#physeq_filt = filter_taxa(physeq, function(x) sum(x &gt; 1) &gt; (0.1*length(x)), TRUE)\n#physeq_filt\n\nNext, we can calculate the summary statistics with the custom summarize_table function we have defined before:\n\n#calculate the number of reads found per otu\nreads_per_OTU_filt &lt;- taxa_sums(physeq_filt)\n\n#summarize the data\nsummarize_table(reads_per_OTU_filt)\n\nTotal number of reads: 190,995 \nNumber of OTUs 1,023 \nNumber of singleton OTUs: 0 \nNumber of doubleton OTUs: 0 \nNumber of OTUs with less than 10 seqs: 360 \nTotal reads for OTUs with less than 10 seqs: 1,827 \nPercentage of reads for OTUs with less than 10 seqs: 0.96%",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#normalize-data",
    "href": "source/Qiime/OTU_table_analysis.html#normalize-data",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "Below, we generate three new phyloseq objects using three different normalization approaches:\n1. *Compositional**: transforms the data into relative abundance\n2. *CLR**: stands for centered log-ratio transform and allows us to compare proportions of OTUs within each sample. After this transformation the values will no longer be counts, but rather the dominance (or lack thereof) for each taxa relative to the geometric mean of all taxa on the logarithmic scale\n3. *Rarefaction**: scales all of your reads down to the lowest total sequencing depth. Notice, that this might drop many OTUs in higher read samples and might lead to under-estimation of low-abundant OTUs. Here, we use the non-filtered data as rarefaction expects singletons, and for that purpose they should be retained, unless you have reason to think they are wrong (and which of them are wrong). Depending on whether you want to filter taxa (such as chloroplast), you might want to filter those before doing rarefaction.\nGenerating different phyloseq objects for different normalization approaches allows us to easily compare analysis steps with different inputs.\nIf you want to familiarize yourself more about why and how to normalize data, feel free to check out the following resources that give some more insights into the topic. As you will see, there is a lot of debate about data treatment and you might want to test and compare different approaches with your data.\n\nMicrobiome Datasets Are Compositional: And This Is Not Optional\nUnderstanding sequencing data as compositions: an outlook and review\nWaste Not, Want Not: Why Rarefying Microbiome Data is Inadmissible\nTo rarefy or not to rarefy: robustness and efficiency trade-offs of rarefying microbiome data\nNormalization and microbial differential abundance strategies depend upon data characteristics\n\n\nphyseq_rel_abundance &lt;- microbiome::transform(physeq_filt, \"compositional\")\nphyseq_clr &lt;- microbiome::transform(physeq_filt, \"clr\")\nphyseq_rarified &lt;- rarefy_even_depth(physeq)",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/OTU_table_analysis.html#visualize-the-data",
    "href": "source/Qiime/OTU_table_analysis.html#visualize-the-data",
    "title": "Analysing OTU tables with R",
    "section": "",
    "text": "Rarefaction curves illustrate how well your sample was sampled. The rarefaction function takes a random subsample of each column in your OTU table of a given size, starting with a very small subsample, and counts how many unique OTUs were observed in that subsample. The analysis is repeated with increasing subsample sizes until the maximum actual read depth for your table is reached.\nIn an ideal dataset, each curve should reach a plateau, i.e. horizontal asymptote, ideally around the same depth. While this almost never is the case, exploring these graphs gives us an idea how well each sample was sampled.\nIn our dataset below, we will use the unfiltered data, since rarefaction is highly dependent on the number of singletons in a sample – the rarer the sequence, the more likely it is that it will be observed only once in a sample.\n\n#create a df from our otu table\ndf &lt;- as.data.frame(otu_table(physeq))\n\n#rarefy data with a step size of 50, using tidy = TRUE will return a dataframe instead of a plot\ndf_rare &lt;- rarecurve(t(df), step=50, cex=0.5, label = FALSE, tidy = TRUE)\n\n#add metadata\ndf_rare &lt;- merge(df_rare, metadata_combined, by.x = \"Site\", by.y = \"sample_id\")\n\n#transform df and plot\nplot_rare &lt;-\n  df_rare |&gt;\n    group_by(Site) |&gt; \n    #find the max value per site\n    mutate(max_sample = max(Sample)) |&gt;\n    #for each site add a label for the max value only (we do this to create a single label to add to the plot)\n    mutate(label = if_else(Sample == max_sample, as.character(Site), NA_character_)) |&gt;\n    #plot\n    ggplot(aes(x = Sample, y = Species, color = treatment, group = interaction(Site, treatment))) + \n    geom_line() + \n    facet_grid(cols = vars(treatment)) +\n    geom_text(aes(label = label),\n              position = position_nudge(x = 2000),\n              na.rm = TRUE, size = 3) +\n    custom_theme()\n\nplot_rare\n\n\n\n\n\n\n\n\nIn this example we can see that almost none of the sample reach a plateau, RH is the closest but not there yet. This suggests that we mainly sampled the abundant members of our community and might miss many rare taxa. This is something to keep in mind for other analyses, such as alpha diversity analyses.\n\n\n\nAlpha diversity measures the diversity within our sample and we distinguish between species richness (i.e. the number of species) and species richness (i.e. how relatively abundant each of the species are).\nSomething to keep in mind: Some richness-based alpha diversity metrics require singletons to estimate OTUs/ASVs observed zero times in the data. These methods try to to estimate how many rare things are around that you didn’t see, and the rare things you did see (that inform you about the unseen others) will be seen 1 or 2 times. Different methods (e.g. rarefaction or Chao’s S1) all have that basic dependence.\nHowever, working with sequencing data we have one caveat: Some sequences are being miss classified as new OTUs/ASVs that simply don’t really exist. More specifically, errors/chimeras/artefacts/contaminants get interpreted as real OTUs (because they are different enough) and those show up largely in the singleton/doubletons and can almost entirely drive richness estimates to nonsensical values.\n\n#calculate different alpha diversity indicators\nrichness_meta &lt;-microbiome::alpha(physeq_filt, index = \"all\")\n\n#add the sample id to table\nrichness_meta$sample_id &lt;- rownames(richness_meta)\n\n#add other metadata data\nrichness_meta  &lt;- merge(richness_meta, metadata_combined, by = \"sample_id\")\n\n#check what parameters are calculated\ncolnames(richness_meta)\n\n [1] \"sample_id\"                  \"observed\"                  \n [3] \"chao1\"                      \"diversity_inverse_simpson\" \n [5] \"diversity_gini_simpson\"     \"diversity_shannon\"         \n [7] \"diversity_fisher\"           \"diversity_coverage\"        \n [9] \"evenness_camargo\"           \"evenness_pielou\"           \n[11] \"evenness_simpson\"           \"evenness_evar\"             \n[13] \"evenness_bulla\"             \"dominance_dbp\"             \n[15] \"dominance_dmn\"              \"dominance_absolute\"        \n[17] \"dominance_relative\"         \"dominance_simpson\"         \n[19] \"dominance_core_abundance\"   \"dominance_gini\"            \n[21] \"rarity_log_modulo_skewness\" \"rarity_low_abundance\"      \n[23] \"rarity_rare_abundance\"      \"treatment\"                 \n[25] \"Date\"                       \"name\"                      \n\n\nNext, we can generate a plot. Based on our discussion on singletons and since we calculated the diversity estimates after removing singletons, we will look at evenness indices, such as the Shannon index.\n\n#generate figure\nalpha_plot &lt;-\n  ggplot(richness_meta, aes(x = treatment, y = diversity_shannon)) +\n    geom_boxplot() +\n    geom_jitter(aes(color = Date), width = 0.2, size = 4) +\n    labs(x = \"\", y = \"Shannon index\") +\n    custom_theme() +\n    theme(legend.key=element_blank())\n\nalpha_plot\n\n\n\n\n\n\n\n#save the figure to our computer\n#ggsave(paste0(\"results/plots/alpha-div.png\"), plot=alpha_plot, device=\"png\")\n\nWe see that there is not a difference in terms of species evenness when comparing our different samples.\n\n\n\nIn contrast to alpha diversity, beta diversity quantifies the dissimilarity between communities (multiple samples).\nCommonly used beta-diversity measures include the:\n\nBray-Curtis index (for compositional/abundance data)\n\nJaccard index (for presence/absence data)\n\nAitchison distance (use Euclidean distance for clr transformed abundances, aiming to avoid the compositionality bias)\n\nUnifrac distance (that takes into account the phylogenetic tree information)\n\n…\n\nIn general it is best to avoid methods that use Euclidean distance as microbiome data are compositional, sparse datasets and better suited for the above mentioned methods. Aitchison is an exception as it calculates an Euclidean distance only after clr transformation (after which our data is suited for an Euclidean space).\nThe methods above generate distance matrices, which can then be used for ordination,which we use to reduce the dimensionality of our data for a more efficient analysis and visualization. Based on the type of algorithm, ordination methods can be divided in two categories:\n\nunsupervised, which measure variation in the data without additional information on covariates or other supervision of the model, including:\n\nPrincipal Coordinate Analysis (PCoA)\n\nPrincipal Component Analysis (PCA)\n\nUniform Manifold Approximation and Projection for Dimension Reduction (UMAP)\n\n\nsupervised ordination:\n\ndistance-based Redundancy Analysis (dbRDA)\n\n\n\n\nFor Bray-Curtis our data should not have negative entries, so we will use the relative abundance (alternatively, you could also look at the rarefied) data:\n\n#calculate PCOA using Phyloseq package\npcoa_bc &lt;- ordinate(physeq_rarified, \"PCoA\", \"bray\") \n\nplot_beta &lt;-\n  plot_ordination(physeq_rarified, pcoa_bc, color = \"treatment\") + \n    geom_point(size = 3) +\n    #add a 95% confidence level for a multivariate t-distribution\n    stat_ellipse(aes(group = treatment), linetype = 2) +\n    custom_theme()\n\nplot_beta\n\nWarning in MASS::cov.trob(data[, vars]): Probable convergence failure\n\n\n\n\n\n\n\n\n\nThis two-dimensions PCOA plot shows 34% of the total variance between the samples. We can also see that our samples are not forming distinct clusters, i.e. microbiomes from the mixed, paper and wood communities appear very similar.\nWe can also perform our statistics to confirm this visual finding:\n\n#Generate distance matrix\nrare_dist_matrix &lt;- phyloseq::distance(physeq_rarified, method = \"bray\") \n\n#ADONIS test\nadonis2(rare_dist_matrix ~ phyloseq::sample_data(physeq_rarified)$treatment, permutations=9999, method=\"bray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSumOfSqs\nR2\nF\nPr(&gt;F)\n\n\n\n\nphyloseq::sample_data(physeq_rarified)$treatment\n2\n0.9508929\n0.0750529\n1.014286\n0.4187\n\n\nResidual\n25\n11.7187500\n0.9249471\nNA\nNA\n\n\nTotal\n27\n12.6696429\n1.0000000\nNA\nNA\n\n\n\n\n\n\nThis confirms that there do not seem to be any statistically significant differences between our samples.\n\n\n\nNext, we generate a beta-diversity ordination using the Aitchison distance as an alternative way to look at beta diversity (and well suited for investigating compositional data). We do this by applying PCA to our centered log-ratio (clr) transformed counts.\n\n#PCA via phyloseq\n#RDA without constraints is a PCA\nord_clr &lt;- phyloseq::ordinate(physeq_clr, \"RDA\")\n\n#Plot scree plot to plot eigenvalues, i.e.the total amount of variance that can be explained by a given principal component\nphyloseq::plot_scree(ord_clr) + \n  geom_bar(stat=\"identity\") +\n  custom_theme() +\n  labs(x = \"\", y = \"Proportion of Variance\\n\") \n\n\n\n\n\n\n\n\n\n#scale axes based on the eigenvalues\nclr1 &lt;- ord_clr$CA$eig[1] / sum(ord_clr$CA$eig)\nclr2 &lt;- ord_clr$CA$eig[2] / sum(ord_clr$CA$eig)\n\n#and plot\nphyloseq::plot_ordination(physeq_filt, ord_clr, color = \"treatment\") + \n  geom_point(size = 2) +\n  coord_fixed(clr2 / clr1) +\n  stat_ellipse(aes(group = treatment), linetype = 2) +\n  custom_theme() \n\n\n\n\n\n\n\n\nWhile PCA is an exploratory data visualization tool, we can test whether the samples cluster beyond that expected by sampling variability using permutational multivariate analysis of variance (PERMANOVA) with the adonis package.\n\n#Generate distance matrix\nclr_dist_matrix &lt;- phyloseq::distance(physeq_clr, method = \"euclidean\") \n\n#ADONIS test\nadonis2(clr_dist_matrix ~ phyloseq::sample_data(physeq_clr)$treatment, permutations=9999, method=\"bray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSumOfSqs\nR2\nF\nPr(&gt;F)\n\n\n\n\nphyloseq::sample_data(physeq_clr)$treatment\n2\n4301.017\n0.0910778\n1.152348\n0.2031\n\n\nResidual\n23\n42922.544\n0.9089222\nNA\nNA\n\n\nTotal\n25\n47223.561\n1.0000000\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nNext, we want to plot the taxa distribution. Let us first look at the most abundant phyla and check how similar our different samples are:\n\n#condense data at specific tax rank, i.e. on phylum level\ngrouped_taxa &lt;- tax_glom(physeq_rel_abundance, \"Phylum\", NArm=FALSE)\n  \n#find top19 most abundant taxa \ntop_taxa &lt;- names(sort(taxa_sums(grouped_taxa), TRUE)[1:19])\n\n#make a list for the remaining taxa\nother_taxa &lt;- names(taxa_sums(grouped_taxa))[which(!names(taxa_sums(grouped_taxa)) %in% top_taxa)]\n\n#group the low abundant taxa into one group\nmerged_physeq = merge_taxa(grouped_taxa, other_taxa, 2)\n  \n#transform phyloseq object into dataframe\ndf &lt;- psmelt(merged_physeq)\n\n#cleanup the names in the df\nnames(df)[names(df) == \"Phylum\"] &lt;- \"tax_level\"\n\n#replace NAs, with other (NAs are generated for the other category)\ndf$tax_level[which(is.na(df$tax_level))] &lt;- \"Other\"\n  \n#create a df to sort taxa labels by abundance\nsorted_labels &lt;- df |&gt; \n  group_by(tax_level) |&gt; \n  summarise(sum = sum(Abundance)) |&gt; \n  arrange(desc(sum))\n  \n#Get list of sorted levels excluding \"Other\"\ndesired_levels &lt;- setdiff(sorted_labels$tax_level, \"Other\")\n  \n#sort df using the sorted levels and ensure that \"Other\" is the last category\ndf$tax_level2 &lt;- factor(df$tax_level, levels = c(desired_levels, \"Other\"))\n  \n#generate color scheme\ncols &lt;- c25[1:length(unique(df$tax_level2))]\ncols[levels(df$tax_level2) == \"Other\"] &lt;- \"#CCCCCC\"\n  \n#plot\nfig &lt;-\n  ggplot(df, aes(x = Sample, y = Abundance, fill = tax_level2) ) +\n    geom_bar(position = \"stack\", stat = \"identity\", width = 0.9) +\n    labs(y = \"Relative abundance\", x = \"\", title = paste0(\"Relative abundance at \", \"Phylum\", \" rank\")) +\n    scale_fill_manual(name = paste0(\"Phylum\",\"_rank\"), labels = levels(df$tax_level2), values = cols) +\n    facet_grid(cols =  vars(treatment), scales = \"free\", space = \"free\") +\n    #scale_y_continuous(expand = c(0, 0), limits = c(0, 1.01)) +\n    custom_theme() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1 ) ) +\n    guides(fill=guide_legend(title = \"Phylum\"))\n  \nfig\n\n\n\n\n\n\n\n\nSince we want to generate one plot for each taxonomic rank, i.e. Phylum, Class, Order,…, we can do this in a loop. The figures will be generated in the folder results/plots/.\n\n#if not there already, create output folder\nimg_path=\"results/plots/\"\ndir.create(img_path, recursive = TRUE)\n\n#generate one barplot for each taxonomic level\nfor (level in colnames(taxonomy_file)){\n  \n  #condense data at specific tax rank, i.e. on phylum level\n  grouped_taxa &lt;- tax_glom(physeq_rel_abundance, level)\n  \n  #find top19 most abundant taxa \n  top_taxa &lt;- names(sort(taxa_sums(grouped_taxa), TRUE)[1:19])\n  \n  #make a list for the remaining taxa\n  other_taxa &lt;- names(taxa_sums(grouped_taxa))[which(!names(taxa_sums(grouped_taxa)) %in% top_taxa)]\n  \n  #group the low abundant taxa into one group\n  merged_physeq = merge_taxa(grouped_taxa, other_taxa, 2)\n  \n  #transform phyloseq object into dataframe\n  df &lt;- psmelt(merged_physeq) \n  \n  #cleanup the dataframe\n  names(df)[names(df) == level] &lt;- \"tax_level\"\n  df$tax_level[which(is.na(df$tax_level))] &lt;- \"Other\"\n  \n  #create a df to sort taxa labels by abundance\n  sorted_labels &lt;- df |&gt; \n    group_by(tax_level) |&gt; \n    summarise(sum = sum(Abundance)) |&gt; \n    arrange(desc(sum))\n  \n  #Get list of sorted levels excluding \"Other\"\n  desired_levels &lt;- setdiff(sorted_labels$tax_level, \"Other\")\n  \n  #sort df using the sorted levels and ensure that \"Other\" is the last category\n  df$tax_level2 &lt;- factor(df$tax_level, levels = c(desired_levels, \"Other\"))\n  \n  #generate color scheme\n  cols &lt;- c25[1:length(unique(df$tax_level2))]\n  cols[levels(df$tax_level2) == \"Other\"] &lt;- \"#CCCCCC\"\n  \n  #plot\n  fig &lt;-\n    ggplot(df, aes(x = Sample, y = Abundance, fill = tax_level2) ) +\n      geom_bar(position = \"stack\", stat = \"identity\", width = 0.9) +\n      labs(y = \"Relative abundance\", x = \"\", title = paste0(\"Relative abundance at \", level, \" rank\")) +\n      scale_fill_manual(name = paste0(level,\"_rank\"), labels = levels(df$tax_level2), values = cols) +\n      facet_grid(cols =  vars(treatment), scales = \"free\", space = \"free\") +\n      scale_y_continuous(expand = c(0, 0), limits = c(0, 1.01)) +\n      custom_theme() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1 ) ) +\n      guides(fill=guide_legend(title=level))\n  \n  ggsave(paste0(img_path, level, \"_barplot_ra.png\"), plot = fig, device=\"png\")\n  }\n\nGenerated plots:\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\nNext, we want to test, whether any OTUs differ among our different conditions, i.e. sampling dates or treatment.\n\n\n\n\n\n\nCaution\n\n\n\nNotice: This is still in development and needs to be optimized, use with care!\n\n\n\n\nFirst, let us compare for differences for a category where we compare two factors. For example, we might want to ask whether there are any significant differences when comparing our samples isolated at different dates.\nLet’s first test this using the non-parametric Wilcoxon rank-sum test.\n\n#Generate data.frame with OTUs and metadata\nps_wilcox &lt;- as.data.frame(t(as.data.frame(otu_table(physeq_clr))))\nps_wilcox$Date &lt;- sample_data(physeq_clr)$Date\n\n#Define functions to pass to map\nwilcox_model &lt;- function(df){\n  wilcox.test(abund ~ Date, data = df)\n}\n\nwilcox_pval &lt;- function(df){\n  wilcox.test(abund ~ Date, data = df)$p.value\n}\n\n#Create nested data frames by OTU and loop over each using map \nwilcox_results &lt;- ps_wilcox |&gt;\n  gather(key = OTU, value = abund, -Date) |&gt;\n  group_by(OTU) |&gt;\n  nest() |&gt;\n  mutate(wilcox_test = map(data, wilcox_model),\n         p_value = map(data, wilcox_pval))  \n\n#Show results\nwilcox_results$wilcox_test[[1]]\n\n\n    Wilcoxon rank sum exact test\n\ndata:  abund by Date\nW = 124, p-value = 0.04412\nalternative hypothesis: true location shift is not equal to 0\n\n\nOk, there seems to be something going on, let’s zoom in and look for differences per OTU level (genus rank in our case). To do this we first will correct for multiple comparison testing via false discovery rate (FDR). We also will filter the table to only include values with an FDR &lt; 0.001.\n\n#unnest df\nwilcox_results &lt;- wilcox_results |&gt;\n  dplyr::select(OTU, p_value) |&gt;\n  unnest(cols = c(p_value))\n\n#Computing FDR corrected p-values (since we do multiple statistical comparisons)\nwilcox_results &lt;- wilcox_results |&gt;\n  arrange(p_value) |&gt;\n  mutate(BH_FDR = p.adjust(p_value, \"BH\")) |&gt;\n  filter(BH_FDR &lt; 0.001) |&gt;\n  dplyr::select(OTU, p_value, BH_FDR, everything())\n\nhead(wilcox_results)  \n\n\n\n\n\n\n\n\n\n\nOTU\np_value\nBH_FDR\n\n\n\n\nBacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Ika33;uncultured bacterium\n0.0000523\n0.0000523\n\n\nBacteria;Bacteroidetes;Bacteroidia;Bacteroidales;NA;uncultured bacterium\n0.0001304\n0.0001304\n\n\nBacteria;Bacteroidetes;Chlorobia;Chlorobiales;Chlorobiaceae;Prosthecochloris\n0.0003840\n0.0003840\n\n\nBacteria;Firmicutes;Clostridia;Clostridiales;Clostridiaceae 1;Clostridium sensu stricto 1\n0.0004930\n0.0004930\n\n\nBacteria;Bacteroidetes;Bacteroidia;Bacteroidales;Marinilabiliaceae;Marinilabilia\n0.0006280\n0.0006280\n\n\nBacteria;Proteobacteria;Deltaproteobacteria;Bdellovibrionales;Bacteriovoracaceae;uncultured\n0.0006280\n0.0006280\n\n\n\n\n\n\nThere, seem to be a few OTUs that are distinct between sampling dates, lets explore them further by generating some plots:\n\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_clr)[as.character(wilcox_results$OTU), ], \"matrix\")),as.data.frame(as(otu_table(physeq_clr)[as.character(wilcox_results$OTU), ], \"matrix\")))\n\ndf_long &lt;- reshape2::melt(df)\ndf_long &lt;- merge(df_long, metadata_combined, by.x = \"variable\", by.y = \"sample_id\")\n\nggplot(df_long, aes(x=Genus, y=value, color = Date)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n  facet_wrap(vars(Genus), scales = \"free\") +\n  scale_color_manual(values = c(\"orange\", \"purple\")) +\n  xlab(\"\") +\n  ylab(\"CLR abundance\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nWe can see that all of the OTUs that showed differences between sampling dates are very low abundant OTUs.\nWilcoxon has some down-sites when it comes to treating zero values, an alternative approach is to use the ANOVA-like differential expression (ALDEx2) method. The aldex function is a wrapper that performs log-ratio transformation and statistical testing in a single line of code, which is why we feed the non-normalized data into it:\n\naldex2_da &lt;- ALDEx2::aldex(data.frame(phyloseq::otu_table(physeq_filt)), phyloseq::sample_data(physeq_filt)$Date, test=\"t\", effect = TRUE, denom=\"iqlr\")\n\nSpecifically, this function:\n\ngenerates Monte Carlo samples of the Dirichlet distribution for each sample,\nconverts each instance using a log-ratio transform,\n\nreturns test results for two sample (Welch’s t, Wilcoxon) or multi-sample (glm, Kruskal-Wallace) tests.\n\nBu using iqlr the function accounts for data with systematic variation and centers the features on the set features that have variance that is between the lower and upper quartile of variance. This provides results that are more robust to asymmetric features between groups.\nNext, we can plot the effect size. This plot shows the median log2 fold difference by the median log2 dispersion (a measure of the effect size by the variability). Differentially abundant taxa will be those where the difference exceeds the dispersion. Points toward the top of the figure are more abundant in our samples from sampling date 2 while those towards the bottom are more abundant in sampling date 1. Taxa with BH-FDR corrected p-values would be shown in red.\n\n#plot effect sizes\nALDEx2::aldex.plot(aldex2_da, type=\"MW\", test=\"wilcox\", called.cex = 1, cutoff.pval = 0.001)\n\n\n\n\n\n\n\n\nFinally, we can print the output with the taxa information added.\n\n#Clean up presentation\nsig_aldex2 &lt;- aldex2_da |&gt;\n  rownames_to_column(var = \"OTU\") |&gt;\n  filter(wi.eBH &lt; 0.001) |&gt;\n  arrange(effect, wi.eBH) |&gt;\n  dplyr::select(OTU, diff.btw, diff.win, effect, wi.ep, wi.eBH)\n\nhead(sig_aldex2)\n\n\n\n\n\nOTU\ndiff.btw\ndiff.win\neffect\nwi.ep\nwi.eBH\n\n\n\n\n\n\n\n\nIn our case an empty dataframe is returned, no significant differences were detected when using this statistical approach.\n\n\n\nNext, we want to test, if there are any OTUs that differ between our three distinct growth conditions, i.e. treatments. One way to do this is to use phyloseqs mt function.\nThe default test applied to each taxa is a two-sample two-sided t.test. In our case this would fail with an error since we want to look at a data variable that contains three classes. To run this, we add test = \"f\" to run a F-test. Additionally, we use FDR as multiple-hypthesis correction. B specifies the number of permutations.\n\n#F-test, by specifying test=\"f\" \nres &lt;- mt(physeq_clr, \"treatment\", method = \"fdr\", test = \"f\", B = 300)\n\nB=300\nb=3 b=6 b=9 b=12    b=15    b=18    b=21    b=24    b=27    b=30    \nb=33    b=36    b=39    b=42    b=45    b=48    b=51    b=54    b=57    b=60    \nb=63    b=66    b=69    b=72    b=75    b=78    b=81    b=84    b=87    b=90    \nb=93    b=96    b=99    b=102   b=105   b=108   b=111   b=114   b=117   b=120   \nb=123   b=126   b=129   b=132   b=135   b=138   b=141   b=144   b=147   b=150   \nb=153   b=156   b=159   b=162   b=165   b=168   b=171   b=174   b=177   b=180   \nb=183   b=186   b=189   b=192   b=195   b=198   b=201   b=204   b=207   b=210   \nb=213   b=216   b=219   b=222   b=225   b=228   b=231   b=234   b=237   b=240   \nb=243   b=246   b=249   b=252   b=255   b=258   b=261   b=264   b=267   b=270   \nb=273   b=276   b=279   b=282   b=285   b=288   b=291   b=294   b=297   b=300   \nr=10    r=20    r=30    r=40    r=50    r=60    r=70    r=80    r=90    r=100   \nr=110   r=120   r=130   r=140   r=150   r=160   r=170   r=180   r=190   r=200   \nr=210   r=220   r=230   r=240   r=250   r=260   r=270   r=280   r=290   r=300   \nr=310   r=320   r=330   r=340   r=350   r=360   r=370   r=380   r=390   r=400   \nr=410   r=420   r=430   r=440   r=450   r=460   r=470   r=480   r=490   r=500   \nr=510   r=520   r=530   r=540   r=550   r=560   r=570   r=580   r=590   r=600   \nr=610   r=620   r=630   r=640   r=650   r=660   r=670   r=680   r=690   r=700   \nr=710   r=720   r=730   r=740   r=750   r=760   r=770   r=780   r=790   r=800   \nr=810   r=820   r=830   r=840   r=850   r=860   r=870   r=880   r=890   r=900   \nr=910   r=920   r=930   r=940   r=950   r=960   r=970   r=980   r=990   r=1000  \nr=1010  r=1020  \n\n#identify significant otus\nres_sig &lt;- res |&gt; \n  filter(adjp &lt; 0.001)\n  \nhead(res_sig)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nindex\nteststat\nrawp\nadjp\nplower\nKingdom\nPhylum\nClass\nOrder\nFamily\nGenus\nfdr\n\n\n\n\n\n\n\n\nWe get an empty dataframe, since no significant differences were detected.\n\n\n\nDESeq2 performs a differential expression analysis based on the Negative Binomial (a.k.a. Gamma-Poisson) distribution. DeSeq normalizes the data throughout its analysis, so we input only the filtered data. For more theory, visit this page.\n\n#convert treatment column to a factor\nsample_data(physeq_filt)$treatment &lt;- as.factor(sample_data(physeq_filt)$treatment)\n\n#Convert the phyloseq object to a DESeqDataSet and run DESeq2:\nds &lt;- phyloseq_to_deseq2(physeq_filt, ~ treatment)\n\n#since our samples contain a lot of 0s (something DeSeq is NOT designed for) we use some alternative means to estimate the size factor\nds &lt;-estimateSizeFactors(ds, type = 'poscounts')\nds &lt;- DESeq(ds)\n\n#extract the results and filter by a FDR cutoff of 0.001 and\n#find significantly differentially abundant OTU between the seasons “paper” and “wood”\nalpha = 0.001\nres = results(ds, contrast=c(\"treatment\", \"paper\", \"wood\"), alpha=alpha)\nres = res[order(res$padj, na.last=NA), ]\nres_sig = res[(res$padj &lt; alpha), ]\n\n#print number of significant results\ndim(res_sig)\n\n[1] 2 6\n\n\nPlot significant OTUs (log2fold change):\n\n#extract relevant data for significant otus\nres_sig = cbind(as(res_sig, \"data.frame\"), as(tax_table(physeq_filt)[rownames(res_sig), ], \"matrix\"))\n\n#plot\nggplot(res_sig, aes(x=Genus, y=log2FoldChange, color=Phylum)) +\n  geom_jitter(size=3, width = 0.2) +\n  custom_theme() +\n  theme(axis.text.x = element_text(angle = -90, hjust = 0, vjust=0.5))\n\n\n\n\n\n\n\n\nPlot significant OTUs (relative abundance):\n\n#extract relevant data for significant otus\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_clr)[rownames(res_sig), ], \"matrix\")),as.data.frame(as(otu_table(physeq_clr)[rownames(res_sig), ], \"matrix\")))\n\n#convert to long df\ndf_long &lt;- reshape2::melt(df)\n\nUsing Kingdom, Phylum, Class, Order, Family, Genus as id variables\n\n#add metadata\ndf_long &lt;- merge(df_long, metadata_combined, by.x = \"variable\", by.y = \"sample_id\")\n\n#plot\nggplot(df_long, aes(x=Genus, y=value, color = treatment)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n  facet_wrap(vars(Genus), scales = \"free\") +\n  scale_color_manual(values = c(\"grey\", \"orange\", \"purple\")) +\n  xlab(\"\") +\n  ylab(\"CLR abundance\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nNotice, how in our plots there seems to be no major difference between the wood and paper samples for these OTUs. Considering that both OTUs have marked outliers, it is likely that our statistics are driven a lot by these outliers and we should interpret our results with care.\nAlso, remember that these two genera are also not returned with the other statistical test we did. If we stay conservative we would say that we see no differences between the different sample types.\n\n\n\ntba\n\n#out = ancom(data = NULL, assay_name = NULL,\n#           tax_level = \"Genus\", phyloseq = physeq_filt,\n#           p_adj_method = \"holm\", prv_cut = 0.10, lib_cut = 1000,\n#           main_var = \"treatment\",\n#           rand_formula = NULL, lme_control = NULL, struc_zero = TRUE,\n#           neg_lb = TRUE, alpha = 0.05, n_cl = 2)\n\n\n#res = out$res\n#res",
    "crumbs": [
      "Welcome page",
      "Amplicon analyses",
      "Analysing OTU tables with R"
    ]
  },
  {
    "objectID": "source/Qiime/readme.html",
    "href": "source/Qiime/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "QIIME 2 is a platform for microbial community analysis such as 16S rRNA gene amplicon analysis. The QIIME 2 website comes with a wealth of documentation and tutorials that are worthwhile to check out before running QIIME 2 on your own data:\n\nInstallation instructions\nThe QIIME 2 documentation.\nThe cancer microbiome tutorial\nq2book a very detailed, but still unfinished, tutorial on QIIME 2.",
    "crumbs": [
      "Omics Analyses",
      "Amplicon analyses"
    ]
  },
  {
    "objectID": "source/Qiime/readme.html#qiime-2",
    "href": "source/Qiime/readme.html#qiime-2",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "QIIME 2 is a platform for microbial community analysis such as 16S rRNA gene amplicon analysis. The QIIME 2 website comes with a wealth of documentation and tutorials that are worthwhile to check out before running QIIME 2 on your own data:\n\nInstallation instructions\nThe QIIME 2 documentation.\nThe cancer microbiome tutorial\nq2book a very detailed, but still unfinished, tutorial on QIIME 2.",
    "crumbs": [
      "Omics Analyses",
      "Amplicon analyses"
    ]
  },
  {
    "objectID": "source/classification/kraken2.html",
    "href": "source/classification/kraken2.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Kraken2 (Wood, Lu, and Langmead 2019) is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.\nAvailable on Crunchomics: Kraken version 2.0.8-beta installed\n\n\n\nIf you want to install kraken2 on your own, its best to install it via mamba:\n\n#setup new conda environment, which we name kraken2\nmamba create --name kraken2 -c bioconda kraken2\n\n\n\n\nFor detailed usage information, check out the kraken2 manual.\n\n\nThe command below will download NCBI taxonomic information, as well as the complete genomes in RefSeq for the bacterial, archaeal, and viral domains, along with the human genome and a collection of known vectors (UniVec_Core). After downloading all this data, the build process begins; this can be the most time-consuming step. If you have multiple processing cores, you can run this process with multiple threads.\nAddtionally, kraken2 comes with several custom databases, such as the SILVA database for 16S rRNA gene analyses. Check the kraken2 manual for detailed information on how to download custom things..\n\n#create a kraken2 database \nkraken2-build --standard --threads 24 --db $DBNAME\n\n\n\n\n\nkraken2 --db $DBNAME seqs.fa --output output.out --report output.report",
    "crumbs": [
      "Welcome page",
      "Sequence classification",
      "Kraken2"
    ]
  },
  {
    "objectID": "source/classification/kraken2.html#kraken2",
    "href": "source/classification/kraken2.html#kraken2",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Kraken2 (Wood, Lu, and Langmead 2019) is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.\nAvailable on Crunchomics: Kraken version 2.0.8-beta installed\n\n\n\nIf you want to install kraken2 on your own, its best to install it via mamba:\n\n#setup new conda environment, which we name kraken2\nmamba create --name kraken2 -c bioconda kraken2\n\n\n\n\nFor detailed usage information, check out the kraken2 manual.\n\n\nThe command below will download NCBI taxonomic information, as well as the complete genomes in RefSeq for the bacterial, archaeal, and viral domains, along with the human genome and a collection of known vectors (UniVec_Core). After downloading all this data, the build process begins; this can be the most time-consuming step. If you have multiple processing cores, you can run this process with multiple threads.\nAddtionally, kraken2 comes with several custom databases, such as the SILVA database for 16S rRNA gene analyses. Check the kraken2 manual for detailed information on how to download custom things..\n\n#create a kraken2 database \nkraken2-build --standard --threads 24 --db $DBNAME\n\n\n\n\n\nkraken2 --db $DBNAME seqs.fa --output output.out --report output.report",
    "crumbs": [
      "Welcome page",
      "Sequence classification",
      "Kraken2"
    ]
  },
  {
    "objectID": "source/cli/cli.html",
    "href": "source/cli/cli.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "In this section you will find a short tutorial on using the Unix command-line interface Bash.\nThe Linux command-line interface (CLI) is an alternative to a graphical user interface (GUI) with which you are likely more familiar. Both interfaces allow a user to interact with an operating system. The key difference between the CLI and GUI is that the interaction with CLI is based on issuing commands. In contrast, the interaction with a GUI involves visual elements, such as windows, buttons, etc. CLI is often also referred to as the shell, terminal, console, prompt or various other names\nBash is a type of interpreter that processes shell commands. A shell interpreter takes commands in plain text format and calls the operating system to do something, for example changing a directory or modifying the content of some files. Bash itself stands for Bourne Again Shell and it is one of the popular command-line shells used to run other programs, many of which are useful for bioinformatic workflows.",
    "crumbs": [
      "Getting Started",
      "Using the command line"
    ]
  },
  {
    "objectID": "source/cli/cli.html#introduction",
    "href": "source/cli/cli.html#introduction",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "In this section you will find a short tutorial on using the Unix command-line interface Bash.\nThe Linux command-line interface (CLI) is an alternative to a graphical user interface (GUI) with which you are likely more familiar. Both interfaces allow a user to interact with an operating system. The key difference between the CLI and GUI is that the interaction with CLI is based on issuing commands. In contrast, the interaction with a GUI involves visual elements, such as windows, buttons, etc. CLI is often also referred to as the shell, terminal, console, prompt or various other names\nBash is a type of interpreter that processes shell commands. A shell interpreter takes commands in plain text format and calls the operating system to do something, for example changing a directory or modifying the content of some files. Bash itself stands for Bourne Again Shell and it is one of the popular command-line shells used to run other programs, many of which are useful for bioinformatic workflows.",
    "crumbs": [
      "Getting Started",
      "Using the command line"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html",
    "href": "source/cli/cli_file_manipulation.html",
    "title": "Working with files in bash",
    "section": "",
    "text": "Next, let’s go through some basic bash commands that are extremely useful when working with files. To this, let’s first download some example files to use.\n\n\n\nwget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/Experiment1.txt\nwget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/Experiment2.txt\nwget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/Experiment3.txt\n\nwget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/PF00900.faa\nwget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/PF01015.faa\n\n#check if this worked\nls downloads/\n\nThe content of the files is not important but what you downloaded are simple text files that contain information about a growth experiment and the files look something like this:\n\n\n\nEx1\ncontrol1\n0.2\n0.3\n#Ex1_all_good\n\n\nEx1\ncontrol2\n0.3\n0.1\n#all_good\n\n\nEx1\nN[0.4uM]\n10.1\n4.7\n#contamination\n\n\nEx1\nN[0.4uM]\n5.2\n4.3\n#all_good\n\n\n\nThe faa files are a file type you might encounter when working with molecular data. The FASTA format is a text-based format for representing either nucleotide sequences or amino acid (protein) sequences, in which nucleotides or amino acids are represented using single-letter codes. A sequence begins with a greater-than character (&gt;) followed by a description of the sequence (all in a single line). The lines immediately following the description line are the sequence representation, with one letter per amino acid or nucleic acid, and are typically no more than 80 characters in length.\n&gt;GCA_000007185\nMVRDKWKDKVWYTILAPDMFDNVEVGETPADDPEKVIGRVLETTLGDVLDDITKHHIKVFFRIYDVEGTTAYSKFEGHRLMRDYVRSLVRRGTSRIDGVIDVVTKDGYKVRVAGLAFTTRRAKTSQQRAIRKEMFKVIEENAKECDFDEFIRRCLSISEEESIPEQIKEAGRKIYPIRQAEIRKTEVLEEPNGLPPYEAVGDRATPELASY\n&gt;GCA_000007305\nMAAKRATTTRDKWKLKQWYII\nIn your case you will look at two protein fasta files in which the header contains the information from what genome, i.e. GCA_000007185, a sequence was extracted from as well as the actual sequence itself.\n\n\n\nThe wc (= wordcount) command in UNIX is a command line utility that:\n\nCan return the number of lines, the number of characters and the number of words in a file\nCan be combined with pipes for general counting operations. We will explain pipes a bit later\n\nThe following commands are simple but essential for quality control and you will use it a lot to check whether your commands worked all right.\n\n#count how many words we have\nwc -w downloads/Experiment1.txt\n\n#count how many lines we have in a file we have\nwc -l downloads/Experiment1.txt\n\n\n\n\nThe grep command is used to search text. It searches the given file for lines containing a match to the given strings or words. Also this command is simple but very useful for sanity checks after file transformations.\n\n#count how often the pattern **control** occurs in our document\ngrep \"control\" downloads/Experiment1.txt\n\n#only give the counts, not the lines\ngrep -c \"control\" downloads/Experiment1.txt\n\n#grep a pattern only if it occurs at the beginning of a line\ngrep \"^Ex\" downloads/Experiment1.txt\n\n#we can also count the number of sequences in the fasta file we downloaded before\ngrep -c \"&gt;\" downloads/*faa\n\n\n\n\nSince the shell uses filenames so much, it provides special characters to help you rapidly specify groups of filenames.\nA Wild-card character can be used as a substitute for any class of characters in a search.\n\nThe * wildcard is the wildcard with the broadest meaning of any of the wildcards, it can represent 0 characters, all single characters or any string of characters. I.e. we can grep for something in any files that end with .txt as follows:\n\n\ngrep -c \"Ex\" downloads/*txt\n\n\nThe ? wildcard matches exactly one character but will ingore files that start with a dot. We could use it as follows:\n\n\ngrep \"control\" downloads/Experiment?.txt\n\n\n[012] wildcard = matches 0 or 1 or 2 exactly once\n\n\ngrep  \"control\" downloads/Experiment[012].txt\n\n\n[0-9] wildcard = matches matches any number exactly one\n\n\ngrep  \"control\" downloads/Experiment[0-9].txt\n\n\ncombining wildcards\n\n\n[A-Z] wildcard = any letter in capitals occurring once\n[a-z]* wildcard = any letter in non-capital letters occurring many times\n\n\ngrep \"control\" downloads/[A-Z][a-z]*[12].txt\n\n\n[a-z]{7} we are exactly looking for 7 letters (as in ‘control’)\nthese 7 letters should be followed by either a 1 or 2\n\n\ngrep \"[a-z]\\{7\\}[12]\" downloads/Experiment[12].txt\n\n\nif we are not sure how many characters we have\n\n\nmatches 3-10 characters\n\n\ngrep \"[a-z]\\{3,10\\}[12]\" downloads/Experiment[12].txt\n\nIf you use grep with special symbols you might notice something:\n\n#this does not work\ngrep \"control?\" downloads/Experiment1.txt\n\n#this works how we want it to work\ngrep -E \"control?\" downloads/Experiment1.txt\n\n\n-E tells grep that the ‘?’ is not taken literally but as a wildcard\n\nUnfortunately different programs have slightly ways to do things, i.e greop uses -E while sed uses different quotes. If you run into problems when using wildcards check the manual or the web.\nExercise\n\nIn the faa files, how many sequences do we have? Notice, sequences always start with a &gt;\nIn the faa files, how often do we have 3 consecutive A’s?\nIn the faa files, how often do we have 2x A’s followed by a T?\nIn the faa files, how often do we have 2x M’s followed by a T or followed by a D?\n\nComment: If you unsure what is happening remove the -c to see what grep is grepping.\n\n\nShow the code\n#question1:\ngrep -c \"&gt;\" downloads/*.faa\n\n#question2\ngrep -c \"[A]\\{3\\}\" downloads/*.faa\n\n#question4\ngrep -c \"[A]\\{2\\}[T]\" downloads/*.faa\n\n#question5\ngrep -c  \"[A]\\{2\\}[TC]\" downloads/*.faa\n\n\nHint: if you are unsure what is happening, redo the command without the -c option\n\n\n\nThe cat command has three main functions related to manipulating text files:\n\nCreate a new file using cat. To do this type the command below, then type Hello! press enter, and *Press “ctrl+c” to save the file:\n\n\ncat &gt; downloads/file_test.txt\n\n\nDisplay the content of an existing file:\n\n\ncat downloads/file_test.txt\n\n\nConcatenate, i.e. combine, several files:\n\n\n#merge files\ncat downloads/file_test.txt downloads/file_test.txt  &gt; downloads/file_merged.txt\n \n#check the content of the file\ncat downloads/file_merged.txt\n\nExercise\n\nView the content of downloads/Experiment1.txt by using cat\nCombine downloads/Experiment1.txt and downloads/Experiment2.txt.\nDo the same as above but use a wildcard to not have to type everything and store the output as downloads/Experiments.txt\n\n\n\nShow the code\n#question1\ncat downloads/Experiment1.txt\n\n#question3\ncat downloads/Experiment1.txt downloads/Experiment2.txt\n\n#question3\ncat downloads/Experiment*.txt &gt; downloads/Experiments.txt\n\n\n\n\n\nPipes are a powerful utility to connect multiple commands together. They allow us to feed the standard output of one command as input into another command.\nIn the example below we first combine two files using cat and use wc -l on the standard output to count the number of lines after concatenating the two files:\n\ncat downloads/Experiment[12].txt | wc -l\n\nExercise\n\nUsing a pipe, combine the two faa files and count the total number of sequences using grep:\n\n\n\nShow the code\n#question1\ncat downloads/*faa | grep -c \"&gt;\"\n\n\n\n\n\nWe can use cut to separate columns. By default cut uses a tab as a delimiter. We could print the first column of our data as follows:\n\ncut -f1 downloads/Experiment1.txt\n\nOr if we only want to get the second column we do this:\n\ncut -f2 downloads/Experiment1.txt\n\nIf we wanted to get columns 1 and 2 we do:\n\ncut -f1-2 downloads/Experiment1.txt\n\nWe can also change how we cut, i.e. we could cut after a # by using the -d argument:\n\ncut -f1 -d \"#\" downloads/Experiment1.txt\n\nExercise\n\nIn PF00900.faa, we want to shorten the fasta header. To do this cut the text of after the first underscore _ (i.e. keep only the first element). Combine this with the head command to only view the first few lines and see if your command worked.\nIn PF00900.faa, cut to only keep the text after the first _ (i.e. keep the second element). Again, combine this with the head command.\n\n\n\nShow the code\n#question1\ncut -f1 -d \"_\" downloads/PF00900.faa | head\n\n#question2\ncut -f2 -d \"_\" downloads/PF00900.faa | head\n\n\nBe careful when using the second options, as this will cut away the &gt; symbol that we need in fasta files to define the header. I.e. fasta headers always should start with this symbol and if we would use such a command we would need the &gt; back in/\n\n\n\nsort – sort lines in a file from A-Z and is useful for file organization. We can sort files like this:\n\nsort downloads/Experiment2.txt\n\nSort by default sorts from left to right, but we also can sort based on a specific column, for example column 4:\n\nsort -k4 downloads/Experiment2.txt\n\nWe can also sort by more than one column, i.e. column 5 followed by column 3:\n\nsort -k5 -k3 downloads/Experiment1.txt\n\n\n\n\nuniq – can be used to remove or find duplicates . However, for this to work the file first needs to be sorted. To do this, we can make use of pipes. Lets, start of by extracting the observations we made with cut:\n\ncut -f5 downloads/Experiment1.txt\n\nWe then can sort the column by adding a pipe:\n\ncut -f5 downloads/Experiment1.txt | sort\n\nAnd only print unique observations with uniq:\n\ncut -f5 downloads/Experiment1.txt | sort | uniq\n\nIf we where interested in duplicated observations, we can add the -d argument:\n\ncut -f5 downloads/Experiment1.txt | sort | uniq -d\n\nCheck out the manual for more options!\nExercise\n\nUsing a pipe: combine the faa files. How many sequences do we have in total?\nUsing a pipe: combine the faa files. Extract only the header and then count how many duplicates do we have?\nSame as above, but how many sequences are not duplicated? Check the manual for uniq on what option allows you to do this.\n\n\n\nShow the code\n#question1\ncat downloads/*faa | grep -c \"&gt;\"\n\n#question 2\ncat downloads/*faa | grep \"&gt;\" | sort | uniq -d | wc -l \n\n#question 3\ncat downloads/*faa | grep \"&gt;\" | sort | uniq -u | wc -l \n\n\n\n\n\nfind searches through the directories for files and directories with a given name, date, size, or any other attribute you care to specify.\n\n#find all files with a txt extension\nfind downloads/ -name \"*txt\"\n\n#find files over a certain size and display results as a long list\nfind downloads/ -size +1M -ls\n\n\n\n\nSed is a stream editor that can be used to perform basic text transformations on an input stream (a file or input from a pipeline).\nThe most basic pattern to use sed for is sed ‘s/find/replace/’ file.\n\n\nLet’s go back to our text files and assume that instead of writing Ex we want to be more clear and write Experiment, we can do this as follows:\n\nsed 's/Ex/Experiment/' downloads/Experiment1.txt \n\nWhen doing this, we see that:\n\nEx only gets replaced the first time it appears in a line, ie. the #Ex1_all_good in the first line gets not replaced\nsed will not overwrite the input file (unless you tell it to do so) but it will print the changed to the screen. If we wanted to save this in a file, we would need to re-direct the output to a new file.\n\nTo change all instances of Ex, we can do the following:\n\nsed 's/Ex/Experiment/g' downloads/Experiment1.txt \n\nSed can be easily used with wildcards. For example, if you would want to replace control1 and control2 just with control you could do:\n\nsed 's/control[0-9]/control/g' downloads/Experiment1.txt \n\nOne important thing to remember is that certain symbols have specific meanings in UNIX. Examples are: comma, brackets, pipes. Do search for these in files, we need to escape them with a \\. For example, we could replace the square with round brackets as follows:\n\n#replace the square bracket with a round bracket. \nsed 's/N\\[0.4uM\\]/N(0.4uM)/g' downloads/Experiment1.txt \n\nExercise\n\nIn downloads/PF00900.faa , replace the GCA_ with Genome_\nIn downloads/PF00900.faa , use wildcards to replace the string of numbers after the GCA with a hello\n\n\n\nShow the code\n#qst 1\nsed 's/GCA_/Genome_/g'  downloads/PF00900.faa \n\n#qst 2\nsed 's/GCA_[0-9]*/GCA_hello/g'  downloads/PF00900.faa \n\n\n\n\n\nSed can also be used to remove things, such as line(s) or patterns. Below is a list of examples that you might encounter or find useful.\n\n# remove 1 first line\nsed '1d' downloads/Experiment1.txt\n\n#remove last line\nsed '$d' downloads/Experiment1.txt\n\n#remove lines 2-4\nsed '2,4d' downloads/Experiment1.txt\n\n#remove lines other than 2-4\nsed '2,4!d' downloads/Experiment1.txt \n\n#remove the first and last line\nsed '1d;$d' downloads/Experiment1.txt\n\n#remove lines beginning with an **L**\nsed '/^L/d' downloads/Experiment1.txt\n\n#delete lines ending with d\nsed '/d$/d' downloads/Experiment1.txt\n\n#delete lines ending with d OR D\nsed '/[dD]$/d' downloads/Experiment1.txt\n\n#delete blank lines ('^$' indicates lines containing nothing, i.e. lines with zero or more spaces)\nsed '/^$/d' downloads/Experiment1.txt\n\n#delete lines that start with capital letters\nsed '/^[A-Z]/d' downloads/Experiment1.txt\n\n#delete lines with the pattern **Ex**\nsed '/Ex/d' downloads/Experiment1.txt\n\n#delete files with the pattern control or uM\nsed '/control\\|uM/d' downloads/Experiment1.txt\n\n#delete the 2nd occurence of each pattern per line (here *o*)\nsed 's/o//2' downloads/Experiment1.txt\n\n#remove all digits across the whole file\nsed 's/[0-9]//g' downloads/Experiment1.txt\n\n#remove all alpha-numerical characters and numbers\nsed 's/[a-zA-Z0-9]//g' downloads/Experiment1.txt\n\n#remove character, here E, regardless of the case\nsed 's/[eE]//g' downloads/Experiment1.txt\n\n\n\n\n\nScreen or GNU Screen is a terminal multiplexer. This means that you can start a screen session and then open any number of windows (virtual terminals) inside that session.\nProcesses running in Screen will continue to run when their window is not visible even if you get disconnected. This is perfect, if we start longer running processes on the server and want to shut down our computer when leaving for the day. As long as the server is still connected to the internet, your process will continue running.\nWe start a screen as follows:\n\nscreen\n\nWe detach, i.e. exit, from a screen with control+a+d.\nIf you run multiple things, it can be useful to give your screens more descriptive names. You can do this as follows:\n\n#start a screen and give it a name\nscreen -S testrun\n\nAfter detaching from a screen you can list all currently running screens with:\n\nscreen -ls\n\nYou can restart a screen like this:\n\n#restart an existing screen\nscreen -r testrun\n\nIf you want to completely close and remove a screen, type the following while being inside of the screen:\n\nexit\n\n\n\n\n\n\nBefore talking about loops, lets learn another command first, echo. echo displays a line of text that can be printed to the screen or stored in a file (why this is useful you see in a second):\nBoth echo and printf print something into the console. However, echo uses different versions across shells, so you might get slighly different outputs depending on the shell you use. Printf is more portable and allows for more formatting options. You might encounter both when finding code in the wild.\n\n#print a string to the console using echo\necho \"Hello everyone\"\n\n#print a string to the console using printf\nprintf \"Hello everyone\"\n\n\n\n\nA for loop is a bash programming language statement which allows code to be repeatedly executed. I.e. it allows us to run a command 2, 3, 5 or 100 times.\nTry running this example in which we want to say hello several times:\n\nfor i in 1 2 3; do echo \"Welcome $i times\"; done\n\nHere, you see what this command does step by step:\n\n\n\nWe could also use a loop to run sed on all our txt files. For example assume that instead of nitrogen (N) we measured sulfur (s) we could replace it among all text files as follows:\n\n#change the N for  P in all our files\nfor i in downloads/*txt; do sed 's/N/S/g' $i; done\n\n\n\n\nIf we want to store the output in new files, we could store them with a slightly different name as follows:\n\nfor i in downloads/*txt; do sed 's/N/S/g' $i &gt; ${i}_2.txt; done\n\n#check that all went alright with \nll downloads/*txt\n\nYou see here, that we added curly brackets to define the borders of our variable i. I.e. if we would not have the brackets unix would look for a variable i_2, which does not exist.\nYou will also see that the name of the output file is not ideal, i.e. it is something like downloads/Experiment3.txt_2.txt. To get more control over the name of the output files we can use a file list for building the loop.\nTo do this let’s use printf to generate a FileList in which list the the two files we want to work with: Experiment1 and Experiment2. We do this using printf and \\n, which is a special character we can use to separate Experiment1 and Experiment using a new line.\n\nprintf \"Experiment1\\nExperiment2\" &gt; downloads/FileList\n\n#view file\nhead downloads/FileList\n\nNow we can use this file to run our loop.\n\nfor i in `cat downloads/FileList`; do sed 's/N/S/g' downloads/${i}.txt &gt; downloads/${i}_2.txt; done\n\n#check that all went alright with \nll downloads/*txt\n\nNotice, that using this option we need to add the file extension! However, the resulting filenames, i.e. downloads/Experiment2_2.txt, are much cleaner.\nExercise\n\nMake a list with all the faa files (do this with the command line). Ideally, we want to have PF00900 and PF01015 in one column. For this you want to list the names of all faa files but at the same time remove the file extension and the folder name.\nUse this list to, in a loop, replace the GCA_ with Genome_. Store the files with the new ending renamed.faa\n\n\n\nShow the code\n#question1\nls downloads/*faa | sed 's/downloads\\///g' | sed 's/\\.faa//g' &gt; downloads/FaaList\n\n#question2\nfor sample in `cat downloads/FaaList`; do sed 's/GCA_/Genome_/g' downloads/${sample}.faa &gt; downloads/${sample}_renamed.faa; done\n\n#check file\nhead downloads/PF00900_renamed.faa\n\n\n\n\n\n\nA variable is a character string to which we assign a value. Environment variables are variables that are set up in your shell when you log in. They are called “environment variables” because most of them affect the way your Unix shell works for you. I.e. one points to your home directory and another to your history file.\nOne common environmental variable is $HOME. This variable holds the path to your home directory, which is like your computer’s main hub.\nWe can use the echo command to display the value of a variable. Let’s use it to look at an environmental variable found on most systems called $HOME:\n\necho $HOME\n\nYou can use this variables in commands, for example you can use it together with cd to move into your home directory:\n\ncd $home\n\nYou can easily make your own variables and use them to store file name, paths or what ever you want. Let’s store a number in a variable:\n\nMY_NUMBER=10\n\nWhen using the export command this lets the system know that this variable is for everyone, not just the current session.\nLets see if this worked:\n\necho \"My math number: $MY_NUMBER\"\n\nOnce you have this, you can use it for any command that you want.\n\n\n\nEach file (and directory) has associated access rights, which may be found by typing ls -l. Additionally, ls -lg gives additional information as to which group owns the file\n\nls -lg downloads/Experiment1.txt \n\n-rw-r--r--  1 staff   156B Sep 28  2019 downloads/Experiment1.txt\nIn the left-hand column is a 10 symbol string consisting of the symbols d, r, w, x, -, and, occasionally, s or S. If d is present, it will be at the left hand end of the string, and indicates a directory: otherwise - will be the starting symbol of the string.\nThe 9 remaining symbols indicate the permissions, or access rights, and are taken as three groups of 3.\n\nThe left group of 3 gives the file permissions for the user that owns the file (or directory)\nthe middle group gives the permissions for the group of people to whom the file (or directory)\nthe rightmost group gives the permissions for all others.\n\nThe symbols r, w, etc., have slightly different meanings depending on whether they refer to a simple file or to a directory.\n\n\n\nr (or -), indicates read permission (or otherwise), that is, the presence or absence of permission to read and copy the file\nw (or -), indicates write permission (or otherwise), that is, the permission (or otherwise) to change a file\nx (or -), indicates execution permission (or otherwise), that is, the permission to execute a file, where appropriate\n\n\n\n\n\nr allows users to list files in the directory;\nw means that users may delete files from the directory or move files into it;\nx means the right to access files in the directory. This implies that you may read files in the directory provided you have read permission on the individual files.\n\nSo, in order to read a file, you must have execute permission on the directory containing that file, and hence on any directory containing that directory as a subdirectory, and so on, up the tree.\n\n\n\nWe can use chmod to change who has access to our files.\nChmod options:\nu = user\ng = group\no = other\na = all\nr = read\nw = write (and delete)\nx = execute (and access directory)\n+ = add permission\n- = take away permission\nFor example, to add read write and execute permissions on the file example.txt for the group (i.e g), type:\n\nchmod g=rwx example.txt \n\nTo remove read write and execute permissions on the file example.txt for the group and others (i.e go), type:\n\nchmod go-rwx example.txt \n\n\n\n\n\nThe code below is a random collection of code the author found useful. This means it might be interesting for you BUT these are so far not linked to example files and some programs might not be avail. on your system by default.\n\n\nCertain characters are significant to the shell; Escaping is a method of quoting single characters. The escape (\\) preceding a character tells the shell to interpret that character literally. Below, we find the special meanings of certain escaped characters:\n\n\n\n\n\n\n\nhidden symbols\n\nSometimes when saving excel documents as text this insert hidden symbols. These can be seen as a blue M when opening the file in vim (sometimes they result in odd errors while parsing tables). Most often you see these issues when you open your files and lines are merged that should not be merged.\nThis symbol can be removed as follows in vim:\n\n:%s/\\r/\\r/g   to remove blue M\n\n\nwrong file types\n\nFiles created on WINDOWS systems are not always compatible with UNIX. In case there is an issue it is always safer to convert. You see if you have an issue if you open your file of interest with nano and check the file format at the bottom.\nIf we see we are dealing with a dos file, we can clean files like this:\n\n#dos to unix\nawk '{ sub(\"\\r$\", \"\"); print }' winfile.txt &gt; unixfile.txt\n\n#unix to dos\nawk 'sub(\"$\", \"\\r\")' unixfile.txt &gt; winfile.txt\n\n\n\n\nGNU datamash is a command-line program which performs basic numeric, textual and statistical operations on input textual data files.\n\ndatamash -sW -g1 collapse 2 collapse 4 &lt; Unimarkers_KO_count_table.txt  &gt; Unimarkers_KO_collapsed.txt\n\n\n\n\nCPDF is a useful program if you want to merge pdfs, remodel them all to A4 etc.\nThis is not available on the server but easy to install in case you are interested.\nFor more information, see here\n\n# merge pdfs\n~/Desktop/Programs/cpdf-binaries-master/OSX-Intel/cpdf -merge *pdf -o All_Main_Figs.pdf\n\n#convert to A4\n~/Desktop/Programs/cpdf-binaries-master/OSX-Intel/cpdf -scale-to-fit a4portrait SI_Figures.pdf -o test.pdf",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#downloading-example-data",
    "href": "source/cli/cli_file_manipulation.html#downloading-example-data",
    "title": "Working with files in bash",
    "section": "",
    "text": "wget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/Experiment1.txt\nwget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/Experiment2.txt\nwget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/Experiment3.txt\n\nwget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/PF00900.faa\nwget -P downloads https://raw.githubusercontent.com/ndombrowski/Unix_tutorial/main/Input_docs/PF01015.faa\n\n#check if this worked\nls downloads/\n\nThe content of the files is not important but what you downloaded are simple text files that contain information about a growth experiment and the files look something like this:\n\n\n\nEx1\ncontrol1\n0.2\n0.3\n#Ex1_all_good\n\n\nEx1\ncontrol2\n0.3\n0.1\n#all_good\n\n\nEx1\nN[0.4uM]\n10.1\n4.7\n#contamination\n\n\nEx1\nN[0.4uM]\n5.2\n4.3\n#all_good\n\n\n\nThe faa files are a file type you might encounter when working with molecular data. The FASTA format is a text-based format for representing either nucleotide sequences or amino acid (protein) sequences, in which nucleotides or amino acids are represented using single-letter codes. A sequence begins with a greater-than character (&gt;) followed by a description of the sequence (all in a single line). The lines immediately following the description line are the sequence representation, with one letter per amino acid or nucleic acid, and are typically no more than 80 characters in length.\n&gt;GCA_000007185\nMVRDKWKDKVWYTILAPDMFDNVEVGETPADDPEKVIGRVLETTLGDVLDDITKHHIKVFFRIYDVEGTTAYSKFEGHRLMRDYVRSLVRRGTSRIDGVIDVVTKDGYKVRVAGLAFTTRRAKTSQQRAIRKEMFKVIEENAKECDFDEFIRRCLSISEEESIPEQIKEAGRKIYPIRQAEIRKTEVLEEPNGLPPYEAVGDRATPELASY\n&gt;GCA_000007305\nMAAKRATTTRDKWKLKQWYII\nIn your case you will look at two protein fasta files in which the header contains the information from what genome, i.e. GCA_000007185, a sequence was extracted from as well as the actual sequence itself.",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#wc-counting-things",
    "href": "source/cli/cli_file_manipulation.html#wc-counting-things",
    "title": "Working with files in bash",
    "section": "",
    "text": "The wc (= wordcount) command in UNIX is a command line utility that:\n\nCan return the number of lines, the number of characters and the number of words in a file\nCan be combined with pipes for general counting operations. We will explain pipes a bit later\n\nThe following commands are simple but essential for quality control and you will use it a lot to check whether your commands worked all right.\n\n#count how many words we have\nwc -w downloads/Experiment1.txt\n\n#count how many lines we have in a file we have\nwc -l downloads/Experiment1.txt",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#grep-finding-patterns-in-files",
    "href": "source/cli/cli_file_manipulation.html#grep-finding-patterns-in-files",
    "title": "Working with files in bash",
    "section": "",
    "text": "The grep command is used to search text. It searches the given file for lines containing a match to the given strings or words. Also this command is simple but very useful for sanity checks after file transformations.\n\n#count how often the pattern **control** occurs in our document\ngrep \"control\" downloads/Experiment1.txt\n\n#only give the counts, not the lines\ngrep -c \"control\" downloads/Experiment1.txt\n\n#grep a pattern only if it occurs at the beginning of a line\ngrep \"^Ex\" downloads/Experiment1.txt\n\n#we can also count the number of sequences in the fasta file we downloaded before\ngrep -c \"&gt;\" downloads/*faa",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#using-wildcards",
    "href": "source/cli/cli_file_manipulation.html#using-wildcards",
    "title": "Working with files in bash",
    "section": "",
    "text": "Since the shell uses filenames so much, it provides special characters to help you rapidly specify groups of filenames.\nA Wild-card character can be used as a substitute for any class of characters in a search.\n\nThe * wildcard is the wildcard with the broadest meaning of any of the wildcards, it can represent 0 characters, all single characters or any string of characters. I.e. we can grep for something in any files that end with .txt as follows:\n\n\ngrep -c \"Ex\" downloads/*txt\n\n\nThe ? wildcard matches exactly one character but will ingore files that start with a dot. We could use it as follows:\n\n\ngrep \"control\" downloads/Experiment?.txt\n\n\n[012] wildcard = matches 0 or 1 or 2 exactly once\n\n\ngrep  \"control\" downloads/Experiment[012].txt\n\n\n[0-9] wildcard = matches matches any number exactly one\n\n\ngrep  \"control\" downloads/Experiment[0-9].txt\n\n\ncombining wildcards\n\n\n[A-Z] wildcard = any letter in capitals occurring once\n[a-z]* wildcard = any letter in non-capital letters occurring many times\n\n\ngrep \"control\" downloads/[A-Z][a-z]*[12].txt\n\n\n[a-z]{7} we are exactly looking for 7 letters (as in ‘control’)\nthese 7 letters should be followed by either a 1 or 2\n\n\ngrep \"[a-z]\\{7\\}[12]\" downloads/Experiment[12].txt\n\n\nif we are not sure how many characters we have\n\n\nmatches 3-10 characters\n\n\ngrep \"[a-z]\\{3,10\\}[12]\" downloads/Experiment[12].txt\n\nIf you use grep with special symbols you might notice something:\n\n#this does not work\ngrep \"control?\" downloads/Experiment1.txt\n\n#this works how we want it to work\ngrep -E \"control?\" downloads/Experiment1.txt\n\n\n-E tells grep that the ‘?’ is not taken literally but as a wildcard\n\nUnfortunately different programs have slightly ways to do things, i.e greop uses -E while sed uses different quotes. If you run into problems when using wildcards check the manual or the web.\nExercise\n\nIn the faa files, how many sequences do we have? Notice, sequences always start with a &gt;\nIn the faa files, how often do we have 3 consecutive A’s?\nIn the faa files, how often do we have 2x A’s followed by a T?\nIn the faa files, how often do we have 2x M’s followed by a T or followed by a D?\n\nComment: If you unsure what is happening remove the -c to see what grep is grepping.\n\n\nShow the code\n#question1:\ngrep -c \"&gt;\" downloads/*.faa\n\n#question2\ngrep -c \"[A]\\{3\\}\" downloads/*.faa\n\n#question4\ngrep -c \"[A]\\{2\\}[T]\" downloads/*.faa\n\n#question5\ngrep -c  \"[A]\\{2\\}[TC]\" downloads/*.faa\n\n\nHint: if you are unsure what is happening, redo the command without the -c option",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#cat-combining-data",
    "href": "source/cli/cli_file_manipulation.html#cat-combining-data",
    "title": "Working with files in bash",
    "section": "",
    "text": "The cat command has three main functions related to manipulating text files:\n\nCreate a new file using cat. To do this type the command below, then type Hello! press enter, and *Press “ctrl+c” to save the file:\n\n\ncat &gt; downloads/file_test.txt\n\n\nDisplay the content of an existing file:\n\n\ncat downloads/file_test.txt\n\n\nConcatenate, i.e. combine, several files:\n\n\n#merge files\ncat downloads/file_test.txt downloads/file_test.txt  &gt; downloads/file_merged.txt\n \n#check the content of the file\ncat downloads/file_merged.txt\n\nExercise\n\nView the content of downloads/Experiment1.txt by using cat\nCombine downloads/Experiment1.txt and downloads/Experiment2.txt.\nDo the same as above but use a wildcard to not have to type everything and store the output as downloads/Experiments.txt\n\n\n\nShow the code\n#question1\ncat downloads/Experiment1.txt\n\n#question3\ncat downloads/Experiment1.txt downloads/Experiment2.txt\n\n#question3\ncat downloads/Experiment*.txt &gt; downloads/Experiments.txt",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#pipes-combining-commands",
    "href": "source/cli/cli_file_manipulation.html#pipes-combining-commands",
    "title": "Working with files in bash",
    "section": "",
    "text": "Pipes are a powerful utility to connect multiple commands together. They allow us to feed the standard output of one command as input into another command.\nIn the example below we first combine two files using cat and use wc -l on the standard output to count the number of lines after concatenating the two files:\n\ncat downloads/Experiment[12].txt | wc -l\n\nExercise\n\nUsing a pipe, combine the two faa files and count the total number of sequences using grep:\n\n\n\nShow the code\n#question1\ncat downloads/*faa | grep -c \"&gt;\"",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#cut-extracting-sections-from-tables",
    "href": "source/cli/cli_file_manipulation.html#cut-extracting-sections-from-tables",
    "title": "Working with files in bash",
    "section": "",
    "text": "We can use cut to separate columns. By default cut uses a tab as a delimiter. We could print the first column of our data as follows:\n\ncut -f1 downloads/Experiment1.txt\n\nOr if we only want to get the second column we do this:\n\ncut -f2 downloads/Experiment1.txt\n\nIf we wanted to get columns 1 and 2 we do:\n\ncut -f1-2 downloads/Experiment1.txt\n\nWe can also change how we cut, i.e. we could cut after a # by using the -d argument:\n\ncut -f1 -d \"#\" downloads/Experiment1.txt\n\nExercise\n\nIn PF00900.faa, we want to shorten the fasta header. To do this cut the text of after the first underscore _ (i.e. keep only the first element). Combine this with the head command to only view the first few lines and see if your command worked.\nIn PF00900.faa, cut to only keep the text after the first _ (i.e. keep the second element). Again, combine this with the head command.\n\n\n\nShow the code\n#question1\ncut -f1 -d \"_\" downloads/PF00900.faa | head\n\n#question2\ncut -f2 -d \"_\" downloads/PF00900.faa | head\n\n\nBe careful when using the second options, as this will cut away the &gt; symbol that we need in fasta files to define the header. I.e. fasta headers always should start with this symbol and if we would use such a command we would need the &gt; back in/",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#sort-sorting-files",
    "href": "source/cli/cli_file_manipulation.html#sort-sorting-files",
    "title": "Working with files in bash",
    "section": "",
    "text": "sort – sort lines in a file from A-Z and is useful for file organization. We can sort files like this:\n\nsort downloads/Experiment2.txt\n\nSort by default sorts from left to right, but we also can sort based on a specific column, for example column 4:\n\nsort -k4 downloads/Experiment2.txt\n\nWe can also sort by more than one column, i.e. column 5 followed by column 3:\n\nsort -k5 -k3 downloads/Experiment1.txt",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#uniq-finding-duplicated-data",
    "href": "source/cli/cli_file_manipulation.html#uniq-finding-duplicated-data",
    "title": "Working with files in bash",
    "section": "",
    "text": "uniq – can be used to remove or find duplicates . However, for this to work the file first needs to be sorted. To do this, we can make use of pipes. Lets, start of by extracting the observations we made with cut:\n\ncut -f5 downloads/Experiment1.txt\n\nWe then can sort the column by adding a pipe:\n\ncut -f5 downloads/Experiment1.txt | sort\n\nAnd only print unique observations with uniq:\n\ncut -f5 downloads/Experiment1.txt | sort | uniq\n\nIf we where interested in duplicated observations, we can add the -d argument:\n\ncut -f5 downloads/Experiment1.txt | sort | uniq -d\n\nCheck out the manual for more options!\nExercise\n\nUsing a pipe: combine the faa files. How many sequences do we have in total?\nUsing a pipe: combine the faa files. Extract only the header and then count how many duplicates do we have?\nSame as above, but how many sequences are not duplicated? Check the manual for uniq on what option allows you to do this.\n\n\n\nShow the code\n#question1\ncat downloads/*faa | grep -c \"&gt;\"\n\n#question 2\ncat downloads/*faa | grep \"&gt;\" | sort | uniq -d | wc -l \n\n#question 3\ncat downloads/*faa | grep \"&gt;\" | sort | uniq -u | wc -l",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#find-locate-files-based-on-conditions",
    "href": "source/cli/cli_file_manipulation.html#find-locate-files-based-on-conditions",
    "title": "Working with files in bash",
    "section": "",
    "text": "find searches through the directories for files and directories with a given name, date, size, or any other attribute you care to specify.\n\n#find all files with a txt extension\nfind downloads/ -name \"*txt\"\n\n#find files over a certain size and display results as a long list\nfind downloads/ -size +1M -ls",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#sed-manipulating-the-content-of-files",
    "href": "source/cli/cli_file_manipulation.html#sed-manipulating-the-content-of-files",
    "title": "Working with files in bash",
    "section": "",
    "text": "Sed is a stream editor that can be used to perform basic text transformations on an input stream (a file or input from a pipeline).\nThe most basic pattern to use sed for is sed ‘s/find/replace/’ file.\n\n\nLet’s go back to our text files and assume that instead of writing Ex we want to be more clear and write Experiment, we can do this as follows:\n\nsed 's/Ex/Experiment/' downloads/Experiment1.txt \n\nWhen doing this, we see that:\n\nEx only gets replaced the first time it appears in a line, ie. the #Ex1_all_good in the first line gets not replaced\nsed will not overwrite the input file (unless you tell it to do so) but it will print the changed to the screen. If we wanted to save this in a file, we would need to re-direct the output to a new file.\n\nTo change all instances of Ex, we can do the following:\n\nsed 's/Ex/Experiment/g' downloads/Experiment1.txt \n\nSed can be easily used with wildcards. For example, if you would want to replace control1 and control2 just with control you could do:\n\nsed 's/control[0-9]/control/g' downloads/Experiment1.txt \n\nOne important thing to remember is that certain symbols have specific meanings in UNIX. Examples are: comma, brackets, pipes. Do search for these in files, we need to escape them with a \\. For example, we could replace the square with round brackets as follows:\n\n#replace the square bracket with a round bracket. \nsed 's/N\\[0.4uM\\]/N(0.4uM)/g' downloads/Experiment1.txt \n\nExercise\n\nIn downloads/PF00900.faa , replace the GCA_ with Genome_\nIn downloads/PF00900.faa , use wildcards to replace the string of numbers after the GCA with a hello\n\n\n\nShow the code\n#qst 1\nsed 's/GCA_/Genome_/g'  downloads/PF00900.faa \n\n#qst 2\nsed 's/GCA_[0-9]*/GCA_hello/g'  downloads/PF00900.faa \n\n\n\n\n\nSed can also be used to remove things, such as line(s) or patterns. Below is a list of examples that you might encounter or find useful.\n\n# remove 1 first line\nsed '1d' downloads/Experiment1.txt\n\n#remove last line\nsed '$d' downloads/Experiment1.txt\n\n#remove lines 2-4\nsed '2,4d' downloads/Experiment1.txt\n\n#remove lines other than 2-4\nsed '2,4!d' downloads/Experiment1.txt \n\n#remove the first and last line\nsed '1d;$d' downloads/Experiment1.txt\n\n#remove lines beginning with an **L**\nsed '/^L/d' downloads/Experiment1.txt\n\n#delete lines ending with d\nsed '/d$/d' downloads/Experiment1.txt\n\n#delete lines ending with d OR D\nsed '/[dD]$/d' downloads/Experiment1.txt\n\n#delete blank lines ('^$' indicates lines containing nothing, i.e. lines with zero or more spaces)\nsed '/^$/d' downloads/Experiment1.txt\n\n#delete lines that start with capital letters\nsed '/^[A-Z]/d' downloads/Experiment1.txt\n\n#delete lines with the pattern **Ex**\nsed '/Ex/d' downloads/Experiment1.txt\n\n#delete files with the pattern control or uM\nsed '/control\\|uM/d' downloads/Experiment1.txt\n\n#delete the 2nd occurence of each pattern per line (here *o*)\nsed 's/o//2' downloads/Experiment1.txt\n\n#remove all digits across the whole file\nsed 's/[0-9]//g' downloads/Experiment1.txt\n\n#remove all alpha-numerical characters and numbers\nsed 's/[a-zA-Z0-9]//g' downloads/Experiment1.txt\n\n#remove character, here E, regardless of the case\nsed 's/[eE]//g' downloads/Experiment1.txt",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#screen-running-things-in-the-background",
    "href": "source/cli/cli_file_manipulation.html#screen-running-things-in-the-background",
    "title": "Working with files in bash",
    "section": "",
    "text": "Screen or GNU Screen is a terminal multiplexer. This means that you can start a screen session and then open any number of windows (virtual terminals) inside that session.\nProcesses running in Screen will continue to run when their window is not visible even if you get disconnected. This is perfect, if we start longer running processes on the server and want to shut down our computer when leaving for the day. As long as the server is still connected to the internet, your process will continue running.\nWe start a screen as follows:\n\nscreen\n\nWe detach, i.e. exit, from a screen with control+a+d.\nIf you run multiple things, it can be useful to give your screens more descriptive names. You can do this as follows:\n\n#start a screen and give it a name\nscreen -S testrun\n\nAfter detaching from a screen you can list all currently running screens with:\n\nscreen -ls\n\nYou can restart a screen like this:\n\n#restart an existing screen\nscreen -r testrun\n\nIf you want to completely close and remove a screen, type the following while being inside of the screen:\n\nexit",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#loops-running-the-same-thing-over-and-over-again",
    "href": "source/cli/cli_file_manipulation.html#loops-running-the-same-thing-over-and-over-again",
    "title": "Working with files in bash",
    "section": "",
    "text": "Before talking about loops, lets learn another command first, echo. echo displays a line of text that can be printed to the screen or stored in a file (why this is useful you see in a second):\nBoth echo and printf print something into the console. However, echo uses different versions across shells, so you might get slighly different outputs depending on the shell you use. Printf is more portable and allows for more formatting options. You might encounter both when finding code in the wild.\n\n#print a string to the console using echo\necho \"Hello everyone\"\n\n#print a string to the console using printf\nprintf \"Hello everyone\"\n\n\n\n\nA for loop is a bash programming language statement which allows code to be repeatedly executed. I.e. it allows us to run a command 2, 3, 5 or 100 times.\nTry running this example in which we want to say hello several times:\n\nfor i in 1 2 3; do echo \"Welcome $i times\"; done\n\nHere, you see what this command does step by step:\n\n\n\nWe could also use a loop to run sed on all our txt files. For example assume that instead of nitrogen (N) we measured sulfur (s) we could replace it among all text files as follows:\n\n#change the N for  P in all our files\nfor i in downloads/*txt; do sed 's/N/S/g' $i; done\n\n\n\n\nIf we want to store the output in new files, we could store them with a slightly different name as follows:\n\nfor i in downloads/*txt; do sed 's/N/S/g' $i &gt; ${i}_2.txt; done\n\n#check that all went alright with \nll downloads/*txt\n\nYou see here, that we added curly brackets to define the borders of our variable i. I.e. if we would not have the brackets unix would look for a variable i_2, which does not exist.\nYou will also see that the name of the output file is not ideal, i.e. it is something like downloads/Experiment3.txt_2.txt. To get more control over the name of the output files we can use a file list for building the loop.\nTo do this let’s use printf to generate a FileList in which list the the two files we want to work with: Experiment1 and Experiment2. We do this using printf and \\n, which is a special character we can use to separate Experiment1 and Experiment using a new line.\n\nprintf \"Experiment1\\nExperiment2\" &gt; downloads/FileList\n\n#view file\nhead downloads/FileList\n\nNow we can use this file to run our loop.\n\nfor i in `cat downloads/FileList`; do sed 's/N/S/g' downloads/${i}.txt &gt; downloads/${i}_2.txt; done\n\n#check that all went alright with \nll downloads/*txt\n\nNotice, that using this option we need to add the file extension! However, the resulting filenames, i.e. downloads/Experiment2_2.txt, are much cleaner.\nExercise\n\nMake a list with all the faa files (do this with the command line). Ideally, we want to have PF00900 and PF01015 in one column. For this you want to list the names of all faa files but at the same time remove the file extension and the folder name.\nUse this list to, in a loop, replace the GCA_ with Genome_. Store the files with the new ending renamed.faa\n\n\n\nShow the code\n#question1\nls downloads/*faa | sed 's/downloads\\///g' | sed 's/\\.faa//g' &gt; downloads/FaaList\n\n#question2\nfor sample in `cat downloads/FaaList`; do sed 's/GCA_/Genome_/g' downloads/${sample}.faa &gt; downloads/${sample}_renamed.faa; done\n\n#check file\nhead downloads/PF00900_renamed.faa",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#variables",
    "href": "source/cli/cli_file_manipulation.html#variables",
    "title": "Working with files in bash",
    "section": "",
    "text": "A variable is a character string to which we assign a value. Environment variables are variables that are set up in your shell when you log in. They are called “environment variables” because most of them affect the way your Unix shell works for you. I.e. one points to your home directory and another to your history file.\nOne common environmental variable is $HOME. This variable holds the path to your home directory, which is like your computer’s main hub.\nWe can use the echo command to display the value of a variable. Let’s use it to look at an environmental variable found on most systems called $HOME:\n\necho $HOME\n\nYou can use this variables in commands, for example you can use it together with cd to move into your home directory:\n\ncd $home\n\nYou can easily make your own variables and use them to store file name, paths or what ever you want. Let’s store a number in a variable:\n\nMY_NUMBER=10\n\nWhen using the export command this lets the system know that this variable is for everyone, not just the current session.\nLets see if this worked:\n\necho \"My math number: $MY_NUMBER\"\n\nOnce you have this, you can use it for any command that you want.",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#accesss-rights",
    "href": "source/cli/cli_file_manipulation.html#accesss-rights",
    "title": "Working with files in bash",
    "section": "",
    "text": "Each file (and directory) has associated access rights, which may be found by typing ls -l. Additionally, ls -lg gives additional information as to which group owns the file\n\nls -lg downloads/Experiment1.txt \n\n-rw-r--r--  1 staff   156B Sep 28  2019 downloads/Experiment1.txt\nIn the left-hand column is a 10 symbol string consisting of the symbols d, r, w, x, -, and, occasionally, s or S. If d is present, it will be at the left hand end of the string, and indicates a directory: otherwise - will be the starting symbol of the string.\nThe 9 remaining symbols indicate the permissions, or access rights, and are taken as three groups of 3.\n\nThe left group of 3 gives the file permissions for the user that owns the file (or directory)\nthe middle group gives the permissions for the group of people to whom the file (or directory)\nthe rightmost group gives the permissions for all others.\n\nThe symbols r, w, etc., have slightly different meanings depending on whether they refer to a simple file or to a directory.\n\n\n\nr (or -), indicates read permission (or otherwise), that is, the presence or absence of permission to read and copy the file\nw (or -), indicates write permission (or otherwise), that is, the permission (or otherwise) to change a file\nx (or -), indicates execution permission (or otherwise), that is, the permission to execute a file, where appropriate\n\n\n\n\n\nr allows users to list files in the directory;\nw means that users may delete files from the directory or move files into it;\nx means the right to access files in the directory. This implies that you may read files in the directory provided you have read permission on the individual files.\n\nSo, in order to read a file, you must have execute permission on the directory containing that file, and hence on any directory containing that directory as a subdirectory, and so on, up the tree.\n\n\n\nWe can use chmod to change who has access to our files.\nChmod options:\nu = user\ng = group\no = other\na = all\nr = read\nw = write (and delete)\nx = execute (and access directory)\n+ = add permission\n- = take away permission\nFor example, to add read write and execute permissions on the file example.txt for the group (i.e g), type:\n\nchmod g=rwx example.txt \n\nTo remove read write and execute permissions on the file example.txt for the group and others (i.e go), type:\n\nchmod go-rwx example.txt",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/cli_file_manipulation.html#random-but-useful",
    "href": "source/cli/cli_file_manipulation.html#random-but-useful",
    "title": "Working with files in bash",
    "section": "",
    "text": "The code below is a random collection of code the author found useful. This means it might be interesting for you BUT these are so far not linked to example files and some programs might not be avail. on your system by default.\n\n\nCertain characters are significant to the shell; Escaping is a method of quoting single characters. The escape (\\) preceding a character tells the shell to interpret that character literally. Below, we find the special meanings of certain escaped characters:\n\n\n\n\n\n\n\nhidden symbols\n\nSometimes when saving excel documents as text this insert hidden symbols. These can be seen as a blue M when opening the file in vim (sometimes they result in odd errors while parsing tables). Most often you see these issues when you open your files and lines are merged that should not be merged.\nThis symbol can be removed as follows in vim:\n\n:%s/\\r/\\r/g   to remove blue M\n\n\nwrong file types\n\nFiles created on WINDOWS systems are not always compatible with UNIX. In case there is an issue it is always safer to convert. You see if you have an issue if you open your file of interest with nano and check the file format at the bottom.\nIf we see we are dealing with a dos file, we can clean files like this:\n\n#dos to unix\nawk '{ sub(\"\\r$\", \"\"); print }' winfile.txt &gt; unixfile.txt\n\n#unix to dos\nawk 'sub(\"$\", \"\\r\")' unixfile.txt &gt; winfile.txt\n\n\n\n\nGNU datamash is a command-line program which performs basic numeric, textual and statistical operations on input textual data files.\n\ndatamash -sW -g1 collapse 2 collapse 4 &lt; Unimarkers_KO_count_table.txt  &gt; Unimarkers_KO_collapsed.txt\n\n\n\n\nCPDF is a useful program if you want to merge pdfs, remodel them all to A4 etc.\nThis is not available on the server but easy to install in case you are interested.\nFor more information, see here\n\n# merge pdfs\n~/Desktop/Programs/cpdf-binaries-master/OSX-Intel/cpdf -merge *pdf -o All_Main_Figs.pdf\n\n#convert to A4\n~/Desktop/Programs/cpdf-binaries-master/OSX-Intel/cpdf -scale-to-fit a4portrait SI_Figures.pdf -o test.pdf",
    "crumbs": [
      "Welcome page",
      "Using the command line",
      "Working with files in bash"
    ]
  },
  {
    "objectID": "source/cli/code_doc.html",
    "href": "source/cli/code_doc.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Documenting your code is crucial for both your future self and anyone else who might work with your code. Documentation serves as a roadmap for your code. It helps others (and your future self) understand the purpose, functionality, and usage of your code.\nA Guide to Reproducible Code in Ecology and Evolution gives detailed information on how to organize project folders and how to write clear and reproducible code. The examples are mainly based on R code but most are general enough to apply to other computational langauges (and scientific disciplines).\n\n\n\n\n\nWhen documenting code, its best to avoid visual editors, such as word, as they are not designed for writing code.\nInstead you can use a plain text editor, such as TextEdit (Mac) or Notepad (Windows). This is the easiest to get started but you will loose some functionality, such as using headers or writing text in bold.\nAlternatives, that offer more functionality, are for example RStudio or VScode.\n\n\n\nRMarkdown is an extension of Markdown that allows you to integrate R code directly into your documentation.\nIf you have not install R and Rstudio, follow these instructions.\nIn RStudio you can create an R Markdown File by:\n\nIn RStudio, go to File -&gt; New File -&gt; R Markdown\nChoose a title, author, and output format\nKnit the Document:\n\nClick the “Knit” button to render your R Markdown document into the chosen output format.\n\n\nFor more information visit the RMarkdown tutorial.\n\n\n\nQuarto is an alternative to RMarkdown for creating dynamic documents in RStudio but can be read by other editors, such as VScode. Compared to RMarkdown it provides enhanced features for document creation and includes many more built in output formats (and many more options for customizing each format).\nIt is installed by default on newer R installations.\n\nIn RStudio, go to File -&gt; New File -&gt; Quarto document\nChoose a title, author, and output format\nRender the Document:\n\nClick the “Render” button to render your R Markdown document into the chosen output format.\n\n\nFor more information (and more functionality) visit the Quarto website.\n\n\n\nVisual Studio Code (VSCode) is a versatile and user-friendly code editor. It provides excellent support for various programming languages, extensions, and a built-in terminal but might take a bit of work to setup to work with different compuational languages.\n\nInstallation:\n\nDownload and install VSCode from here.\n\nExtensions:\n\nInstall extensions relevant to your programming language (e.g., Python, R). These extensions enhance code highlighting and provide additional features.\n\n\n\n\n\n\nMarkdown is a lightweight markup language that’s easy to read and write. It allows you to add formatting elements to plain text documents.\nHeaders:\nUse # for headers. The more # symbols, the smaller the header. When writing a header make sure to always put a space between the # and the header name.\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers don’t have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nCreate links to external resources or within your documentation.\n[Link Text](https://www.example.com)\nEmphasis:\nUse * or _ for italic and ** or __ for bold.\n*italic*\n**bold**\nPictures\nYou can easily add images to your documentation as well:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/image.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|\n\n\n\nIf you want to see an example for documented code, check out an example of a notebook in which I started working with some sequencing data.\nThe link above leads you to an example for:\n\nHow could I use github to make my code available to others\nHow could a “code book” look like? The example you see in the folder is provided a Quarto markdown file here and for convenience the report was also rendered as a html to make it easier to read for potential collaborators. To view the HTML report, you need to download it first.\n\nPlease note that this is just an example to get you started and such a report might look different depending on your needs.\nBelow you find some snippets from the report rendered to HTML and some thoughts for what to put into different sections.\n\n\n\n\n\n\n\nIn this first section I add everything that a user that wants to use my workflow has to change and I try to standardize the code below, so that another user could just run this as is without having to edit anything. Typically things to add are:\n\nFrom where to start the analyses\nCustom paths, to for example for databases that need to be downloaded from elsewhere\nCustom variables: In the example above I store the link to the data in a variable called download_link, I then use the variable in the code below to download the data. By doing it this way I have one location in the code another person needs to change the code when for example the path to the data changes. The code below stays the same. When writing code try to think ahead and minimize the number of instances where things need to be changed if for example the location to your data changes.\n…\n\nI tend to NOT add the information how I use scp to log into an HPC to keep my user name and login information private.\n\n\n\n\nWhen I first start working with new data, I try to add as many sanity checks as possible to ensure that my data looks good. That way I avoid that I don’t notice an issue and run into trouble further down the line. I at the same time understand my data more and learn with how many samples and how much data I am working with.\nI also add such sanity checks whenever I modify my data. For example, when I merge individual files into a large file I might count the lines for the individual files and the combined file simply to ensure that I used my wildcards correctly.\nRemember: The computer is only as smart as the person using it and will blindly run your commands. Because of that the computer can do unexpected things and you need to account for that.\n\n\n\n\nIn the example above I use markdown to not only document my code but also add some comments whenever I might need it. For example here, I added some notes about what the results from FastQC actually told me.\nThis kind of documentation can be useful for:\n\nJustifying decisions further down the line. In this example, I might decide on how to clean the sequencing data. For example, if I would have found a lot of low quality reads or the adapter still being part of the sequence then I would have specifically cleaned my sequences to deal with that\nFuture you. If you read the report a month, or a year, later you have the key information in your report and don’t have to open the HTML reports again.\n\n\n\n\n\nThis part might make more sense after you have worked through the part of the tutorial about using an HPC. But what you see here is how I have written down code that simply says that I submitted a script to an HPC but it does not actually say how I ran the FastQC software. The actual code is “hidden” inside of the run_fastqc.sh script. This also means that a person reading your workflow does not have the code right away. You can deal with this in two ways.\n\nInstead of the sbatch command, you can add the actual line of code that was run on the compute node.\nWhen publishing your code with your manuscript, add the whole scripts folder to where you publish the main code, i.e. on github or zenodo\n\nI tend to prefer number 2 because I like to record the code in my notebooks exactly as I ran them but you can do it differently as long as all the code you ran is recorded and accessible to others once you publish your data.",
    "crumbs": [
      "Welcome page",
      "Documenting your code"
    ]
  },
  {
    "objectID": "source/cli/code_doc.html#documenting-code",
    "href": "source/cli/code_doc.html#documenting-code",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Documenting your code is crucial for both your future self and anyone else who might work with your code. Documentation serves as a roadmap for your code. It helps others (and your future self) understand the purpose, functionality, and usage of your code.\nA Guide to Reproducible Code in Ecology and Evolution gives detailed information on how to organize project folders and how to write clear and reproducible code. The examples are mainly based on R code but most are general enough to apply to other computational langauges (and scientific disciplines).",
    "crumbs": [
      "Welcome page",
      "Documenting your code"
    ]
  },
  {
    "objectID": "source/cli/code_doc.html#choose-your-editor",
    "href": "source/cli/code_doc.html#choose-your-editor",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "When documenting code, its best to avoid visual editors, such as word, as they are not designed for writing code.\nInstead you can use a plain text editor, such as TextEdit (Mac) or Notepad (Windows). This is the easiest to get started but you will loose some functionality, such as using headers or writing text in bold.\nAlternatives, that offer more functionality, are for example RStudio or VScode.\n\n\n\nRMarkdown is an extension of Markdown that allows you to integrate R code directly into your documentation.\nIf you have not install R and Rstudio, follow these instructions.\nIn RStudio you can create an R Markdown File by:\n\nIn RStudio, go to File -&gt; New File -&gt; R Markdown\nChoose a title, author, and output format\nKnit the Document:\n\nClick the “Knit” button to render your R Markdown document into the chosen output format.\n\n\nFor more information visit the RMarkdown tutorial.\n\n\n\nQuarto is an alternative to RMarkdown for creating dynamic documents in RStudio but can be read by other editors, such as VScode. Compared to RMarkdown it provides enhanced features for document creation and includes many more built in output formats (and many more options for customizing each format).\nIt is installed by default on newer R installations.\n\nIn RStudio, go to File -&gt; New File -&gt; Quarto document\nChoose a title, author, and output format\nRender the Document:\n\nClick the “Render” button to render your R Markdown document into the chosen output format.\n\n\nFor more information (and more functionality) visit the Quarto website.\n\n\n\nVisual Studio Code (VSCode) is a versatile and user-friendly code editor. It provides excellent support for various programming languages, extensions, and a built-in terminal but might take a bit of work to setup to work with different compuational languages.\n\nInstallation:\n\nDownload and install VSCode from here.\n\nExtensions:\n\nInstall extensions relevant to your programming language (e.g., Python, R). These extensions enhance code highlighting and provide additional features.",
    "crumbs": [
      "Welcome page",
      "Documenting your code"
    ]
  },
  {
    "objectID": "source/cli/code_doc.html#markdown-for-documentation",
    "href": "source/cli/code_doc.html#markdown-for-documentation",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Markdown is a lightweight markup language that’s easy to read and write. It allows you to add formatting elements to plain text documents.\nHeaders:\nUse # for headers. The more # symbols, the smaller the header. When writing a header make sure to always put a space between the # and the header name.\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers don’t have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nCreate links to external resources or within your documentation.\n[Link Text](https://www.example.com)\nEmphasis:\nUse * or _ for italic and ** or __ for bold.\n*italic*\n**bold**\nPictures\nYou can easily add images to your documentation as well:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/image.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|",
    "crumbs": [
      "Welcome page",
      "Documenting your code"
    ]
  },
  {
    "objectID": "source/cli/code_doc.html#an-example-notebook",
    "href": "source/cli/code_doc.html#an-example-notebook",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "If you want to see an example for documented code, check out an example of a notebook in which I started working with some sequencing data.\nThe link above leads you to an example for:\n\nHow could I use github to make my code available to others\nHow could a “code book” look like? The example you see in the folder is provided a Quarto markdown file here and for convenience the report was also rendered as a html to make it easier to read for potential collaborators. To view the HTML report, you need to download it first.\n\nPlease note that this is just an example to get you started and such a report might look different depending on your needs.\nBelow you find some snippets from the report rendered to HTML and some thoughts for what to put into different sections.\n\n\n\n\n\n\n\nIn this first section I add everything that a user that wants to use my workflow has to change and I try to standardize the code below, so that another user could just run this as is without having to edit anything. Typically things to add are:\n\nFrom where to start the analyses\nCustom paths, to for example for databases that need to be downloaded from elsewhere\nCustom variables: In the example above I store the link to the data in a variable called download_link, I then use the variable in the code below to download the data. By doing it this way I have one location in the code another person needs to change the code when for example the path to the data changes. The code below stays the same. When writing code try to think ahead and minimize the number of instances where things need to be changed if for example the location to your data changes.\n…\n\nI tend to NOT add the information how I use scp to log into an HPC to keep my user name and login information private.\n\n\n\n\nWhen I first start working with new data, I try to add as many sanity checks as possible to ensure that my data looks good. That way I avoid that I don’t notice an issue and run into trouble further down the line. I at the same time understand my data more and learn with how many samples and how much data I am working with.\nI also add such sanity checks whenever I modify my data. For example, when I merge individual files into a large file I might count the lines for the individual files and the combined file simply to ensure that I used my wildcards correctly.\nRemember: The computer is only as smart as the person using it and will blindly run your commands. Because of that the computer can do unexpected things and you need to account for that.\n\n\n\n\nIn the example above I use markdown to not only document my code but also add some comments whenever I might need it. For example here, I added some notes about what the results from FastQC actually told me.\nThis kind of documentation can be useful for:\n\nJustifying decisions further down the line. In this example, I might decide on how to clean the sequencing data. For example, if I would have found a lot of low quality reads or the adapter still being part of the sequence then I would have specifically cleaned my sequences to deal with that\nFuture you. If you read the report a month, or a year, later you have the key information in your report and don’t have to open the HTML reports again.\n\n\n\n\n\nThis part might make more sense after you have worked through the part of the tutorial about using an HPC. But what you see here is how I have written down code that simply says that I submitted a script to an HPC but it does not actually say how I ran the FastQC software. The actual code is “hidden” inside of the run_fastqc.sh script. This also means that a person reading your workflow does not have the code right away. You can deal with this in two ways.\n\nInstead of the sbatch command, you can add the actual line of code that was run on the compute node.\nWhen publishing your code with your manuscript, add the whole scripts folder to where you publish the main code, i.e. on github or zenodo\n\nI tend to prefer number 2 because I like to record the code in my notebooks exactly as I ran them but you can do it differently as long as all the code you ran is recorded and accessible to others once you publish your data.",
    "crumbs": [
      "Welcome page",
      "Documenting your code"
    ]
  },
  {
    "objectID": "source/cli/slurm_basics.html",
    "href": "source/cli/slurm_basics.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Next, let’s go over some basic commands that are useful when working on an HPC.\nGenerally, you schedule jobs on an HPC for them to run effectively. The scheduler used on Crunchomics is called slurm and allows you to:\n\nallocate resources on compute nodes\nhelps you start, run, and keep track of work on the assigned node\nhandles conflicts for resources by keeping track of a waiting list for pending work\n\n\n\n\n\n\n\nImportant\n\n\n\nSince you share resources with other people when using an HPC:\nChoose the resources you use wisely and allow other users to also run some work. For big projects on the Uva Crunchomics HPC you are encouraged to contact Wim de Leeuw w.c.deleeuw@uva.nl beforehand.\n\n\n\n\n\nSSH (Secure Shell) is a network protocol that enables secure remote connections between two systems. The general ssh command that you can use to login into any HPC looks as follows:\n\nssh -X username@server\n\nOptions:\n\n-X option enables untrusted X11 forwarding in SSH. Untrusted means = your local client sends a command to the remote machine and receives the graphical output. Put simply this option enables us to run graphical applications on a remote server and this for example allows us to view a pdf.\n\nIf you have access to and want to connect to Crunchomics you would edit the command above to look like this:\n\nssh -X uvanetid@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to log into Crunchomics from outside of UvA you need to be connected to the VPN. If you have not set that up and/or have trouble doing so, please contact ICT.\n\n\n\n\n\nIf you have access to Crunchomics and have not used Crunchomics before, then you want to first run a small Bash script that:\n\nAllows you to use system-wide installed software, by adding /zfs/omics/software/bin to your path. This basically means that bash knows that there is an extra folder in which to look for any software that is already installed\nSets up a python3 environment and some useful python packages\nHave a link for your 500 GB personal directory in your home directory\n\nTo set this up, run the following command in the cli:\n\n/zfs/omics/software/script/omics_install_script\n\n\n\n\n\n\nLet’s start by getting some basic information about what nodes are avaiable on a cluster by typing the following command into the cli:\n\nsinfo\n\nWe will see something like this:\n\n\n\n\n\nHere, you see information about the:\n\npartition: the queues that are available\nstate: if a node is busy or not\n\nmix : consumable resources partially allocated\nidle : available to requests consumable resources\ndrain : unavailable for use per system administrator request\nalloc : consumable resources fully allocated\ndown : unavailable for use.\n\nNodes: The number of nodes\nNodeList: the names of the nodes omics-cn001 to omics-cn005\n\n\n\n\nThe following commands gives us some information about how busy the HPC is:\n\nsqueue\n\nAfter running this, you can see all jobs scheduled on the HPC:\n\n\n\n\n\n\nJOBID: every job gets a number and you can manipulate jobs via this number\nST: Job state codes that describe the current state of the job. The full list of abbreviations can be found here\n\n\n\n\nIf you are unsure about how many resources a compute node has available you can get more information using the following command:\n\nscontrol show node\n\n\n\n\n\n\nThis is a lot of information but here are some useful things to look at:\n\nNodeName=omics-cn005: This is the name of the node you’re inspecting. Each node has a unique name for identification.\nCPUAlloc=4 CPUTot=64 CPULoad=1.00:\n\nCPUAlloc=4: Currently, 4 CPUs are allocated or actively being used.\nCPUTot=64: The total number of CPUs available on this node is 64.\nCPULoad=1.00: The CPU load gives you an idea of how busy the CPUs are. A load of 1.00 means they’re fully utilized.\n\nRealMemory=514900 AllocMem=20480 FreeMem=510438:\n\nRealMemory=514900: The total physical memory (RAM) on this node is 514900 megabytes or 512 Gb.\nAllocMem=20480: Currently, 20 gigabytes of memory are allocated or in use.\nFreeMem=510438: There are 510438 megabytes of free memory available.\n\nAllocTRES=cpu=4,mem=20G:\n\nAllocTRES shows the resources currently allocated. 4 CPUs and 20 gigabytes of memory are currently in use.\n\n\nIf unsure about what resources are available:\n\nCheck CPUTot for the total CPUs on the node\nLook at CPUAlloc for the currently allocated CPUs.\nExamine RealMemory for total available memory and FreeMem for the unused portion.\n\n\n\n\nTo get a better feel about how many resources a job used we can use sacct, which displays accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database.\nIn the command below 419847 is the job ID of a currently running or finished job and -with --format we create a custom report with some useful information:\n\nsacct -j 419847 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,nnodes,ncpus,nodelist\n\n\n\n\n\n\nThis tells us:\n\nIn the example the first row corresponds to the job itself and the other rows are job steps. If you run things in parallel you will get several additional rows and can see how many resources each job used\nHow long the job run (or each job for parallel jobs)\nMaxRSS: the highest amount of memory your job used while it was running in kilobytes. In our example we see the job used 3525948 kilobytes or 3.5 Gigabytes.\n\n\n\n\nHtop gives us an overview about how busy a node is, typing htop will give a screen that looks something like this:\n\n\n\nThe numbers from 1-64 are the 64 CPUs available on the Crunchomics login ndode and the fuller the bar is, the more of a single CPU is currently in use. This is also summarized under tasks. Another important line is the memory listing how much in total of the avail. memory is in use.\nYou exit the htop window by typing q.\n\n\n\ndf monitors the available space on the different file systems. There might be a lot of different files listed but it quickly allows you to look for the folders you have access to and check how much space you have available.\n\n\n\n\n\n\ndu monitors how much space specific folders take up. In the example below we look at all the folders in folder called personal. If your folder is large and has many individual files, give this a bit of time to run.\n\ndu -sh personal/\n\n\n\n\n\n\n\nscp stands for Secure Copy Protocol and allows us to securely copy files and directories between remote hosts. When transferring data the transfer is prepared from the terminal of your local computer and not from the HPCs login node.\nThe basic syntax is:\nscp [options] SOURCE DESTINATION\nLet’s assume we have a file, file.txt, on our personal computer and want to move it to an HPC. To be able to do this, we run our commands from the terminal of the local machine, not on the HPC itself:\n\nscp file.txt username@server:/path_to_target_folder\n\nFor the case of us having a text file on the server and want to move to our personal computer we can do this as follows. Again, we run this command from a terminal on your computer, not while being logged in the HPC:\n\nscp username@server:/path_to_target_folder/file.txt Desktop\n\nThere are graphical interace filetransfer systems that can make the job easier, i.e. FileZilla, that you can check up on.\n\n\n\nWhile scp is handy for transferring files, rsync takes it a step further by efficiently synchronizing files and directories. This is particularly useful for large datasets (imagine having to move several terrabytes) and for updating files without transferring the entire content.\nThis can be useful for transferring large files from an HPC to your computer but also when transferring large files from one to another location on the same machine.\nTo copy a file from your local machine to an HPC using rsync, use the following command:\n\nrsync -avz file.txt username@server:/path_to_target_folder\n\nUsed options:\n\n-a: Archive mode, preserving permissions and other attributes.\n-v: Verbose, showing the files being copied.\n-z: Compress data during transfer, reducing the amount of data sent over the network.\n\nConversely, to copy a file from the server to your local machine, execute the following command:\n\nrsync -avz username@server:/path_to_source_folder/file.txt Desktop\n\nFor more examples, check out this page.\n\n\n\n\n\n\nsrun is used when you want to run tasks interactively or have more control over the execution. You directly issue srun commands in the terminal, specifying the tasks to be executed and their resource requirements.\nUse srun when:\n\nYou want to run tasks interactively and need immediate feedback.\nYou are testing or debugging your commands before incorporating them into a script.\nYou need more control over the execution of tasks.\n\nA simple example:\n\nsrun echo \"Hello interactively\"\n\nAssuming you want to run a more complex interactive task with srun it is good to specify the resources your job needs by adding flags, i.e.\n\nsrun --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=1G echo \"Hello interactively\"\n\n\n--nodes=1: Specifies the number of nodes. In this case, it’s set to 1 and tells slurm that we want to use a full node. Only use this if you make use of all resources on that node, otherwise omit.\n--ntasks=1: Defines the number of tasks to run. Here, it’s set to 1\n--cpus-per-task=1: Specifies the number of CPUs per task. Adjust this based on the computational requirements of your task.\n--mem=1G: Sets the memory requirement for the task. Modify this based on your task’s memory needs.\necho \"Hello interactively: The actual command you want to run interactively.\n\n\n\n\nWhen you’re just starting, deciding on the right resources to request for your computational job can be a bit challenging. The resource requirements can vary significantly based on the specific tool or workflow you are using. Here are some general guidelines to help you make informed choices:\n\nUse default settings: Begin by working with the default settings provided by the HPC cluster. These are often set to provide a balanced resource allocation for a wide range of tasks\nCheck the software documentation: Consult the documentation of the software or tool you are using. Many tools provide recommendations for resource allocation based on the nature of the computation.\nTesting with Small Datasets: For initial testing and debugging, consider working with a smaller subset of your dataset. This allows for faster job turnaround times, helping you identify and resolve issues more efficiently.\nMonitor the resources usage:\n\nUse sacct to check what resources a finished job has used. Look for columns like MaxRSS (maximum resident set size) to check if the amount of memory allocated (–mem) was appropriate.\nFor instance, with --cpus-per-task=4 --mem=4G, you would expect to use a total of 16 GB of memory (4 CPUs * 4 GB). Verify this with sacct to ensure your job’s resource requirements align with its actual usage.\n\nFine-Tuning Resource Requests: If you encounter performance issues or your jobs are not completing successfully, consider iteratively adjusting resource requests. This might involve increasing or decreasing the number of CPUs, memory allocation, or other relevant parameters.\n\n\n\n\nsbatch is your go-to command when you have a script (a batch script) that needs to be executed without direct user interaction.\nUse sbatch when:\n\nYou have long-running or resource-intensive tasks.\nYou want to submit jobs that can run independently without your immediate supervision\nYou want to submit multiple jobs at once\n\nTo run a job script, you:\n\ncreate a script that contains all the commands and configurations needed for your job\nuse sbatch to submit this script to the Slurm scheduler, and it takes care of the rest.\n\nTo get started, assume we have created a script named first_batch_script.sh with the following content in which we want to run two commands after each other:\n\n#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=2G\n\n# Job Step 1\necho \"Hello from your extended batch script\"\n\n# Job Step 2\necho \"Today we are using slurm\"\n\nThe we can submit first_batch_script.sh with:\n\nsbatch first_batch_script.sh\n\nAfter running this, you will see that a new file is generated that will look something like this slurm-425707.out. When you submit a batch job using sbatch, Slurm redirects the standard output and standard error streams of your job to a file named in the format slurm-JOBID.out, where JOBID is the unique identifier assigned to your job.\nThis file is useful as it:\n\nCaptures the output of our batch scripts and stores them in a file\nCan be used for debugging, since if something goes wrong with your job, examining the contents of this file can provide valuable insights into the issue. Error messages, warnings, or unexpected outputs are often recorded here.\n\n\n\n\nIn bioinformatics its very common to run the same command on several input files. Instead of copy pasting the same command several times, one option is to write the command once and use a slurm array to run the command in parallel on multiple files.\nAssume we have three samples and for each sample we have generated a file with sequencing data:\n\nsample1.fastq.gz\nsample2.fastq.gz\nsample3.fastq.gz\n\nWe might want to do some cleaning of the reads, for example remove adaptors from all three files. We start with making a file that list all samples we want to work with (i.e. using nano or cp from excel) called sample_list which might look like this:\nsample1\nsample2\nsample3\nWe then can write a sbatch script, i.e. arrays.sh, as follows:\n\n#!/bin/bash\n\n#SBATCH --job-name=our_job\n#SBATCH --output=%A_%a.out\n#SBATCH --error=%A_%a.err\n#SBATCH --array=1-3%2\n#SBATCH --cpus-per-task=20\n#SBATCH --mem-per-cpu=5G\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=user@uva.nl\n\n#calculate the index of the current job within the batch\nINDEX=$((SLURM_ARRAY_TASK_ID ))\n\n#build array structure via ale file names\nCURRENT_SAMPLE=$(cat sample_list | sort -V  | sed -n \"${INDEX}p\")\n\necho \"Now Job${INDEX} runs on ${CURRENT_SAMPLE}\"\n\nporechop --input ${CURRENT_SAMPLE}.fastq.gz \\\n  --output outputfolder/${CURRENT_SAMPLE}_filtered.fastq.gz \\\n  --threads 20 \\\n  --discard_middle\n\nThe first section contains parameters for Slurm, the job scheduler.\n\n--job-name: Specifies the name of the job.\n--output and --error: Define the naming pattern for the standard output and error files. %A represents the job ID assigned by Slurm, and %a represents the array task ID, which is the index of the current job within the array. After running the job, you will get files that might be named 123456_1.out or 123456_1.err, where 123456 is the job ID and 1 is the array task ID.\n--array: Sets up a job array, specifying the range (1-3) and stride (%2) for the array. It says that we want to run 3 jobs and two jobs should be run at the same time\n--cpus-per-task: Indicates the number of CPU cores each task in the array will use\n--mem-per-cpu: Specifies the memory requirement per CPU core.\n--mail-type and --mail-user: Configure email notifications for job completion or failure.\n\nThe job does the following:\n\nThe INDEX variable is calculated using the SLURM_ARRAY_TASK_ID, representing the index of the current job within the array. In our case this will be first 1, then 2 and finally 3.\nNext, we build the array structure. The CURRENT_SAMPLE variable is created by reading the sample_list file, sorting it using sort -V (version sorting), and extracting the sample at the calculated index using sed. So for the first index we extract sample1 and store it in the variable CURRENT_SAMPLE.\nThe porechop command is executed using the CURRENT_SAMPLE variable, processing the corresponding FASTQ file. So for the first array, it will trim adapters from sample1.fastq.gz",
    "crumbs": [
      "Getting Started",
      "Start working on an HPC"
    ]
  },
  {
    "objectID": "source/cli/slurm_basics.html#getting-started",
    "href": "source/cli/slurm_basics.html#getting-started",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Next, let’s go over some basic commands that are useful when working on an HPC.\nGenerally, you schedule jobs on an HPC for them to run effectively. The scheduler used on Crunchomics is called slurm and allows you to:\n\nallocate resources on compute nodes\nhelps you start, run, and keep track of work on the assigned node\nhandles conflicts for resources by keeping track of a waiting list for pending work\n\n\n\n\n\n\n\nImportant\n\n\n\nSince you share resources with other people when using an HPC:\nChoose the resources you use wisely and allow other users to also run some work. For big projects on the Uva Crunchomics HPC you are encouraged to contact Wim de Leeuw w.c.deleeuw@uva.nl beforehand.",
    "crumbs": [
      "Getting Started",
      "Start working on an HPC"
    ]
  },
  {
    "objectID": "source/cli/slurm_basics.html#connecting-to-a-sever",
    "href": "source/cli/slurm_basics.html#connecting-to-a-sever",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "SSH (Secure Shell) is a network protocol that enables secure remote connections between two systems. The general ssh command that you can use to login into any HPC looks as follows:\n\nssh -X username@server\n\nOptions:\n\n-X option enables untrusted X11 forwarding in SSH. Untrusted means = your local client sends a command to the remote machine and receives the graphical output. Put simply this option enables us to run graphical applications on a remote server and this for example allows us to view a pdf.\n\nIf you have access to and want to connect to Crunchomics you would edit the command above to look like this:\n\nssh -X uvanetid@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to log into Crunchomics from outside of UvA you need to be connected to the VPN. If you have not set that up and/or have trouble doing so, please contact ICT.",
    "crumbs": [
      "Getting Started",
      "Start working on an HPC"
    ]
  },
  {
    "objectID": "source/cli/slurm_basics.html#setup-for-first-time-crunchomics-users",
    "href": "source/cli/slurm_basics.html#setup-for-first-time-crunchomics-users",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "If you have access to Crunchomics and have not used Crunchomics before, then you want to first run a small Bash script that:\n\nAllows you to use system-wide installed software, by adding /zfs/omics/software/bin to your path. This basically means that bash knows that there is an extra folder in which to look for any software that is already installed\nSets up a python3 environment and some useful python packages\nHave a link for your 500 GB personal directory in your home directory\n\nTo set this up, run the following command in the cli:\n\n/zfs/omics/software/script/omics_install_script",
    "crumbs": [
      "Getting Started",
      "Start working on an HPC"
    ]
  },
  {
    "objectID": "source/cli/slurm_basics.html#slurm-basics",
    "href": "source/cli/slurm_basics.html#slurm-basics",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Let’s start by getting some basic information about what nodes are avaiable on a cluster by typing the following command into the cli:\n\nsinfo\n\nWe will see something like this:\n\n\n\n\n\nHere, you see information about the:\n\npartition: the queues that are available\nstate: if a node is busy or not\n\nmix : consumable resources partially allocated\nidle : available to requests consumable resources\ndrain : unavailable for use per system administrator request\nalloc : consumable resources fully allocated\ndown : unavailable for use.\n\nNodes: The number of nodes\nNodeList: the names of the nodes omics-cn001 to omics-cn005\n\n\n\n\nThe following commands gives us some information about how busy the HPC is:\n\nsqueue\n\nAfter running this, you can see all jobs scheduled on the HPC:\n\n\n\n\n\n\nJOBID: every job gets a number and you can manipulate jobs via this number\nST: Job state codes that describe the current state of the job. The full list of abbreviations can be found here\n\n\n\n\nIf you are unsure about how many resources a compute node has available you can get more information using the following command:\n\nscontrol show node\n\n\n\n\n\n\nThis is a lot of information but here are some useful things to look at:\n\nNodeName=omics-cn005: This is the name of the node you’re inspecting. Each node has a unique name for identification.\nCPUAlloc=4 CPUTot=64 CPULoad=1.00:\n\nCPUAlloc=4: Currently, 4 CPUs are allocated or actively being used.\nCPUTot=64: The total number of CPUs available on this node is 64.\nCPULoad=1.00: The CPU load gives you an idea of how busy the CPUs are. A load of 1.00 means they’re fully utilized.\n\nRealMemory=514900 AllocMem=20480 FreeMem=510438:\n\nRealMemory=514900: The total physical memory (RAM) on this node is 514900 megabytes or 512 Gb.\nAllocMem=20480: Currently, 20 gigabytes of memory are allocated or in use.\nFreeMem=510438: There are 510438 megabytes of free memory available.\n\nAllocTRES=cpu=4,mem=20G:\n\nAllocTRES shows the resources currently allocated. 4 CPUs and 20 gigabytes of memory are currently in use.\n\n\nIf unsure about what resources are available:\n\nCheck CPUTot for the total CPUs on the node\nLook at CPUAlloc for the currently allocated CPUs.\nExamine RealMemory for total available memory and FreeMem for the unused portion.\n\n\n\n\nTo get a better feel about how many resources a job used we can use sacct, which displays accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database.\nIn the command below 419847 is the job ID of a currently running or finished job and -with --format we create a custom report with some useful information:\n\nsacct -j 419847 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,nnodes,ncpus,nodelist\n\n\n\n\n\n\nThis tells us:\n\nIn the example the first row corresponds to the job itself and the other rows are job steps. If you run things in parallel you will get several additional rows and can see how many resources each job used\nHow long the job run (or each job for parallel jobs)\nMaxRSS: the highest amount of memory your job used while it was running in kilobytes. In our example we see the job used 3525948 kilobytes or 3.5 Gigabytes.\n\n\n\n\nHtop gives us an overview about how busy a node is, typing htop will give a screen that looks something like this:\n\n\n\nThe numbers from 1-64 are the 64 CPUs available on the Crunchomics login ndode and the fuller the bar is, the more of a single CPU is currently in use. This is also summarized under tasks. Another important line is the memory listing how much in total of the avail. memory is in use.\nYou exit the htop window by typing q.\n\n\n\ndf monitors the available space on the different file systems. There might be a lot of different files listed but it quickly allows you to look for the folders you have access to and check how much space you have available.\n\n\n\n\n\n\ndu monitors how much space specific folders take up. In the example below we look at all the folders in folder called personal. If your folder is large and has many individual files, give this a bit of time to run.\n\ndu -sh personal/",
    "crumbs": [
      "Getting Started",
      "Start working on an HPC"
    ]
  },
  {
    "objectID": "source/cli/slurm_basics.html#transferring-files",
    "href": "source/cli/slurm_basics.html#transferring-files",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "scp stands for Secure Copy Protocol and allows us to securely copy files and directories between remote hosts. When transferring data the transfer is prepared from the terminal of your local computer and not from the HPCs login node.\nThe basic syntax is:\nscp [options] SOURCE DESTINATION\nLet’s assume we have a file, file.txt, on our personal computer and want to move it to an HPC. To be able to do this, we run our commands from the terminal of the local machine, not on the HPC itself:\n\nscp file.txt username@server:/path_to_target_folder\n\nFor the case of us having a text file on the server and want to move to our personal computer we can do this as follows. Again, we run this command from a terminal on your computer, not while being logged in the HPC:\n\nscp username@server:/path_to_target_folder/file.txt Desktop\n\nThere are graphical interace filetransfer systems that can make the job easier, i.e. FileZilla, that you can check up on.\n\n\n\nWhile scp is handy for transferring files, rsync takes it a step further by efficiently synchronizing files and directories. This is particularly useful for large datasets (imagine having to move several terrabytes) and for updating files without transferring the entire content.\nThis can be useful for transferring large files from an HPC to your computer but also when transferring large files from one to another location on the same machine.\nTo copy a file from your local machine to an HPC using rsync, use the following command:\n\nrsync -avz file.txt username@server:/path_to_target_folder\n\nUsed options:\n\n-a: Archive mode, preserving permissions and other attributes.\n-v: Verbose, showing the files being copied.\n-z: Compress data during transfer, reducing the amount of data sent over the network.\n\nConversely, to copy a file from the server to your local machine, execute the following command:\n\nrsync -avz username@server:/path_to_source_folder/file.txt Desktop\n\nFor more examples, check out this page.",
    "crumbs": [
      "Getting Started",
      "Start working on an HPC"
    ]
  },
  {
    "objectID": "source/cli/slurm_basics.html#submitting-a-job",
    "href": "source/cli/slurm_basics.html#submitting-a-job",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "srun is used when you want to run tasks interactively or have more control over the execution. You directly issue srun commands in the terminal, specifying the tasks to be executed and their resource requirements.\nUse srun when:\n\nYou want to run tasks interactively and need immediate feedback.\nYou are testing or debugging your commands before incorporating them into a script.\nYou need more control over the execution of tasks.\n\nA simple example:\n\nsrun echo \"Hello interactively\"\n\nAssuming you want to run a more complex interactive task with srun it is good to specify the resources your job needs by adding flags, i.e.\n\nsrun --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=1G echo \"Hello interactively\"\n\n\n--nodes=1: Specifies the number of nodes. In this case, it’s set to 1 and tells slurm that we want to use a full node. Only use this if you make use of all resources on that node, otherwise omit.\n--ntasks=1: Defines the number of tasks to run. Here, it’s set to 1\n--cpus-per-task=1: Specifies the number of CPUs per task. Adjust this based on the computational requirements of your task.\n--mem=1G: Sets the memory requirement for the task. Modify this based on your task’s memory needs.\necho \"Hello interactively: The actual command you want to run interactively.\n\n\n\n\nWhen you’re just starting, deciding on the right resources to request for your computational job can be a bit challenging. The resource requirements can vary significantly based on the specific tool or workflow you are using. Here are some general guidelines to help you make informed choices:\n\nUse default settings: Begin by working with the default settings provided by the HPC cluster. These are often set to provide a balanced resource allocation for a wide range of tasks\nCheck the software documentation: Consult the documentation of the software or tool you are using. Many tools provide recommendations for resource allocation based on the nature of the computation.\nTesting with Small Datasets: For initial testing and debugging, consider working with a smaller subset of your dataset. This allows for faster job turnaround times, helping you identify and resolve issues more efficiently.\nMonitor the resources usage:\n\nUse sacct to check what resources a finished job has used. Look for columns like MaxRSS (maximum resident set size) to check if the amount of memory allocated (–mem) was appropriate.\nFor instance, with --cpus-per-task=4 --mem=4G, you would expect to use a total of 16 GB of memory (4 CPUs * 4 GB). Verify this with sacct to ensure your job’s resource requirements align with its actual usage.\n\nFine-Tuning Resource Requests: If you encounter performance issues or your jobs are not completing successfully, consider iteratively adjusting resource requests. This might involve increasing or decreasing the number of CPUs, memory allocation, or other relevant parameters.\n\n\n\n\nsbatch is your go-to command when you have a script (a batch script) that needs to be executed without direct user interaction.\nUse sbatch when:\n\nYou have long-running or resource-intensive tasks.\nYou want to submit jobs that can run independently without your immediate supervision\nYou want to submit multiple jobs at once\n\nTo run a job script, you:\n\ncreate a script that contains all the commands and configurations needed for your job\nuse sbatch to submit this script to the Slurm scheduler, and it takes care of the rest.\n\nTo get started, assume we have created a script named first_batch_script.sh with the following content in which we want to run two commands after each other:\n\n#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=2G\n\n# Job Step 1\necho \"Hello from your extended batch script\"\n\n# Job Step 2\necho \"Today we are using slurm\"\n\nThe we can submit first_batch_script.sh with:\n\nsbatch first_batch_script.sh\n\nAfter running this, you will see that a new file is generated that will look something like this slurm-425707.out. When you submit a batch job using sbatch, Slurm redirects the standard output and standard error streams of your job to a file named in the format slurm-JOBID.out, where JOBID is the unique identifier assigned to your job.\nThis file is useful as it:\n\nCaptures the output of our batch scripts and stores them in a file\nCan be used for debugging, since if something goes wrong with your job, examining the contents of this file can provide valuable insights into the issue. Error messages, warnings, or unexpected outputs are often recorded here.\n\n\n\n\nIn bioinformatics its very common to run the same command on several input files. Instead of copy pasting the same command several times, one option is to write the command once and use a slurm array to run the command in parallel on multiple files.\nAssume we have three samples and for each sample we have generated a file with sequencing data:\n\nsample1.fastq.gz\nsample2.fastq.gz\nsample3.fastq.gz\n\nWe might want to do some cleaning of the reads, for example remove adaptors from all three files. We start with making a file that list all samples we want to work with (i.e. using nano or cp from excel) called sample_list which might look like this:\nsample1\nsample2\nsample3\nWe then can write a sbatch script, i.e. arrays.sh, as follows:\n\n#!/bin/bash\n\n#SBATCH --job-name=our_job\n#SBATCH --output=%A_%a.out\n#SBATCH --error=%A_%a.err\n#SBATCH --array=1-3%2\n#SBATCH --cpus-per-task=20\n#SBATCH --mem-per-cpu=5G\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=user@uva.nl\n\n#calculate the index of the current job within the batch\nINDEX=$((SLURM_ARRAY_TASK_ID ))\n\n#build array structure via ale file names\nCURRENT_SAMPLE=$(cat sample_list | sort -V  | sed -n \"${INDEX}p\")\n\necho \"Now Job${INDEX} runs on ${CURRENT_SAMPLE}\"\n\nporechop --input ${CURRENT_SAMPLE}.fastq.gz \\\n  --output outputfolder/${CURRENT_SAMPLE}_filtered.fastq.gz \\\n  --threads 20 \\\n  --discard_middle\n\nThe first section contains parameters for Slurm, the job scheduler.\n\n--job-name: Specifies the name of the job.\n--output and --error: Define the naming pattern for the standard output and error files. %A represents the job ID assigned by Slurm, and %a represents the array task ID, which is the index of the current job within the array. After running the job, you will get files that might be named 123456_1.out or 123456_1.err, where 123456 is the job ID and 1 is the array task ID.\n--array: Sets up a job array, specifying the range (1-3) and stride (%2) for the array. It says that we want to run 3 jobs and two jobs should be run at the same time\n--cpus-per-task: Indicates the number of CPU cores each task in the array will use\n--mem-per-cpu: Specifies the memory requirement per CPU core.\n--mail-type and --mail-user: Configure email notifications for job completion or failure.\n\nThe job does the following:\n\nThe INDEX variable is calculated using the SLURM_ARRAY_TASK_ID, representing the index of the current job within the array. In our case this will be first 1, then 2 and finally 3.\nNext, we build the array structure. The CURRENT_SAMPLE variable is created by reading the sample_list file, sorting it using sort -V (version sorting), and extracting the sample at the calculated index using sed. So for the first index we extract sample1 and store it in the variable CURRENT_SAMPLE.\nThe porechop command is executed using the CURRENT_SAMPLE variable, processing the corresponding FASTQ file. So for the first array, it will trim adapters from sample1.fastq.gz",
    "crumbs": [
      "Getting Started",
      "Start working on an HPC"
    ]
  },
  {
    "objectID": "source/metagenomics/fama_readme.html",
    "href": "source/metagenomics/fama_readme.html",
    "title": "FAMA",
    "section": "",
    "text": "FAMA\n\nIntroduction\nFama is a fast pipeline for functional and taxonomic analysis of shotgun metagenomic sequences.\n\n\nInstallation\nIf you want to install FAMA on your own, follow these instructions in the FAMA manual.\nAvailable on Crunchomics: Yes, via the metatools share. For access contact Anna Heintz Buschart a.u.s.heintzbuschart@uva.nl\n\n\nInstructions\n\n#activate environment\nconda activate /zfs/omics/projects/metatools/TOOLS/miniconda3/envs/FAMA\n\n#get template and add \n#(if you downloaded FAMA by yourself then the ini file can be found in the folder you downloaded)\ncp /zfs/omics/projects/metatools/DB/fama_new/project.ini.sample Annotations/Fama/my.project.config\n\n#run FAMA \npython3 /zfs/omics/projects/metatools/DB/fama_new/py/fama.py -c /zfs/omics/projects/metatools/DB/fama_new/config.ini -p my.project.config\n\nExample content of Annotations/Fama/my.project.config:\n[DEFAULT]\nproject_name = 'Test sample sulfur'\ncollection = test_sulfur_v1\nref_output_name = ref_tabular_output.txt\nbackground_output_name = bgr_tabular_output.txt\nref_hits_list_name = ref_hits.txt\nref_hits_fastq_name = ref_hits.fq\nreads_fastq_name = reads.fq\npe_reads_fastq_name = reads_pe.fq\noutput_subdir = out_sulfur\nreport_name = report.txt\nxml_name = krona.xml\nhtml_name = functional_profile.html\nreads_json_name = reads.json\nassembly_subdir = assembly\nwork_dir = /zfs/omics/projects/thiopac-mgx/nina_wdir/Annotations/Fama\n\n[test_sample]\nsample_id = TPS_fame_sulfur\nfastq_pe1 = /zfs/omics/projects/thiopac-mgx/nina_wdir/faa/All_genomes.faa\nsample_dir = /zfs/omics/projects/thiopac-mgx/nina_wdir/Annotations/Fama\nreplicate = 0\n#fastq_pe1_readcount = 30\n#fastq_pe1_basecount = 4509\n#fastq_pe2_readcount = 30\n#fastq_pe2_basecount = 4346\n#rpkg_scaling = 0.18962061540779365\n#insert_size = 233.4950097660472\n\n\nAdding more databases to FAMA\nIt is possible to add new databases to FAMA. Below is an example on how to add a custom sulfur database.\nSulfur database installation instructions:\n\nDownload and unpack the archive into a separate directory:https://iseq.lbl.gov/mydocs/fama_downloads/fama_sulfur_dataset.tar.gz\nCreate diamond databases:\n\n\ndiamond makedb --in classification_database.faa --db classification_database\ndiamond makedb --in selection_database_clustered.faa --db selection_database\n\n\nAppend your config.ini file with a new section and replace “” with real paths:\n\n\n[test_sulfur_v1]\nfunctions_file = &lt;path to db files&gt;/collection_functions.txt\ntaxonomy_file = &lt;path to db files&gt;/collection_taxonomy.txt\nreference_diamond_db = &lt;path to db files&gt;/selection_database.dmnd\nbackground_diamond_db = &lt;path to db files&gt;/classification_database.dmnd\nreference_db_size = 6990766\nbackground_db_size = 99120132",
    "crumbs": [
      "Welcome page",
      "Metagenomics",
      "FAMA"
    ]
  },
  {
    "objectID": "source/metagenomics/interproscan_readme.html",
    "href": "source/metagenomics/interproscan_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "InterPro is a database which integrates together predictive information about proteins’ function from a number of partner resources, giving an overview of the families that a protein belongs to and the domains and sites it contains (Blum et al. 2021).\nUsers who have novel nucleotide or protein sequences that they wish to functionally characterise can use the software package InterProScan to run the scanning algorithms from the InterPro database in an integrated way. Sequences are submitted in FASTA format. Matches are then calculated against all of the required member database’s signatures and the results are then output in a variety of formats (Jones et al. 2014).\n\n\n\n\n\nNotice: The newest version does NOT run on crunchomics unless you do some extra steps during the isntallation since we need a newer java version and we need to update a tool used to analyse Prosite specific databases.\nThese changes require to move a few things around, so if you feel uncomfortable with this feel free to install an older version (for installation, see section #### Version for Crunchomics below) that works well with the default java version installed on crunchomics.\nIf you want to work with the newest version and are not afraid of some extra steps do the following:\n\n#go into a folder in which you have all your software installed\ncd software_dir\n\n#make a new folder for the interproscan installation and go into the folder\nmkdir interproscan\ncd interproscan \n\n#download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum -c interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n#decompress the downloaded folder\ntar -pxvzf interproscan-5.64-96.0-*-bit.tar.gz\n\n#index hmm models\ncd interproscan-5.64-96.0\npython3 setup.py -f interproscan.properties\n\n#setup a conda environment with the newest java version\nconda deactivate\nmamba create --name java_f_iprscan -c conda-forge openjdk\n\n#activate teh environment and install some more dependencies\nconda activate java_f_iprscan\nmamba install -c conda-forge gfortran\n\n#test error outside of iprscan\n#bin/prosite/pfsearchV3\n\n#get new pftools version via conda\nmamba install -c bioconda pftools\n\n#remove old pftools \ncd bin/prosite\nrm pfscanV3\nrm pfsearchV3\n\n#replace with new pftools we have installed (and which are found in the mamba env folder)\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscan .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscanV3 .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearch .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearchV3 .\ncd ..\n\n\n#do testrun (ProSiteProfiles and ProSitePatterns not working yet)\nsrun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv -dp --appl TIGRFAM,SFLD,SUPERFAMILY,PANTHER,GENE3D,Hamap,Coils,SMART,CDD,PRINTS,PIRSR,AntiFam,Pfam,MobiDBLite,PIRSF,ProSiteProfiles \n\nsrun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv\nconda deactivate\n\n\n\n\nOn Crunchomics, newer versions of interproscan do not run due to an incompatibility with the installed Java version. To get it running without complicated environmental setups, you can install an older version:\n\n#download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n#decompress the folder\ntar -pxvzf interproscan-5.36-75.0-64-bit.tar.gz\n\n#index hmm models\ncd interproscan-5.36-75.0\n\n#do a test run to check the installation\nsrun -n1 --cpus-per-task 8 --mem=8G ./interproscan.sh -i test_proteins.fasta\n\n\n\n\n\nRequired inputs:\n\nProtein fasta file\n\nGenerated outputs:\n\nTSV: a simple tab-delimited file format\nXML: the new “IMPACT” XML format\nGFF3: The GFF 3.0 format\nJSON\nSVG\nHTML\n\nNotice:\n\nInterproscan does not like * symbols inside the protein sequence. Some tools for protein calling, like prokka, use * add the end of a protein to indicate that the full protein was found. If your files have such symbols, use the code below to remove it first. Beware: using sed -i overwrites the content of your file. If that behaviour is not wanted use sed 's/*//g' Proteins_of_interest.faa &gt; Proteins_of_interest_new.faa instead.\nIf you are on Crunchomics (or most other servers): DO NOT run jobs on the head node, but add something like srun -n 1 --cpus-per-task 4 before the actual command\n\nExample code:\n\n#clean faa file \n#remove `*` as interproscan does not like that symbol\nsed -i 's/*//g' Proteins_of_interest.faa\n\n#run interproscan\n&lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --cpu 4 -i Proteins_of_interest.faa -d outputfolder -T outputfolder/temp --iprlookup --goterms\n\nTo check available options use &lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --help or for more detailed information, see the documentation):",
    "crumbs": [
      "Welcome page",
      "Metagenomics",
      "Interproscan"
    ]
  },
  {
    "objectID": "source/metagenomics/interproscan_readme.html#sec-interproscan",
    "href": "source/metagenomics/interproscan_readme.html#sec-interproscan",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "InterPro is a database which integrates together predictive information about proteins’ function from a number of partner resources, giving an overview of the families that a protein belongs to and the domains and sites it contains (Blum et al. 2021).\nUsers who have novel nucleotide or protein sequences that they wish to functionally characterise can use the software package InterProScan to run the scanning algorithms from the InterPro database in an integrated way. Sequences are submitted in FASTA format. Matches are then calculated against all of the required member database’s signatures and the results are then output in a variety of formats (Jones et al. 2014).\n\n\n\n\n\nNotice: The newest version does NOT run on crunchomics unless you do some extra steps during the isntallation since we need a newer java version and we need to update a tool used to analyse Prosite specific databases.\nThese changes require to move a few things around, so if you feel uncomfortable with this feel free to install an older version (for installation, see section #### Version for Crunchomics below) that works well with the default java version installed on crunchomics.\nIf you want to work with the newest version and are not afraid of some extra steps do the following:\n\n#go into a folder in which you have all your software installed\ncd software_dir\n\n#make a new folder for the interproscan installation and go into the folder\nmkdir interproscan\ncd interproscan \n\n#download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum -c interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n#decompress the downloaded folder\ntar -pxvzf interproscan-5.64-96.0-*-bit.tar.gz\n\n#index hmm models\ncd interproscan-5.64-96.0\npython3 setup.py -f interproscan.properties\n\n#setup a conda environment with the newest java version\nconda deactivate\nmamba create --name java_f_iprscan -c conda-forge openjdk\n\n#activate teh environment and install some more dependencies\nconda activate java_f_iprscan\nmamba install -c conda-forge gfortran\n\n#test error outside of iprscan\n#bin/prosite/pfsearchV3\n\n#get new pftools version via conda\nmamba install -c bioconda pftools\n\n#remove old pftools \ncd bin/prosite\nrm pfscanV3\nrm pfsearchV3\n\n#replace with new pftools we have installed (and which are found in the mamba env folder)\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscan .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscanV3 .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearch .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearchV3 .\ncd ..\n\n\n#do testrun (ProSiteProfiles and ProSitePatterns not working yet)\nsrun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv -dp --appl TIGRFAM,SFLD,SUPERFAMILY,PANTHER,GENE3D,Hamap,Coils,SMART,CDD,PRINTS,PIRSR,AntiFam,Pfam,MobiDBLite,PIRSF,ProSiteProfiles \n\nsrun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv\nconda deactivate\n\n\n\n\nOn Crunchomics, newer versions of interproscan do not run due to an incompatibility with the installed Java version. To get it running without complicated environmental setups, you can install an older version:\n\n#download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n#decompress the folder\ntar -pxvzf interproscan-5.36-75.0-64-bit.tar.gz\n\n#index hmm models\ncd interproscan-5.36-75.0\n\n#do a test run to check the installation\nsrun -n1 --cpus-per-task 8 --mem=8G ./interproscan.sh -i test_proteins.fasta\n\n\n\n\n\nRequired inputs:\n\nProtein fasta file\n\nGenerated outputs:\n\nTSV: a simple tab-delimited file format\nXML: the new “IMPACT” XML format\nGFF3: The GFF 3.0 format\nJSON\nSVG\nHTML\n\nNotice:\n\nInterproscan does not like * symbols inside the protein sequence. Some tools for protein calling, like prokka, use * add the end of a protein to indicate that the full protein was found. If your files have such symbols, use the code below to remove it first. Beware: using sed -i overwrites the content of your file. If that behaviour is not wanted use sed 's/*//g' Proteins_of_interest.faa &gt; Proteins_of_interest_new.faa instead.\nIf you are on Crunchomics (or most other servers): DO NOT run jobs on the head node, but add something like srun -n 1 --cpus-per-task 4 before the actual command\n\nExample code:\n\n#clean faa file \n#remove `*` as interproscan does not like that symbol\nsed -i 's/*//g' Proteins_of_interest.faa\n\n#run interproscan\n&lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --cpu 4 -i Proteins_of_interest.faa -d outputfolder -T outputfolder/temp --iprlookup --goterms\n\nTo check available options use &lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --help or for more detailed information, see the documentation):",
    "crumbs": [
      "Welcome page",
      "Metagenomics",
      "Interproscan"
    ]
  },
  {
    "objectID": "source/metagenomics/readme.html",
    "href": "source/metagenomics/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Metagenomics is the study of genetic material recovered directly from environmental or other samples using sequencing technologies. Below you find some tutorials explaining how to work with sequencing data, assemble metagenome-assembled genomes (MAGs) from this sequencing data and how to analyse these MAGS. Notice: These tutorials where not developed at UvA and will be updated in the future but will nevertheless give a good starting point in these kind of analyses.\n\n\n\nAssembling a metagenome\nGenomic binning\nAnnotating microbial genomes\nHow to do phylogenetic analyses",
    "crumbs": [
      "Omics Analyses"
    ]
  },
  {
    "objectID": "source/metagenomics/readme.html#general",
    "href": "source/metagenomics/readme.html#general",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Metagenomics is the study of genetic material recovered directly from environmental or other samples using sequencing technologies. Below you find some tutorials explaining how to work with sequencing data, assemble metagenome-assembled genomes (MAGs) from this sequencing data and how to analyse these MAGS. Notice: These tutorials where not developed at UvA and will be updated in the future but will nevertheless give a good starting point in these kind of analyses.\n\n\n\nAssembling a metagenome\nGenomic binning\nAnnotating microbial genomes\nHow to do phylogenetic analyses",
    "crumbs": [
      "Omics Analyses"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/sortmerna.html",
    "href": "source/metatranscriptomics/sortmerna.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "SortMeRNA is a tool for the fast and accurate filtering of ribosomal RNAs in metatranscriptomic data.\n\nManual\nPaper: Please do not forget to cite the paper whenever you use the software (Kopylova, Noé, and Touzet 2012).\n\nAvailable on Crunchomics: No\n\n\n\nSortMeRNA can be easily installed with conda/mamba:\n\nmamba create -n sortmerna\nmamba install -n sortmerna -c conda-forge -c bioconda -c defaults sortmerna \n\n\n\n\nSortMeRNA has many different options, so its best to also have a look at the manual. Below you find a quick example to try out.\n\nRequired input:\n\nA reference database\nSingle-end or paired-end FastQ files in the following formats: FASTA/FASTQ/FASTA.GZ/FASTQ.GZ\n\nSortmerna generates multiple output folders:\n\nkvdb/ key-value datastore for alignment results\nidx/ index database\nout/ output files: Here you find the fastq (or fastq.gz) files that aligned or not-aligned (depending in the used options) to the reference database.\n\nUseful arguments (not extensive, check manual for all arguments as well as use sortmerna -h as not all options are listed in the manual):\n\n-ref{PATH}: Reference file (FASTA) absolute or relative path. Can be used multiple times, once per a reference file, if working with more than one reference\n-reads {PATH}: Raw reads file. Use twice for files with paired reads\n-wordir {PATH}: Working directory for storing the Reference index, Key-value database, Output. Default location is USRDIR/sortmerna/run/\n-fastx: Output the reads that aligned to the reference database into FASTA/FASTQ file\n-other: Create output file for the non-aligned reads output file. Must be used with fastx\n--paired_out: Flags the paired-end reads as Non-aligned, when either of them is non-aligned\n--out2: Output paired reads into separate files. Must be used with fastx. If a single reads file is provided, this options implies interleaved paired reads\n--sout: Separate paired and singleton aligned reads. Must be used with fastx and Cannot be used with ‘paired_in’ | ‘paired_out’\n…\n\n\n\n\nIn the example below we start with downloading a database provided by sortmerna into a db folder. When running this on your own, ensure that there are no newer releases or other databases that might be of interest to you. For us, this downloads the following databases:\n\nsmr_v4.3_default_db.fasta -&gt; bac-16S 90%, 5S & 5.8S seeds, rest 95% (benchmark accuracy: 99.899%)\nsmr_v4.3_sensitive_db.fasta -&gt; all 97% (benchmark accuracy: 99.907%) and thus the most complete database\nsmr_v4.3_sensitive_db_rfam_seeds.fasta -&gt; all 97%, except RFAM database which includes the full seed database sequences\n\nAfterwards, we run Sortmerna on an imaginary metatranscriptomic sample1 from which we have paired-end sequencing data that we beforehand cleaned with other tools, such as FastP. Since our goal is in this example to remove rRNA reads from metatranscriptomic data, we would continue working with the unaligned files. However, at the same time sortmerna can be used to extract and work further with rRNA reads if desired.\n\n#generate some folder to better structure the data \nmkdir dbs\nmkdir -p sortmerna/sample1\n\n#get rRNA db\nwget https://github.com/biocore/sortmerna/releases/download/v4.3.4/database.tar.gz\ntar -xvf database.tar.gz -C dbs/\nrm database.tar.gz \n\n#run sortmerna \nsortmerna \\\n  --ref dbs/smr_v4.3_sensitive_db.fasta \\\n  --reads data/sample1_forward_filtered.fq.gz \\\n  --reads data/sample1_reverse_filtered.fq.gz \\\n  --fastx --other --out2 --paired_out \\\n  --workdir sortmerna/sample1 \\\n  --threads 20 -v",
    "crumbs": [
      "Welcome page",
      "Metatranscriptomics",
      "SortMeRNA"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/sortmerna.html#sortmerna",
    "href": "source/metatranscriptomics/sortmerna.html#sortmerna",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "SortMeRNA is a tool for the fast and accurate filtering of ribosomal RNAs in metatranscriptomic data.\n\nManual\nPaper: Please do not forget to cite the paper whenever you use the software (Kopylova, Noé, and Touzet 2012).\n\nAvailable on Crunchomics: No\n\n\n\nSortMeRNA can be easily installed with conda/mamba:\n\nmamba create -n sortmerna\nmamba install -n sortmerna -c conda-forge -c bioconda -c defaults sortmerna \n\n\n\n\nSortMeRNA has many different options, so its best to also have a look at the manual. Below you find a quick example to try out.\n\nRequired input:\n\nA reference database\nSingle-end or paired-end FastQ files in the following formats: FASTA/FASTQ/FASTA.GZ/FASTQ.GZ\n\nSortmerna generates multiple output folders:\n\nkvdb/ key-value datastore for alignment results\nidx/ index database\nout/ output files: Here you find the fastq (or fastq.gz) files that aligned or not-aligned (depending in the used options) to the reference database.\n\nUseful arguments (not extensive, check manual for all arguments as well as use sortmerna -h as not all options are listed in the manual):\n\n-ref{PATH}: Reference file (FASTA) absolute or relative path. Can be used multiple times, once per a reference file, if working with more than one reference\n-reads {PATH}: Raw reads file. Use twice for files with paired reads\n-wordir {PATH}: Working directory for storing the Reference index, Key-value database, Output. Default location is USRDIR/sortmerna/run/\n-fastx: Output the reads that aligned to the reference database into FASTA/FASTQ file\n-other: Create output file for the non-aligned reads output file. Must be used with fastx\n--paired_out: Flags the paired-end reads as Non-aligned, when either of them is non-aligned\n--out2: Output paired reads into separate files. Must be used with fastx. If a single reads file is provided, this options implies interleaved paired reads\n--sout: Separate paired and singleton aligned reads. Must be used with fastx and Cannot be used with ‘paired_in’ | ‘paired_out’\n…\n\n\n\n\nIn the example below we start with downloading a database provided by sortmerna into a db folder. When running this on your own, ensure that there are no newer releases or other databases that might be of interest to you. For us, this downloads the following databases:\n\nsmr_v4.3_default_db.fasta -&gt; bac-16S 90%, 5S & 5.8S seeds, rest 95% (benchmark accuracy: 99.899%)\nsmr_v4.3_sensitive_db.fasta -&gt; all 97% (benchmark accuracy: 99.907%) and thus the most complete database\nsmr_v4.3_sensitive_db_rfam_seeds.fasta -&gt; all 97%, except RFAM database which includes the full seed database sequences\n\nAfterwards, we run Sortmerna on an imaginary metatranscriptomic sample1 from which we have paired-end sequencing data that we beforehand cleaned with other tools, such as FastP. Since our goal is in this example to remove rRNA reads from metatranscriptomic data, we would continue working with the unaligned files. However, at the same time sortmerna can be used to extract and work further with rRNA reads if desired.\n\n#generate some folder to better structure the data \nmkdir dbs\nmkdir -p sortmerna/sample1\n\n#get rRNA db\nwget https://github.com/biocore/sortmerna/releases/download/v4.3.4/database.tar.gz\ntar -xvf database.tar.gz -C dbs/\nrm database.tar.gz \n\n#run sortmerna \nsortmerna \\\n  --ref dbs/smr_v4.3_sensitive_db.fasta \\\n  --reads data/sample1_forward_filtered.fq.gz \\\n  --reads data/sample1_reverse_filtered.fq.gz \\\n  --fastx --other --out2 --paired_out \\\n  --workdir sortmerna/sample1 \\\n  --threads 20 -v",
    "crumbs": [
      "Welcome page",
      "Metatranscriptomics",
      "SortMeRNA"
    ]
  },
  {
    "objectID": "source/nanopore/chopper.html",
    "href": "source/nanopore/chopper.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Chopper is a tool for quality filtering of long read data. It is a Rust implementation of two other tools for long-read quality filtering, NanoFilt and NanoLyse, both originally written in Python. This tool, intended for long read sequencing such as PacBio or ONT, filters and trims a fastq file (De Coster and Rademakers 2023).\nAvailable on Crunchomics: Not by default\n\n\n\nIt is easiest to install chopper with conda. If you do not have mamba (or conda) installed. Check out the installation instructions for mamba and conda. Mamba is a newer version of conda and from experience faster but since its newer this might mean that not all software can yet be installed with mamba.\n\nmamba create --name chopper -c bioconda chopper\n\n\n\n\nRequired input:\n\nFASTQ files\n\nOutput:\n\nFASTQ files\n\nExample usage:\n\nchopper -q 10 \\\n        --headcrop 0 \\\n        --tailcrop 0   \\\n        -l 250 \\\n        --maxlength 3000  \\\n        --threads 1 \n\nUseful arguments:\n\n--headcrop Trim N nucleotides from the start of a read [default: 0]\n--maxlength Sets a maximum read length [default: 2147483647]\n-l, --minlength Sets a minimum read length [default: 1]\n-q, --quality Sets a minimum Phred average quality score [default: 0]\n--tailcrop Trim N nucleotides from the end of a read [default: 0]\n--threads Number of parallel threads to use [default: 4]\n--contam Fasta file with reference to check potential contaminants against [default None]}",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "Chopper"
    ]
  },
  {
    "objectID": "source/nanopore/chopper.html#chopper",
    "href": "source/nanopore/chopper.html#chopper",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "Chopper is a tool for quality filtering of long read data. It is a Rust implementation of two other tools for long-read quality filtering, NanoFilt and NanoLyse, both originally written in Python. This tool, intended for long read sequencing such as PacBio or ONT, filters and trims a fastq file (De Coster and Rademakers 2023).\nAvailable on Crunchomics: Not by default\n\n\n\nIt is easiest to install chopper with conda. If you do not have mamba (or conda) installed. Check out the installation instructions for mamba and conda. Mamba is a newer version of conda and from experience faster but since its newer this might mean that not all software can yet be installed with mamba.\n\nmamba create --name chopper -c bioconda chopper\n\n\n\n\nRequired input:\n\nFASTQ files\n\nOutput:\n\nFASTQ files\n\nExample usage:\n\nchopper -q 10 \\\n        --headcrop 0 \\\n        --tailcrop 0   \\\n        -l 250 \\\n        --maxlength 3000  \\\n        --threads 1 \n\nUseful arguments:\n\n--headcrop Trim N nucleotides from the start of a read [default: 0]\n--maxlength Sets a maximum read length [default: 2147483647]\n-l, --minlength Sets a minimum read length [default: 1]\n-q, --quality Sets a minimum Phred average quality score [default: 0]\n--tailcrop Trim N nucleotides from the end of a read [default: 0]\n--threads Number of parallel threads to use [default: 4]\n--contam Fasta file with reference to check potential contaminants against [default None]}",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "Chopper"
    ]
  },
  {
    "objectID": "source/nanopore/nanoclass.html",
    "href": "source/nanopore/nanoclass.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoClass2 build upon NanoClass, a snakemake workflow originally developed by IBED’s former bioinformatician Evelien Jongepier.\nNanoClass2 is a taxonomic meta-classifier for meta-barcoding 16S/18S rRNA gene sequencing data generated with the Oxford Nanopore Technology. With a single command, this Snakemake pipeline installs all programs and databases and will do the following:\nNanoClass2 is a snakemake workflow that will:\n\nRemove adapters with porechop\nPerform quality filtering with chopper\nGenerate read quality plots post quality cleaning with pistis\nSubsample reads to compare different classifiers (can be turned of)\nClassify reads with any one of these tools: blastn, centrifuge, dcmegablast, idtaxa, megablast, minimap, mothur, qiime, rdp, spingo, kraken. By default NanoClass2 will use the SILVA_138.1_SSURef_NR99 database to classify reads. If you are interested in using other bases, check out these instructions.\n\nFor more details, feel free to also have a look at the manual.\n\n\n\n\n\nNanoClass2 is installed on the UvA crunchomics HPC. If you have access to crunchomics you can be added to the amplicomics share in which NanoClass2 is set up. To be added, please send an email with your Uva netID to Nina Dombrowski.\nIf you want or need to set up NanoClass2 by yourself, expand the code below to view an example on how to install the software:\n\n\nShow the code\n#install snakemake as a conda/mamba environment\nmamba create -c conda-forge -c bioconda --prefix /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2 python=3.9.7 snakemake=6.8.0 tabulate=0.8\n\n#go to folder in which to install software \ncd /zfs/omics/projects/amplicomics/bin\n\n#install NanoClass2\ngit clone https://github.com/ndombrowski/NanoClass2.git\n\n\nTo be able to use software installed on the amplicomics share, you first need to ensure that conda is installed. If it is, then you can run the following command:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\nThis command will add pre-installed conda environments in the amplicomics share to your conda env list. After you run conda env list you should see several tools from amplicomics, including QIIME2 and Snakemake. Snakemake is what we need to run NanoClass2.\nNext, you can setup your working environment. Change the path of the working directory to wherever you want to analyze your data:\n\n#define the directory in which you want to run your analyses\nwdir=\"/home/$USER/personal/testing/amplicomics_test\"\n\n#go into the working directory\ncd $wdir\n\n#activate the snakemake conda environment \nconda activate snakemake_nanoclass2\n\n\n\n\nThe test run will be performed with some example data provided with NanoClass2. The test run will subsample the reads and use the subsetted reads to test all classifiers available with NanoClass2.\n\n#cp some test data over (comes with NanoClass2) \nmkdir data \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/example_files/*fastq.gz data/\n\n#copy the mapping file (comes with NanoClass2) \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/example_files/mapping.csv .\n\n#ensure that the path to the fastq data is correct in mapping.csv\n#exchange the path in `replace` with wherever you copied the fastq.gz files\nsearch='your_path/example_files/'\nreplace='/home/ndombro/personal/testing/amplicomics_test/data/'\n\nsed -i -e  \"s|$search|$replace|\" mapping.csv\n\n#copy config yaml (comes with NanoClass2) \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/config.yaml .\n\nIn the config.yaml file we can tell NanoClass2 where the data is located and how to run NanoClass2. For the test-run, we only need to:\n\nexchange \"example_files/mapping.csv\" with \"mapping.csv\"\nkeep the rest as is, i.e. we want to subsample our reads for the test run\n\nBy default the config.yaml is setup to run NanoClass2 as follows:\n\nSubsample the fastq files to only work with 100 reads per sample. To turn this of, set skip: true\nRun NanoClass2 using all 11 classifiers. If you already have a favorite classifier you can only select the tools that you want to use.\nQuality filter the samples:\n\nDiscard reads shorter than 1400 bp (minlen)\nDiscard reads longer than 1600bp (maxlen)\nDiscard reads with a phred score less than 10 (quality)\nDo not trim nucleotides from the start (headcrop)\nDo not trim nucleotides from the end (tailcrop)\n\nFeel free to check the config file for more details about these and other parameters that can be adjusted\n\n\n\nWhen running snakemake with -np this will not run NanoClass itself but only perform a dry-run, which is useful to ensure that everything works correctly. The command below should work as it is as long as the config.yaml is located in the same directory from which you start the analysis.\n\n#perform a dry-run\nsnakemake --cores 1 \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml \\\n    --use-conda --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --nolock --rerun-incomplete -np\n\nIf the above command works and you see This was a dry-run (flag -n). The order of jobs does not reflect the order of execution. all is good and you can submit things to a compute node.\n\n\n\nThe NanoClass2 installation comes with an example sbatch script that you can move over to where you do your analyses. This script allows you to submit the job to the compute nodes.\n\n#cp template batch script (comes with NanoClass2) \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/jobscript.sh .\n\nThe jobscript is setup to work with NanoClass2 in the amplicomics environment and you should not need to change anything if your config.yaml is set up correctly.\nThe script does the following:\n\nRequests 58 crunchomics cpus and 460G of memory. One can set this lower, but for testing purposes you might keep these values. The can be changed with #SBATCH --cpus-per-task=58 and #SBATCH --mem=460G\nEnsures that conda is loaded properly by running source ~/.bashrc. The bashrc is a script executed once a user logs in and holds special configurations, such as telling the shell where conda is installed. If you run into issues that are related to conda/mamba not being found then open the this file with nano ~/.bashrc and check that you have a section with text similar to # &gt;&gt;&gt; conda initialize &gt;&gt;&gt;. If this is not present you might need to run conda init bash to add it.\nActivates the snakemake conda environment that is found in the amplicomics share with conda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\nRuns NanoClass2. When using the script as is, the script assumes that NanoClass2 is installed at this path /zfs/omics/projects/amplicomics/bin/NanoClass2/ and that the config.yaml is located in the folder from which you analyse the data. If that is not the case, change the paths accordingly\n\nNext, we can submit the job as follows:\n\n#Submitted batch job 427090\nsbatch jobscript.sh\n\n#for checking how far things are along:\n#check the end of the log file i.e. \ntail *.log \n\n#check if sbatch script is still running \nsqueue\n\n\n\n\nFinally, we can create a report with the following command:\n\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile\n\nTo view the report, its best to copy the html to your computer and open it via any web browser.\nThe report holds the key information, however, there are other useful outputs, all of which can be found in the results folder. The NanoClass2 website gives some more information about the generated outputs.\n\n\n\n\nMost classification tools implemented in NanoClass2 can use alternative databases supplied by the user, provided they are formatted correctly. The database that is provided in the amplicomics share is the SILVA 16S or 18S databases.\nIf you want to work with a custom database, users will need to:\n\nInstall NanoClass2 for themselves as one installation is needed for each database of interest\nProvide the databases themselves and store them in the db/common/ subdirectory of the NanoClass2 directory. These will then be automatically detected once NanoClass2 is started and NanoClass2 will create and reformat all tools-specific databases based on this user-provided database.\n\nFor an example on how to format a custom database, check out these instructions.",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "NanoClass2"
    ]
  },
  {
    "objectID": "source/nanopore/nanoclass.html#nanoclass2",
    "href": "source/nanopore/nanoclass.html#nanoclass2",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoClass2 build upon NanoClass, a snakemake workflow originally developed by IBED’s former bioinformatician Evelien Jongepier.\nNanoClass2 is a taxonomic meta-classifier for meta-barcoding 16S/18S rRNA gene sequencing data generated with the Oxford Nanopore Technology. With a single command, this Snakemake pipeline installs all programs and databases and will do the following:\nNanoClass2 is a snakemake workflow that will:\n\nRemove adapters with porechop\nPerform quality filtering with chopper\nGenerate read quality plots post quality cleaning with pistis\nSubsample reads to compare different classifiers (can be turned of)\nClassify reads with any one of these tools: blastn, centrifuge, dcmegablast, idtaxa, megablast, minimap, mothur, qiime, rdp, spingo, kraken. By default NanoClass2 will use the SILVA_138.1_SSURef_NR99 database to classify reads. If you are interested in using other bases, check out these instructions.\n\nFor more details, feel free to also have a look at the manual.\n\n\n\n\n\nNanoClass2 is installed on the UvA crunchomics HPC. If you have access to crunchomics you can be added to the amplicomics share in which NanoClass2 is set up. To be added, please send an email with your Uva netID to Nina Dombrowski.\nIf you want or need to set up NanoClass2 by yourself, expand the code below to view an example on how to install the software:\n\n\nShow the code\n#install snakemake as a conda/mamba environment\nmamba create -c conda-forge -c bioconda --prefix /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2 python=3.9.7 snakemake=6.8.0 tabulate=0.8\n\n#go to folder in which to install software \ncd /zfs/omics/projects/amplicomics/bin\n\n#install NanoClass2\ngit clone https://github.com/ndombrowski/NanoClass2.git\n\n\nTo be able to use software installed on the amplicomics share, you first need to ensure that conda is installed. If it is, then you can run the following command:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\nThis command will add pre-installed conda environments in the amplicomics share to your conda env list. After you run conda env list you should see several tools from amplicomics, including QIIME2 and Snakemake. Snakemake is what we need to run NanoClass2.\nNext, you can setup your working environment. Change the path of the working directory to wherever you want to analyze your data:\n\n#define the directory in which you want to run your analyses\nwdir=\"/home/$USER/personal/testing/amplicomics_test\"\n\n#go into the working directory\ncd $wdir\n\n#activate the snakemake conda environment \nconda activate snakemake_nanoclass2\n\n\n\n\nThe test run will be performed with some example data provided with NanoClass2. The test run will subsample the reads and use the subsetted reads to test all classifiers available with NanoClass2.\n\n#cp some test data over (comes with NanoClass2) \nmkdir data \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/example_files/*fastq.gz data/\n\n#copy the mapping file (comes with NanoClass2) \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/example_files/mapping.csv .\n\n#ensure that the path to the fastq data is correct in mapping.csv\n#exchange the path in `replace` with wherever you copied the fastq.gz files\nsearch='your_path/example_files/'\nreplace='/home/ndombro/personal/testing/amplicomics_test/data/'\n\nsed -i -e  \"s|$search|$replace|\" mapping.csv\n\n#copy config yaml (comes with NanoClass2) \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/config.yaml .\n\nIn the config.yaml file we can tell NanoClass2 where the data is located and how to run NanoClass2. For the test-run, we only need to:\n\nexchange \"example_files/mapping.csv\" with \"mapping.csv\"\nkeep the rest as is, i.e. we want to subsample our reads for the test run\n\nBy default the config.yaml is setup to run NanoClass2 as follows:\n\nSubsample the fastq files to only work with 100 reads per sample. To turn this of, set skip: true\nRun NanoClass2 using all 11 classifiers. If you already have a favorite classifier you can only select the tools that you want to use.\nQuality filter the samples:\n\nDiscard reads shorter than 1400 bp (minlen)\nDiscard reads longer than 1600bp (maxlen)\nDiscard reads with a phred score less than 10 (quality)\nDo not trim nucleotides from the start (headcrop)\nDo not trim nucleotides from the end (tailcrop)\n\nFeel free to check the config file for more details about these and other parameters that can be adjusted\n\n\n\nWhen running snakemake with -np this will not run NanoClass itself but only perform a dry-run, which is useful to ensure that everything works correctly. The command below should work as it is as long as the config.yaml is located in the same directory from which you start the analysis.\n\n#perform a dry-run\nsnakemake --cores 1 \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml \\\n    --use-conda --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --nolock --rerun-incomplete -np\n\nIf the above command works and you see This was a dry-run (flag -n). The order of jobs does not reflect the order of execution. all is good and you can submit things to a compute node.\n\n\n\nThe NanoClass2 installation comes with an example sbatch script that you can move over to where you do your analyses. This script allows you to submit the job to the compute nodes.\n\n#cp template batch script (comes with NanoClass2) \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/jobscript.sh .\n\nThe jobscript is setup to work with NanoClass2 in the amplicomics environment and you should not need to change anything if your config.yaml is set up correctly.\nThe script does the following:\n\nRequests 58 crunchomics cpus and 460G of memory. One can set this lower, but for testing purposes you might keep these values. The can be changed with #SBATCH --cpus-per-task=58 and #SBATCH --mem=460G\nEnsures that conda is loaded properly by running source ~/.bashrc. The bashrc is a script executed once a user logs in and holds special configurations, such as telling the shell where conda is installed. If you run into issues that are related to conda/mamba not being found then open the this file with nano ~/.bashrc and check that you have a section with text similar to # &gt;&gt;&gt; conda initialize &gt;&gt;&gt;. If this is not present you might need to run conda init bash to add it.\nActivates the snakemake conda environment that is found in the amplicomics share with conda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\nRuns NanoClass2. When using the script as is, the script assumes that NanoClass2 is installed at this path /zfs/omics/projects/amplicomics/bin/NanoClass2/ and that the config.yaml is located in the folder from which you analyse the data. If that is not the case, change the paths accordingly\n\nNext, we can submit the job as follows:\n\n#Submitted batch job 427090\nsbatch jobscript.sh\n\n#for checking how far things are along:\n#check the end of the log file i.e. \ntail *.log \n\n#check if sbatch script is still running \nsqueue\n\n\n\n\nFinally, we can create a report with the following command:\n\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile\n\nTo view the report, its best to copy the html to your computer and open it via any web browser.\nThe report holds the key information, however, there are other useful outputs, all of which can be found in the results folder. The NanoClass2 website gives some more information about the generated outputs.\n\n\n\n\nMost classification tools implemented in NanoClass2 can use alternative databases supplied by the user, provided they are formatted correctly. The database that is provided in the amplicomics share is the SILVA 16S or 18S databases.\nIf you want to work with a custom database, users will need to:\n\nInstall NanoClass2 for themselves as one installation is needed for each database of interest\nProvide the databases themselves and store them in the db/common/ subdirectory of the NanoClass2 directory. These will then be automatically detected once NanoClass2 is started and NanoClass2 will create and reformat all tools-specific databases based on this user-provided database.\n\nFor an example on how to format a custom database, check out these instructions.",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "NanoClass2"
    ]
  },
  {
    "objectID": "source/nanopore/nanoqc_readme.html",
    "href": "source/nanopore/nanoqc_readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoQC is a quality control tool for long read sequencing data aiming to replicate some of the plots made by fastQC (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\n\nNanoQC is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools using pip\n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\n\n\nInputs: Fastqc.gz file\nOutput: An HTML with quality information\n\nExample code:\n\n#start environment\nconda activate nanopack\n\n#run on a single file\nnanoQC myfile.fastq.gz -o outputfolder\n\nUseful arguments (for the full version, check the manual):\n\n-l, --minlen {int} Minimum length of reads to be included in the plots. This also controls the length plotted in the graphs from the beginning and end of reads (length plotted = minlen / 2)",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "NanoQC"
    ]
  },
  {
    "objectID": "source/nanopore/nanoqc_readme.html#nanoqc",
    "href": "source/nanopore/nanoqc_readme.html#nanoqc",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "NanoQC is a quality control tool for long read sequencing data aiming to replicate some of the plots made by fastQC (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\n\nNanoQC is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools using pip\n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\n\n\nInputs: Fastqc.gz file\nOutput: An HTML with quality information\n\nExample code:\n\n#start environment\nconda activate nanopack\n\n#run on a single file\nnanoQC myfile.fastq.gz -o outputfolder\n\nUseful arguments (for the full version, check the manual):\n\n-l, --minlen {int} Minimum length of reads to be included in the plots. This also controls the length plotted in the graphs from the beginning and end of reads (length plotted = minlen / 2)",
    "crumbs": [
      "Welcome page",
      "Nanopore analyses",
      "NanoQC"
    ]
  },
  {
    "objectID": "source/nanopore/readme.html",
    "href": "source/nanopore/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "In this section you find tools to analyse long-read sequencing data. There might be a bit of overlap with the section “Amplicon sequencing data” but in the Nanopore analysis section you specifically will find tools suitable for dealing with long read data generated with Nanopore.",
    "crumbs": [
      "Omics Analyses",
      "Nanopore analyses"
    ]
  },
  {
    "objectID": "source/nanopore/readme.html#general",
    "href": "source/nanopore/readme.html#general",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "In this section you find tools to analyse long-read sequencing data. There might be a bit of overlap with the section “Amplicon sequencing data” but in the Nanopore analysis section you specifically will find tools suitable for dealing with long read data generated with Nanopore.",
    "crumbs": [
      "Omics Analyses",
      "Nanopore analyses"
    ]
  },
  {
    "objectID": "source/databases/readme.html",
    "href": "source/databases/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "tba"
  }
]