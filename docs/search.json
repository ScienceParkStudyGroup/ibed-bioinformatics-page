[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Software information",
    "section": "",
    "text": "On this website you can find documentation about software that might be useful for bioinformatic data analyses. Please note: This is very much a work in progress and the page will be slowly updated over time.\nIf you want to add additional information you are welcome to do so. For now, feel free to send an email to n.dombrowski@uva.nl with a markdown, quarto or text file with all relevant information and I can integrate this into the webpage. In the future, a “how-to-add-your-own-information” section will be added as well.\nOn each page, you will find a note if the software is installed on Crunchomics in the introduction. Additionally, each page will give you a brief installation instruction as well as some basic information about using the tool."
  },
  {
    "objectID": "source/ITSx/readme.html",
    "href": "source/ITSx/readme.html",
    "title": "Software information",
    "section": "",
    "text": "“ITSx is an open source software utility to extract the highly variable ITS1 and ITS2 subregions from ITS sequences, which is commonly used as a molecular barcode for e.g. fungi.”\n\nWebsite\nManual\nPaper. Please do not forget to cite the ITSx paper whenever you use the software (Bengtsson-Palme et al. 2013).\n\nAvailable on Crunchomics: No\n\n\n\nITSx can be installed from scratch but below is the code needed to install the software with mamba (or conda if that is preferred). If you don’t have mamba/conda installed find the correction version for your system here for mamba and here for conda. Examples for setting up either mamba or conda are found in the code cell below.\nIf there is an issue with the mamba/conda installation, the software can also be downloaded with wget https://microbiology.se/sw/ITSx_1.1.3.tar.gz. After the download, decompress the folder and follow the information in the readme.txt. The download also comes with a test.fasta which can be used to test either installation.\nData for testing can also be found here.\n\n\n\n#install conda (run only if not yet installed)\n#adjust the file name based on your system needs\nwget https://repo.anaconda.com/archive/Anaconda3-2023.07-2-Linux-x86_64.sh\n\nbash Anaconda3-2023.07-2-Linux-x86_64.sh\n\n#install mamba (run only if not yet installed)\n#adjust the file name based on your system needs\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\n\nbash Mambaforge-Linux-x86_64.sh\n\n\n\n\n\nmamba create -n fungi_its\nmamba install -n fungi_its -c bioconda itsx\n\n\n\n\n\nmamba install -c bioconda itsx\n\n\n\n\n\n\nRequired input: FASTA format (aligned or unaligned, DNA or RNA)\nGenerated output:\n\none summary file of the entire run\none more detailed table containing the positions in the respective sequences where the ITS subregions were found\none “semi-graphical” representation of hits\none FASTA file of all identified ITS sequences\none FASTA file for the ITS1 and ITS2 regions\nif entries that did not contain any ITS region are found, a list of sequence IDs representing those entries (optional)\n\nUseful arguments (not extensive, check manual for all arguments):\n\n--save_regions: Get all regions of interest, not only ITS1/2\n-E {value}: E-value cutoff (default 1e-5)\n-S {value}: Domain score cutoff (default 0)\n--cpu {value }: Number of cpus to use (default 1)\n--multi_thread {T/F}: Multi-thread the HMMER-search. On (T) by default if the number of CPUs/cores is larger than one (–cpu option > 1), else off (F)\n--preserve {T/F}: If on, ITSx will preserve the sequence headers from the input file instead of replacing them with ITSx headers in the output. Off (F) by default.\n--only_full {T/F}: If true, the output is limited to full-length ITS1 and ITS2 regions only. Off (F) by default.\n--minlen {value} Minimum length the ITS regions must be to be outputted in the concatenated file (see –concat above). Default is zero (0).\n\n\n\n\n\n#activate the right environment (if using environment)\nmamba deactivate\nmamba activate fungi_its\n\n#download data for testing the install (optional)\nmkdir testing\nwget -P testing https://raw.githubusercontent.com/ScienceParkStudyGroup/software_information/main/data/itsx/test.fasta\n\n#testrun (adjust path of test.fasta to where ever you downloaded the software)\nITSx -i testing/test.fasta --save_regions all -o testing/ITS_test_v1\n\n#sanity checking\ngrep -c \">\" testing/*fasta\n\n#deactivate environment (if using environment)\nmamba deactivate\n\nRegions extracted from test file (notice how the full fasta ONLY contains sequences with all regions):\n\ntesting/ITS_test_v1.5_8S.fasta:50\ntesting/ITS_test_v1.ITS1.fasta:50\ntesting/ITS_test_v1.ITS2.fasta:50\ntesting/ITS_test_v1.LSU.fasta:32\ntesting/ITS_test_v1.SSU.fasta:31\ntesting/ITS_test_v1.full.fasta:19\ntesting/ITS_test_v1_no_detections.fasta:0\ntesting/test.fasta:50"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html",
    "href": "source/Qiime/qiime_cmi_tutorial.html",
    "title": "Software information",
    "section": "",
    "text": "Notice:\n\nThis tutorial was not written by myself but taken from QIIME tutorial. Additionally, the notes you find here were expended by copying from several spots in the QIIME documentation to explore things.\nThe code was run on my personal computer (Windows, WSL2).\nIf you follow the link for the tutorial, you will see that the tutorial is also available using the Galaxy interface and python API.\n\nOther useful resources to check out:\n\nQIIME 2 view: web-based viewer for .qza and .qzv files\nQIIME forum\nQIIME2 docs. Notice, the tutorial runs on v2021.2, which we will use for now and later update to the newest QIIME version\nQIIME2 Library. Useful to check for new plugins/functionality\nOld QIIME tutorials\nNEW QIIME tutorials\n\n\n\n\n\n\nAll files generated by QIIME 2 are either .qza or .qzv files, and these are simply zip files that store your data alongside some QIIME 2-specific metadata. You can unzip them with unzip xxx.qza\nThe .qza file extension is an abbreviation for QIIME Zipped Artifact, and the .qzv file extension is an abbreviation for QIIME Zipped Visualization. .qza files (which are often simply referred to as artifacts) are intermediary files in a QIIME 2 analysis, usually containing raw data of some sort.\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files (generally .qza and .qzv files) without requiring a QIIME installation.\nPlugins are software packages that can be developed by anyone and define actions, which are steps in an analysis workflow. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses.\nQIIME actions:\n\nA method, i.e.alpha-phylogenetic, accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts (qza file) as output.\nA visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input and generate qzv files. In contrast to a method, a visualizer produces exactly one visualization as output. An example of a QIIME 2 visualizer is the beta-group-significance action in the q2-diversity plugin.\nA pipeline can generate one or more .qza and/or .qzv as output. Pipelines are special in that they’re a type of action that can call other actions. They are often used by developers to define simplify common workflows so they can be run by users in a single step. For example, the core-metrics-phylogenetic action in the q2-diversity plugin is a Pipeline that runs both alpha-phylogenetic and beta-phylogenetic, as well as several other actions, in a single command.\n\nTypes used in QIIME 2:\n\nFile type: the format of a file used to store some data. For example, newick is a file type that is used for storing phylogenetic trees.\nData type: refer to how data is represented in a computer’s memory (i.e., RAM) while it’s actively in use.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. This is a representation of the meaning of the data. For example, two semantic types used in QIIME 2 are Phylogeny[Rooted] and Phylogeny[Unrooted], which are used to represent rooted and unrooted trees, respectively. QIIME 2 methods will describe what semantic types they take as input(s), and what semantic types they generate as output(s).\n\n\n\n\n\n\nFind out more also here.\n\nMetadata:\n\nSample metadata may include technical details, such as the DNA barcodes that were used for each sample in a multiplexed sequencing run, or descriptions of the samples, such as which subject, time point, and body site each sample came from\nFeature metadata is often a feature annotation, such as the taxonomy assigned to an amplicon sequence variant (ASV).\nQIIME 2 does not place restrictions on what types of metadata are expected to be present; there are no enforced “metadata standards”.\nthe MIxS and MIMARKS standards [1] provide recommendations for microbiome studies and may be helpful in determining what information to collect in your study. If you plan to deposit your data in a data archive (e.g. ENA or Qiita), it is also important to determine the types of metadata expected by that resource.\nIn QIIME 2, we always refer to these files as metadata files, but they are conceptually the same thing as QIIME 1 mapping files. QIIME 2 metadata files are backwards-compatible with QIIME 1 mapping files, meaning that you can use existing QIIME 1 mapping files in QIIME 2 without needing to make modifications to the file.\n\n\n\n\n\nUsually provided as TSV file (.tsv or .txt file extension) that provides data in form of rows and columns\nFirst row = non-comment, non-empty column headers containing a unique identifier for each metadata entry\nRows whose first cell begins with the pound sign (#) are interpreted as comments and may appear anywhere in the file. Comment rows are ignored by QIIME 2 and are for informational purposes only. Inline comments (i.e., comments that begin part-way through a row or at the end of a row) are not supported.\nEmpty rows (e.g. blank lines or rows consisting solely of empty cells) may appear anywhere in the file and are ignored.\nColumn 1: identifier (ID) column. This column defines the sample or feature IDs associated with your study. It is not recommended to mix sample and feature IDs in a single metadata file; keep sample and feature metadata stored in separate files. The ID column name (also referred to as the ID column header) must be:\n\nCase-insenitive (i.e., uppercase or lowercase, or a mixing of the two, is allowed): id, sampleid, sample id, sample-id, featureid feature id, feature-id\nIDs may consist of any Unicode characters, with the exception that IDs must not start with the pound sign (#), as those rows would be interpreted as comments and ignored.\nIDs cannot be empty (i.e. they must consist of at least one character).\nIDs must be unique (exact string matching is performed to detect duplicates).\nAt least one ID must be present in the file.\nIDs cannot be any of the reserved ID headers listed above.\n\nThe ID column is the first column in the metadata file, and can optionally be followed by additional columns defining metadata associated with each sample or feature ID. Metadata files are not required to have additional metadata columns, so a file containing only an ID column is a valid QIIME 2 metadata file.\nThe contents of a metadata file following the ID column and header row (excluding comments and empty lines) are referred to as the metadata values. A single metadata value, defined by an (ID, column) pair, is referred to as a cell. The following rules apply to metadata values and cells:\n\nMay consist of any Unicode characters.\nEmpty cells represent missing data. Other values such as NA are not interpreted as missing data; only the empty cell is recognized as “missing”. Note that cells consisting solely of whitespace characters are also interpreted as missing data\nIf any cell in the metadata contains leading or trailing whitespace characters (e.g. spaces, tabs), those characters will be ignored when the file is loaded.\n\n\nRecommendations for identifiers:\n\nIdentifiers should be 36 characters long or less.\nIdentifiers should contain only ASCII alphanumeric characters (i.e. in the range of [a-z], [A-Z], or [0-9]), the period (.) character, or the dash (-) character.\nNote that some bioinformatics tools may have more restrictive requirements on identifiers than the recommendations that are outlined here. For example, Illumina sample sheet identifiers cannot have . characters, while we do include those in our set of recommended characters\n[cual-id] (https://github.com/johnchase/cual-id) can be used to help create identifiers and the associated paper also includes a discussion on how to choose identifiers [2].\n\nColumn types:\n\nQIIME 2 currently supports categorical and numeric metadata columns and will automatically attempt to infer the type of each metadata column\nQIIME 2 supports an optional comment directive to allow users to explicitly state a column’s type.\nThe comment directive must appear directly below the header row. The value in the ID column in this row must be #q2:types to indicate the row is a comment directive. Subsequent cells in this row may contain the values categorical or numeric (both case-insensitive). The empty cell is also supported if you do not wish to assign a type to a column\n\nMetadata validation:\n\nQIIME 2 will automatically validate a metadata file anytime it is used. Loading your metadata in QIIME 2 will typically present only a single error at a time, which can make identifying and resolving validation issues cumbersome, especially if there are many issues with the metadata.\nSample and feature metadata files stored in Google Sheets can additionally be validated using Keemei [3]\n\n\n\n\n\nThis tutorial focuses on data reused a Compilation of longitudinal microbiota data and hospitalome from hematopoietic cell transplantation patients [4]\n\n\n\n\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help\n\n\n\n\n\n\n\nRun this if needed. If you are not using mamba yet, replace mamba with conda to use this as an alternative installer.\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n#mamba activate qiime2-2023.7\n\n\n\n\n\n#set wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/QIIME/qiime_cmi_tutorial\"\ncd $wdir \n\n#activate QIIME environment \nmamba activate qiime2-2023.7\n\n\n\n\n\n\nmkdir data\nmkdir visualizations\n\n#download metadata table\nwget \\\n  -O 'sample-metadata.tsv' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/020-metadata/sample-metadata.tsv'\n\n#prepare metadata for qiime view \nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv \\\n  --o-visualization visualizations/metadata-summ-1.qzv\n\n#download sequencing data (already demultiplexed)\nwget \\\n  -O 'data_to_import.zip' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/030-importing/data_to_import.zip'\n\nunzip -d data_to_import data_to_import.zip\n\nrm data_to_import.zip\n\n\n\n\n\n#import data into qiime\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-format CasavaOneEightSingleLanePerSampleDirFmt \\\n  --input-path data_to_import \\\n  --output-path demultiplexed-sequences.qza\n\n#generate a summary of the imported data\nqiime demux summarize \\\n  --i-data demultiplexed-sequences.qza \\\n  --o-visualization visualizations/demultiplexed-sequences-summ.qzv\n\nqiime demux: supports demultiplexing of single-end and paired-end sequence reads and visualization of sequence quality information.\n\n\n\n\nqiime tools peek  demultiplexed-sequences.qza\n\n\n\n\nIf you have reads from multiple samples in the same file, you’ll need to demultiplex your sequences.\nIf your barcodes are still in your sequences, you can use functions from the cutadapt plugin. The cutadapt demux-single method looks for barcode sequences at the beginning of your reads (5’ end) with a certain error tolerance, removes them, and returns sequence data separated by each sample. The QIIME 2 forum has a tutorial on various functions available in cutadapt, including demultiplexing. You can learn more about how cutadapt works under the hood by reading their documentation.\nNote: Currently q2-demux and q2-cutadapt do not support demultiplexing dual-barcoded paired-end sequences, but only can demultiplex with barcodes in the forward reads. So for the time being, this type of demultiplexing needs to be done outside of QIIME 2 using other tools, for example bcl2fastq.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here.\nThere are two plugins to check out:\n\nq2-demux\ncutadapt\n\n\n\n\nWhether or not you need to merge reads depends on how you plan to cluster or denoise your sequences into amplicon sequence variants (ASVs) or operational taxonomic units (OTUs). If you plan to use deblur or OTU clustering methods next, join your sequences now. If you plan to use dada2 to denoise your sequences, do not merge — dada2 performs read merging automatically after denoising each sequence.\nIf you need to merge your reads, you can use the QIIME 2 q2-vsearch plugin with the merge-pairs method.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here.\n\n\n\nIf your data contains any non-biological sequences (e.g. primers, sequencing adapters, PCR spacers, etc), you should remove these.\nThe q2-cutadapt plugin has comprehensive methods for removing non-biological sequences from paired-end or single-end data.\nIf you’re going to use DADA2 to denoise your sequences, you can remove biological sequences at the same time as you call the denoising function. All of DADA2’s denoise fuctions have some sort of –p-trim parameter you can specify to remove base pairs from the 5’ end of your reads. (Deblur does not have this functionality yet.)\n\n\n\nThe names for these steps are very descriptive:\n\nWe denoise our sequences to remove and/or correct noisy reads.\nWe dereplicate our sequences to reduce repetition and file size/memory requirements in downstream steps.\nWe cluster sequences to collapse similar sequences (e.g., those that are ≥ 97% similar to each other) into single replicate sequences. This process, also known as OTU picking, was once a common procedure, used to simultaneously dereplicate but also perform a sort of quick-and-dirty denoising procedure (to capture stochastic sequencing and PCR errors, which should be rare and similar to more abundant centroid sequences). Use denoising methods instead if you can.\n\nThe denoising methods currently available in QIIME 2 include DADA2 and Deblur. Note that deblur (and also vsearch dereplicate-sequences) should be preceded by basic quality-score-based filtering, but this is unnecessary for dada2. Both Deblur and DADA2 contain internal chimera checking methods and abundance filtering, so additional filtering should not be necessary following these methods.\nTo put it simply, these methods filter out noisy sequences, correct errors in marginal sequences (in the case of DADA2), remove chimeric sequences, remove singletons, join denoised paired-end reads (in the case of DADA2), and then dereplicate those sequences.\n\n\nThe final products of all denoising and clustering methods/workflows are a FeatureTable[Frequency] (feature table) artifact and a FeatureData[Sequence] (representative sequences) artifact. These are two of the most important artifacts in an amplicon sequencing workflow, and are used for many downstream analyses.\nMany operations on these tables can be performed with q2-feature-table.\nWant to see which sequences are associated with each feature ID? Use qiime metadata tabulate with your FeatureData[Sequence] artifact as input.\n\n\n\nQuality control or denoising of the sequence data will be performed with DADA2 [5]. DADA2 is an model-based approach for correcting amplicon errors without constructing OTUs. Can be applied to every gene of interest and is not limited to the 16S.\nThe DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).\nAlso check out this tutorial for using DADA2. One important parameter to check is truncLen to trim low quality read ends. Also check out this paper discussing quality filtering [6].\nIn QIIME the denoise_paired action in the q2-dada2 plugin. This performs quality filtering, chimera checking, and paired- end read joining.\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots and identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. Your reads must still overlap after truncation in order to merge them later!\nSome thoughts on this:\n\nReviewing the data we notice that the twenty-fifth percentile quality score drops below 30 at position 204 in the forward reads and 205 in the reverse reads. We chose to use those values for the required truncation lengths. This truncates the 3’/5’ end of the of the input sequences. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads.\nSince the first base of the reverse reads is slightly lower than those that follow, I choose to trim that first base in the reverse reads, but apply no trimming to the forward reads. This trimming is probably unnecessary here, but is useful here for illustrating how this works.\nFigaro is a tool to automatically choose the trunc_len\nChimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2\n\nDesired overlap length\n\nOften between 10 and 20 for most applications.\nDADA2 often recommends 20\nRemember: longer overlaps means more 3’ end must be kept at a cost of decreased overall sequence quality\n\nOther parameters:\n\n--p-max-ee-f/r {number}: Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]. If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads (eg. maxEE=c(2,5))\n--p-trunc-q {integeer}: Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-for trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap {INTEGER}: The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method {TEXT Choices(‘independent’, ‘pseudo’)}: he method used to pool samples for denoising. By default, the dada function processes each sample independently. However, pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method {TEXT Choices(‘consensus’, ‘none’, ‘pooled’)}: The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-n-threads {INTEGER}: default 1\n\nOutputs:\n\nThe feature table describes which amplicon sequence variants (ASVs) were observed in which samples, and how many times each ASV was observed in each sample.\nThe feature data in this case is the sequence that defines each ASV. Generate and explore the summaries of each of these files.\nSanity check: in stats-dada2 check that outside of filtering, there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nSequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the asv table.\n\n\n#denoise data\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs demultiplexed-sequences.qza \\\n  --p-trunc-len-f 204 \\\n  --p-trim-left-r 1 \\\n  --p-trunc-len-r 205 \\\n  --o-representative-sequences asv-sequences-0.qza \\\n  --o-table feature-table-0.qza \\\n  --o-denoising-stats dada2-stats.qza\n\n#generate summaries\nqiime feature-table summarize \\\n  --i-table feature-table-0.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-0-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data asv-sequences-0.qza \\\n  --o-visualization visualizations/asv-sequences-0-summ.qzv\n\nqiime metadata tabulate \\\n  --m-input-file dada2-stats.qza \\\n  --o-visualization visualizations/stats-dada2.qzv\n\nOutputs:\n\nASV table,\nthe representative sequences,\nstatistics on the procedure\n\n\n\n\n\nWe’ll next obtain a much larger feature table representing all of the samples included in the study dataset. These would take too much time to denoise in this course, so we’ll start with the feature table, sequences, and metadata provided by the authors and filter to samples that we’ll use for our analyses.\nA full description can be foun on the QIIME website\n\n#cleanup first dataset\nmkdir upstream_tutorial\nmv *qza upstream_tutorial/\n\n#get the data\nwget \\\n  -O 'feature-table.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/feature-table.qza'\n\nwget \\\n  -O 'rep-seqs.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/rep-seqs.qza'\n\n#summarize table\nqiime feature-table summarize \\\n  --i-table feature-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data rep-seqs.qza \\\n  --o-visualization visualizations/rep-seqs-summ.qzv\n\nOutput explanation:\n\nA feature is essentially any unit of observation, e.g., an OTU, a sequence variant, a gene, a metabolite, etc, and a feature table is a matrix of sample X feature abundances (the number of times each feature was observed in each sample).\nminimum frequency is 5342 - if you click the “Interactive Sample Detail” tab and scroll to the bottom, you will see that the sample with the lowest feature frequency has 5342 observations. Similarly, the sample with the highest frequency of features has 193491 total observations - meaning that many/most features were seen more than once.\nmean frequency of features (881)? Does it mean that one feature is found in average 881 times throughout all the samples\n\n\n\n\n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table autofmt-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/autofmt-table-summ.qzv\n\n#filter time window\nqiime feature-table filter-samples \\\n  --i-table autofmt-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'DayRelativeToNearestHCT BETWEEN -10 AND 70' \\\n  --o-filtered-table filtered-table-1.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\n#done to reduce runtime (so optional)\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table filtered-table-2.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-2-summ.qzv\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\nWe can also also filter tables by lists:\n\n#create imaginary list of ids to keep\necho L1S8 >> samples-to-keep.tsv\n\n#filter \nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nSome examples using where:\n\n–p-where “[subject]=‘subject-1’”\n\n–p-where “[body-site] IN (‘left palm’, ‘right palm’)”\n\n–p-where “[subject]=‘subject-1’ AND [body-site]=‘gut’”\n\n–p-where “[body-site]=‘gut’ OR [reported-antibiotic-usage]=‘Yes’”\n\n–p-where “[subject]=‘subject-1’ AND NOT [body-site]=‘gut’”\n\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza\n\n\n\n\n\nTo identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods: - classify-consensus-blast and classify-consensus-vsearch are both alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained - Machine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! - Most users do not even need to follow that tutorial and perform that training step, because the lovely QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods we’ve tested based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nIn the example below we use a pre-trained Naive Bayes taxonomic classifier. This particular classifier was trained on the Greengenes 13-8 database, where sequences were trimmed to represent only the region between the 515F / 806R primers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [7].\nCheck out this study discussing reproducible sequence taxonomy reference database management [8].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [9].\n\n\n\n\n#get classifier\nwget \\\n  -O 'gg-13-8-99-nb-classifier.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/020-taxonomy/gg-13-8-99-nb-classifier.qza'\n\n#assign taxonomic info to ASV sequences\nqiime feature-classifier classify-sklearn \\\n  --i-classifier gg-13-8-99-nb-classifier.qza \\\n  --i-reads filtered-sequences-1.qza \\\n  --o-classification taxonomy.qza\n\n#generate summary\nqiime metadata tabulate \\\n  --m-input-file taxonomy.qza \\\n  --o-visualization visualizations/taxonomy.qzv\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza\n\n\n\n\nYou may have noticed when looking at feature table summaries earlier that some of the samples contained very few ASV sequences. These often represent samples which didn’t amplify or sequence well, and when we start visualizing our data low numbers of sequences can cause misleading results, because the observed composition of the sample may not be reflective of the sample’s actual composition. For this reason it can be helpful to exclude samples with low ASV sequence counts from our samples. Here, we’ll filter out samples from which we have obtained fewer than 10,000 sequences.\n\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#summarize data\nqiime feature-table summarize \\\n  --i-table filtered-table-4.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-4-summ.qzv\n\n\n\n\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-4.qza \\\n  --o-filtered-data filtered-sequences-2.qza\n\n\n\n\n\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/taxa-bar-plots-1.qzv\n\nNotice: We can also generate heatmaps with feature-table heatmap\n\n\n\n\n\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. >10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose\n\n\n\n\n\n\n\nNotice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [10]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [11].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [12]\nNormalization and microbial differential abundance strategies depend upon data characteristics [11]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible.\n\n\n\n\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)\n\n\n\nIf continuous sample metadata columns (e.g., days-since-experiment-start) are correlated with alpha diversity, we can test for those associations here. If you’re interested in performing those tests (for this data set, or for others), you can use the qiime diversity alpha-correlation command.\nWe can also analyze sample composition in the context of categorical metadata using PERMANOVA using the beta-group-significance command. The code below was taken from another test but would allow to test whether distances between samples within a group, such as samples from the same body site (e.g., gut), are more similar to each other then they are to samples from the other groups (e.g., tongue, left palm, and right palm). If you call this command with the --p-pairwise parameter, as we’ll do here, it will also perform pairwise tests that will allow you to determine which specific pairs of groups (e.g., tongue and gut) differ from one another, if any. This command can be slow to run, especially when passing --p-pairwise, since it is based on permutation tests. So, unlike the previous commands, we’ll run beta-group-significance on specific columns of metadata that we’re interested in exploring, rather than all metadata columns to which it is applicable. Here we’ll apply this to our unweighted UniFrac distances, using two sample metadata columns, as follows.\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column body-site \\\n  --o-visualization core-metrics-results/unweighted-unifrac-body-site-significance.qzv \\\n  --p-pairwise\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column subject \\\n  --o-visualization core-metrics-results/unweighted-unifrac-subject-group-significance.qzv \\\n  --p-pairwise\n\nCheck this link for an example output.\nIf continuous sample metadata are correlated with sample composition, we can use the qiime metadata distance-matrix in combination with qiime diversity mantel and qiime diversity bioenvcommands.\n\n\n\nIn order to manage the repeated measures, we will use a linear mixed-effects model. In a mixed-effects model, we combine fixed-effects (your typical linear regression coefficients) with random-effects. These random effects are some (ostensibly random) per-group coefficient which minimizes the error within that group. In our situation, we would want our random effect to be the PatientID as we can see each subject has a different baseline for richness (and we have multiple measures for each patient). By making that a random effect, we can more accurately ascribe associations to the fixed effects as we treat each sample as a “draw” from a per-group distribution.\nIt is called a random effect because the cause of the per-subject deviation from the population intercept is not known. Its source is “random” in the statistical sense. That is randomness is not introduced to the model, but is instead tolerated by it.\nThere are several ways to create a linear model with random effects, but we will be using a random-intercept, which allows for the per-subject intercept to take on a different average from the population intercept (modeling what we saw in the group-significance plot above).\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column DayRelativeToNearestHCT \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-HCT.qzv\n\nHere we see a significant association between richness and the bone marrow transplant.\nOptions:\n\n–p-state-column TEXT Metadata column containing state (time) variable information.\n–p-individual-id-column TEXT Metadata column containing IDs for individual subjects.\n–p-metric TEXT Dependent variable column name. Must be a column name located in the metadata or feature table files.\n\nWe may also be interested in the effect of the auto fecal microbiota transplant. It should be known that these are generally correlated, so choosing one model over the other will require external knowledge.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-FMT.qzv\n\nWe also see a downward trend from the FMT. Since the goal of the FMT was to ameliorate the impact of the bone marrow transplant protocol (which involves an extreme course of antibiotics) on gut health, and the timing of the FMT is related to the timing of the marrow transplant, we might deduce that the negative coefficient is primarily related to the bone marrow transplant procedure. (We can’t prove this with statistics alone however, in this case, we are using abductive reasoning).\nLooking at the log-likelihood, we also note that the HCT result is slightly better than the FMT in accounting for the loss of richness. But only slightly, if we were to perform model testing it may not prove significant.\nIn any case, we can ask a more targeted question to identify if the FMT was useful in recovering richness.\nBy adding the autoFmtGroup to our linear model, we can see if there are different slopes for the two groups, based on an interaction term.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-treatmentVScontrol.qzv\n\nHere we see that the autoFmtGroup is not on its own a significant predictor of richness, but its interaction term with Q(‘day-relative-to-fmt’) is. This implies that there are different slopes between these groups, and we note that given the coding of Q(‘day-relative-to-fmt’):autoFmtGroup[T.treatment] we have a positive coefficient which counteracts (to a degree) the negative coefficient of Q(‘day-relative-to-fmt’).\nNotice: The slopes of the regression scatterplot in this visualization are not the same fit as our mixed-effects model in the table. They are a naive OLS fit to give a sense of the situation. A proper visualization would be a partial regression plot which can condition on other terms to show some effect in “isolation”. This is not currently implemented in QIIME 2.\n\n\n\n\n#test group significance \nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-faith-pd.qzv\n\n#lme\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric faith_pd \\\n  --o-visualization visualizations/lme-faith-pd-treatmentVScontrol.qzv\n\n\n\n\nBeta diversity is between sample diversity. This is useful for answering the question, how different are these microbial communities?\nList of available indices:\n\nJaccard distance (a qualitative measure of community dissimilarity)\nBray-Curtis distance (a quantitative measure of community dissimilarity)\nunweighted UniFrac distance (a qualitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\nweighted UniFrac distance (a quantitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\n\n\nqiime diversity beta-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metric braycurtis \\\n  --p-clustering-method nj \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/braycurtis-rarefaction-plot.qzv\n\n\numap is an ordination method that can be used in place of PCoA and has been shown to better resolve differences between microbiome samples in ordination plots [13].\nLike PCoA, umap operates on distance matrices. We’ll compute this on our weighted and unweighted UniFrac distance matrices.\n\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/unweighted_unifrac_distance_matrix.qza \\\n  --o-umap uu-umap.qza\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/weighted_unifrac_distance_matrix.qza \\\n  --o-umap wu-umap.qza\n\nIn the next few steps, we’ll integrate our unweighted UniFrac umap axis 1 values, and our Faith PD, evenness, and Shannon diversity values, as metadata in visualizations. This will provide a few different ways of interpreting these values.\n\nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization expanded-metadata-summ.qzv\n\nTo see how this information can be used, let’s generate another version of our taxonomy barplots that includes these new metadata values.\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization visualizations/taxa-bar-plots-2.qzv\n\nWe’ll start by integrating these values as metadata in our ordination plots. We’ll also customize these plots in another way: in addition to plotting the ordination axes, we’ll add an explicit time axis to these plots. This is often useful for visualization patterns in ordination plots in time series studies. We’ll add an axis for week-relative-to-hct.\n\nqiime emperor plot \\\n  --i-pcoa uu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa wu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/unweighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-pcoa-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/weighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-pcoa-emperor-w-time.qzv\n\nTip:\nQIIME 2’s q2-diversity plugin provides visualizations for assessing whether microbiome composition differs across groups of independent samples (for example, individuals with a certain disease state and healthy controls) and for assessing whether differences in microbiome composition are correlated with differences in a continuous variable (for example, subjects’ body mass index). These tools assume that all samples are independent of one another, and therefore aren’t applicable to the data used in this tutorial where multiple samples are obtained from the same individual. We therefore don’t illustrate the use of these visualizations on this data, but you can learn about these approaches and view examples in the Moving Pictures tutorial. The Moving Pictures tutorial contains example data and commands, like this tutorial does, so you can experiment with generating these visualizations on your own.\n\n\n\n\nANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [14].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use.\n\n\n\n\n\nBefore applying these analyses, we’re going to perform some additional operations on the feature table that will make these analyses run quicker and make the results more interpretable.\nFirst, we are going to use the taxonomic information that we generated earlier to redefine our features as microbial genera. To do this, we group (or collapse) ASV features based on their taxonomic assignments through the genus level. This is achieved using the q2-taxa plugin’s collapse action.\n\nqiime taxa collapse \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-level 6 \\\n  --o-collapsed-table genus-table.qza\n\nThen, to focus on the genera that are likely to display the most interesting patterns over time (and to reduce the runtime of the steps that come next), we will perform even more filtering. This time we’ll apply prevalence and abudnance based filtering. Specifically, we’ll require that a genus’s abundance is at least 1% in at least 10% of the samples.\nNote: The prevalence-based filtering applied here is fairly stringent. In your own analyses you may want to experiment with relaxed settings of these parameters. Because we want the commands below to run quickly, stringent filtering is helpful for the tutorial.\n\nqiime feature-table filter-features-conditionally \\\n  --i-table genus-table.qza \\\n  --p-prevalence 0.1 \\\n  --p-abundance 0.01 \\\n  --o-filtered-table filtered-genus-table.qza\n\nFinally, we’ll convert the counts in our feature table to relative frequencies. This is required for some of the analyses that we’re about to perform.\n\nqiime feature-table relative-frequency \\\n  --i-table filtered-genus-table.qza \\\n  --o-relative-frequency-table genus-rf-table.qza\n\n\n\n\nThe first plots we’ll generate are volatility plots. We’ll generate these using two different time variables. First, we’ll plot based on week-relative-to-hct.\nThe volatility visualizer generates interactive line plots that allow us to assess how volatile a dependent variable is over a continuous, independent variable (e.g., time) in one or more groups. See also the QIIME notes\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-hct \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-1.qzv\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-2.qzv\n\n\n\n\nThe last plots we’ll generate in this section will come from a QIIME 2 pipeline called feature-volatility. These use supervised regression to identify features that are most associated with changes over time, and add plotting of those features to a volatility control chart.\nAgain, we’ll generate the same plots but using two different time variables on the x-axes. First, we’ll plot based on week-relative-to-hct.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-hct \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility-2\n\nOutputs:\n\nvolatility-plot contains an interactive feature volatility plot. This is very similar to the plots produced by the volatility visualizer described above, with a couple key differences. First, only features are viewable as “metrics” (plotted on the y-axis). Second, feature metadata (feature importances and descriptive statistics) are plotted as bar charts below the volatility plot. The relative frequencies of different features can be plotted in the volatility chart by either selecting the “metric” selection tool, or by clicking on one of the bars in the bar plot. This makes it convenient to select features for viewing based on importance or other feature metadata. By default, the most important feature is plotted in the volatility plot when the visualization is viewed. Different feature metadata can be selected and sorted using the control panel to the right of the bar charts. Most of these should be self-explanatory, except for “cumulative average change” (the cumulative magnitude of change, both positive and negative, across states, and averaged across samples at each state), and “net average change” (positive and negative “cumulative average change” is summed to determine whether a feature increased or decreased in abundance between baseline and end of study).\naccuracy-results display the predictive accuracy of the regression model. This is important to view, as important features are meaningless if the model is inaccurate. See the sample classifier tutorial for more description of regressor accuracy results.\nfeature-importance contains the importance scores of all features. This is viewable in the feature volatility plot, but this artifact is nonetheless output for convenience. See the sample classifier tutorial for more description of feature importance scores.\nfiltered-table is a FeatureTable[RelativeFrequency] artifact containing only important features. This is output for convenience.\nsample-estimator contains the trained sample regressor. This is output for convenience, just in case you plan to regress additional samples. See the sample classifier tutorial for more description of the SampleEstimator type.\n\n\n\n\n\nIf you’re a veteran microbiome scientist and don’t want to use QIIME 2 for your analyses, you can extract your feature table and sequences from the artifact using the export tool. While export only outputs the data, the extract tool allows you to also extract other metadata such as the citations, provenance etc.\nNote that this places generically named files (e.g. feature-table.txt) into the output directory, so you may want to immediately rename the files to something more information (or somehow ensure that they stay in their original directory)!\nYou can also use the handy qiime2R package to import QIIME 2 artifacts directly into R."
  },
  {
    "objectID": "source/Qiime/readme.html",
    "href": "source/Qiime/readme.html",
    "title": "Software information",
    "section": "",
    "text": "QIIME 2 is a platform for microbial community analysis such as 16S amplicon analysis. The QIIME 2 website comes with a wealth of documentation and tutorials that are worthwhile to check out before running QIIME 2 on your own data:\n\nInstallation instructions\nThe QIIME 2 documentation.\nThe cancer microbiome tutorial\nq2book a very detailed, but still unfinished, tutorial on QIIME 2."
  },
  {
    "objectID": "source/R/readme.html",
    "href": "source/R/readme.html",
    "title": "Using R",
    "section": "",
    "text": "R is a language and environment for statistical computing and graphics and is useful to analyse computational data.\nSome useful information to get started:\n\nInstallation guide for R and RStudio\nAn R cookbook including some example files\nTutorial on data manipulation with dplyr\nTutorial on data visualization with ggplot2"
  },
  {
    "objectID": "source/cli/readme.html",
    "href": "source/cli/readme.html",
    "title": "Software information",
    "section": "",
    "text": "The command line interface (cli) is not a software but nevertheless useful for bioinformaticians since it is a text-based user interface used to run programs, manage computer files and interact with the computer.\nYou can use cli as follows (installation instructions tba):\n\nMac: Use the terminal tool (easy to find with spotlight)\nLinux: Use xterm or konsole\nWindows: Use putty, mobaxterm orwindows subsystem for linux (WSL)\n\nIf you want to use Crunchomics, the Genomics Compute Environment for SILS and IBED please check out this documentation.\nIf you are completely unfamiliar with using the command line check out:\n\nAn older session on using the command line. Will be updated to work on Crunchomics in the future\nA tutorial on using AWK, an excellent command line tool for filtering tables, extracting patterns, etc… If you want to follow this tutorial then you can download the required input files from here"
  },
  {
    "objectID": "source/metagenomics/readme.html",
    "href": "source/metagenomics/readme.html",
    "title": "Software information",
    "section": "",
    "text": "Past workshop materials\nMetagenomics is the study of genetic material recovered directly from environmental or other samples using sequencing technologies. Below you find some tutorials explaining how to work with sequencing data, assemble metagenome-assembled genomes (MAGs) from this sequencing data and how to analyse these MAGS. Notice: These tutorials where not developed at UvA and will be updated in the future but will nevertheless give a good starting point in these kind of analyses.\n\nAssembling a metagenome\nGenomic binning\nAnnotating microbial genomes\nHow to do phylogenetic analyses"
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html",
    "title": "Software information",
    "section": "",
    "text": "These are the notes following Evelyn Jongepier’s tutorial on using QIIME2. A large part of the text found here was taken directly from this tutorial but extended as was seen fit.\nThis was run on a personal Windows computer as well as Crunchomics.\nA test data set to run QIIME 2 on was provided and consists of already demultiplexed sequencing data (provided by the sequencing facility). The files we are working with are:\n\nThe forward, R1, reads\nThe reverse, R2, reads\nThe metadata file describing what kind of data we work with (in our case that is simply a sample ID)\n\nOther comments:\n\nbarcode length = ~20\nread length 250 bp\nThe primers used are for 515F-926R (fragment length = 411)\nlength of the primer trimmed fragment (i.e. target region) = 371\ncalculating overlap: (length of forward read) + (length of reverse read) − (length of amplicon) = length of overlap\n\n230 + 230 - 411 = 50\n\n\nOther resources:\n\nQIIME2 docs. Notice, the tutorial runs on v2021.2, which we will use for now and later update to the newest QIIME version\nOld QIIME tutorials\nNEW QIIME tutorials\n\n\n\nNotes were copied from the QIIME2docs.\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (i.e. provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files (generally .qza and .qzv files) without requiring a QIIME installation.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. Semantic types enable QIIME 2 to identify artifacts that are suitable inputs to an analysis. For example, if an analysis expects a distance matrix as input, QIIME 2 can determine which artifacts have a distance matrix semantic type and prevent incompatible artifacts from being used in the analysis (e.g. an artifact representing a phylogenetic tree).\nPlugins are software packages that can be developed by anyone. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses. A method accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts as output. A visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input. In contrast to a method, a visualizer produces exactly one visualization as output.\n\n\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help\n\n\n\n\n\n\n\n\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\ncd metabarcoding-qiime2-datapackage-v2021.06.2\n\n\n\n\nNotice:\n\nChange the path stored in wdir to wherever you downloaded the data\nThis is not needed but useful to have if you resume an analysis after closing the terminal\nIf you don’t have QIIME installed on your personal computer then follow these install instructions. The easiest is to follow the conda/mamba installation instructions.\n\n\n#set working environment\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/QIIME/Evelyn_Qiime/metabarcoding-qiime2-datapackage-v2021.06.2/\"\ncd $wdir\n\n#activate QIIME environment \nconda deactivate\nmamba activate qiime2-2023.7\n\n\n\n\n\n\n\nThe manifest file includes the sample ids, the path to where each sequence read file, i.e. fastq.gz file, is stored, and its orientation. Because we have paired-end data and thus two files for each sample, we will list each sample twice, once for the forward and once for the reverse orientation. This is what the first few lines of the manifest file look like (or run head data/MANIFEST.csv):\nsample-id,absolute-filepath,direction\nDUMMY10,$PWD/data/Sample-DUMMY10_R1.fastq.gz,forward\nDUMMY10,$PWD/data/Sample-DUMMY10_R2.fastq.gz,reverse\nDUMMY11,$PWD/data/Sample-DUMMY11_R1.fastq.gz,forward\nDUMMY11,$PWD/data/Sample-DUMMY11_R2.fastq.gz,reverse\nDUMMY12,$PWD/data/Sample-DUMMY12_R1.fastq.gz,forward\nDUMMY12,$PWD/data/Sample-DUMMY12_R2.fastq.gz,reverse\n\n\n\nComments:\n\nA general tutorial on importing data can be found here\nYou can import data by:\n\nproviding the path to the sequences with --input-path\nOr you can import sequences using the maifest file, which maps the sample identifiers to a fastq.gz or fastq file. It requires a file path, which as to be an absolute filepath (but can use $PWD/) as well as the direction of the reads. The manifest file is compatible with the metadata format\n\nThere are different phred scores used, Phred33 and Phred64. In Phred 64 files, you should not be seeing phred values <64. But you can also look at upper case and lower case of quality representations (ascii letters). In general, lower case means 64.\n\n\nmkdir -p prep\n\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path data/MANIFEST.csv \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#check artifact type\nqiime tools peek prep/demux-seqs.qza \n\n#generate summary\nqiime demux summarize \\\n    --i-data prep/demux-seqs.qza \\\n    --o-visualization prep/demux-seqs.qzv\n\nUseful Options:\n\n--type {TEXT} The semantic type of the artifact that will be created upon importing. Use –show-importable-types to see what importable semantic types are available in the current deployment. [required]\n--input-path {PATH} Path to file or directory that should be imported. [required]\n--output-path {ARTIFACT} Path where output artifact should be written. [required]\n--input-format {TEXT} The format of the data to be imported. If not provided, data must be in the format expected by the semantic type provided via –type.\n--show-importable-types Show the semantic types that can be supplied to –type to import data into an artifact.\n\n\n\n\n\n\n\nFastQC is a tool to judge the quality of sequencing data (in a visual way). It is installed on Crunchomics and if you want to install it on your own machine follow the instructions found here.\n\nmkdir prep/fastqc\nfastqc data/*gz -o prep/fastqc\n\n\n\n\nThe next step is to remove any primer sequences. We will use the cutadapt QIIME2 plugin for that. Because we have paired-end data, there is a forward and reverse primer, referenced by the parameters --p-front-f and --p-front-r\nUseful options:\n\ntrim-paired: Find and remove adapters in demultiplexed paired-end sequences\n-p-front-f {TEXT…} Sequence of an adapter ligated to the 5’ end.\n--p-error-rate {PROPORTION Range(0, 1, inclusive_end=True)} Maximum allowed error rate. [default: 0.1]\n\nNotice:\n\nI played a bit with the error threshold to see how it behaves\n\n\nqiime cutadapt trim-paired \\\n    --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f GTGYCAGCMGCCGCGGTAA \\\n  --p-front-r CCGYCAATTYMTTTRAGTTT \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose\n\nqiime cutadapt trim-paired \\\n    --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f GTGYCAGCMGCCGCGGTAA \\\n  --p-front-r CCGYCAATTYMTTTRAGTTT \\\n  --p-error-rate 0.1 \\\n  --o-trimmed-sequences prep/trimmed-seqs_err0_1.qza \\\n  --p-cores 2 \\\n  --verbose\n\nqiime cutadapt trim-paired \\\n    --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f GTGYCAGCMGCCGCGGTAA \\\n  --p-front-r CCGYCAATTYMTTTRAGTTT \\\n  --p-error-rate 0.2 \\\n  --o-trimmed-sequences prep/trimmed-seqs_err0_2.qza \\\n  --p-cores 2 \\\n  --verbose\n\nUseful options (for the full list, check the help function):\n\nThere are many ways an adaptor can be ligated. Check the help function for all options and if you do not know where the adaptor is contact your sequencing center.\n--p-front-f TEXT… Sequence of an adapter ligated to the 5’ end. Searched in forward read\n--p-front-r TEXT… Sequence of an adapter ligated to the 5’ end. Searched in reverse read\n--p-error-rate PROPORTION Range(0, 1, inclusive_end=True) Maximum allowed error rate. [default: 0.1]\n--p-indels / --p-no-indels Allow insertions or deletions of bases when matching adapters. [default: True]\n--p-times INTEGER Remove multiple occurrences of an adapter if it is Range(1, None) repeated, up to times times. [default: 1]\n--p-match-adapter-wildcards / --p-no-match-adapter-wildcards Interpret IUPAC wildcards (e.g., N) in adapters. [default: True]\n--p-minimum-length INTEGER Range(1, None) Discard reads shorter than specified value. Note, the cutadapt default of 0 has been overridden, because that value produces empty sequence records. [default: 1]\n--p-max-expected-errors NUMBER Range(0, None) Discard reads that exceed maximum expected erroneous nucleotides. [optional] --p-max-n NUMBER Discard reads with more than COUNT N bases. If Range(0, None) COUNT_or_FRACTION is a number between 0 and 1, it is interpreted as a fraction of the read length. [optional] --p-quality-cutoff-5end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 5 prime end. [default: 0] --p-quality-cutoff-3end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 3 prime end. [default: 0] --p-quality-base INTEGER Range(0, None) How the Phred score is encoded (33 or 64). [default: 33]\n\n\n#generate summary \nqiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\nqiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs_err0_1.qzv\n\nqiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs_err0_2.qzv\n\n#view trimmed sequences\nmkdir prep/trimmed_remove_primers\n\nqiime tools extract \\\n    --output-path prep/trimmed_remove_primers \\\n    --input-path prep/trimmed-seqs.qza\n\n#check for R1 sequences: 1,323,201 (all in the beginning it seems) to 38\ncat data/Sample-DUMMY*R1.fastq.gz | zcat | grep -c \"GTG[CT]CAGC[AC]GCCGCGGTAA\"\n\ncat prep/trimmed_remove_primers/c22435d6-299d-414e-931a-098ae6915e2e/data/DUMMY*R1*.fastq.gz | zcat | grep -c \"GTG[CT]CAGC[AC]GCCGCGGTAA\"\n\n#check for R2 sequences: 1,439,600 (all in the beginning it seems) to 38\ncat data/Sample-DUMMY*R2.fastq.gz | zcat | grep -c \"CCG[CT]CAATT[CT][AC]TTT[AG]AGTTT\"\n\ncat prep/trimmed_remove_primers/c22435d6-299d-414e-931a-098ae6915e2e/data/DUMMY*R2*.fastq.gz | zcat | grep -c \"CCG[CT]CAATT[CT][AC]TTT[AG]AGTTT\"\n\n\nThe 38 adaptor sequences found were found inside the sequence, not at the beginning.\nWhen using the 0.1 error rate, we can see that some positions at the end of the reverse reads have quite a low quality score, so we either need to watch this or use the more stringent cutoff.\n\n\n\n\n\nIn order to minimize the risks of sequencer error in targeted sequencing, clustering approaches were initially developed. Clustering approaches are based upon the idea that related/similar organisms will have similar target gene sequences and that rare sequencing errors will have a trivial contribution, if any, to the consensus sequence for these clusters, or operating taxonomic units (OTUs)\nASV methods generating an error model tailored to an individual sequencing run and employing algorithms that use the model to distinguish between true biological sequences and those generated by error. The two ASV methods we explore in this tutorial are Deblur and DADA2.\nDADA2 implements a novel algorithm that models the errors introduced during amplicon sequencing, and uses that error model to infer the true sample composition. DADA2 replaces the traditional “OTU-picking” step in amplicon sequencing workflows, producing instead higher-resolution tables of amplicon sequence variants (ASVs).\nDeblur uses pre-computed error profiles to obtain putative error-free sequences from Illumina MiSeq and HiSeq sequencing platforms. Unlike, Deblur operates on each sample independently. Additionally, reads are depleted from sequencing artifacts either using a set of known sequencing artifacts (such as PhiX) (negative filtering) or using a set of known 16S sequences (positive filtering).\n\n\nNotice:\n\nDepending on the version you might need to change qiime vsearch join-pairs to vsearch merge-pairs.\nSome more details on merging reads can be found here\nIf you plan to use DADA2 to join and denoise your paired end data, do not join your reads prior to denoising with DADA2; DADA2 expects reads that have not yet been joined, and will join the reads for you during the denoising process.\nSome notes on overlap calculation for dada2\nAnother option for joining is fastq-join\n\n\nmkdir -p deblur\n\nqiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 2 \\\n  --verbose\n\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nUseful options:\n\n--p-minlen {INTEGER} Sequences shorter than minlen after truncation are Range(0, None) discarded. [default: 1]\n--p-maxns {INTEGER} Sequences with more than maxns N characters are Range(0, None) discarded. [optional]\n--p-maxdiffs {INTEGER} Maximum number of mismatches in the area of overlap Range(0, None) during merging. [default: 10]\n--p-minmergelen {INTEGER Range(0, None) } Minimum length of the merged read to be retained. [optional]\n--p-maxee {NUMBER} Maximum number of expected errors in the merged read Range(0.0, None) to be retained. [optional]\n--p-threads {INTEGER Range(0, 8, inclusive_end=True)} The number of threads to use for computation. Does not scale much past 4 threads. [default: 1]\n\nExample output from what is printed to the screen:\nMerging reads 100% 45195 Pairs 40730 Merged (90.1%) 4465 Not merged (9.9%)\nPairs that failed merging due to various reasons: 50 too few kmers found on same diagonal 30 multiple potential alignments 1878 too many differences 1790 alignment score too low, or score drop too high 717 staggered read pairs\nStatistics of all reads: 233.02 Mean read length\nStatistics of merged reads: 374.39 Mean fragment length 14.67 Standard deviation of fragment length 0.32 Mean expected error in forward sequences 0.78 Mean expected error in reverse sequences 0.51 Mean expected error in merged sequences 0.21 Mean observed errors in merged region of forward sequences 0.71 Mean observed errors in merged region of reverse sequences 0.92 Mean observed errors in merged region\nIn the end we end up with 1,346,501 joined reads (out of 1,554,929 trimmed reads); so ~86.5% of reads maintained which seems decent.\nNotice that when looking at the plot we see an increased quality score in the middle of the region. When merging reads with tools like vsearch, the quality scores often increase, not decrease, in the region of overlap. The reason being, if the forward and reverse read call the same base at the same position (even if both are low quality), then the quality estimate for the base in that position goes up. That is, you have two independent observations of that base in that position. This is often the benefit of being able to merge paired reads, as you can recover / increase your confidence of the sequence in the region of overlap.\n\n\n\nNotice: Both DADA and deblur will do some quality filtering, so we do not need to be too strict here. Once can consider whether to even include this step, running this might be useful to reduce the dataset site.\n\nqiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\n#summarize stats\nqiime metadata tabulate \\\n   --m-input-file deblur/filt-stats.qza \\\n   --o-visualization deblur/filt-stats.qzv\n\nDefault settings:\n\n--p-min-quality4 All PHRED scores less that this value are considered to be low PHRED scores.\n--p-quality-window 3: The maximum number of low PHRED scores that can be observed in direct succession before truncating a sequence read.\n--p-min-length-fraction 0.75: The minimum length that a sequence read can be following truncation and still be retained. This length should be provided as a fraction of the input sequence length. --p-max-ambiguous 0: The maximum number of ambiguous (i.e., N) base calls.\n\nSummary:\n\nInitially we had 1,346,501 joined reads , after filtering we have 1,346,501 (so nothing was lost). A few reads were trimmed, as can be seen based on the 2% percentile\n\nThese results actually look very, very similar to the summary vizualisation of the joined sequences. In fact, when you look at the trimming stats, nothing really seems to have happened. This is somewhat unusual but so is the quality of this particular data set. Just be prepared that your own data may be of lower quality.\n\n\n\n\nWhen we continue with deblur-denoise in the next step, you need to truncate your fragments such that they are all of the same length. The position at which sequences are truncated is specified by the --p-trim-length parameter. Any sequence that is shorter than this value will be lost from your analyses. Any sequence that is longer will be truncated at this position.\nEvelyn selected a –p-trim-length of 370, because that resulted in minimal data loss. That is, only <9% of the reads were discarded for being too short, and only 4 bases were trimmed off from sequences of median length. Also, we can see that sequences afterwards drop in quality\nPerform sequence quality control for Illumina data using the Deblur workflow with a 16S reference as a positive filter. Only forward reads are supported at this time. The specific reference used is the 88% OTUs from Greengenes 13_8. This mode of operation should only be used when data were generated from a 16S amplicon protocol on an Illumina platform. The reference is only used to assess whether each sequence is likely to be 16S by a local alignment using SortMeRNA with a permissive e-value; the reference is not used to characterize the sequences\ndeblur does an excellent job of denoising and likely produces less false-positives, compared to DADA2 and UNOISE3. It does however seem to get more conservative with longer reads.\n\n\n#default settings\nqiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length 370 \\\n  --o-representative-sequences deblur/deblur-reprseqs2.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 2 \\\n  --verbose\n\nUseful options:\n\n--p-trim-length INTEGER Sequence trim length, specify -1 to disable trimming. [required]\n--p-left-trim-len {INTEGER} Range(0, None) Sequence trimming from the 5’ end. A value of 0 will disable this trim. [default: 0]\n--p-mean-error NUMBER The mean per nucleotide error, used for original sequence estimate. [default: 0.005]\n--p-indel-probNUMBER Insertion/deletion (indel) probability (same for N indels). [default: 0.01]\n--p-indel-max INTEGER Maximum number of insertion/deletions. [default: 3]\n--p-min-reads INTEGER Retain only features appearing at least min-reads times across all samples in the resulting feature table. [default: 10]\n--p-min-size INTEGER In each sample, discard all features with an abundance less than min-size. [default: 2]\n--p-jobs-to-start INTEGER Number of jobs to start (if to run in parallel). [default: 1]\n\nGenerated outputs:\n\nA feature table artifact with the frequencies per sample and feature.\nA representative sequences artifact with one single fasta sequence for each of the features in the feature table.\nA stats artifact with details of how many reads passed each filtering step of the deblur procedure.\n\n\n#look at stats\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats_noremoval.qza \\\n  --o-visualization deblur/deblur-stats_noremova.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table_noremoval.qza \\\n  --o-visualization deblur/deblur-table_noremova.qzv\n\nIf we check the visualization with QIIME 2 view (or qiime tools view deblur/deblur-table.qzv) we get some feelings about how many ASVs were generated and where we lost things. Often times the Interactive sample detail page can be particularily interesting. This shows you the frequency per sample. If there is large variation in this number between samples, it means it is difficult to directly compare samples. In that case it is often recommended to standardize your data by for instance rarefaction. Rarefaction will not be treated in detail in the tutorial, but the idea is laid out below.\n\nOut of the 1,346,802 filtered and merged reads we get 3,636 features with a total frequency of 290,671. So we only retained ~21%. If we compare this to the QIIME CMI tutorial we have 662 features with a total frequency of 1,535,691 (~84) so this is definitely something to watch out for.\nWith this dataset we loose a lot to “fraction-artifact-with-minsize” . This value is related to the --p-min-size parameter which sets the min abundance of a read to be retained (default 2). So we simply might loose things because it is a subsetted dataset and it means that most of your reads are singletons and so are discarded. For a real dataset this could be due to improper merging, not having removed primer/barcodes from your reads, or that there just is that many singletons (though unlikely imo)\nloss of sequences with merged reads in deblur compared to just the forward reads has been observed and discussed before see here\n\n\n\n\nThe code below is not required to be run. However, the code is useful in case something goes wrong and you need to check if the file you work with is corrupted.\n\n#check file: Result deblur/filt-seqs.qza appears to be valid at level=max.\nqiime tools validate deblur/filt-seqs.qza\n\n\n\n\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots to: - identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. - If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. - If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads - Figaro is a tool to automatically choose the trunc_len - Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2 - Your reads must still overlap after truncation in order to merge them later!\n\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trim-left-f 0 \\\n  --p-trim-left-r 0 \\\n  --p-trunc-len-f 230 \\\n  --p-trunc-len-r 220 \\\n  --p-n-threads 4 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose\n\nUseful options:\n\n--p-trunc-len-f INTEGER Position at which forward read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trunc-len-r INTEGER Position at which reverse read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trim-left-f INTEGER Position at which forward read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-trim-left-r INTEGER Position at which reverse read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-max-ee-f NUMBER Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-max-ee-r NUMBER Reverse reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-trunc-q INTEGER Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-f or trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap INTEGER Range(4, None) The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method TEXT Choices(‘independent’, ‘pseudo’) The method used to pool samples for denoising. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method TEXT Choices(‘consensus’, ‘none’, ‘pooled’) The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-min-fold-parent-over-abundance NUMBER The minimum abundance of potential parents of a sequence being tested as chimeric, expressed as a fold-change versus the abundance of the sequence being tested. Values should be greater than or equal to 1 (i.e. parents should be more abundant than the sequence being tested). This parameter has no effect if chimera-method is “none”. [default: 1.0]\n--p-n-threads INTEGER The number of threads to use for multithreaded processing. If 0 is provided, all available cores will be used. [default: 1]\n--p-n-reads-learn INTEGER The number of reads to use when training the error model. Smaller numbers will result in a shorter run time but a less reliable error model. [default: 1000000]\n\nLet’s now summarize the DADA2 output:\n\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nNotes:\n\nSanity check: in the stats visualizations for both DADA2 and deblur check that outside of filtering, there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nsequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the asv table.\n\nComments on the two methods:\n\nOverall there seem to be a bit less reads passing through the DADA2 filters\nDeblur:\n\n3,636 features\n290,671 total frequency (out of 1,346,802 filtered and merged reads, ca 21%)\nnothing at frequencies below 2500, most samples with frequencies between 2500-7500, 1 sample with 20 000\n\nDada2:\n\n13,934 features\n648,666 total frequency (out of 1554929 trimmed reads, ~41%)\nmost samples at 10,000 frequency/sample peak, histrogram looks closer to normal distribution, 1 sample with ~37 000 frequencies\n\n\nOveral Dada2 seems beneficial in that less reads are lost but without some deeper dive into the data the qzv files are not informative enough to decide what happened to the sequences. From some quick checks, it seems that more sequences in dada2 pass the filtering steps, while a lot is lost in the deblur section “fraction-artifact-with-minsize”: - Fraction-artifact-with-minsize is the fraction of sequences detected as artifactual, including those that fall below the minimum length threshold (specified by the –p-trim-length parameter). Fraction-artifact is the fraction of raw sequences that were identified as artifactual - If we check the min frequency we get 2000 with deblur and 0 with dada2, so these two are really not comparable at this stage - If we set both to a sampling depth of 2000 we get - Retained 94,000 (32.34%) features in 47 (100.00%) samples (deblur) instead of 290,671 (100.00%) features in 47 (100.00%) - Retained 94,000 (14.49%) features in 47 (95.92%) samples (dada2) instead of 648,666 (100.00%) features in 49 (100.00%)\nBased on that it does not seem to make much of a difference what one uses.\n\n\n\n\nTo identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods: - classify-consensus-blast and classify-consensus-vsearch are both alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained - Machine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! - Most users do not even need to follow that tutorial and perform that training step, because the lovely QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods we’ve tested based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [@bokulich2018].\nCheck out this study discussing reproducible sequence taxonomy reference database management [@ii2021].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [@kaehler2019].\n\nFor the steps below a classifier using SILVA_138_99 is pre-computed the classifier. Steps outlining how to do this can be found here.\nRequirements:\n\nThe reference sequences\nThe reference taxonomy\n\n\n\n\nqiime tools import \\\n  --type \"FeatureData[Sequence]\" \\\n  --input-path db/SILVA_138_99_16S-ref-seqs.fna \\\n  --output-path db/SILVA_138_99_16S-ref-seqs.qza\n\nqiime tools import \\\n  --type \"FeatureData[Taxonomy]\" \\\n  --input-format HeaderlessTSVTaxonomyFormat \\\n  --input-path db/SILVA_138_99_16S-ref-taxonomy.txt \\\n  --output-path db/SILVA_138_99_16S-ref-taxonomy.qza\n\n#check data: Avg length --> 1456.89\nhead -2 db/SILVA_138_99_16S-ref-seqs.fna\ngrep CP013078.2406498.2408039 db/SILVA_138_99_16S-ref-taxonomy.txt\n\nawk '{/>/&&++a||b+=length()}END{print b/a}' db/SILVA_138_99_16S-ref-seqs.fna\n\n\nUsing full length references to look at partial amplicons seems to have similar performances (depending on the algorithm), maybe slightly lower, check out this paper.\nTrimmed might give better species-level resolution and might make computation faster see here\nTrimming will exclude non-target DNA, effectively denoising the kmer profiles used for training\nIt has been shown that taxonomic classification accuracy of 16S rRNA gene sequences improves when a Naive Bayes classifier is trained on only the region of the target sequences that was sequenced Werner et al., 2012\n\n\n\n\nFrom the trimming step yesterday, we know which primer sequences were used. We can now use these same sequences to extract the corresponding region of the 16S rRNA sequences from the SILVA database, like so:\n\nqiime feature-classifier extract-reads \\\n  --i-sequences db/SILVA_138_99_16S-ref-seqs.qza \\\n  --p-f-primer GTGYCAGCMGCCGCGGTAA \\\n  --p-r-primer CCGYCAATTYMTTTRAGTTT \\\n  --o-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --p-n-jobs 2\n\nOptions:\n\n--p-trim-right 0\n--p-trunc-len 0\n--p-identity {NUMBER} minimum combined primer match identity threshold. [default: 0.8] –p-min-length {INTEGER} Minimum amplicon length. Shorter amplicons are Range(0, None) discarded. Applied after trimming and truncation, so be aware that trimming may impact sequence retention. Set to zero to disable min length filtering. [default: 50]\n--p-max-length 0\n\nThis results in :\n\nMin length: 56 (we have partial sequences)\nMax length: 2316 (indicates we might have some non-16S seqs)\nMean length: 394 (What we would expect based on our 16S data)\n\n\n\n\nNot run on Windows PC since this requires >32 GB RAM\n\nThis classifier is not very specific with respect to which environment the samples come from, because it assumes that all species in the reference database are equally likely to be observed in your samples (i.e., that sea-floor microbes are just as likely to be found in a stool sample as microbes usually associated with stool).\nIt is possible to incorporate environment-specific taxonomic abundance information to improve species inference. This bespoke method has been shown to improve classification accuracy when compared to traditional Naive Bayes classifiers Kaehler et al. 2019. However, there are also some down-sides discussed here\nTo train a classifier using this bespoke method, we need to provide an additional file with taxonomic weigths (see –i-class-weight in the qiime feature-classifier fit-classifier-naive-bayes help function) Pre-assembled taxonomic weights can be found in the readytowear collection at https://github.com/BenKaehler/readytowear.\n\n\nqiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --i-reference-taxonomy db/SILVA_138_99_16S-ref-taxonomy.qza \\\n  --o-classifier db/SILVA_138_99_16S-ref-classifier.qza\n\nCheck on options online, since the help function is not very specific\n\n\n\nrequires ~50 GB of RAM. Don’t run this yourself unless you have lots of RAM on your system.\n\nqiime feature-classifier classify-sklearn \\\n  --i-classifier db/SILVA_138_99_16S-ref-classifier.qza \\\n  --p-n-jobs 16 \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza\n\nArguments:\n\n--p-confidence {VALUE Float % Range(0, 1, inclusive_end=True) | Str % Choices(‘disable’)} Confidence threshold for limiting taxonomic depth. Set to “disable” to disable confidence calculation, or 0 to calculate confidence but not apply it to limit the taxonomic depth of the assignments. [default: 0.7]\n--p-read-orientation {TEXT Choices(‘same’, ‘reverse-complement’, ‘auto’)} Direction of reads with respect to reference sequences. same will cause reads to be classified unchanged; reverse-complement will cause reads to be reversed and complemented prior to classification. “auto” will autodetect orientation based on the confidence estimates for the first 100 reads. [default: ‘auto’]\n\nNext, view the data with:\n\nqiime metadata tabulate \\\n --m-input-file taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nqiime2 tools view taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nWe see:\n\nThat this also reports a confidence score ranging between 0.7 and 1. The lower limit of 0.7 is the default value (see also the qiime feature-classifier classify-sklearn help function). You can opt for a lower value to increase the number of features with a classification, but beware that that will also increase the risk of spurious classifcations!\n\n\n\n\nBefore running the command, we will need to prepair a metadata file. This metadata file should contain information on the samples. For instance, at which depth was the sample taken, from which location does it come, was it subjected to experimental or control treatment etc. etc. This information is of course very specific to the study design but at the very least it should look like this (see also data/META.tsv):\n#SampleID\n#q2:types\nDUMMY1\nDUMMY10\n...\nbut we can add any variables, like so:\n#SampleID    BarcodeSequence Location        depth   location        treatment       grainsize       flowrate        age\n#q2:types    categorical     categorical     categorical     catechorical    categorical     numeric numeric numeric\n<your data>\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n  --m-metadata-file data/META.tsv \\\n  --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv\n\nqiime tools view taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv\n\n\n\n\n\n\n\nThese are just some general comments on filtering ASV tables and are not yet implemented in this specific tutorial. A full description of filtering methods can be found on the QIIME website\n\n#filter using the metadata info \n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#filter out samples from which we have obtained fewer than 10,000 sequences\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#create imaginary list of ids to keep and use this to filter\necho L1S8 >> samples-to-keep.tsv\n\nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\n\n\n\n\n#filter the representative sequences based on a filtered ASV table\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza\n\n\n\n\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza\n\n\n\n\n\nNote: This are general notes and not yet implemented in this tutorial\n\n\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\n\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\n\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. >10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose\n\n\n\n\n\n\n\nNotice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [@gloor2017]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [@weiss2017].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [@mcmurdie2014]\nNormalization and microbial differential abundance strategies depend upon data characteristics [@weiss2017]\n\n\n\n\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible.\n\n\n\n\nNote: These are general notes and not yet implemented in this tutorial\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\n\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)\n\n\n\n\nANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [@mandal2015].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use.\n\n\n\nIf submitting jobs via the srun command it can be useful to submit them via a screen for long running jobs. Some basics on using a screen can be found here.\n\n\nGeneral notice: In Evelyn’s tutorial qiime2-2021.2 was used. For this run, qiime2 was updated to a newer version and the tutorial code adjusted if needed.\nTo be able to access the amplicomics share, and the QIIME 2 installation and necessary databases, please contact Nina Dombrowski with your UvAnetID.\n\n#check what environments we already have\nconda env list\n\n#add amplicomics environment to be able to use the QIIME conda install\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\n#exit server contenction (so steps below were run from fresh login)\n#install newer qiime2 version\n#wget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\n#mamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n\n#to install it in personal environment add hash before amplicomics envs in \n#nano /home/ndombro/.condarc\n#mamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml \n\n\n\n\nIf you have not used SLURM before, please check out the Crunchomics tutorial. Below are two key commands to check memory usage and running things via SLURM is explained in a bit more detail in Section 1.12.\n\n#find past job ids \nsacct -X -o <JOB-ID> --starttime 2023-09-12\n\n#determine run time and mem requirements\nsacct -j <JOB-ID> --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n\n\n\nIf you run this the first time, start by downloading the tutorial data.\n\n#download data data\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\n\n\n\n\nBelow, we set some environmental variables describing the location for key files we want to use. The only thing that you need to change yourself is the path to the working directory, ie. wdir, and the path_to_where_you_downloaded_the_data.\n\nwdir=\"/home/ndombro/tutorials/evelyn_qiime\"\ncd $wdir\n\n#make symbolic link to have the structures in the tutorial work\nln -s path_to_where_you_downloaded_the_data/metabarcoding-qiime2-datapackage-v2021.06.2/data ./\n\n#set environmental variables\nexport MANIFEST=\"data/MANIFEST.csv\"\nexport META=\"data/META.tsv\"\nexport PRIMFWD=\"GTGYCAGCMGCCGCGGTAA\"\nexport PRIMREV=\"CCGYCAATTYMTTTRAGTTT\"\nexport TRIMLENG=370\nexport TRUNKFWD=230\nexport TRUNKREV=220\n\nexport DBSEQ=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-seqs.qza\"\nexport DBTAX=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-tax.qza\"\nexport DBPREFIX=\"SILVA_138_99_16S\"\n\n#activate qiime environment\nmamba activate qiime2-2023.7\n\n\n\n\n\nmkdir -p logs\nmkdir -p prep\nmkdir -p deblur\nmkdir -p dada2\nmkdir -p db\nmkdir -p taxonomy\n\n\n\n\nWhen using Slurm we use the following settings to say that:\n\nthe job consists of 1 task (-n 1)\nthe job needs to be run on 1 cpu (–cpus-per-task 1)\n\n\nsrun -n 1 --cpus-per-task 1 qiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path $MANIFEST \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#create visualization\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/demux-seqs.qza \\\n  --o-visualization prep/demux-seqs.qzv\n\n#instructions to view qvz (tool not available, need to check if I can find it) \n#how-to-view-this-qzv prep/demux-seqs.qzv\n\nSummary of statistics:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\nsrun -n 1 --cpus-per-task 4 qiime cutadapt trim-paired \\\n  --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f $PRIMFWD \\\n  --p-front-r $PRIMREV \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose \\\n  2>&1 | tee logs/qiime-cutadapt-trim-paired.log\n\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#make a table out of this (quick, so ok to run on headnode)\nsrun -n 1 --cpus-per-task 1 qiime tools extract \\\n   --input-path prep/trimmed-seqs.qzv \\\n   --output-path visualizations/trimmed-seqs\n\n\nInformation about the 2>&1 is a redirection operator :\n\nIt says to “redirect stderr (file descriptor 2) to the same location as stdout (file descriptor 1).” This means that both stdout and stderr will be combined and sent to the same destination\n\nThe tee command is used to:\n\nLogging: It captures the standard output (stdout) and standard error (stderr) of the command that precedes it and saves it to a file specified as its argument. In this case, it saves the output and errors to the file logs/qiime-cutadapt-trim-paired.log.\nDisplaying Output: In addition to saving the output to a file, tee also displays the captured output and errors on the terminal in real-time. This allows you to see the progress and any error messages while the command is running.\n\nIf we would want to run the command without tee we could do 2>&1 > logs/qiime-cutadapt-trim-paired.log but this won’t print output to the screen\n\nSummary of run:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\n\n\n\nNote that for the qiime cutadapt trim-paired-command you used p-cores while here you need p-threads\n\nsrun -n 1 --cpus-per-task 4 qiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 4 \\\n  --verbose \\\n  2>&1 | tee logs/qiime-vsearch-join-pairs.log\n\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nNotice: This step cannot be sped up because the qiime quality-filter q-score command does not have an option to use more cpus or threads.\n\nsrun -n 1 --cpus-per-task 1 qiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose \\\n  2>&1 | tee logs/qiime-quality-filter-q-score.log\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\nqiime metadata tabulate \\\n  --m-input-file deblur/filt-stats.qza \\\n  --o-visualization deblur/filt-stats.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\n\nBelow we:\n\nWe defined a memory allocation of 16GB. This is the total amount of RAM you expect to need for this job (+ a bit more to be on the safe side). How much you need is not always easy to predict and requires some experience/trial and error. As a rule of thumb: if you get an Out Of Memory error, double it and try again. Note that you can also define mem-per-cpu, which may be easier to work with if you often change the number of cpus between analyses.\nWe get a warning warn' method is deprecated, use 'warning' instead, but we can ignore that\n\n\n#job id: 398288\nsrun -n 1 --cpus-per-task 10 --mem=16GB qiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length $TRIMLENG \\\n  --o-representative-sequences deblur/deblur-reprseqs.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --p-min-size 2 \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 10 \\\n  --verbose \\\n  2>&1 | tee logs/qiime-deblur-denoise-16S.log\n\n#check memory requirements: 1.6GB\n#maxrss = maximum amount of memory used at any time by any process in that job\nsacct -j 398288 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\ncat deblur.log >> logs/qiime_deblur_denoise-16S.log && rm deblur.log\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data deblur/deblur-reprseqs.qza \\\n  --o-visualization deblur/deblur-reprseqs.qzv\n\n\n\n\n\nDADA2 performs filtering, joining and denoising all with one single command, and therefor takes longer to run. Here, it takes ca. 30m5.334s using 8 cpus. It is tempting to just increase the number of cpus when impatient, but please note that not all commands are very efficient at running more jobs in parallel (i.e. on multiple cpus). The reason often is that the rate-limiting step in the workflow is unable to use multiple cpus, so all but one are idle.\n\n\n#job id: 398291\ntime srun -n 1 --cpus-per-task 8 --mem=32GB qiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trunc-len-f $TRUNKFWD \\\n  --p-trunc-len-r $TRUNKREV \\\n  --p-n-threads 8 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose \\\n  2>&1 | tee logs/qiime-dada2-denoise-paired.log\n\n#get mem usage\nsacct -j 398291 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#get summaries\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nUsage info –> 7.64878 GB needed and 50 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398291            qiime          8              00:49:55\n398291.exte+     extern          8          0   00:49:55\n398291.0          qiime          8   7648780K   00:49:55\n\n\n\n\n\n\n\n#job id: 398333\ntime srun -n 1 --cpus-per-task 8 --mem=4GB qiime feature-classifier extract-reads \\\n  --i-sequences $DBSEQ \\\n  --p-f-primer $PRIMFWD \\\n  --p-r-primer $PRIMREV \\\n  --o-reads db/$DBPREFIX-ref-frags.qza \\\n  --p-n-jobs 8 \\\n  --verbose \\\n  2>&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398333 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nUsage info –> 1.43846 GB needed in 4 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398333            qiime          8              00:04:33\n398333.exte+     extern          8          0   00:04:33\n398333.0          qiime          8   1438460K   00:04:33\n\n\n\nTraining a classifier takes quite some time, especially because the qiime feature-classifier fit-classifier-naive-bayes- command cannot use multiple cpus in parallel (see qiime feature-classifier fit-classifier-naive-bayes –help). Here, it took ca. 145m56.836s. You can however often re-use your classifier, provided you used the same primers and db.\n\n#job id: 398335\ntime srun -n 1 --cpus-per-task 1 --mem=32GB qiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/$DBPREFIX-ref-frags.qza \\\n  --i-reference-taxonomy $DBTAX \\\n  --o-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --p-verbose \\\n  2>&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398335 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nResources needed –> 33.4GB\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398335            qiime          2              02:19:24\n398335.exte+     extern          2          0   02:19:24\n398335.0          qiime          1  33469976K   02:19:24\n\n\n\nImportant\nThe classifier takes up quite some disc space. If you ‘just’ run it you are likely to get a No space left on device-error. You can avoid this by changing the directory where temporary files are written to, but make sure you have sufficient space there as well of course.\n\n#prepare tmp dir \nmkdir -p tmptmp\nexport TMPDIR=$PWD/tmptmp/\n\n#job id: 398339\ntime srun -n 1 --cpus-per-task 32 --mem=160GB qiime feature-classifier classify-sklearn \\\n  --i-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --p-n-jobs 32 \\\n  --verbose \\\n  2>&1 | tee logs/qiime-feature-classifier-classify-sklearn.log\n\n#get mem usage\nsacct -j 398339 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#cleanup\nrm -fr tmptmp\n\nqiime metadata tabulate \\\n  --m-input-file taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxonomy.qzv\n\nResource need –> 158.7 GB:\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398339            qiime         32              00:13:57\n398339.exte+     extern         32          0   00:13:57\n398339.0          qiime         32 158749076K   00:13:57\n\n\n\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --m-metadata-file $META \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxplot.qzv\n\n\n\n\n\n\nscp crunchomics:'/home/ndombro/tutorials/evelyn_qiime/*/*.qzv' crunchomics_files/"
  }
]