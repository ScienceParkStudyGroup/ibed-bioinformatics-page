[
  {
    "objectID": "index.html#useful-tutorials",
    "href": "index.html#useful-tutorials",
    "title": "Bioinformatics guidance page",
    "section": "Useful tutorials",
    "text": "Useful tutorials\nThe Carpentries teaches workshops around the world on the foundational skills to work effectively and reproducibly with data and code and they are an excellent resource to check out if you want to get started with bioinformatics\n\nThe software carpentries provides tutorials on:\n\nBash\nGit\nPython\nR\n\nData carpentries provides domain-specific tutorials, such as for ecology or genomics\nLibrary carpentries contain some useful tutorials if you want to transform data frames, map data to each other and work effectively with data\n\nNext, to the carpentries you will find a list of tutorials for more specific topics below.\n\nGetting started with bash\n\nA tutorial on using bash and an HPC\nVersion control with git\nA tutorial on using AWK, a command line tool for filtering tables, extracting patterns, etc… If you want to follow this tutorial then you can download the required input files from here\n\n\n\nUsing R\n\nAn R cookbook including some example files if you want to code along\nTutorial on data manipulation with dplyr\nTutorial on data visualization with ggplot2\n\n\n\nBioinformatic workflows\n\nFrom sequence file to OTU table with Qiime\nAnalysing an OTU table with R\nAssembling a metagenome\nMetagenomic binning\nAnnotating microbial genomes\nHow to do generate a species tree\nAccessing data from NCBI",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "index.html#bioinformatic-tools-a-z",
    "href": "index.html#bioinformatic-tools-a-z",
    "title": "Bioinformatics guidance page",
    "section": "Bioinformatic tools A-Z",
    "text": "Bioinformatic tools A-Z\n\n\n\n\n\nBioinformatic Tools A-Z\n\n\n\n\n\nA-D\n\n\n\n\nATLAS: A metagenomic pipeline for QC, assembly binning and annotation\n\n\nAugustus: A program that predicts genes in eukaryotic genomic sequences\n\n\nAutocycler: A tool for generating consensus long-read assemblies for microbial genomes\n\n\nBakta: A tool for the rapid & standardized annotation of bacterial genomes and plasmids from both isolates and MAGs\n\n\nBarrnap: A tool to predict the location of ribosomal RNA genes in genomes.\n\n\nBMGE: A program to select regions in a multiple sequence alignment that are suited for phylogenetic inference\n\n\nBowtie2: A tool for aligning sequencing reads to genomes and other reference sequences\n\n\nBUSCO: Quality assessment of (meta)genomes, transcriptomes and proteomes\n\n\nCheckM2: A tool to assess the quality of a genome assembly\n\n\nChopper: A tool for quality filtering of long read data\n\n\nCoverM: A DNA read coverage and relative abundance calculator focused on metagenomics applications\n\n\nDeepLoc: A tool to predict the subcellular localization(s) of eukaryotic proteins\n\n\nDiamond: A sequence aligner for protein and translated DNA searches\n\n\nDnaapler: A tool to re-orient a genome, for example at dnaA\n\n\nDeSeq2: Analyse gene expression data in R\n\n\n\n\nE-H\n\n\n\n\nFAMA: A fast pipeline for functional and taxonomic analysis of metagenomic sequences\n\n\nFastP: A tool for fast all-in-one preprocessing of FastQ files\n\n\nFastQC: A quality control tool for read sequencing data\n\n\nFeatureCounts: A read summarization program that counts mapped reads for genomic features\n\n\nFiltlong: A tool for filtering long reads\n\n\nFlye: A de novo assembler for single-molecule sequencing reads\n\n\nGTDB_tk: A software toolkit for assigning objective taxonomic classifications to bacterial and archaeal genomes\n\n\nHMMER: A tool for searching sequence databases for sequence homologs, and for making sequence alignments\n\n\nHomopolish: A tool for the removal of systematic errors in nanopore sequencing by homologous polishing\n\n\n\n\nI-L\n\n\n\n\nIQ-TREE: A tool for phylogenomic inferences\n\n\nInterproscan: A tool to scan protein and nucleic sequences against InterPro signatures\n\n\nITSx: A tool to extract ITS1 and ITS2 subregions from ITS sequences\n\n\nKraken2: A taxonomic sequence classifier using kmers\n\n\n\n\nM-P\n\n\n\n\nMafft: A multiple sequence alignment program\n\n\nMedaka: A tool for assembly polishing\n\n\nMETABOLIC: A tool to predict functional trait profiles in genome datasets\n\n\nMetaCerberus: A tool for functional assignment\n\n\nMOTUS: A tool to estimate microbial abundances in Illumina and Nanopore sequencing data\n\n\nMinimap2: A program to align DNA or mRNA sequences against a reference database\n\n\nMultiQC: A program to summarize analysis reports\n\n\nNanoClass2: A taxonomic meta-classifier for long-read 16S/18S rRNA gene sequencing data\n\n\nNanoITS: A taxonomic meta-classifier for long-read ITS operon sequencing data\n\n\nNanophase: A pipeline to generate MAGs using Nanopore long and Illumina short reads from metagenomes\n\n\nNanoPlot: Plotting tool for long read sequencing data\n\n\nNanoQC: A quality control tool for long read sequencing data\n\n\nNGSpeciesID: A tool for clustering and consensus forming of long-read amplicon sequencing data\n\n\nPorechop: A tool for finding and removing adapters from Nanopore reads\n\n\nProkka: A tool to annotate bacterial, archaeal and viral genomes\n\n\nPseudofinder: A tool that detects pseudogene candidates from annotated genbank files of bacterial and archaeal genomes\n\n\npyCirclize: A tool for circular visualization, i.e. genome plots, in python\n\n\npyGenomeTracks: A tool to produce high-quality genome browser tracks\n\n\n\n\nQ-T\n\n\n\n\nQUAST: A Quality Assessment Tool for Genome Assemblies\n\n\nRibodetector: Detect and remove rRNA sequences from metagenomic, metatranscriptomic, and ncRNA sequencing data\n\n\nRSeQC: A tool to evaluate high throughput sequence data especially RNA-seq data\n\n\nRSEM: A software package for estimating gene and isoform expression levels from RNA-Seq data\n\n\nSamtools: A tool to manipulating alignments in SAM/BAM format\n\n\nSeqKit: A tool for FASTA/Q file manipulation\n\n\nSignalP6: A tool to predict the presence of signal peptides\n\n\nSingleM: A tool for taxonomic profiling of shotgun metagenomes\n\n\nSortMerNa: A tool to filter ribosomal RNAs in metatranscriptomic data\n\n\nSTAR: An ultrafast universal RNA-seq aligner\n\n\nStringTie: A a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts\n\n\nTransDecoder: A tool to identify candidate coding regions within transcript sequences\n\n\nTrycyler: A tool for generating consensus long-read assemblies for bacterial genomes\n\n\nTrinity: A tool to assemble transcript sequences from Illumina RNA-Seq data\n\n\n\n\nU-Z",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "index.html#bioinformatic-toolbox",
    "href": "index.html#bioinformatic-toolbox",
    "title": "Bioinformatics guidance page",
    "section": "Bioinformatic toolbox",
    "text": "Bioinformatic toolbox\n\nFor-loops-in-bash: How can I scale my research and run software not only on one but multiple files?",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "index.html#useful-databases-a-z",
    "href": "index.html#useful-databases-a-z",
    "title": "Bioinformatics guidance page",
    "section": "Useful databases A-Z",
    "text": "Useful databases A-Z\n\nCOG\nKOfam\nNCBI-nr\nPfam\nPGAP database\nTIGRFAM",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "source/template.html#tool",
    "href": "source/template.html#tool",
    "title": "Bioinformatics guidance page",
    "section": "Tool",
    "text": "Tool\n\nIntroduction\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nProkka v1.14.6 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\n\nUsage\nUseful options:"
  },
  {
    "objectID": "source/phylogenomics/mafft.html#mafft",
    "href": "source/phylogenomics/mafft.html#mafft",
    "title": "Bioinformatics guidance page",
    "section": "Mafft",
    "text": "Mafft\n\nIntroduction\nMAFFT (Katoh 2002) is a multiple sequence alignment program for unix-like operating systems. It offers a range of multiple alignment methods, L-INS-i (accurate; for alignment of &lt;∼200 sequences), FFT-NS-2 (fast; for alignment of &lt;∼30,000 sequences), and so on. For more information, please visit the mafft website.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nMafft v7.525 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -n mafft -c bioconda mafft\n\n\n\nUsage\nExample usage:\n\nconda activate mafft_7.525\n\nmafft-linsi --reorder --thread 20 \\\n    my_protein_file.faa &gt; my_protein_file.aln\n\nconda deactivate\n\nNotice:\n\nlinsi is an alias for an accurate option (L-INS-i) for an alignment of up to ∼200-1000 sequences × ∼2,000 sites.\nBy default, mafft uses a fast option (FFT-NS-2)\nIf unsure what alignment option to use, you can also use --auto or check all available options in the manual\n\nUseful options (for a full list, please visit the manual):\n\n--auto Automatically selects an appropriate strategy from L-INS-i, FFT-NS-i and FFT-NS-2, according to data size. Default: off (always FFT-NS-2)\n\n--maxiterate number number cycles of iterative refinement are performed. Default: 0\n--reorder: Output order: aligned. Default: off (inputorder). This can be useful if you visually inspect the alignments, as outliers tend to appear at the bottom\n--anysymbol: To be able to allow unusual characters (e.g., U as selenocysteine in protein sequence; i as inosine in nucleotide sequence),we have to use this option\n\nIf you want to inspect your alignment, we also provide a tool on the bioinformatics server to be able to do this (to better find conserved sites go to Colour –&gt; Clustal):\n\nconda activate jalview_2.11.3.3\n\njalview my_protein_file.aln\n\nconda deactivate",
    "crumbs": [
      "Sequence data analyses",
      "Phylogenomics",
      "Mafft"
    ]
  },
  {
    "objectID": "source/phylogenomics/bmge.html#bmge",
    "href": "source/phylogenomics/bmge.html#bmge",
    "title": "Bioinformatics guidance page",
    "section": "BMGE",
    "text": "BMGE\n\nIntroduction\nBMGE (Block Mapping and Gathering with Entropy) is a command line program written in Java to select regions in a multiple sequence alignment that are suited for phylogenetic inference (Criscuolo and Gribaldo 2010). To find out more, visit the tools website and the manual.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nBMGE v1.12 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -n bmge -c bioconda bmge\n\n\n\nUsage\nExample usage:\n\nbmge -i my_alignment.aln -t AA -m BLOSUM30 -h 0.55 -of my_alignment_trimmed.aln\n\nUseful options (for a full set of options, please visit the manual):\n\n-t [AA,DNA,CODON]: sequence coding of the input\n-of: name of output file and generates the selected characters in FASTA format\n-m BLOSUMn: Used similarity matrix.\n\nWith amino acid input sequences (-t AA), BMGE uses by default the popular BLOSUM62 matrix (Eddy 2004). However, one can use another BLOSUM matrix as shown in the example above. The trimming is progressively more stringent as the BLOSUM index increases (e.g. BLOSUM95); reciprocally, the trimming is progressively more relaxed as the BLOSUM index is lower (e.g. BLOSUM30). In practice, it is recommended to use BLOSUM95 with closely related sequences, and BLOSUM30 with distantly related sequences\nFor nucleotide input sequences (-t DNA), BMGE uses PAM matrices with a fixed transition/transition ratio. BMGE can be used with all possible PAM matrices, from the most stringent (i.e. -m DNAPAM1) to highly relaxed ones (e.g. -m DNAPAM500). It is also possible to indicate a transition/transversion ratio to better define the PAM matrices. For example, if one wishes to estimate entropy-like scores with a (relaxed) PAM-250 matrix and a transition/transversion ratio of 4, then one uses the following DNAPAM250:4\n\n-h n: Entropy cutoff. Following the smoothing operation of the entropy-like score values across characters, BMGE selects characters associated with a score value below a fixed threshold. This cut-off is set to 0.5 by default, but it can be modified with the option -h. In the example above, BMGE estimates stringent entropy-like scores, but it only selects the characters with a score smaller than 0.55\n-g col_rate: BMGE allows characters containing too many gaps to be removed with the option -g. By default, BMGE removes all characters with a gap frequency greater than 0.2. For example, to perform default trimming operations and the removal of all characters with more than 5% we can use -g 0.05",
    "crumbs": [
      "Sequence data analyses",
      "Phylogenomics",
      "BMGE"
    ]
  },
  {
    "objectID": "source/nanopore/readme.html#general",
    "href": "source/nanopore/readme.html#general",
    "title": "Bioinformatics guidance page",
    "section": "General",
    "text": "General\nIn this section you find tools to analyse long-read sequencing data. There might be a bit of overlap with the section “Amplicon sequencing data” but in the Nanopore analysis section you specifically will find tools suitable for dealing with long read data generated with Nanopore."
  },
  {
    "objectID": "source/nanopore/ngspeciesid.html#ngspeciesid",
    "href": "source/nanopore/ngspeciesid.html#ngspeciesid",
    "title": "Bioinformatics guidance page",
    "section": "NGSpeciesID",
    "text": "NGSpeciesID\n\nIntroduction\nNGSpeciesID is a tool for clustering and consensus forming of long-read amplicon sequencing data (has been used with both PacBio and Oxford Nanopore data) (Sahlin, Lim, and Prost 2021).\nFor more information, please visit the tools Github page.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nNGSpeciesID v0.3.0 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\n# Setup environment\nmamba create -p NGSpeciesID_0.3.0 python=3.6 pip \n\n# Install NGSpeciesID inside the new env\nmamba activate NGSpeciesID_0.3.0 \nmamba install --yes -c conda-forge -c bioconda medaka==0.11.5 openblas==0.3.3 spoa racon minimap2 pysam==0.15.2\npip install NGSpeciesID\npip install absl-py==1.1.0\n\n# Check if help is available\nNGSpeciesID --help\n\n# Test install with provided files found on the github repo \ncurl -LO https://raw.githubusercontent.com/ksahlin/NGSpeciesID/master/test/sample_h1.fastq\nNGSpeciesID --ont --fastq sample_h1.fastq --outfolder ./sample_h1 --consensus --medaka\nconda deactivate\n\n\n\nUsage\n\nBasic Usage\nGeneral comments:\n\nNGSpeciesID takes as input a fastq file generated by an Oxford Nanopore basecaller\nThere are different settings to control that will differ depending on your dataset (single species amplicon versus amplicons generated from a community) and quality of your data. The most critical parameters to adjust are:\n\n--abundance_ratio: This setting defines what clusters are refined. Only clusters larger than a fraction of number of total reads are considered (default 0.1). When working with community amplicon data it is recommended to set this value lower than the default as for example with a read depth of 10,000 reads only clusters are considered if they are covered by 1000 reads thereby removing many low abundant taxa. However, you also do not want to go too low and want clusters be represented by at least 50-100 reads in order to generate a useful consensus sequence.\n--aligned_threshold: Defines the the minimum fraction of the read that must be aligned to be included in a cluster (default 0.4). This means that at least 40 bases out of the 100 bases must be aligned to the reference, even if those 40 bases include mismatches or gaps. This threshold is affected by the quality of the reads.\n--mapped_threshold: Defines the minimum fraction of a read that must be classified as mapped to a reference to be considered part of a cluster (default: 0.7). This threshold is affected by the read length variation. Assume you work with a 1500 bp amplicon, using the default cutoff this means that 70% of the read, i.e. 1050 bp, must be mapped.\n\n\nBasic usage:\n\nconda activate NGSpeciesID_0.3.0 \n\nNGSpeciesID --ont --fastq data/my_data.fastq \\\n--outfolder results/ \\\n--consensus --medaka --t 20 --aligned_threshold 0.8 --abundance_ratio 0.01 --mapped_threshold 0.7\n\nconda deactivate\n\nUseful options (for a full list of options please use the tools help function and visit the manual):\n\n--q QUALITY_THRESHOLD: Filters reads with average phred quality value under this number (default = 7.0). (default: 7.0)\n--consensus After clustering, (1) run spoa on all clusters, (2) detect reverse complements, (3) run medaka.\n--abundance_ratioABUNDANCE_RATIO Threshold for –consensus algorithm. Consider only clusters larger than a fraction of number of total reads (default 0.1) (default: 0.1)\n--m TARGET_LENGTH Intended amplicon length. Invoked to filter out reads with length greater than m + s or smaller than m - s (default = 0 which means no filtering) (default: 0)\n--s TARGET_DEVIATION Maximum allowed amplicon-length deviation. Invoked to filter out reads with length greater than m + s or smaller than m - s (default = 0 which means no filtering) (default: 0)\n--rc_identity_threshold RC_IDENTITY_THRESHOLD: Threshold for –consensus algorithm. Define a reverse complement if identity is over this threshold (default 0.9) (default: 0.9)\n--sample_size SAMPLE_SIZE Use sample_size reads in the NGSpecies pipeline (default = 0 which means all reads considered). If sample size is larger than actual number of reads, all reads will be used. (default: 0)\n--mapped_threshold MAPPED_THRESHOLD Minmum mapped fraction of read to be included in cluster. The density of minimizers to classify a region as mapped depends on quality of the read. (default: 0.7)\n--aligned_threshold ALIGNED_THRESHOLD Minmum aligned fraction of read to be included in cluster. Aligned identity depends on the quality of the read. (default: 0.4)\n\nGenerated outputs:\n\nPolished consensus sequence(s). A folder named “medaka_cl_id_X”[/“racon_cl_id_X”] is created for each predicted consensus. Each such folder contains a sequence “consensus.fasta” which is the final output of NGSpeciesID.\nDraft spoa consensus sequences of each of the clusters are given as consensus_reference_X.fasta (where X is a number).\nThe final cluster information is given in a tsv file final_clusters.tsv present in the specified output folder.\n\nGeneral recommendations:\n\nThe ideal settings will depend on the quality of your data and it is recommended to test different thresholds for the abundance_ratio (depending on the average phred score of your data) as well as mapped_threshold and aligned_threshold (depending on the average phred score and length of your reads)\nTo compare results you can look at:\n\nHow many consensus sequences were generated for each run\nHow many multi-mappers map back to the consensus sequences\n\n\n\n\nFor multiple samples (advanced)\nTo run Autocycler on more than one genome you can run the code above with either a for loop or with a job scheduler, such as SLURM. To do so, we also provide a bash script. This script:\n\nruns NGSpeciesID on every .fastq.gz file found in the input directory. Output files are named using the name of the input file, so name these accordingly\nparses the output of NGSpeciesID. Specifically, the header of the consensus sequences is shortened and individual consensus sequences are combined into a single file named based on the input file\n\nThe script, ngspeciesid_pipeline_polished.sh, is available on Crunchomics at /zfs/omics/projects/bioinformatics/scripts/ or can be downloaded here. This scripts requires the following arguments to run:\n\n-r &lt;desired_read_nr&gt; Total reads threshold (default: 100). Consider only clusters larger than a number of total reads. Important: This behaves a bit different than the default settings found in NGSpeciesID. Instead of using a proportion, which can result in varying thresholds when running this on samples with different total read numbers, this wrapper tries to only work with clusters that have approximately 100 reads when using default settings.\n-a &lt;aln_thres&gt; Alignment threshold (default: 0.4). Minimum aligned fraction of read to be included in the cluster\n-m &lt;mapped_thres&gt; Mapped threshold (default: 0.7). Minimum mapped fraction of read to be included in the cluster\n-p &lt;polishing_method&gt; The polishing method to use: ‘medaka’ or ‘racon’.\n-d &lt;run_dir&gt; A unique name for this run, used to organize output files. Useful when testing different input parameters.\n-i &lt;input_dir&gt; The directory containing input FASTQ files. Workflow does not work on compressed files or files with an extension other than fastq.\n-o &lt;output_dir&gt; The base directory where output files will be saved.\n-l &lt;log_dir&gt; The directory where log files will be saved.\n-t &lt;threads&gt; Number of threads to use (default: 20).\n\nExample usage:\n\nconda activate NGSpeciesID_0.3.0 \n\nbash ./ngspeciesid_pipeline_polished.sh \\\n    -r 100 -m 0.7 -a 0.6 -t 20 -d run1 -p medaka \\\n    -i data/ \\\n    -o results/ngspeciesID -l logs\n\nconda deactivate",
    "crumbs": [
      "Sequence data analyses",
      "Nanopore analyses",
      "NGSpeciesID"
    ]
  },
  {
    "objectID": "source/nanopore/nanoplot_readme.html#nanoplot",
    "href": "source/nanopore/nanoplot_readme.html#nanoplot",
    "title": "Bioinformatics guidance page",
    "section": "Nanoplot",
    "text": "Nanoplot\n\nIntroduction\nNanoPlot is a plotting tool for long read sequencing data and alignment (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nNanoplot v1.42.0 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nNanoPlot is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools \n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\nUsage\nPossible input formats :\n\nfastq files (can be bgzip, bzip2 or gzip compressed)\nfastq files generated by albacore, guppy or MinKNOW containing additional information (can be bgzip, bzip2 or gzip compressed)\n\nsorted bam files\n\nsequencing_summary.txt output table generated by albacore, guppy or MinKnow basecalling (can be gzip, bz2, zip and xz compressed)\nfasta files (can be bgzip, bzip2 or gzip compressed); Multiple files of the same type can be offered simultaneously\n\nOutput:\n\na statistical summary\na number of plots\na html summary file\n\nExample code:\n\n#start environment\nconda activate nanoplot_1.42.0\n\n#run on a single file (if you are using other inputs, check the readme for the appropriate flag)\nNanoPlot --fastq myfile.fastq.gz -o outputfolder --threads 1\n\nconda deactivate\n\nUseful arguments (for the full version, check the manual):\n\n--tsv_stats Output the stats file as a properly formatted TSV.\n--info_in_report Add NanoPlot run info in the report.\n--barcoded Use if you want to split the summary file by barcode\n-f, --format {[{png,jpg,jpeg,webp,svg,pdf,eps,json} …] } Specify the output format of the plots, which are in addition to the html files\n\n\n\nReferences",
    "crumbs": [
      "Sequence data analyses",
      "Nanopore analyses",
      "Nanoplot"
    ]
  },
  {
    "objectID": "source/nanopore/nanoclass.html#nanoclass2",
    "href": "source/nanopore/nanoclass.html#nanoclass2",
    "title": "Bioinformatics guidance page",
    "section": "NanoClass2",
    "text": "NanoClass2\n\nIntroduction\nNanoClass2 is an updated version of NanoClass, a snakemake workflow originally developed by IBED’s former bioinformatician Evelien Jongepier.\nNanoClass2 is a taxonomic meta-classifier for meta-barcoding 16S/18S rRNA gene sequencing data generated with the Oxford Nanopore Technology. With a single command, this Snakemake pipeline installs all programs and databases and will do the following:\n\nRemove adapters with porechop\nPerform quality filtering with chopper\nGenerate read quality plots post quality cleaning with pistis\nSubsample reads to compare different classifiers (can be turned of)\nClassify reads with any one of these tools: blastn, centrifuge, dcmegablast, idtaxa, megablast, minimap, mothur, qiime, rdp, spingo, kraken. By default NanoClass2 will use the SILVA_138.1_SSURef_NR99 database to classify reads. If you are interested in using other databases, check out these instructions.\n\nFor more details, feel free to also have a look at the manual.\n\n\nRunning NanoClass2 on Crunchomics\n\nGetting started\nNanoClass2 is installed on the amplicomics share on the UvA Crunchomics HPC. If you have access to Crunchomics you can be added to the amplicomics share in which NanoClass2 is set up. To be added, please send an email with your Uva netID to Nina Dombrowski.\nIf you want or need to set up NanoClass2 by yourself, expand the code below to view an example on how to install the software:\n\n\nShow the code\n# Install snakemake as a conda/mamba environment\nmamba create -c conda-forge -c bioconda --prefix /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2 python=3.9.7 snakemake=6.8.0 tabulate=0.8\n\n# Change directory to folder in which to install software \ncd /zfs/omics/projects/amplicomics/bin\n\n# Install NanoClass2\ngit clone https://github.com/ndombrowski/NanoClass2.git\n\n\nTo be able to use software installed on the amplicomics share, you first need to ensure that conda is installed. If it is, then you can run the following command:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\nThis command will add pre-installed conda environments in the amplicomics share to your conda env list. After you run conda env list you should see several tools from amplicomics, including QIIME2 and Snakemake. Snakemake is the software that we need in order to run NanoClass2.\n\n\nSetup a testrun\nThe test run will be performed with some example data provided with NanoClass2. The test run will subsample the reads and use the subsetted reads to test all classifiers available with NanoClass2.\nFirst, we set up a working environment. Change the path of the working directory to wherever you want to analyze your data:\n\n# Define the directory in which you want to run your analyses\nwdir=\"/home/${USER}/personal/testing/amplicomics_test\"\n\n# Change to the working directory\ncd $wdir\n\n# Activate the snakemake conda environment \nconda activate snakemake_nanoclass2\n\nNext, we copy over the test data, the mapping file, and the config file that comes with NanoClass2.\n\n# Copy some test data over (comes with NanoClass2) \nmkdir data \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2_Silva_1.0/example_files/*fastq.gz data/\n\n# Copy the mapping file (comes with NanoClass2) \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2_Silva_1.0/example_files/mapping.csv .\n\n# Ensure that the path to the fastq data is correct in mapping.csv\n# you can keep the text in the `search` variable as is\n# you need to change the path in `replace` and add the path to where you copied the fastq.gz files\nsearch='your_path/example_files/'\nreplace='/home/ndombro/personal/testing/amplicomics_test/data/'\n\nsed -i -e  \"s|$search|$replace|\" mapping.csv\n\n# Copy config yaml (comes with NanoClass2) \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2_Silva_1.0/config.yaml .\n\nWhen setting up mapping.csv for your own samples, make sure to use the absolute path. In the example above, we use /home/ndombro/personal/testing/amplicomics_test/data/ and not data/. Don’t use the relative path, as NanoClass2 will look for the input files relative to where it is installed and not to where your working directory is (or from where you want to execute NanoClass2).\nIn the config.yaml file we can tell NanoClass2 where the data is located and how to run NanoClass2. For the test-run, we only need to:\n\nexchange \"example_files/mapping.csv\" with \"mapping.csv\"\nkeep the rest as is, i.e. we want to subsample our reads for the test run\n\nBy default the config.yaml is setup to run NanoClass2 as follows:\n\nSubsample the fastq files to only work with 100 reads per sample.\n\nThis allows to test all or a subset of classifiers with a subset of the data.\nTo turn this of, set skip: true.\nImportant: Once you decided on a final classifier you want to turn subsampling off by setting skip:true in order to run NanoClass2 on all reads.\n\nRun NanoClass2 using all 11 classifiers. If you already have a favorite classifier you can only select the tool that you want to use.\nQuality filter the samples:\n\nDiscard reads shorter than 1400 bp (minlen)\nDiscard reads longer than 1600bp (maxlen)\nDiscard reads with a phred score less than 10 (quality)\nDo not trim nucleotides from the start (headcrop)\nDo not trim nucleotides from the end (tailcrop)\n\nFeel free to check the config file for more details about these and other parameters that can be adjusted\n\n\nPerform a dry-run\nWhen running snakemake with -np this will not run NanoClass itself but only perform a dry-run, which is useful to ensure that everything works correctly. The command below should work as it is as long as the config.yaml is located in the same directory from which you start the analysis.\n\n# Perform a dry-run\nsnakemake --cores 1 \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2_Silva_1.0/Snakefile \\\n    --configfile config.yaml \\\n    --use-conda --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2_Silva_1.0/.snakemake/conda \\\n    --nolock --rerun-incomplete -np\n\nIf the above command works and you see This was a dry-run (flag -n). The order of jobs does not reflect the order of execution. all is good and you can submit things to a compute node.\n\n\nSubmit sbatch job\nThe NanoClass2 installation comes with an example sbatch script that you can move over to where you do your analyses. This script allows you to submit the job to the compute nodes.\n\n# Copy a template batch script (comes with NanoClass2) \ncp /zfs/omics/projects/amplicomics/bin/NanoClass2_Silva_1.0/jobscript.sh .\n\nThe jobscript is setup to work with NanoClass2 in the amplicomics environment and you should not need to change anything if your config.yaml is set up correctly.\nThe script does the following:\n\nRequests 58 crunchomics cpus and 460G of memory. One can set this lower, but for testing purposes you might keep these values. The can be changed with #SBATCH --cpus-per-task=58 and #SBATCH --mem=460G\nEnsures that conda is loaded properly by running source ~/.bashrc. The bashrc is a script executed once a user logs in and holds special configurations, such as telling the shell where conda is installed. If you run into issues that are related to conda/mamba not being found then open the this file with nano ~/.bashrc and check that you have a section with text similar to # &gt;&gt;&gt; conda initialize &gt;&gt;&gt;. If this is not present you might need to run conda init bash to add it.\nActivates the snakemake conda environment that is found in the amplicomics share with conda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\nRuns NanoClass2. When using the script as is, the script assumes that NanoClass2 is installed at this path /zfs/omics/projects/amplicomics/bin/NanoClass2_Silva_1.0/ and that the config.yaml is located in the folder from which you analyse the data. If that is not the case, change the paths accordingly\n\nNext, we can submit the job as follows:\n\n# Submit a batch job\nsbatch jobscript.sh\n\n# For checking how far things are along:\n# check the end of the log file i.e. \ntail *.log \n\n# Check if sbatch script is still running \nsqueue\n\n\n\nGenerate report\nFinally, we can create a report with the following command:\n\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoClass2_Silva_1.0/Snakefile\n\nTo view the report, its best to copy the html to your computer and open it via any web browser.\nThe report holds the key information, however, there are other useful outputs, all of which can be found in the results folder. The NanoClass2 website gives some more information about the generated outputs.\n\n\n\nRunning NanoClass2 with other databases\nMost classification tools implemented in NanoClass2 can use alternative databases supplied by the user, provided they are formatted correctly. The database that is provided in the amplicomics share is the SILVA 16S or 18S databases.\nIf you want to work with a custom database, users will need to:\n\nInstall NanoClass2 for themselves as one installation is needed for each database of interest\nProvide the databases themselves and store them in the db/common/ subdirectory of the NanoClass2 directory. These will then be automatically detected once NanoClass2 is started and NanoClass2 will create and reformat all tools-specific databases based on this user-provided database.\n\nFor an example on how to format a custom database, check out these instructions.\n\n\nAdditional Resources for UvA Users: Cyanoseq database\nFor UvA researchers, the Cyanoseq v1.3 database with GSR-DB as the bacterial database is available on the amplicomics share. This database is a curated database of 16S rRNA gene sequences from cyanobacteria.\nTo run NanoClass2 with Cyanoseq, you can follow the notes of this tutorial. However, instead of using the Nanoclass2 installation in /zfs/omics/projects/amplicomics/bin/NanoClass2_Silva_1.0/, you should use the installation in /zfs/omics/projects/amplicomics/bin/NanoClass2_CyanoSeq_1.0/.",
    "crumbs": [
      "Sequence data analyses",
      "Nanopore analyses",
      "NanoClass2"
    ]
  },
  {
    "objectID": "source/nanopore/filtlong.html#filtlong",
    "href": "source/nanopore/filtlong.html#filtlong",
    "title": "Bioinformatics guidance page",
    "section": "FiltLong",
    "text": "FiltLong\n\nIntroduction\nFiltlong is a tool for filtering long reads by quality. It can take a set of long reads and produce a smaller, better subset. It uses both read length (longer is better) and read identity (higher is better) when choosing which reads pass the filter.\nFor more information, visit the manual\n\n\nInstallation\nInstalled on crunchomics: Yes, as part of the bioinformatics share. If you have access to crunchomics you can be added to the bioinformatics share by sending an email with your Uva netID to Nina Dombrowski.\nIf you want to install it yourself, you can run:\n\nmamba create --name filtlong_0.2.1 -c bioconda filtlong=0.2.1\n\n\n\nUsage\n\n#throw out the worst 10% of reads. This is measured by bp, not by read count. So this option throws out the worst 10% of read bases\n#a length weight of 10 (instead of the default of 1) makes read length more important when choosing the best read\nfiltlong --min_length 1000 --keep_percent 90 --mean_q_weight 10 input.fastq.gz | gzip &gt; input_filtered.fastq.gz\n\nUseful options:\n\noutput thresholds:\n\n-t[int], --target_bases [int] keep only the best reads up to this many total bases\n-p[float], --keep_percent [float] keep only this percentage of the best reads (measured by bases)\n--min_length [int] minimum length threshold\n--max_length [int] maximum length threshold\n--min_mean_q [float] minimum mean quality threshold\n--min_window_q [float] minimum window quality threshold\n\nexternal references (if provided, read quality will be determined using these instead of from the Phred scores):\n\n-a[file], --assembly [file] reference assembly in FASTA format\n-1[file], --illumina_1 [file] reference Illumina reads in FASTQ format\n-2[file], --illumina_2 [file] reference Illumina reads in FASTQ format\n\nscore weights (control the relative contribution of each score to the final read score): --length_weight [float] weight given to the length score (default: 1) --mean_q_weight [float] weight given to the mean quality score (default: 1) --window_q_weight [float] weight given to the window quality score (default: 1)\nread manipulation: --trim trim non-k-mer-matching bases from start/end of reads --split [split] split reads at this many (or more) consecutive non-k-mer-matching bases\nother:\n\n--window_size [int] size of sliding window used when measuring window quality (default: 250) ---verbose verbose output to stderr with info for each read\n--version display the program version and quit\n\n-h, --help display this help menu",
    "crumbs": [
      "Sequence data analyses",
      "Nanopore analyses",
      "FiltLong"
    ]
  },
  {
    "objectID": "source/nanopore/autocycler.html#autocycler",
    "href": "source/nanopore/autocycler.html#autocycler",
    "title": "Bioinformatics guidance page",
    "section": "Autocycler",
    "text": "Autocycler\n\nIntroduction\nAutocycler is a tool for generating consensus long-read assemblies for microbial genomes (Wick 2025). It is the successor to Trycyler, and for most users it is recommended to Autocycler over Trycycler. If you want to use Autocycler, keep in mind that the input assemblies need to mostly be complete: one sequence per piece of DNA in the genome. Therefore, its best used for microbial genomes (archaea, bacteria, mitochondria, chloroplast).\nFor installation instructions, usage, deeper explanations and more, head over to the Autocycler wiki!\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nAutocycler v0.1.2 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run the following to install the tool from source:\n\n# Change to directory you want to download the github repository to\ncd software/\n\n# Create an empty conda env \nmamba create -p /zfs/omics/projects/bioinformatics/software/miniconda3/envs/autocycler_0.1.2 \nconda activate autocycler_0.1.2\n\n# Install autocycler\n# This will build an executable in ``target/release/autocycler``\ngit clone https://github.com/rrwick/Autocycler.git\ncd Autocycler\ncargo build --release \n\n# Install various different assemblers \n# for this first remove the name field to install it in our empty conda environment we setup before\n# ensure that you run mamba env update only after you activated the autocycler_0.1.2 environment\ngrep -v \"name\" scripts/environment.yml &gt; scripts/environment_edit.yml\nmamba env update --file scripts/environment_edit.yml\n\n# Optional but recommended: Install GNU parallel\nmamba install conda-forge::parallel \n\n# Check that all packages exist\nconda list \n\n# Copy scripts to conda bin folder to be able to access them without having to write out the path path\ncp target/release/autocycler \"$CONDA_PREFIX\"/bin/\ncp scripts/*.py scripts/*.sh \"$CONDA_PREFIX\"/bin/\n\n\n\nUsage\n\nSingle sample, one assembly at a time\nIf you want to know what is happening at each step, please visit the tools wiki.\n\n# Activate the right conda environment\nconda activate autocycler_0.1.2\n\n# Go to the project directory in which you also have the sequencing data\ncd autocyler_analysis\n\n# Define necessary input variables\n# The genome size can also be set manually if you know the value\nreads=data/my_data.fastq.gz\nthreads=8  \ngenome_size=$(genome_size_raven.sh \"$reads\" \"$threads\") \n\n# View estimated genome size:\n# In this example, we have an estimated genome size of 4,497,636 bp\necho $genome_size\n\n# Step 1: subsample the long-read set into multiple files. This generates 4 sub-sampled read sets by default\n# Make sure to record some of the useful statistics such as: used default for min_read_depth 25, input fastq read count: 213459 and N50 length: 3760 bp; Total read depth: 131.5x; Mean read length: 2770 bp\nautocycler subsample --reads \"$reads\" --out_dir subsampled_reads --genome_size \"$genome_size\"\n\n# Step 2: assemble each of the 4 subsampled files with 6 different assemblers (this can take a bit, for suggestions to speed this up, see below)\n# Adjust in case you could not install all assemblers or don't want to use any of the listed assemblers\nmkdir assemblies\nfor assembler in canu flye miniasm necat nextdenovo raven; do\n    for i in 01 02 03 04; do\n        srun --cpus-per-task $threads --mem=50G \\\n            \"$assembler\".sh subsampled_reads/sample_\"$i\".fastq assemblies/\"$assembler\"_\"$i\" \"$threads\" \"$genome_size\"\n    done\ndone\n\n# Sanity check: Count number of contigs/assembly\ngrep -c \"&gt;\" assemblies/*fasta\n\n# Optional step: remove the subsampled reads to save space\nrm subsampled_reads/*.fastq\n\n# Step 3: compress the input assemblies into a unitig graph\nautocycler compress -i assemblies -a autocycler_out\n\n# Step 4: cluster the input contigs into putative genomic sequences\nautocycler cluster -a autocycler_out\n\n# Steps 5 and 6: trim and resolve each QC-pass cluster\nfor c in autocycler_out/clustering/qc_pass/cluster_*; do\n    autocycler trim -c \"$c\"\n    autocycler resolve -c \"$c\"\ndone\n\n# Step 7: combine resolved clusters into a final assembly\n# Record assembly statistics: 1 unitig, 1 link, total length:  4491993 bp\n# The final consensus assembly will be named: autocycler_out/consensus_assembly.fasta\nautocycler combine -a autocycler_out -i autocycler_out/clustering/qc_pass/cluster_*/5_final.gfa\n\n# Optional: generate a TSV line from the various metrix\nautocycler table &gt; metrics.tsv\nautocycler table -a autocycler_out &gt;&gt; metrics.tsv\n\n\n\nSingle sample, several assemblies run in parallel\nTo speed things up we can run the assembly (Step 2), which is the most time intensive step, with GNU parallel. This allows us to run several assemblies at the same time.\n\n# Define our input variables\n# Ensure that this works with your computer specs, i.e. here we run 4 jobs in parallel each with 8 cpus\n# So here we need to have 4x8 = 24 threads available for things to run\njobs=4\nthreads=8 \n\nmkdir -p assemblies\nrm -f assemblies/jobs.txt\n\nfor assembler in canu flye miniasm necat nextdenovo raven; do\n    for i in 01 02 03 04; do\n        echo \"srun --cpus-per-task $threads --mem=50G $assembler.sh subsampled_reads/sample_$i.fastq assemblies/${assembler}_$i $threads $genome_size\" &gt;&gt; assemblies/jobs.txt\n    done\ndone\n\nparallel --jobs \"$jobs\" --joblog assemblies/joblog.txt --results assemblies/logs &lt; assemblies/jobs.txt\n\n\n\nSeveral samples run in parallel\nTo run Autocycler on more than one genome you can run the code above with either a for loop or with a job scheduler, such as SLURM. To do so, we provide two bash scripts.\nFor these to work ensure that both Autocycler and GNU parallel are installed on your system (when running this outside of Crunchomics).\n\nVersion 1: Without a job scheduler\nTo run Autocycler on several genomes when you have no job scheduler available you can use autocycler_bash.sh. This scripts requires the following arguments to run:\n\n-d: The path to the folder with the raw read in .fastq.gz format. Note, that the name of the file will be used as the sample name throughout the script, so name those files accordingly.\n-t: The number of threads to use\n-m: The minimum number of CPUs per assembly. These assemblies will be run in parallel based on that number so set this accordingly. For example, if you use a total of 16 threads and want to run at least four assemblies in parallel you would set this to 4.\n\nThe results of this script will be generated in the results folder which will contain a sub-directory for each genome. The final assembly will be found in results/&lt;Genome_name&gt;/autocycler_out/consensus_assembly.fasta.\n\ncd project_dir\n\n# Make the bash script executable (needs to be run only once if you run this outside of Crunchomics)\nchmod +x scripts/autocycler_bash.sh \n\n# Run Autocycler on several genomes\nconda activate autocycler_0.1.2\n\nautocycler_bash.sh -d data -t 32 -m 8\n\n# Generate metrics file\nautocycler table &gt; metrics.tsv \n\nfor i in data/*fastq.gz; do\n    sample=$(basename $i .fastq.gz)\n    echo $sample\n    autocycler table -a results/${sample}/autocycler_out -n ${sample} &gt;&gt; metrics.tsv  # append a TSV row\ndone\n\nconda deactivate\n\n# Optional: The scripts generates a log file for each genome in the log folder, we can extract some key information as follows \nfor i in logs/*log; do \n  sample=$(basename $i .log)\n  \n  echo -e \"\\nInformation on Assembly $sample\"\n  echo \"----------------------------------------\"\n  echo \"Estimated $(grep \"Genome size for\" $i | uniq)\"\n  \n  echo -e \"\\nInformation on Input FastQ for $sample\"\n  grep \"Read count:\" $i | sed 's/^[ \\t]*//'\n  grep \"Read bases:\" $i | sed 's/^[ \\t]*//'\n  grep \"Read N50 length:\" $i | sed 's/^[ \\t]*//'\n  grep \"Total read depth:\" $i\n  grep \"Mean read length:\" $i\n  grep \"reads per subset\" $i | sed 's/^[ \\t]*//'\n\n  echo -e \"\\nInformation on the assembled genome for $sample\"\n  grep -E \"[0-9]* unitig, [0-9]* link\"  $i | tail -n1 | sed 's/^[ \\t]*//'\n  grep \"total length:\" logs/Genome1.log | tail -n1 | sed 's/^[ \\t]*//'\n  grep \"fully resolv\" $i\ndone \n\n\n\nVersion 2: With the SLURM scheduler\nIf you want to run Autocycler on an HPC with SLURM, such as Crunchomics, you can use autocycler_array.sh which is available on Crunchomics at /zfs/omics/projects/bioinformatics/scripts/ or can be downloaded here.\nThis is the recommended option when working with many genomes, since some assemblers, such as CANU, can take some time to finish.\nBefore running the script, you should open the script once and adjust the following:\n\n#SBATCH --cpus-per-task=32: Provide the number of threads to use, adjust based on what the system has available.\n#SBATCH --array=1-2: Define the job array size, i.e. the number of genomes to run this script on. Set the second number to the total number of genomes that you want to analyse (here: 2). If you have many genomes to analyse it is recommended to not start all at once to leave resources for others. For example, if you have 100 genomes and want to start them in batches of five, you would set this to #SBATCH --array=1-100%5\ndata_folder=\"data\": Provide the path to the folder that contains the assemblies (here: data). Note, that the name of the file will be used as the sample name throughout the script, so name those files accordingly.\nMIN_CPUS_PER_ASSEMBLY=8. Provide the minimum number of CPUs per assembly. These assemblies will be run in parallel based on that number so set this accordingly. For example, if you use a total of 32 threads and want to run at least four assemblies in parallel you would set this to 8.\nconda activate autocycler_0.1.2: Defines the name of the conda environment that has autocycler and GNU parallel installed. If you have these tools installed outside of a conda environment you can delete this line.\n\nThe results of this script will be generated in the results folder which will contain a sub-directory for each genome. The final assembly will be found in results/&lt;Genome_name&gt;/autocycler_out/consensus_assembly.fasta.\n\ncd project_dir\n\n# Run Autocycler on several genomes using SLURM\nsbatch scripts/autocycler_array.sh\n\n# Generate metrics file\nconda activate autocycler_0.1.2\n\nautocycler table &gt; metrics.tsv \n\nfor i in data/*fastq.gz; do\n    sample=$(basename $i .fastq.gz)\n    echo $sample\n    autocycler table -a results/${sample}/autocycler_out -n ${sample} &gt;&gt; metrics.tsv  # append a TSV row\ndone\n\nconda deactivate\n\n# Optional: The scripts generates a log file for each genome in the log folder, we can extract some key information as follows \nfor i in logs/*log; do \n  sample=$(basename $i .log)\n  \n  echo -e \"\\nInformation on Assembly $sample\"\n  echo \"----------------------------------------\"\n  echo \"Estimated $(grep \"Genome size for\" $i | uniq)\"\n  \n  echo -e \"\\nInformation on Input FastQ for $sample\"\n  grep \"Read count:\" $i | sed 's/^[ \\t]*//'\n  grep \"Read bases:\" $i | sed 's/^[ \\t]*//'\n  grep \"Read N50 length:\" $i | sed 's/^[ \\t]*//'\n  grep \"Total read depth:\" $i\n  grep \"Mean read length:\" $i\n  grep \"reads per subset\" $i | sed 's/^[ \\t]*//'\n\n  echo -e \"\\nInformation on the assembled genome for $sample\"\n  grep -E \"[0-9]* unitig, [0-9]* link\"  $i | tail -n1 | sed 's/^[ \\t]*//'\n  grep \"total length:\" logs/Genome1.log | tail -n1 | sed 's/^[ \\t]*//'\n  grep \"fully resolv\" $i\ndone \n\n\n\n\n\nThings to keep in mind\n\nWhen to use autocycler: For Autocycler to work, the input assemblies need to mostly be complete: one sequence per piece of DNA in the genome. Therefore, its best used for microbial genomes (archaea, bacteria, mitochondria, chloroplast). However, if you sequence eukaryotes and if T2T assemblies are possible, then Autocycler should work as well.\nPolishing: Since Autocycler assemblies are long-read-only, they may still contain errors. If assembling Oxford Nanopore reads you can consider polishing with for example Medaka. If you also have short reads Polypolish and Pypolca are options to consider\nGenome orientation: Autocycler does not rotate circular sequences to start at a particular gene (e.g. dnaA). To do this, check out Dnaapler.\nAssessing the assembly\n\nThe Autocycler combine command produces a final assembly by combining all of the clusters. Hopefully, each cluster resolved to a single contig, in which case it will print this at the end of its stderr output: Consensus assembly is fully resolved. The Metrics page describes all of the assembly metrics generated by Autocycler, but for assessment purposes, the most useful is likely the consensus_assembly_fully_resolved metric, which can be true or false. You can find this metric in the consensus_assembly.yaml file made by Autocycler combine in your Autocycler output directory.\nIf your assembly went poorly you can consider doing the following:\n\nTry using other assemblers\nTry different parameters when making your input assemblies. Some assemblers (e.g. Canu) have a large number of parameters that can influence the result.\nManually curate your input assemblies before using them with Autocycler. Specifically, discard any assemblies that appear to be incomplete.\nIf none of the above work well, then your read set is likely insufficient, in which case you may need to sequence again aiming for deeper and longer reads.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Autocycler"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/transdecoder.html#transdecoder",
    "href": "source/metatranscriptomics/transdecoder.html#transdecoder",
    "title": "Bioinformatics guidance page",
    "section": "TransDecoder",
    "text": "TransDecoder\n\nIntroduction\nTransDecoder identifies candidate coding regions within transcript sequences, such as those generated by de novo RNA-Seq transcript assembly using Trinity, or constructed based on RNA-Seq alignments to the genome using StringTie (Haas 2025).\nTransDecoder is applied to an entire transcriptome for a single organism involving thousands of transcript sequences as input. Therefore, TransDecoder is unlikely to work if you provide a small number of sequences as input, as it requires training a species-specific model based on hundreds of candidates derived from the inputs.\nTransDecoder identifies likely coding sequences based on the following criteria:\n\na minimum length open reading frame (ORF) is found in a transcript sequence\na log-likelihood score similar to what is computed by the GeneID software is &gt; 0.\nthe above coding score is greatest when the ORF is scored in the 1st reading frame as compared to scores in the other 2 forward reading frames.\nif a candidate ORF is found fully encapsulated by the coordinates of another candidate ORF, the longer one is reported. However, a single transcript can report multiple ORFs (allowing for operons, chimeras, etc).\na PSSM is built/trained/used to refine the start codon prediction.\noptional: the putative peptide has a match to a Pfam/Uniprot domain above the noise cutoff score.\n\nImportant: As of March 2024, TransDecoder is no longer actively supported by the developer. Please continue to use as it fits your needs.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nTransDecoder v5.7.1 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name transdecoder_5.7.1 -c bioconda transdecoder=5.7.1\n\n\n\nUsage\n\nBasic usage\n\nconda activate transdecoder_5.7.1\n\n# Extract the long open reading frames\nTransDecoder.LongOrfs -t target_transcripts.fasta --output_dir results\n\n# Predict the likely coding regions\n# The final set of candidate coding regions can be found as files '.transdecoder.' where extensions include .pep, .cds, .gff3, and .bed\nTransDecoder.Predict -t target_transcripts.fasta --output_dir results\n\nconda deactivate\n\nOutput files generated:\n\ntranscripts.fasta.transdecoder.pep : peptide sequences for the final candidate ORFs; all shorter candidates within longer ORFs were removed.\ntranscripts.fasta.transdecoder.cds : nucleotide sequences for coding regions of the final candidate ORFs\ntranscripts.fasta.transdecoder.gff3 : positions within the target transcripts of the final selected ORFs\ntranscripts.fasta.transdecoder.bed : bed-formatted file describing ORF positions, best for viewing using GenomeView or IGV.\n\nUseful options:\nTransDecoder.LongOrfs:\n\n-t: Transcripts fasta file\n--gene_trans_map: gene-to-transcript identifier mapping file (tab-delimited, gene_idtrans_id )\n-m &lt;int&gt; : minimum protein length (default: 100)\n--genetic_code | -G &lt;string&gt;: genetic code (default: universal). View help for all options\n--complete_orfs_only: yields only complete ORFs (peps start with Met (M), end with stop (*))\n\nTransDecoder.Predict:\n\n--single_best_only: Retain only the single best orf per transcript (prioritized by homology then orf length)\n--retain_long_orfs_mode &lt;string&gt;: ‘dynamic’ or ‘strict’ (default: dynamic). In dynamic mode, sets range according to 1%FDR in random sequence of same GC content.\n--retain_long_orfs_length &lt;int&gt;: under ‘strict’ mode, retain all ORFs found that are equal or longer than these many nucleotides even if no other evidence marks it as coding (default: 1000000) so essentially turned off by default.\n--retain_pfam_hits &lt;string&gt;: domain table output file from running hmmscan to search Pfam (see transdecoder.github.io for info)Any ORF with a pfam domain hit will be retained in the final output.\n--retain_blastp_hits &lt;string&gt;: blastp output in ‘-outfmt 6’ format. Any ORF with a blast match will be retained in the final output.\n--no_refine_starts: start refinement identifies potential start codons for 5’ partial ORFs using a PWM, process on by default\n-T &lt;int&gt;: Top longest ORFs to train Markov Model (hexamer stats) (default: 500). Note, 10x this value are first selected for removing redundancies, and then this -T value of longest ORFs are selected from the non-redundant set.\n\n\n\nStarting from a genome-based transcript structure GTF file\nThe process here is identical to the above with the exception that we must first generate a fasta file corresponding to the transcript sequences, and in the end, we recompute a genome annotation file in GFF3 format that describes the predicted coding regions in the context of the genome.\n\n# Construct the transcript fasta file using the genome and the transcripts.gtf \ngtf_genome_to_cdna_fasta.pl stringtie.gtf genome.fasta &gt; transcripts.fasta\n\n# Convert gtf to gff\ngtf_to_alignment_gff3.pl stringtie.gtf &gt; stringtie.gff3\n\n# Extract open reading frames\nTransDecoder.LongOrfs -t transcripts.fasta --output_dir results\n\n# Predict the likely coding regions\nTransDecoder.Predict -t transcripts.fasta --output_dir results\n\n# Generate a genome-based coding region annotation file\ncdna_alignment_orf_to_genome_orf.pl results/transcripts.fasta.transdecoder.gff3 \\\n   stringtie.gff3 \\\n   transcripts.fasta &gt; results/transcripts.fasta.transdecoder.genome.gff3   \n\n# Convert to gtf as companion to the gff3 file\ngff3_gene_to_gtf_format.pl results/transcripts.fasta.transdecoder.genome.gff3  \\\n  genome.fasta &gt; \\\n  results/transcripts.fasta.transdecoder.genome.gtf\n\n# Optional: Clean the gtf file \npython /zfs/omics/projects/bioinformatics/scripts/edit_transdecoder_gtf.py \\\n    --input results/transcripts.fasta.transdecoder.genome.gtf \\\n    --output results/transcripts.fasta.transdecoder.genome.clean.gtf\n\n\n\nInclude homology searches as ORF retention criteria\n\n# Extract the long open reading frames\nTransDecoder.LongOrfs -t target_transcripts.fasta --output_dir results\n\n# Blastp Against uniprot database\nblastp -query results/transcripts.fasta.transdecoder_dir/longest_orfs.pep  \\\n    -db /zfs/omics/projects/bioinformatics/databases/uniprot/uniprot_sprot_070524.fasta  -max_target_seqs 1 \\\n    -outfmt 6 -evalue 1e-5 -num_threads 30 &gt; results/blastp.outfmt6\n\n# Hmmsearch against PFAM \nhmmsearch --cpu 30 -E 1e-10 --domtblout results/pfam.domtblout \\\n  /zfs/omics/projects/bioinformatics/databases/pfam/release_36.0/2-Pfam-A.hmm \\\n  results/transcripts.fasta.transdecoder_dir/longest_orfs.pep\n\n# Integrate the homology searches in the prediction step\nTransDecoder.Predict -t target_transcripts.fasta --output_dir results \\\n    --retain_pfam_hits results/pfam.domtblout \\\n    --retain_blastp_hits results/blastp.outfmt6\n\nIf you want to generate your own databases, you can do the following:\n\n# Get the Uniprot database\ntoday=$(date -I)\nwget https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\ngzip -d uniprot_sprot.fasta.gz\nmv uniprot_sprot.fasta uniprot_sprot_${today}.fasta\nmakeblastdb -in uniprot_sprot_${today}.fasta -parse_seqids -dbtype prot\n\n# Get the Pfam database\nwget ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz\nwget https://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.clans.tsv.gz\ngzip -d *gz",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "TransDecoder"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/star.html#star",
    "href": "source/metatranscriptomics/star.html#star",
    "title": "Bioinformatics guidance page",
    "section": "STAR",
    "text": "STAR\n\nIntroduction\nSTAR (Spliced Transcripts Alignment to a Reference) is an ultra-fast universal RNA-seq aligner that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure (Dobin et al. 2012). In addition to unbiased de novo detection of canonical junctions, STAR can discover non-canonical splices and chimeric (fusion) transcripts, and is also capable of mapping full-length RNA sequences.\nFor more information, please have a look at the manual.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nSTAR 2.7.10b_alpha_230301 is installed by default on Crunchomics\nSTAR v2.7.11b is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name star -c bioconda star\n\n\n\nUsage\nThere are many different option you can set for generating the genome index and doing the read mapping. For a lot of cases the default settings should be sufficient. However, we recommend that you have a look at the manual to familiarize yourself with the possibly options for each step.\n\nStep 1: Generating genome index\nSTAR requires two inputs to generate an index from a genome: The genome as fasta file as well as a gtf file with the genome annotation. While STAR can be run without annotations, using annotations is highly recommended whenever they are available.\nIf you are working with GFF3 formatted annotations then:\n\n--sjdbGTFtagExonParentTranscript Parent should be used. In general, for -sjdbGTFfile files STAR only processes lines which have --sjdbGTFfeatureExon (=exon by default) in the 3rd field. The exons are then assigned to the transcripts using parent-child relationships defined by the --sjdbGTFtagExonParentTranscript (=transcript_id by default)\nIn most cases it should be fine to convert GFF3 to GTF with gffread via gffread assembly.gff3 -T -o assembly.gtf\n\nExample for generating a genome index:\n\nSTAR --runThreadN 10 \\\n    --runMode genomeGenerate \\\n    --genomeDir results/star/index \\\n    --genomeFastaFiles 03_data/genome_files/assembly.fasta \\\n    --sjdbGTFfile data/genome_files/assembly.gtf \\\n    --sjdbOverhang 100 \\\n    --genomeSAindexNbases 11\\\n\nNotice:\n\nYou might get a warning that says something like ” –genomeSAindexNbases 14 is too large for the genome size=56491843, which may cause seg-fault at the mapping step. Re-run genome generation with recommended –genomeSAindexNbases 11”. If you see, this, then change the value accordingly\nTo index the genome with STAR for RNA-seq analysis, the sjdbOverhang option needs to be specified for detecting possible splicing sites. It usually equals to the minimum read size minus 1; it tells STAR what is the maximum possible stretch of sequence that can be found on one side of a spicing site. If you trimmed the data and have longer reads, you can use the default option of 100 as also discussed in this thread\n\n\n\nStep 2: Mapping reads to a genome\nYou can map the reads by either running STAR in a loop when working with multiple input files or alternatively, multiple files can be mapped in one run with a single output.\n\nExample 1: Run Star on an individual file\nBelow an example for running STAR on an individual file:\n\nSTAR --genomeDir results/star/index \\\n     --runThreadN 6 \\\n     --readFilesIn sample1_R1.fastq.gz sample1_R2.fastq.gz \\\n     --outFileNamePrefix results/star/mapping/sample1_ \\\n     --outSAMtype BAM SortedByCoordinate \\\n     --outSAMunmapped Within \\\n     --readFilesCommand zcat \\\n     --outSAMattributes Standard \\\n     --quantMode GeneCounts\n\nNotice:\n\nFor larger libraries, you might need to limit the amount of RAM used. If you run into an error because of lack of memory, STAR will give you recommendations on how to adjust the --limitBAMsortRAM option accordingly\n--readFilesCommand is used when working with compressed files. For example, for gzipped files use --readFilesCommand zcat OR --readFilesCommand gunzip -c. For bzip2-compresse files, use --readFilesCommand bunzip2 -c.\noutSAMtype outputs the alignment directly in bam format and, if desired, sorts the output by coordinate or give an unsorted output.\nWith --quantMode you can output transcript coordinates. Check the manual for the different options that exist.\nUnmapped reads can be output into the SAM/BAM Aligned.* file(s) with --outSAMunmapped\n\n\n\nExample 2: Multiple files in a for-loop\nIf you work with multiple files, you can map them individually using a for-loop (or an array when using SLURM). To keep the code readable, its recommended to use variables to store different values as follows:\n\n# Set the input directories and parameters\ninput_dir=\"results/fastp\"\noutput_dir=\"results/star/mapping\"\ngenome_dir=\"results/star/index\"\nthreads=6\n\n# Make sure the output directory exists\nmkdir -p $output_dir\n\n# Find all R1 files in the input folder and loop through them\nfor R1 in ${input_dir}/*_R1_trim.fastq.gz; do\n    \n    # Derive the corresponding R2 filename by replacing R1 with R2\n    R2=${R1/_R1_/_R2_}\n\n    # Get the sample name by extracting the base name without the path and the suffix\n    sample_name=$(basename ${R1} _R1_trim.fastq.gz)\n\n    # Run STAR for each sample\n    STAR --genomeDir $genome_dir \\\n         --runThreadN $threads \\\n         --readFilesIn $R1 $R2 \\\n         --outFileNamePrefix ${output_dir}/${sample_name}_ \\\n         --outSAMtype BAM SortedByCoordinate \\\n         --outSAMunmapped Within \\\n         --readFilesCommand zcat \\\n         --outSAMattributes Standard\ndone\n\n\n\nExample 3: Multiple files with a single output\nIf you work with multiple files, then these samples can be mapped in one run with a single output. Check the section “3.2 Mapping multiple files in one run.” in the manual for more information.\nThe following code should work to dynamically supply the file names but it was not tested, so use with care:\n\n# Set the input directories and parameters\ninput_dir=\"results/fastp\"\noutput_prefix=\"results/star/mapping/combined_\"\ngenome_dir=\"results/star/index\"\n\n# Find all R1 and R2 files and store them in an array, each element separated with a comma\nR1_files=$(ls ${input_dir}/*_R1_trim.fastq.gz | tr '\\n' ',')\nR2_files=$(ls ${input_dir}/*_R2_trim.fastq.gz | tr '\\n' ',')\n\n# Remove the trailing comma from the lists\nR1_files=${R1_files%,}\nR2_files=${R2_files%,} \n\n# Run STAR\nSTAR --genomeDir $genome_dir \\\n     --runThreadN $threads \\\n     --readFilesIn ${R1_files},${R2_files} \\\n     --outFileNamePrefix ${output_prefix} \\\n     --outSAMtype BAM SortedByCoordinate \\\n     --outSAMunmapped Within \\\n     --readFilesCommand zcat \\\n     --outSAMattributes Standard \\\n     --quantMode GeneCounts\n\n\n\nExample 4: Mapping reads with a 2-pass procedure\nFor the most sensitive novel junction discovery, it is recommended to run STAR in 2-pass mode. It does not significantly increase the number of detected novel junctions, but allows to detect more splices reads mapping to novel junctions. The basic idea is to run 1st pass of STAR mapping with the usual parameters, then collect the splice junctions (SJ) detected in the first pass, and use them as “annotated” junctions for the 2nd pass mapping.\nIf working with more than one sample, Alex Dobin (developer of STAR) recommends to use junctions from all samples, not only from one. So first you do normal mapping of all your samples, collect all junctions, and insert them into the second step for each sample. Note, that the BAM files in the 1st pass are irrelevant and since they are very large you can avoid them with --outSAMtype None.\nBefore running the second pass, you might want to filter the splice junction files to remove low-confidence hits.\nExample:\n\n# Count number of junctions before filtering \ncat results/first_pass_junctions/*SJ.out.tab | cut -f1-6 | sort | uniq | wc -l \n\n# Filter, with for example the following rules:\n# Filter non-canonical junctions (column 5 &gt; 0),\n# Filter junctions supported by multimappers only (column7 &gt; 0)\n# Filter out junctions supported by too few reads (column7 &gt; 2)\n# Do not count annotated junctions since they are part of the gtf anyhow (column6 == 0)\ncat results/first_pass_junctions/*SJ.out.tab | awk '($1 != \"chrM\" && $5 &gt; 0 && $7 &gt; 2 && $6 == 0)' | cut -f1-6 | sort | uniq &gt; results/first_pass_junctions/SJ.filtered.tab\n\n# Count how many junctions are left\ncat results/first_pass_junctions/*SJ.out.tab | awk '($5 &gt; 0 && $7 &gt; 2 && $6 == 0)' | cut -f1-6 | sort | uniq | wc -l\n\n# Run Star's second pass (for a single sample)\nSTAR --genomeDir $genome_dir \\\n --runThreadN 10 \\\n --readFilesIn Sample1_R1.fastq.gz Sample1_R2.fastq.gz \\\n --outFileNamePrefix results/2ndpass/sample1_ \\\n --outSAMtype BAM SortedByCoordinate \\\n --outSAMstrandField intronMotif \\\n --outSAMunmapped Within \\\n --readFilesCommand zcat \\\n --outSAMattributes Standard \\\n --quantMode TranscriptomeSAM \\\n --outWigType bedGraph \\\n --outWigStrand Stranded \\\n --outWigNorm RPM \\\n --sjdbFileChrStartEnd results/first_pass_junctions/SJ.filtered.tab\n\nSettings:\n\n--outSAMstrandField intronMotif: Adds the XS tag to bam files, which is necessary to run down-stream tools like stringtie. This ensures that any SAM record with a spliced alignment (i.e. having a read alignment across at least one junction) has the XS tag (or the ts tag for long read data) which indicates the transcription strand, the genomic strand from which the RNA that produced the read originated\nUsing outWigType, outWigStrand and outWigNorm is optional but useful to visualize the data per strand with tools like IGV or pyGenomeTracks.\n\n\n\n\n\nInterpreting the outputs\nSTAR will produce different output files, which are briefly described below.\nLog.final.out contains the summary mapping statistics of the run and can be used to evaluate both the mapping performance and the quality of the RNA-seq library. Notice that by default STAR does not allow any unpaired alignments, i.e. where only one read mapped, or not concordantly mapped pairs, and these alignments are not counted in the summary statistics.\nIn here the most important value to look at is the Uniquely mapped reads % or mapping rate. This is defined as a proportion of uniquely mapped reads of all the input reads. Generall, a very good library exceeds 90% and for good libraries the value should be above 80% (notice, this might differ a bit depending on the organism you are working with). Values below 50% indicate problems with either library preparation or processing and might be due to:\n\nInsufficient depletion of rRNA. Most rRNA contains highly sequence-similar paralogues and reads will be mapped to multiple loci. Percentages above 15% of multi-mapping reads can be indicative of insufficient depletion of rRNA\nPoor sequence quality. Sequence error rates can be estimated from the Mismatch rate per base, Deletion rate per base and Insertion rate per base. These metrics includes sequencing errors but also genotype variants. For Illumina typical mismatch error rates are below 0.5% and indel error rates below 0.05%. Another indicator of poor sequence quality is a reduction of the average mapped length with respect to the average input read length.\nExogenous RNA/DNA contamination. If a large percentage of reads are in the categories unmapped too short or % of reads unmapped other this can indicate contamination. In this case it is recommended to BLAST several of the unmapped reads to the NCBI database to identify possible sources of contamination\n\nLog.out contains various run-time information and messages, and is typically used for debugging.\nAligned.out.sam is the main output file containing read alignments in the SAM format. If --outSAMtype BAM was used, then a unsorted (or sorted, depending on the used settings) BAM file will be given. Notice, converting SAM to BAM is quite time-consuming and also saves disk-space, so its generally recommeded to let STAR convert the SAM to BAM automatically.\nSJ.out.tab contains high confidence collapsed splice junctions in tab-delimited format. The columns for this file are as follows:\n1.  contig name\n2.  first base of the splice junction (1-based)\n3.  last base of the splice junction (1-based)\n4.  strand (0: undefined, 1: +, 2: -)\n5.  intron motif: 0: noncanonical, 1: GT/AG, 2: CT/AC, 3: GC/AG, 4: CT/GC, 5: AT/AC, 6: GT/AT\n6.  0: unannotated, 1: annotated, only if an input gene annotations file was used\n7.  number of uniquely mapping reads spanning the splice junction\n8.  number of multimapping reads spanning the splice junction",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "STAR"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/rseqc.html#rseqc",
    "href": "source/metatranscriptomics/rseqc.html#rseqc",
    "title": "Bioinformatics guidance page",
    "section": "RSeQC",
    "text": "RSeQC\n\nIntroduction\nThe RSeQC package provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data (Wang, Wang, and Li 2012). Some basic modules quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while RNA-seq specific modules evaluate sequencing saturation, mapped reads distribution, coverage uniformity, strand specificity, transcript level RNA integrity etc.\nThis page will only give very specific usage examples, to find out more about all available options, please visit the tools manual.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nRSeQC v5.0.3-1 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name rseq_5.0.3-1 -c bioconda rseqc\n\n\n\nUsage\n\ninfer_experiment.py\nThis program is used to infer the strandedness of RNA-seq libraries by comparing the orientation of sequenced reads with the annotated strand of transcripts. It is particularly useful when the library preparation protocol is unknown or unclear.\nThis script requires the following inputs:\n\nA bam file (we usually get this from mappers like STAR. If you only have a sam file you can use samtools to convert sam to bam)\nA bed file, a text file format used to store genomic regions as coordinates and associated annotations. Often times you have other formats available, i.e. gff. In such cases you can use gff2bed or gffread to convert between different file formats.\n\nExample running RSeQC on several bam files:\n\nconda activate rseq_5.0.3-1\n\nfor i in results/star/mapping/*_Aligned.sortedByCoord.out.bam; do\n    sample_name=$(basename \"$i\" _Aligned.sortedByCoord.out.bam)\n    echo ${sample_name} &gt; results/star/mapping/${sample_name}_strand.txt\n\n    infer_experiment.py \\\n        --i ${i} \\\n        -r results/genome_files/my_genome.bed \\\n        &gt;&gt;  results/star/mapping/${sample_name}_strand.txt\ndone \n\ncat results/star/mapping/*_strand.txt &gt; results/star/mapping/all_strand.txt\n\nconda deactivate\n\nOutputs:\nFor paired-end RNA-seq, there are two common stranded library preparation protocols (e.g., Illumina ScriptSeq)\n\n1++,1--,2+-,2-+, library preparation: FR/fr-secondstrand/forward stranded\n\nread1 mapped to ‘+’ strand indicates parental gene on ‘+’ strand, i.e. read 1 mapped to the + strand when the gene itself is on the plus strand\nread1 mapped to ‘-’ strand indicates parental gene on ‘-’ strand\nread2 mapped to ‘+’ strand indicates parental gene on ‘-’ strand\nread2 mapped to ‘-’ strand indicates parental gene on ‘+’ strand\n\n1+-,1-+,2++,2-- , library preparation: RF/fr-firststrand/reverse stranded\n\nread1 mapped to ‘+’ strand indicates parental gene on ‘-’ strand\nread1 mapped to ‘-’ strand indicates parental gene on ‘+’ strand\nread2 mapped to ‘+’ strand indicates parental gene on ‘+’ strand\nread2 mapped to ‘-’ strand indicates parental gene on ‘-’ strand\n\n\nExample output for non-strand specific data:\nThis is PairEnd Data\nFraction of reads failed to determine: 0.0172\nFraction of reads explained by \"1++,1--,2+-,2-+\": 0.4903\nFraction of reads explained by \"1+-,1-+,2++,2--\": 0.4925\nExample output for fr-firststrand data:\nThis is PairEnd Data\nFraction of reads failed to determine: 0.0292\nFraction of reads explained by \"1++,1--,2+-,2-+\": 0.0062\nFraction of reads explained by \"1+-,1-+,2++,2--\": 0.9646\nYou can also confirm these results with IGV by:\n\nOpen IGV and load sorted bam files and reference gtf file\nRight click on the alignment track and select the option “Colour Alignments By -&gt; First-of-Pair Strand”\n\nFirst-of-pair colors both reads of a read pair with the color of the direction of mapping (forward of reverse) of Read1\nif Read1 is forward, the pair “Read 1”+“Read 2” is colored red\nIf Read1 is reverse, both members are coloured blue\n\nTo determine the strandedness\n\nFor a given transcript, non-stranded libraries will show an equal mix of red and blue reads aligning to the locus, as there is no preservation of strand information\nIn the case of forward-stranded (secondstrand), Read 1 aligns to the same strand as the gene. For a gene on the forward (plus) strand, Read 1 will be forward (red), and for a gene on the reverse (minus) strand, Read 1 will be reverse (blue).\nIn the case of reverse-stranded (firststrand), Read 1 aligns to the opposite strand of the gene. For a gene on the forward (plus) strand, Read 1 will be reverse (blue), and for a gene on the reverse (minus) strand, Read 1 will be forward (red).",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "RSeQC"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/fastp.html#fastp",
    "href": "source/metatranscriptomics/fastp.html#fastp",
    "title": "Bioinformatics guidance page",
    "section": "FastP",
    "text": "FastP\n\nIntroduction\nFastP is a tool designed to provide fast all-in-one preprocessing for FastQ files.\n\nManual\nPaper. Please do not forget to cite the paper whenever you use the software (Chen 2023).\n\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nFastP v0.23.4 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -n fastp\nmamba install -n fastp -c bioconda fastp\n\n\n\nUsage\nFastP has many different options, so its best to also have a look at the manual. Below you find a quick example to try out.\n\nRequired input: Single-end or paired-end FastQ files (can be compressed)\nGenerated output: Quality filtered fastq files\nUseful arguments (not extensive, check manual for all arguments):\n\n-q, --qualified_quality_phred: the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified\n-l, --length_required: reads shorter than length_required will be discarded, default is 15\nLow complexity filter: The low complexity filter is disabled by default, and you can enable it by -y or --low_complexity_filter. The complexity is defined as the percentage of base that is different from its next base (base[i] != base[i+1]).\nAdaptor removal: Adaptors are removed by default and fastp contains some built-in known adapter sequences for better auto-detection. For SE data, the adapters are evaluated by analyzing the tails of first ~1M reads. This evaluation may be inacurrate, and you can specify the adapter sequence by -a or --adapter_sequence option. For PE data, the adapters can be detected by per-read overlap analysis, which seeks for the overlap of each pair of reads. This method is robust and fast, so normally you don’t have to input the adapter sequence even you know it. But you can still specify the adapter sequences for read1 by --adapter_sequence, and for read2 by --adapter_sequence_r2.\nRead cutting by quality score: fastp supports per read sliding window cutting by evaluating the mean quality scores in the sliding window. There are 3 different operations, and you enable one or all of them:\n\n-5, --cut_front move a sliding window from front (5’) to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The leading N bases are also trimmed. Use cut_front_window_size to set the widnow size, and cut_front_mean_quality to set the mean quality threshold. If the window size is 1, this is similar as the Trimmomatic LEADING method.\n-3, --cut_tail move a sliding window from tail (3’) to front, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise. Default is disabled. The trailing N bases are also trimmed. Use cut_tail_window_size to set the widnow size, and cut_tail_mean_quality to set the mean quality threshold. If the window size is 1, this is similar as the Trimmomatic TRAILING method.\n-r, --cut_right move a sliding window from front to tail, if meet one window with mean quality &lt; threshold, drop the bases in the window and the right part, and then stop. Use cut_right_window_size to set the widnow size, and cut_right_mean_quality to set the mean quality threshold. This is similar as the Trimmomatic SLIDINGWINDOW method.\nIf you don’t set window size and mean quality threshold for these function respectively, fastp will use the values from -W, --cut_window_size(Range: 1~1000, default: 4) and -M, --cut_mean_quality (Range: 1~36 default: 20)\n\nGlobal trimming: fastp supports global trimming, which means trim all reads in the front or the tail. This function is useful since sometimes you want to drop some cycles of a sequencing run.\n\nFor read1 or SE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1.\nFor read2 of PE data, the front/tail trimming settings are given with -F, --trim_front2 and -T, --trim_tail2. But if these options are not specified, they will be as same as read1 options, which means trim_front2 = trim_front1 and trim_tail2 = trim_tail1.\n\n-D, --dedup: enable deduplication to drop the duplicated reads/pairs\n\n\n\nExample code\nTo give a simple example for using FastP, assume we work with paired-end (i.e. reverse and forward) reads from a sample and we want to remove adaptors, reads with a quality using a phred-score cutoff of 20 and remove reads shorter than 100 bp.\n\n#activate the right environment\nmamba activate fastp\n\n#run fastp\nfastp \\\n  -i data/sample1_F.fastq.gz -I data/sample1_R.fastq.gz \\\n  -o filtered_data/sample1_F_filtered.fq.gz -O filtered_data/sample1_R_filtered.fq.gz \\\n  --thread 5 -q 20 -l 100\n\n#deactivate environment (if using environment)\nmamba deactivate",
    "crumbs": [
      "Sequence data analyses",
      "Quality control",
      "FastP"
    ]
  },
  {
    "objectID": "source/metagenomics/trycycler.html#trycycler",
    "href": "source/metagenomics/trycycler.html#trycycler",
    "title": "Bioinformatics guidance page",
    "section": "Trycycler",
    "text": "Trycycler\n\nIntroduction\nTrycycler is a tool for generating consensus long-read assemblies for bacterial genomes (Wick et al. 2021). I.e. if you have multiple long-read assemblies for the same isolate, Trycycler can combine them into a single assembly that is better than any of your inputs.\nLong-read assembly has come a long way in the last few years, and there are many good assemblers available, including Canu, Flye, Raven and Redbean. Since bacterial genomes are relatively simple (not too large and not too many repeats), a completed assembly (one contig per replicon) is often possible when assembling long reads. But even the best assemblers are not perfect and how do we decide what the best assembler for your purposes is?\nTrycycler is a tool that takes as input multiple separate long-read assemblies of the same genome (e.g. from different assemblers or different read subsets) and produces a consensus long-read assembly.\nIn brief, Trycycler does the following:\n\nClusters the contig sequences, so the user can distinguish complete contigs (i.e. those that correspond to an entire replicon) from spurious and/or incomplete contigs.\nReconciles the alternative contig sequences with each other and repairs circularisation issues.\nPerforms a multiple sequence alignment (MSA) of the alternative sequences.\nConstructs a consensus sequence from the MSA by choosing between variants where the sequences differ.\n\nFor a detailed usage information, deeper explanations and more, head over to the Trycycler wiki.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nTrycycler v0.5.5 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nThis installation also comes with several assemblers: Flye, Raven, Unicyler and miniasm/minipolish. If you want to use any of these assemblers independent of Trycycler, you can do so by simply activating the conda environment\nYou can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nAfterwards, you can activate the environment with:\n\nconda activate trycycler_0.5.5\n\nIf you want to install it yourself, you can run:\n\nmamba create -c bioconda -c conda-forge --name trycycler_0.5.5 trycycler=0.5.5\n\n#add different assemblers into the conda env\nmamba activate trycycler_0.5.5\n\nmamba install -c bioconda flye\n\npip3 install git+https://github.com/rrwick/Minipolish.git\nwget https://raw.githubusercontent.com/rrwick/Minipolish/main/miniasm_and_minipolish.sh -O &lt;path-to-trycyler-conda-environment&gt;/trycycler_0.5.5/bin/miniasm_and_minipolish.sh \nchmod +x /zfs/omics/projects/bioinformatics/software/miniconda3/envs/trycycler_0.5.5/bin/miniasm_and_minipolish.sh\nmamba install -c bioconda any2fasta\n\nmamba install -c bioconda raven-assembler\n\nmamba install -c bioconda unicycler\n\nconda deactivate\n\n\n\nUsage\nThe Trycyler assembly approach consists of multiple steps, and we strongly recommend that the users follows the step-by-step manual.\nAdditionally, you can find an example how a microbial genome was assembled here. This tutorial starts with raw Nanopore data and includes information how to do quality cleaning, assembly with 4 different assemblers, combining the different assemblies with Trycycler, polishing the assembly and assessing the assembly quality.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Trycycler"
    ]
  },
  {
    "objectID": "source/metagenomics/readme.html#general",
    "href": "source/metagenomics/readme.html#general",
    "title": "Bioinformatics guidance page",
    "section": "General",
    "text": "General\nIn this section you find a set of tools useful to analyse sequencing data. Due to the increased popularity in long-read technologies, you can find a separate section for tools suitable for Nanopore analyses.\nBelow you find a list of tutorials explaining how assemble and bin metagenome-assembled genomes (MAGs) from short-read sequencing data and how to analyse these MAGS. Notice: These tutorials were not developed at UvA and will be updated in the future but will nevertheless give a good starting point into these kind of analyses.\n\nTutorials\n\nAssembling a metagenome\nGenomic binning\nAnnotating microbial genomes\nHow to do phylogenetic analyses",
    "crumbs": [
      "Sequence data analyses"
    ]
  },
  {
    "objectID": "source/metagenomics/pygenometracks.html#pygenometracks",
    "href": "source/metagenomics/pygenometracks.html#pygenometracks",
    "title": "Bioinformatics guidance page",
    "section": "pyGenomeTracks",
    "text": "pyGenomeTracks\n\nIntroduction\npyGenomeTracks aims to produce high-quality genome browser tracks that are highly customizable (Lopez-Delisle et al. 2020). Currently, it is possible to plot:\n\nbigwig\nbed/gtf (many options)\nbedgraph\nbedgraph matrices (like TAD-separation scores)\nepilogos\nnarrow peaks\nlinks (represented as arcs, triangles or squares)\nHi-C matrices (as triangle or squares)\nfasta\nmaf (multiple alignment format)\n\nFor more examples, please visit the tools github or documentation.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\npyGenomeTracks v3.9 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name pygenometracks_3.9 -c bioconda -c conda-forge pygenometracks=3.9\n\n# Optional: Add tool to convert bam files into other formats\nconda activate pygenometracks_3.9\nmamba install -c conda-forge -c bioconda deeptools\n\nconda deactivate\n\n\n\nUsage\n\nOptional: Convert bam files\npyGenomeTracks can work with bam files but for some applications it can be useful to generate other output formats, for example if we wanted to depict coverage information separately for each strand.\nTo convert bam to bedgraph format with one file for each strand you can use bamCoverage from the deepTools software:\n\nbamCoverage --bam sample1.bam \\\n    --outFileFormat bedgraph \\\n    -o sample1.bam_forward.bedgraph \\\n    --filterRNAstrand forward\n\nbamCoverage --bam sample1.bam \\\n    --outFileFormat bedgraph \\\n    -o sample1.bam_reverse.bedgraph \\\n    --filterRNAstrand reverse\n\nGenerated output:\nBedgraph files give coordinate information (chr, start and end) as well as a numerical 4th column coverage information. The coverage is calculated as the number of reads per bin, where bins are short consecutive counting windows of a defined size. It is possible to extended the length of the reads to better reflect the actual fragment length. bamCoverage offers normalization by scaling factor, Reads Per Kilobase per Million mapped reads (RPKM), counts per million (CPM), bins per million mapped reads (BPM) and 1x depth (reads per genome coverage, RPGC).\nBy default the output is not normalized, if this is desired, visit the bamCoverage manual for more information.\n\n\nGenerate a configuration file\nTo run pyGenomeTracks a configuration file describing the tracks is required. The easiest way to create this file is using the program make_tracks_file which creates a configuration file with defaults that can be easily changed. The format is:\n\nmake_tracks_file --trackFiles &lt;file1.bedgraph&gt; &lt;file2.bedgraph&gt; ... -o tracks.ini\n\nIf you have several files, you can also provide them as a list:\n\n# Create a list with first all forward followed by all reverse files\nls *forward.bedgraph  &gt; img/tracks_bg\nls *reverse.bedgraph  &gt;&gt; img/tracks_bg\n\n# Generate a configuration file also including two genome annotation files as gtf (if we for example want to compare results from 2 assembly approaches)\nmake_tracks_file --trackFiles $(cat img/tracks_bg ) \\\n    assembly1.gtf \\\n    assembly2.gtf  \\\n    -o img/tracks.ini \n\n# Optional: Modify the configuration file \n# This can be done by manually opening the file or editing via sed\n# Example: Set labels = false to labels true and define where labels are depicted \nsed -i \"s/labels = false/labels = true/g\" img/tracks.ini\nsed -i \"s/#all_labels_inside = true/all_labels_inside = true/g\" img/tracks.ini\nsed -i \"s/#labels_in_margin = true/labels_in_margin = true/g\" img/tracks.ini\n\n# You can also add spacers to better separate different tracks, for example:\nsed -i '/\\[assembly1\\]/i \\\n[spacer]\\nheight = 0.5\\n' img/tracks.ini\n  \nsed -i '/\\[assembly2\\]/i \\\n[spacer]\\nheight = 0.5\\n' img/tracks.ini\n\n\n\nGenerate an image\nWith the configuration file, a region can be plotted using:\n\npyGenomeTracks --tracks img/tracks.ini --region chr2:10,000,000-11,000,000 --outFileName nice_image.pdf\n\nYou can also automate the plotting. For example, if we are interested in certain genes that are listed in the 9th column of the gtf file we can use some bash scripting to automatically extract the coordinates. Below is an example but this might need adjustment depending on the exact structure of your gtf files.\n\n# Define gene we care about, keep the \\\" for exact matches\nto_find=\"MSTRG.1125\\\"\"\n\n# Search the gtf file for the gene of interest and confirm that the right information is printed\nawk -F \"\\t\" -v gene=\"$to_find\" '$9 ~ gene' assembly1.gtf\n\n# If the above is correct, store the gene ID in a variable\ngene_id=$(awk -F \"\\t\" -v gene=\"$to_find\" '$9 ~ gene' assembly1.gtf | \\\n            cut -f9 | cut -f2 -d \" \" | sort -u)\n\n# Use the geneID to automatically label or output image\nfile_name=$(echo $gene_id | sed 's/\"//g;s/;//g')\n\n# Extract the coordinates and add some space to the left and right\ncoordinates=$(grep ${gene_id} assembly1.gtf | \\\n    awk '$3 == \"transcript\" {if (min == \"\" || $4 &lt; min) min = $4; if (max == \"\" || $5 &gt; max) max = $5} \\\n    END {printf \"%s:%\\047d-%\\047d\", $1, min-500, max+500, min, max}')\n\n# Confirm that the right coordinates were extracted\necho $coordinates\n\n# Generate an image \npyGenomeTracks --tracks img/tracks.ini --region $coordinates --outFileName img/${file_name}.pdf\n\nExample output:\n\nDescription of other possible arguments: \noptions:\n  -h, --help            show this help message and exit\n  --tracks TRACKS       File containing the instructions to plot the tracks.\n                        The tracks.ini file can be genarated using the\n                        `make_tracks_file` program.\n  --region REGION       Region to plot, the format is chr:start-end\n  --BED BED             Instead of a region, a file containing the regions to\n                        plot, in BED format, can be given. If this is the\n                        case, multiple files will be created. It will use the\n                        value of --outFileName as a template and put the\n                        coordinates between the file name and the extension.\n  --width WIDTH         figure width in centimeters (default is 40)\n  --plotWidth PLOTWIDTH\n                        width in centimeters of the plotting (central) part\n  --height HEIGHT       Figure height in centimeters. If not given, the figure\n                        height is computed based on the heights of the tracks.\n                        If given, the track height are proportionally scaled\n                        to match the desired figure height.\n  --title TITLE, -t TITLE\n                        Plot title\n  --outFileName OUTFILENAME, -out OUTFILENAME\n                        File name to save the image, file prefix in case\n                        multiple images are stored\n  --fontSize FONTSIZE   Font size for the labels of the plot (default is 0.3 *\n                        figure width)\n  --dpi DPI             Resolution for the image in case the ouput is a raster\n                        graphics image (e.g png, jpg) (default is 72)\n  --trackLabelFraction TRACKLABELFRACTION\n                        By default the space dedicated to the track labels is\n                        0.05 of the plot width. This fraction can be changed\n                        with this parameter if needed.\n  --trackLabelHAlign {left,right,center}\n                        By default, the horizontal alignment of the track\n                        labels is left. This alignemnt can be changed to right\n                        or center.\n  --decreasingXAxis     By default, the x-axis is increasing. Use this option\n                        if you want to see all tracks with a decreasing\n                        x-axis.\n  --version             show program's version number and exit",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "pyGenomeTracks"
    ]
  },
  {
    "objectID": "source/metagenomics/pseudofinder.html#pseudofinder",
    "href": "source/metagenomics/pseudofinder.html#pseudofinder",
    "title": "Bioinformatics guidance page",
    "section": "Pseudofinder",
    "text": "Pseudofinder\n\nIntroduction\nPseudofinder is a bioinformatics tool that detects pseudogene candidates from annotated genbank files of bacterial and archaeal genomes (Syberg-Olsen et al. 2022).\nIt has been tested mostly on genbank (.gbf/.gbk) files annotated by Prokka with the --compliant flag (i.e. including both /gene and /CDS annotations).\nThere are alternative programs for pseudogene finding and annotation (e.g. the NCBI Prokaryotic Genome Annotation Pipeline), but to the best of the software developers knowledge, none of them are open source or allow easy fine-tuning of parameters.\nFor more information, please check the wiki.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nPseudofinder v1.1.0 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\ncd /home/ndombro/personal/software\ngit clone https://github.com/filip-husnik/pseudofinder.git\ncd pseudofinder\nbash setup.sh\nconda activate pseudofinder\n\n#install a reference database, i.e. swissprot \ncd path_to_databases/uniprot\nwget https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\ngzip -d uniprot_sprot.fasta.gz \nmakeblastdb -in uniprot_sprot.fasta -parse_seqids -dbtype prot\n\n\n\nUsage\nInput files:\n\nPseudofinder requires the user to provide the genome in genbank format as well as a non-redundant protein database formatted for BlastP/BlastX searches. If possible, providing a reference genome allows Pseudofinder to include dN/dS calculations to identify pseudogenes.\nThe software developers recommend genbank (.gbf/.gbk) files generated by Prokka with the --compliant and --rfam flags. Annotating rRNAs, tRNAs, and other ncRNAs in Prokka is recommended to eliminate any false positive ‘pseudogene’ candidates. ORFs overlapping with non-coding RNAs such as rRNA can be sometimes misannotated in databases as ‘hypothetical proteins’.\nUsing very strict gene length cutt-offs in Pseudofinder (--length_pseudo &gt;0.90) should be avoided since it can lead to biased pseudogene calls in short proteins due to the signal peptide presence/absence (&lt;35 AA difference).\n\nDatabase recommendations:\n\nDatabase selection is critical to the speed and sensitivity of Pseudofinder. Users can provide any database they would like provided it is a non-redundant protein database formatted for BlastP/BlastX searches, but must keep in mind that larger databases will increase runtime while smaller databases could suffer in sensitivity if they lack relevant protein sequences. For those who don’t have manually curated databases tailored to their specific microbe, we recommend NCBI-NR (non-redundant) protein database (or similar such as SwissProt).\nIf you have access to the bioinformatics share, then SwissProt is already installed\n\nMore on how pseudogenes are detected and what categories are identified by Pseudofinder is described here.\n\nmkdir -p results/pseudofinder\n\n#run pseudofinder without reference\nconda activate pseudofinder_1.1.0\n\n#set variable tp access the reference protein database\npseudo_db=\"/zfs/omics/projects/bioinformatics/databases/uniprot/uniprot_sprot_070524.fasta\"\n\nsrun --cpus-per-task 20 --mem=10G pseudofinder.py annotate \\\n    --genome data/prokka/GCF_000005845.gbk \\\n    --outprefix results/pseudofinder/pseudofinder \\\n    --database $pseudo_db --threads 20\n\nconda deactivate\n\nCheck out this page for more information about the individual output files.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Pseudofinder"
    ]
  },
  {
    "objectID": "source/metagenomics/motus.html",
    "href": "source/metagenomics/motus.html",
    "title": "mOTUS",
    "section": "",
    "text": "mOTUS\n\nIntroduction\nThe mOTU profiler is a computational tool that estimates relative taxonomic abundance of known and currently unknown microbial community members using metagenomic shotgun sequencing data (Ruscheweyh et al. 2022). Check the wiki for more information.\n\n\nInstallation\nAvailable on Crunchomics: No\nYou can install mOTUs with conda/mamba as follows:\n\n#install the conda environment\nconda create -n motus_3.1.0\nconda install -n motus_3.1.0 -c bioconda\n\n#download the required databases\n#the database will get downloaded to gets downloaded to: conda_env_folder/motus_3.1.0/lib/python3.9/site-packages/motus\nconda activate motus_3.1.0\nmotus downloadDB\n\n\n\nUsage\nRequired inputs: Illumina reads (forward, reverse, unpaired) or Nanopore reads in fastq(.gz) format\nGenerated output(s):\n\na file with the relative abundance (or read counts) of the prokaryotic species found in the sample. More about the output can be found here\nthe results of the read mapping in BAM format (optional)\n\n\nExample for short-read Illumina data\n\n#prepare folders\nmkdir data\nmkdir -p results/motus_sr\n\n#run motus\nmotus profile -f data/SRR17913199_1.fastq -r data/SRR17913199_2.fastq \\\n    -n SRR17913199 \\\n    -t 30 \\\n    -o results/motus_sr/taxonomy_profile.txt\n\n#parse the results and extract only hits &gt;0 in decreasing order \nawk -F'\\t' -v OFS=\"\\t\" '!/^#/ && $2 &gt; 0' results/motus_sr/taxonomy_profile.txt | sort -t$'\\t' -k2,2nr\n\n\n\nExample for long-read data\n\n#prepare folders \nmkdir -p results/motus_lr\n\n#prepare long-reads to be profiled by mOTUs, this splits long reads into short reads of length 300\nsrun --cpus-per-task 1 --mem=10GB  motus prep_long -i data/SRR17913199.fastq \\\n    -o data/SRR17913199_prepped.fastq -n SRR17913199\n\n#run motus\nsrun --cpus-per-task 30 --mem=10GB motus profile -s data/SRR17913199_prepped.fastq \\\n    -n SRR17913199 \\\n    -t 30 \\\n    -o results/motus_lr/taxonomy_profile.txt\n\n#parse the results and extract only hits &gt;0 in decreasing order \nawk -F'\\t' -v OFS=\"\\t\" '!/^#/ && $2 &gt; 0' results/motus_lr/taxonomy_profile.txt | sort -t$'\\t' -k2,2nr\n\nUseful arguments for motus profile:\nInput options:\n\n-f FILE[,FILE] input file(s) for reads in forward orientation, fastq(.gz)-formatted\n-r FILE[,FILE] input file(s) for reads in reverse orientation, fastq(.gz)-formatted\n-s FILE[,FILE] input file(s) for unpaired reads, fastq(.gz)-formatted\n-n STR sample name [‘unnamed sample’]\n-i FILE[,FILE] provide SAM or BAM input file(s) (generated by motus map_tax)\n-m FILE provide a mgc reads count file (generated by motus calc_mgc)\n-db DIR provide a different database directory\n\nOutput options:\n\n-o FILE output file name [stdout]\n-I FILE save the result of BWA in BAM format (output of motus map_tax)\n-M FILE save the mgc reads count (output of motus calc_mgc)\n-e only species with reference genomes (ref-mOTUs)\n-u print the full name of the species\n-c print result as counts instead of relative abundances\n-p print NCBI taxonomy identifiers\n-B print result in BIOM format\n-C STR print result in CAMI format (BioBoxes format 0.9.1), Values: [precision, recall, parenthesis]\n-q print the full rank taxonomy\n-A print all taxonomic levels together (kingdom to mOTUs, override -k)\n-k STR taxonomic level [mOTU], Values: [kingdom, phylum, class, order, family, genus, mOTU]\n\nAlgorithm options:\n\n-g INT number of marker genes cutoff: 1=higher recall, 6=higher precision [3]\n-l INT min length of the alignment (bp) [75]\n-t INT number of threads [1]\n-v INT verbosity level: 1=error, 2=warning, 3=message, 4+=debugging [3]\n-y STR type of read counts [insert.scaled_counts], Values: [base.coverage, insert.raw_counts, insert.scaled_counts]\n\n\n\nMerging profiles\nIf you generated several profiles for different samples, you can merge them by providing a list of samples like this:\n\nmotus merge -i sampleX.motus,sampleY.motus,sampleZ.motus \\\n    -o results/motus/merged.txt\n\nNotice: For this to work its best to use the -n option when using motus profile to ensure that each sample is has a clear identifier in the abundance table.\nYou can also merge different profiles if they are all in the same directory like this:\n\nmotus merge -d results/motus \\\n    -o results/motus/merged.txt\n\n\n\n\n\n\n\n\nReferences\n\nRuscheweyh, Hans-Joachim, Alessio Milanese, Lucas Paoli, Nicolai Karcher, Quentin Clayssen, Marisa Isabell Keller, Jakob Wirbel, et al. 2022. “Cultivation-Independent Genomes Greatly Expand Taxonomic-Profiling Capabilities of mOTUs Across Various Environments.” Microbiome 10 (1). https://doi.org/10.1186/s40168-022-01410-z.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "mOTUS"
    ]
  },
  {
    "objectID": "source/metagenomics/metabolic.html#metabolic",
    "href": "source/metagenomics/metabolic.html#metabolic",
    "title": "Bioinformatics guidance page",
    "section": "Metabolic",
    "text": "Metabolic\n\nIntroduction\nMetabolic is a workflow developed by the AnantharamanLab. This software enables the prediction of metabolic and biogeochemical functional trait profiles to any given genome datasets. These genome datasets can either be metagenome-assembled genomes (MAGs), single-cell amplified genomes (SAGs) or isolated strain sequenced genomes.\nMETABOLIC has two main implementations, which are METABOLIC-G and METABOLIC-C. METABOLIC-G.pl allows for generation of metabolic profiles and biogeochemical cycling diagrams of input genomes and does not require input of sequencing reads. METABOLIC-C.pl generates the same output as METABOLIC-G.pl, but as it allows for the input of metagenomic read data, it will generate information pertaining to community metabolism.\nCheck out the manual for all usage options and more details about what the software exactly does (Zhou et al. 2022).\n\n\nInstallation\nAvailable on crunchomics: Yes,\n\nMetabolic v4.0 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAdditionally, the GTDB r214 database that is used by metabolic is installed on the metatools share. To get access to metatools, send an email with your Uva netID to Anna Heintz Buschart, a.u.s.heintzbuschart@uva.nl.\n\nAfter you were added to the bioinformatics and metatools share, you can add the conda environments that are installed in these shares as follows (if you have already done this in the past, you don’t need to run these commands):\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\nconda config --add envs_dirs /zfs/omics/projects/metatools/TOOLS/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\n#go into folder in which to install software\ncd ~/personal/software/METABOLIC_4.0\n\n#download environmental yaml (which tells conda what software to install)\nmkdir envs\nwget https://raw.githubusercontent.com/AnantharamanLab/METABOLIC/master/METABOLIC_v4.0_env.yml -P envs/\n\n#install dependencies via the environmental yaml (this env will be named METABOLIC_v4.0)\nmamba env create -f envs/METABOLIC_v4.0_env.yml\n\n#activate environment (NEEDS to be active to run the setup steps below)\nconda activate METABOLIC_v4.0\n\n#download a git clone of the METABOLIC workflow\ngit clone https://github.com/AnantharamanLab/METABOLIC.git\n\n#run bash setup script (needs some time, have patience)\ncd METABOLIC\nbash run_to_setup.sh\n\n\n\n\n\n\n\nNote\n\n\n\nThe command bash run_to_setup.sh installs some public databases, which might be useful for other things. These are:\n\nThe kofam database: KOfams are a customized HMM database of KEGG Orthologs (KOs). The KO (KEGG Orthology) database is a database of molecular functions represented in terms of functional orthologs and is useful to assign functions to your proteins of interest. The script will download the database from scratch and you will therefore always have the newest version installed when installing METABOLIC.\nThe dbCAN2 database: A database that can be used for carbohydrate-active enzyme annotation. The script downloads dbCAN v10.\nThe Meropds database: A database for peptidases (also termed proteases, proteinases and proteolytic enzymes) and the proteins that inhibit them. The script will download the most recent version from the internet.\n\n\n\nAdditionally to these databases, metabolic makes use of the gtdb database. If desired, you can install the GTDB database yourself as follows:\n\n#go into the folder into which you want to download the database\ncd /path/to/target \n\n#Manually download the latest reference data\n#Check, if you work with the latest database. When installing Metabolic, you might get some pointers how to set up gtdb and we recommend to follow those\nwget https://data.gtdb.ecogenomic.org/releases/release207/207.0/auxillary_files/gtdbtk_r207_v2_data.tar.gz\n\n#Extract the archive to a target directory:\n#change `/path/to/target/db` to whereever you want to install the db\ntar -xvzf gtdbtk_r207_v2_data.tar.gz -c \"/path/to/target/db\" --strip 1 &gt; /dev/null\n\n#cleanup\nrm gtdbtk_r207_v2_data.tar.gz\n\n#while the conda env for METABOLIC is activate link gtdb database\n#change \"/path/to/target/db\" to wherever you downloaded the database\nconda env config vars set GTDBTK_DATA_PATH=\"/path/to/target/db\"\n\n#reactivate env for the variable to be recognized\nconda deactivate\nconda activate METABOLIC_v4.0\n\n\n\nUsage example\nMETABOLIC has two scripts:\n\nMETABOLIC-G.pl: Allows for classification of the metabolic capabilities of input genomes.\nMETABOLIC-C.pl: Allows for classification of the metabolic capabilities of input genomes, calculation of genome coverage, creation of biogeochemical cycling diagrams, and visualization of community metabolic interactions and contribution to biogeochemical processes by each microbial group.\n\nInput:\n\nNucleotide fasta files (use -in-gn in the perl script)\nProtein faa files (use -in in the perl script)\nIllumina reads (use -r flag in the metabolic-c perl script) provided as un-compressed fastq files. This option requires you to provide the full path to your paired reads. Note that the two different sets of paired reads are separated by a line return (new line), and two reads in each line are separated by a “,” but not ” ,” or ” , ” (no spaces before or after comma). Blank lines are not allowed\nNanopore/PacBio long-reds (use r togher with -st illumina/pacbio/pacbio_hifi/nanopore to provide information about the sequencing type)\nIf you provide the raw reads, the should be given as a text file with the absolute paths that looks something like this:\n\n#Read pairs: \n/path/to/your/reads/file/SRR3577362_sub_1.fastq,/path/to/your/reads/file/SRR3577362_sub_2.fastq\n/path/to/your/reads/file/SRR3577362_sub2_1.fastq,/path/to/your/reads/file/SRR3577362_sub2_2.fastq\nImportant:\n\nEnsure that the fasta headers of each “.fasta” or “.faa” file is unique (all fasta or faa files will be concatenated together to make a “total.fasta” or “total.faa” file; be sure that all sequence headers are unique)\nthat your file names do not contain spaces (suggest to only use alphanumeric characters and underscores in the file names)\nbe sure that in the genomes folder, only the genomes are placed but not other files, for example, non-genome metagenomic assemblies, since METABOLIC will take in all the files within the folder as genomes.\nIf you want to use METABOLIC-C, only “fasta” files and the “-in-gn” flag are allowed to perform the analysis correctly.\n\nSome example files for testing can be found in METABOLIC_test_files/. The steps below show how to get the files and also includes some sanity checks that can be useful to test, whether your files are suitable to be used by metabolic:\n\nPreparing the input files\nWe start with downloading proteins from two genomes and do some cleaning, here we:\n\nMake sure the file header is concise and does not have ANY spaces and that ideally uses a ‘-’ (or any other unique delimiter) to separate the genome ID from the protein ID. Also avoid any unusual symbols, such as |, (, ), {, }…\nAdd not only the protein ID but also the genome ID sequence header\nIf you have a concise header + the bin ID in the header, it is easy for METABOLIC to concatenate the protein sequences of your genomes into one single file and still easily know from what genome the sequence originally came from\n\n\nmkdir -p results/faa/renamed\n\n#download some example genomes from NCBI\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/008/085/GCA_000008085.1_ASM808v1/GCA_000008085.1_ASM808v1_protein.faa.gz  -P results/faa/\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/017/945/GCA_000017945.1_ASM1794v1/GCA_000017945.1_ASM1794v1_protein.faa.gz  -P results/faa/\ngzip -d results/faa/*gz\n\n#view the header of one file\n#we see the header looks like this: &gt;ABU81185.1 translation initiation factor aIF-2 [Ignicoccus hospitalis KIN4/I]\n#this is too long and the extra characters, i.e. spaces, can disrupt downstream analysis, so let's fix that first\nhead results/faa/GCA_000017945.1_ASM1794v1_protein.faa \n\n#remove everything after the space in the fasta headers\nfor i in results/faa/*faa; do \nfilename=$(basename $i .faa)\nsed '/^&gt;/ s/ .*$//g' $i &gt; results/faa/${filename}_temp.faa\ndone\n\n#afterwards, the header looks like this; &gt;ABU81185.1\nhead results/faa/GCA_000017945.1_ASM1794v1_protein_temp.faa \n\n#next, we add the filename into the fasta header and store them in our new folder\n#this allows us to combine all genomes into 1 file has the benefit that you only need to run hmmsearch 1x\n#for this to work proberly its a good practice to add the genomeID into the fasta header so that you can easily distinguish from where each protein originally came\nfor i in results/faa/*_temp.faa; do \nfilename=$(basename $i _temp.faa | cut -f1 -d \".\")\nawk -v fname=\"$filename\" '/&gt;/{sub(\"&gt;\",\"&\"fname\"-\")}1' $i &gt; results/faa/renamed/$filename.faa\ndone\n\n#afterwards, the header looks like this; &gt;ABU81185.1\n#notice: how using basename with cut made our filename a bit shorter?\nhead results/faa/renamed/GCA_000017945.faa \n\n#cleanup the temporary files we made\nrm results/faa/*temp.faa\n\n\n\n\n\n\n\nTip: How does the sed command work\n\n\n\n\n\nLet’s look into how this code works:\n\nfor i in results/faa/*faa; do \nfilename=$(basename $i .faa)\nsed '/^&gt;/ s/ .*$//g' $i &gt; results/faa/${filename}_temp.faa\ndone\n\n\nfor i in results/faa/*faa; do: This line starts a loop that iterates over each file in the directory results/faa/ that ends with the extension .faa. The loop variable i will hold the path of each file in turn during each iteration.\nfilename=$(basename $i .faa): Inside the loop, this line extracts the base name of the current file ($i) by removing the directory path and the .faa extension. It assigns this base name to the variable filename.\nsed '/^&gt;/ s/ .*$//g' $i: This line uses the sed command to edit the contents of the current file ($i). Let’s break down the sed command:\n\n'/^&gt;/ s/ .*$//g': This is a regular expression pattern that sed will use to find and replace text in the file.\n\n^&gt;: This part of the pattern matches lines that start with &gt;.\ns/ .*$//g: This part of the pattern replaces any space character and everything after it on lines that match the pattern with nothing (i.e., it removes everything after the first space). Here, the s stands for substitution, and the .*$ matches any character (.) until the end of the line ($).\n\n$i: This is the file that sed will operate on, which is the current file in the loop.\n\nWe then store the output to a new file with &gt; results/faa/${filename}_temp.faa. This makes use of the filename variable to flexibly edit the filename.\n\ndone: This marks the end of the loop. So, in summary, this code loops through each file in the directory results/faa/ with the .faa extension, extracts the base name of each file, and then edits each file in-place to remove any text after the first space on lines that start with &gt;.\n\n\n\n\n\n\n\n\n\n\nTip: How does the awk command work\n\n\n\n\n\nLet’s look into how this code works:\n\nfor i in results/faa/*_temp.faa; do \nfilename=$(basename $i _temp.faa | cut -f1 -d \".\")\nawk -v fname=\"$filename\" '/&gt;/{sub(\"&gt;\",\"&\"fname\"-\")}1' $i &gt; results/faa/renamed/$filename.faa\ndone\n\n\nfor i in  results/faa/*_temp.faa; do: This line initiates a loop that iterates over each file in the directory results/faa/ with the _temp.faa extension. During each iteration, the variable i holds the path of the current file.\nfilename=$(basename $i _temp.faa | cut -f1 -d \".\"): Inside the loop, this line extracts the base name of the current file ($i) by removing the directory path and the _temp.faa extension using basename, and then it uses cut to split the filename at the “.” character and extracts the first part. The extracted base name is stored in the variable filename.\nawk -v fname=\"$filename\" '/&gt;/{sub(\"&gt;\",\"&\"fname\"-\")}1' $i &gt; results/faa/renamed/$filename.faa: This line utilizes the awk command to process the contents of the current file ($i). Let’s break down the awk command:\n\n-v fname=\"$filename\": This option passes the value of the shell variable filename to awk as an awk variable named fname.\n/&gt;/: This part of the pattern matches lines that contain &gt;.\n{sub(\"&gt;\",\"&\"fname\"-\")}: This action performs a substitution on lines that match the pattern:\n\nsub(\"&gt;\",\"&\"fname\"-\"): This substitutes the &gt; character with itself (&) followed by the value of fname and a hyphen (-). So, it’s essentially appending the value of fname followed by a hyphen after the &gt; character.\n\n1: This is a condition that always evaluates to true, triggering the default action of awk, which is to print the current line.\n$i: This is the file that awk will operate on.\n&gt; results/faa/renamed/$filename.faa: This redirects the output of awk to a new file with the same base name as the original file ($filename.faa), but located in the results/faa/renamed/ directory.\n\n\nSo, in summary, this code loops through each file in the directory results/faa/ with the .faa extension, extracts the base name of each file, and then uses awk to modify each file’s contents. It appends the base name followed by a hyphen after any line that starts with &gt;, and saves the modified content to a new file in the results/faa/renamed/ directory with the same base name as the original file.\n\n\n\n\n\nRun the G mode with faa files as input:\nImportant:\n\nMetabolic combines the faa files and generates a file named total.faa in the folder with the fasta sequences, if you re-run Metabolic, ensure that you first delete that file!\nAs described above, metabolic runs in different modes and also can take fasta files or raw reads as input. For details on how to do that, please visit the manual\n\n\nmkdir results/metabolic/ \n\nconda activate METABOLIC_v4.0 \n\nperl /zfs/omics/projects/bioinformatics/software/metabolic/v4.0.0/METABOLIC-G.pl \\\n  -in results/faa/renamed/ \\\n  -t 20 \\\n  -o results/metabolic\n\n#get a table with the raw results \n#this table is good to use for sanity checks and for example test if your protein is really absent\ncat results/metabolic/intermediate_files/Hmmsearch_Outputs/*hmmsearch_result.txt | sed -n '/^#/!p' &gt; results/metabolic/hmmer_raw.txt\n\nconda deactivate\n\nGenerated outputs (for more detail, check the manual):\n\nAll_gene_collections_mapped.depth.txt: The gene depth of all input genes (METABOLIC-C only)\n\nEach_HMM_Amino_Acid_Sequence/: The faa collection for each hmm file\nintermediate_files/: The hmmsearch, peptides (MEROPS), CAZymes (dbCAN2), and GTDB-Tk (only for METABOLIC-C) running intermediate files\nKEGG_identifier_result/: The hit and result of each genome by Kofam database\nMETABOLIC_Figures/: All figures output from the running of METABOLIC\nMETABOLIC_Figures_Input/: All input files for R-generated diagrams\nMETABOLIC_result_each_spreadsheet/: TSV files representing each sheet of the created METABOLIC_result.xlsx file\nMW-score_result/: The resulted table for MW-score (METABOLIC-C only)\n\nMETABOLIC_result.xlsx: The resulting excel file of METABOLIC\n\nRequired/Optional flags: (for a detail explanation, please read the manual)\n\n-in-gn [required if you are starting from nucleotide fasta files] Defines the location of the FOLDER containing the genome nucleotide fasta files ending with “.fasta” to be run by this program\n-in [required if you are starting from faa files] Defines the location of the FOLDER containing the genome amino acid files ending with “.faa” to be run by this program\n-r [required] Defines the path to a text file containing the location of paried reads\n-rt [optional] Defines the option to use “metaG” or “metaT” to indicate whether you use the metagenomic reads or metatranscriptomic reads (default: ‘metaG’). Only required when using METABOLIC-C\n-st [optional] To use “illumina” (for Illumina short reads), or “pacbio” (for PacBio CLR reads), or “pacbio_hifi” (for PacBio HiFi/CCS genomic reads (v2.19 or later)), or “pacbio_asm20” (for PacBio HiFi/CCS genomic reads (v2.18 or earlier)), or “nanopore” (for Oxford Nanopore reads) to indicate the sequencing type of metagenomes or metatranscriptomes (default: ‘illumina’; Note that all “illumina”, “pacbio”, “pacbio_hifi”, “pacbio_asm20”, and “nanopore” should be provided as lowercase letters and the underscore “_” should not be typed as “-” or any other marks)\n-t [optional] Defines the number of threads to run the program with (Default: 20)\n-m-cutoff [optional] Defines the fraction of KEGG module steps present to designate a KEGG module as present (Default: 0.75)\n-kofam-db [optional] Defines the use of the full (“full”) or reduced (“small”) KOfam database by the program (Default: ‘full’). “small” KOfam database only contains KOs present in KEGG module, using this setting will significantly reduce hmmsearch running time.\n-tax [optional] To calculate MW-score contribution of microbial groups at the resolution of which taxonomical level (default: “phylum”; other options: “class”, “order”, “family”, “genus”, “species”, and “bin” (MAG itself)). Only required when using METABOLIC-C\n-p [optional] Defines the prodigal method used to annotate ORFs (“meta” or “single”)(Default: “meta”)\n-o [optional] Defines the output directory to be created by the program (Default: current directory)\n\n\n\n\nAddon: Verifying your results\nIn bioinformatics, it’s important to look at your results with a critical eye. This is particularly crucial in functional assignments, where reliance on automated tools may yield inaccuracies. Let’s delve into this with a practical example:\nConsider Ignicoccus hospitalis (GCA_000017945), a genome purported to lack an ATPase according to our analysis. However, upon closer inspection of the literature, we discover that Ignicoccus does indeed possess an ATPase. This dissonance underscores the importance of critically evaluating computational findings.\n\nIf we make a list of the IDs that are required to make up an V/ATPase in prokaryotes and search the raw hmm results, we actually see that most KOs are found albeit some with lower bitscores.\n\ngrep -f atp_list results/metabolic/hmmer_raw.txt | grep \"GCA_000017945\"\n\nFor each hit, Metabolic uses specific bitscore cutoff values to decide whether it sees a hit as present or absent. This approach usually works well but can be too stringent as seen in our example. Due to that we can not be completely sure if a gene/module/pathway that is labelled as absent is truly absent.\nSo what do we do about this when we analyse our own genomes? There are a few things that can help:\n\nUtilize multiple annotation tools and compare their outcomes. Discrepancies among results can flag potential errors or ambiguities.\nReview raw output data (if possible) to assess the reliability of individual predictions. In the section about Hmmsearch you also find some code how you could run a hmmsearch against the KOs yourself.\nInstead of relying solely on one database, cross-reference findings from multiple sources such as Pfam, PGAP, COG, or arCOG. Consistency across databases enhances confidence in the results.\nCompare your findings against functional annotations of closely related genomes. For example, you can check the literature or check whether your organisms is part of the KEGG genome database.\nValidate specific protein functions using specialized tools like Blast or interproscan\n\n\n\nReferences",
    "crumbs": [
      "Sequence data analyses",
      "Functional annotation",
      "Metabolic"
    ]
  },
  {
    "objectID": "source/metagenomics/interproscan_readme.html#sec-interproscan",
    "href": "source/metagenomics/interproscan_readme.html#sec-interproscan",
    "title": "Bioinformatics guidance page",
    "section": "Interproscan",
    "text": "Interproscan\n\nIntroduction\nInterPro is a database which integrates together predictive information about proteins’ function from a number of partner resources, giving an overview of the families that a protein belongs to and the domains and sites it contains (Blum et al. 2021).\nUsers who have novel nucleotide or protein sequences that they wish to functionally characterise can use the software package InterProScan to run the scanning algorithms from the InterPro database in an integrated way. Sequences are submitted in FASTA format. Matches are then calculated against all of the required member database’s signatures and the results are then output in a variety of formats (Jones et al. 2014).\n\n\nInstallation\n\nNewest version\nNotice: The newest version does NOT run on crunchomics unless you do some extra steps during the installation since we need a newer java version and we need to update a tool used to analyse Prosite specific databases.\nThese changes require to move a few things around, so if you feel uncomfortable with this feel free to install an older version (for installation, see section #### Version for Crunchomics below) that works well with the default java version installed on Crunchomics.\nIf you want to work with the newest version and are not afraid of some extra steps do the following:\n\n# Go into a folder in which you have all your software installed\ncd software_dir\n\n# Make a new folder for the interproscan installation and go into the folder\nmkdir interproscan\ncd interproscan \n\n# Download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.64-96.0/interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum -c interproscan-5.64-96.0-64-bit.tar.gz.md5\n\n# Decompress the downloaded folder\ntar -pxvzf interproscan-5.64-96.0-*-bit.tar.gz\n\n# Index hmm models\ncd interproscan-5.64-96.0\npython3 setup.py -f interproscan.properties\n\n# Setup a conda environment with the newest java version\nconda deactivate\nmamba create --name java_f_iprscan -c conda-forge openjdk\n\n# Activate the environment and install some more dependencies\nconda activate java_f_iprscan\nmamba install -c conda-forge gfortran\n\n# The default installation will give an error with pfsearchV3 to solve this:\n# Get new pftools version via conda (v3.2.12)\nmamba install -c bioconda pftools\n\n# Remove old pftools that came with interprocscan\ncd bin/prosite\nrm pfscanV3\nrm pfsearchV3\n\n# Replace with new pftools we have installed via mamba (and which are found in the mamba env folder)\n# !!! exchange &lt;~/personal/mambaforge/&gt; with where your conda environments are installed!!!\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscan .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfscanV3 .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearch .\ncp ~/personal/mambaforge/envs/java_f_iprscan/bin/pfsearchV3 .\ncd ../..\n\n# Do testrun (ProSiteProfiles and ProSitePatterns not working yet, therefore this works best when setting what applications to run manually)\n#srun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv\nsrun -n 1 --cpus-per-task 1 --mem=8G ./interproscan.sh -i test_all_appl.fasta -f tsv -dp --appl TIGRFAM,SFLD,SUPERFAMILY,PANTHER,GENE3D,Hamap,Coils,SMART,CDD,PRINTS,PIRSR,AntiFam,Pfam,MobiDBLite,PIRSF,ProSiteProfiles \n\nconda deactivate\n\n\n\nVersion for Crunchomics\nOn Crunchomics, newer versions of interproscan do not run due to an incompatibility with the installed Java version. However, this version has an incompatibility with the local blast install, so some additional steps are also needed to get things working as well.\n\n# Download software\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz\nwget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.36-75.0/interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n# Recommended checksum to confirm the download was success\n# Must return *interproscan-5.64-96.0-64-bit.tar.gz: OK*\nmd5sum interproscan-5.36-75.0-64-bit.tar.gz.md5\n\n# Decompress the folder\ntar -pxvzf interproscan-5.36-75.0-64-bit.tar.gz\n\n# Index hmm models\ncd interproscan-5.36-75.0\n\n#fix a dependency issue with the blast install that comes with this version of interproscan\n## Setup new blast env (installed blast 2.16.0)\nmamba create -n python3.6.8 -c conda-forge python=3.6.8\nconda activate python3.6.8\n\n## Install a working version of blast\nmamba install -c bioconda blast=2.16.0\n\n## Replace problematic blast versions via symlinking\n## !!! Replace &lt;/zfs/omics/personal/ndombro/mambaforge/&gt; with the path to your conda environments!!!\nrm bin/blast/ncbi-blast-2.9.0+/rpsblast\nln -s /zfs/omics/personal/ndombro/mambaforge/envs/python3.6.8/bin/rpsblast bin/blast/ncbi-blast-2.9.0+/rpsblast\n\n# Do a test run to check the installation\nsrun -n1 --cpus-per-task 8 --mem=8G ./interproscan.sh -i test_proteins.fasta\n\n# Close the environment\nconda deactivate\n\n\n\n\nUsage\nRequired inputs:\n\nProtein fasta file\n\nGenerated outputs:\n\nTSV: a simple tab-delimited file format\nXML: the new “IMPACT” XML format\nGFF3: The GFF 3.0 format\nJSON\nSVG\nHTML\n\nNotice:\n\nInterproscan does not like * symbols inside the protein sequence. Some tools for protein calling, like prokka, use * add the end of a protein to indicate that the full protein was found. If your files have such symbols, use the code below to remove it first. Beware: using sed -i overwrites the content of your file. If that behaviour is not wanted use sed 's/*//g' Proteins_of_interest.faa &gt; Proteins_of_interest_new.faa instead.\nIf you are on Crunchomics (or most other servers): DO NOT run jobs on the head node, but add something like srun -n 1 --cpus-per-task 4 before the actual command\n\nExample code:\n\n#clean faa file \n#remove `*` as interproscan does not like that symbol\nsed -i 's/*//g' Proteins_of_interest.faa\n\n#run interproscan\n&lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --cpu 4 -i Proteins_of_interest.faa -d outputfolder -T outputfolder/temp --iprlookup --goterms\n\nTo check available options use &lt;path_to_install&gt;/interproscan-5.36-75.0/interproscan.sh --help or for more detailed information, see the documentation):",
    "crumbs": [
      "Sequence data analyses",
      "Functional annotation",
      "Interproscan"
    ]
  },
  {
    "objectID": "source/metagenomics/gtdb.html#gtdb_tk",
    "href": "source/metagenomics/gtdb.html#gtdb_tk",
    "title": "Bioinformatics guidance page",
    "section": "GTDB_tk",
    "text": "GTDB_tk\n\nIntroduction\nGTDB_Tk is a software toolkit for assigning objective taxonomic classifications to bacterial and archaeal genomes (Chaumeil et al. 2019). It uses the GTDB database to assign your genome(s) of interest to a taxonomy (Parks et al. 2020). For more information, visit the tool`s github and website.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nGTDB_Tk v2.4.0 and the GTDB v220 database are installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\n\nAfter you were added to the bioinformatics share you can add the conda environments that are installed in this share as follows (if you have already done this in the past, you don’t need to run this command):\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -n gtdbtk_2.4.0 -c bioconda gtdbtk=2.4.0\n\n#get gtdb data, v220 \ncd &lt;my_database_folder&gt;\nwget https://data.gtdb.ecogenomic.org/releases/latest/auxillary_files/gtdbtk_package/full_package/gtdbtk_data.tar.gz\ntar xvzf gtdbtk_data.tar.gz \n\n#link data to gtdbtk\nconda activate gtdbtk_2.4.0\nconda env config vars set GTDBTK_DATA_PATH=\"&lt;my_database_folder&gt;/gtdb/release220\";\nconda deactivate\n\n\n\nUsage\n\nmkdir -p results/gtdb \n\nconda activate gtdbtk_2.4.0\n\ngtdbtk classify_wf --genome_dir  genome_dir/ \\\n  --extension fasta \\\n  --out_dir results/gtdb \\\n   --mash_db  /zfs/omics/projects/bioinformatics/databases/gtdb/release220/gtdb_ref_sketch.msh \\\n  --cpus 20\n\nconda deactivate\n\nThe files with the name gtdbtk.*.summary.tsv will contain key information about the taxonomic assignment of your genome(s).\nFor a full set of options and description how the tool works, please visit the manual.\n\n\nCommon Issues and Solutions\n\nIssue 1: Running out of memory as described here\n\nSolution 1: Use --scratch_dir and --pplacer_cpus 1\n\nIssue 2: Using a gtdbtk version with the incorrect database\n\nSolution 2: Ensure that you use the right gtdbtk-db combination, as listed here",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "GTDB_tk"
    ]
  },
  {
    "objectID": "source/metagenomics/fastqc_readme.html#fastqc",
    "href": "source/metagenomics/fastqc_readme.html#fastqc",
    "title": "Bioinformatics guidance page",
    "section": "FastQC",
    "text": "FastQC\n\nIntroduction\nFastQC is a quality control tool for high throughput sequence data. For help with interpreting the output, please visit the website with some very good examples.\nFastQC can be run both on short- and long-read data.\nAvailable on Crunchomics: Yes\n\n\nInstallation\nFastQC already is installed on Crunchomics, if you want to install it on your own system check out the instructions found here.\n\n\nUsage\n\nInputs: FastQC can process bam,sam,bam_mapped,sam_mapped and fastq files\nOutput: An HTML quality report\n\nExample code:\n\n#get help \nfastqc --help\n\n#run on a single file\nfastqc myfile.fastq.gz -o outputfolder --threads 1\n\n#run on multiple files (assumes that the fastq files are in a folder called my_reads)\nfastqc  my_reads/*gz -o outputfolder --threads 20\n\n\n\nReferences",
    "crumbs": [
      "Sequence data analyses",
      "Quality control",
      "FastQC"
    ]
  },
  {
    "objectID": "source/metagenomics/dnaapler.html#dnaapler",
    "href": "source/metagenomics/dnaapler.html#dnaapler",
    "title": "Bioinformatics guidance page",
    "section": "dnaapler",
    "text": "dnaapler\n\nIntroduction\ndnaapler is a simple python program that takes a single nucleotide input sequence (in FASTA format), finds the desired start gene using blastx against an amino acid sequence database, checks that the start codon of this gene is found, and if so, then reorients the chromosome to begin with this gene on the forward strand (Bouras et al. 2024).\nIt was originally designed to replicate the reorientation functionality of Unicycler with dnaA, but for for long-read first assembled chromosomes. We have extended it to work with plasmids (dnaapler plasmid) and phages (dnaapler phage), or for any input FASTA desired with dnaapler custom, dnaapler mystery or dnaapler nearest.\nFor bacterial chromosomes, dnaapler chromosome should ensure the chromosome breakpoint never interrupts genes or mobile genetic elements like prophages. It is intended to be used with good-quality completed bacterial genomes, generated with methods such as Trycycler, Dragonflye or my own pipeline hybracter.\nAdditionally, you can also reorient multiple bacterial chromosomes/plasmids/phages at once using the dnaapler bulk subcommand.\nIf your input FASTA is mixed (e.g. has chromosome and plasmids), you can also use dnaapler all, with the option to ignore some contigs with the --ignore parameter.\nIf no DnaA sequence was found in your genome, you also can check out Circulator.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\ndnaapler v0.7.0 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\nmamba create -p /zfs/omics/projects/bioinformatics/software/miniconda3/envs/dnaapler_0.7.0 -c bioconda dnaapler=0.7.0\n\n\nUsage\nAvailable commands:\n\ndnaapler all: Reorients 1 or more contigs to begin with any of dnaA, terL, repA.\n\nPractically, this should be the most useful command for most users.\n\ndnaapler chromosome: Reorients your sequence to begin with the dnaA chromosomal replication initiator gene\ndnaapler plasmid: Reorients your sequence to begin with the repA plasmid replication initiation gene\ndnaapler phage: Reorients your sequence to begin with the terL large terminase subunit gene\ndnaapler custom: Reorients your sequence to begin with a custom amino acid FASTA format gene that you specify\ndnaapler mystery: Reorients your sequence to begin with a random CDS\ndnaapler largest: Reorients your sequence to begin with the largest CDS\ndnaapler nearest: Reorients your sequence to begin with the first CDS (nearest to the start). Designed for fixing sequences where a CDS spans the breakpoint.\ndnaapler bulk: Reorients multiple contigs to begin with the desired start gene - either dnaA, terL, repA or a custom gene.\n\n\nmkdir results/v3_trycycler/dnaapler/\n\n#find dnaA and reorient a genome to begin at dnaA\ndnaapler chromosome -i my_genome.fasta \\\n    -o results/dnaapler/ -p J4 -t 8\n\nFor a full list of options, check the manual or use dnappler command -h",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "dnaapler"
    ]
  },
  {
    "objectID": "source/metagenomics/coverm.html#coverm",
    "href": "source/metagenomics/coverm.html#coverm",
    "title": "Bioinformatics guidance page",
    "section": "CoverM",
    "text": "CoverM\n\nIntroduction\nCoverM aims to be a configurable, easy to use and fast DNA read coverage and relative abundance calculator focused on metagenomics applications (Aroney et al. 2025).\nCoverM calculates coverage of genomes/MAGs via coverm genome (help) or individual contigs via coverm contig (help). Calculating coverage by read mapping, its input can either be BAM files sorted by reference, or raw reads and reference genomes in various formats.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nCoverM v0.7.0 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name coverm_0.7.0 -c bioconda coverm=0.7.0\n\n\n\nUsage\n\nconda activate coverm_0.7.0\n\nmkdir results/coverm/\n\n#  Calculate read coverage per-genome\ncoverm genome \\\n  --interleaved data/sample1.fastq.gz data/sample2.fastq.gz data/sample3.fastq.gz \\\n  --genome-fasta-directory genome_folder \\\n  --genome-fasta-extension fasta \\\n  -t 8 \\\n  -m mean relative_abundance covered_fraction \\\n  -o results/coverm/output_coverm.tsv\n\nconda deactivate\n\nUseful settings:\n\n-1 PATH ..: Forward FASTA/Q file(s) for mapping. These may be gzipped or not.\n-2 PATH ..: Reverse FASTA/Q file(s) for mapping. These may be gzipped or not.\n-c, --coupled PATH ..: One or more pairs of forward and reverse possibly gzipped FASTA/Q files for mapping in order &lt;sample1_R1.fq.gz&gt; &lt;sample1_R2.fq.gz&gt; &lt;sample2_R1.fq.gz&gt; &lt;sample2_R2.fq.gz&gt; ..\n--interleaved PATH ..: Interleaved FASTA/Q files(s) for mapping. These may be gzipped or not.\n--single PATH ..: Unpaired FASTA/Q files(s) for mapping. These may be gzipped or not.\n-f, --genome-fasta-files PATH ..: Path(s) to FASTA files of each genome e.g. pathA/genome1.fna pathB/genome2.fa.\n-d, --genome-fasta-directory PATH: Directory containing FASTA files of each genome.\n-x, --genome-fasta-extension EXT: File extension of genomes in the directory specified with -d/–genome-fasta-directory. [default: fna]\n--genome-fasta-list PATH: File containing FASTA file paths, one per line.\n-p, --mapper NAME: Underlying mapping software used [default: minimap2-sr]. minimap2-sr, bwa-mem, bwa-mem2, minimap2-ont, minimap2-pb, minimap2-hifi, minimap2-no-preset\n-m, --methods METHOD: Method(s) for calculating coverage [default: relative_abundance]. A more thorough description of the different methods is available at here.\n\nFor a full set of options visit the coverm genome manual.\nIf you want to run CoverM on contigs, visit the coverm contig manual.\n\n\nTrouble shooting\nCoverM is quite memory intensive and might not run efficiently on many samples. For such cases, it might be preferable to run CoverM on a single sample at a time and optimize the run by using bash arrays or tools like GNU parallel.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "CoverM"
    ]
  },
  {
    "objectID": "source/metagenomics/barrnap.html#barrnap",
    "href": "source/metagenomics/barrnap.html#barrnap",
    "title": "Bioinformatics guidance page",
    "section": "Barrnap",
    "text": "Barrnap\n\nIntroduction\nBarrnap predicts the location of ribosomal RNA genes in genomes. It supports bacteria (5S, 23S, 16S), archaea (5S, 5.8S, 23S, 16S), metazoan mitochondria (12S,16S) and eukaryotes (5S,5.8S,28S,18S).\nIt takes FASTA DNA sequence as input, and write GFF3 as output. It uses the nhmmer tool that comes with HMMER 3.1 for HMM searching in RNA:DNA style. Multithreading is supported and one can expect roughly linear speed-ups with more CPUs.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nBarrnap v0.9 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name barrnap_0.9 -c bioconda barrnap=0.9\n\n\n\nUsage\nBarrnap takes as input a genome.fna file that NEEDS to have the fna extension. With other extensions you will encounter the following error: ERROR: No input file on command line or stdin.\n\nSingle genome\n\nconda activate barrnap_0.9 \n\nbarrnap \\\n  --kingdom bac --threads 20 \\\n  --outseq results/barrnap/genome_barrnap.fasta my_genome.fna\n\n\n\nMultiple genomes\n\n# Several genomes\nfor i in $(ls genome_folder/*fna); do \n  genome=$(basename $i .fna)\n\n  barrnap \\\n  --kingdom bac --threads 20 \\\n  --outseq results/barrnap/${genome}_barrnap.fasta \\\n  ${i}\ndone\n\nUseful options:\n\n--help show help and exit\n--version print version in form barrnap X.Y and exit\n--citation print a citation and exit\n--kingdom is the database to use: Bacteria:bac, Archaea:arc, Eukaryota:euk, Metazoan Mitochondria:mito\n--threads is how many CPUs to assign to nhmmer search\n--evalue is the cut-off for nhmmer reporting, before further scrutiny\n--lencutoff is the proportion of the full length that qualifies as partial match\n--reject will not include hits below this proportion of the expected length\n--quiet will not print any messages to stderr\n--incseq will include the full input sequences in the output GFF\n--outseq creates a FASTA file with the hit sequences",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Barrnap"
    ]
  },
  {
    "objectID": "source/metagenomics/augustus.html#augustus",
    "href": "source/metagenomics/augustus.html#augustus",
    "title": "Bioinformatics guidance page",
    "section": "Augustus",
    "text": "Augustus\n\nIntroduction\nAUGUSTUS is a program that predicts genes in eukaryotic genomic sequences (Stanke et al. 2008). It can be run on this web server, on a new web server for larger input files or be downloaded and run locally. If you also have transcriptomic data available, consider to also have a look at BRAKER to optimize the gene prediction process.\nAugustus can be used as an ab initio program, which means it bases its prediction purely on the sequence. AUGUSTUS may also incorporate hints on the gene structure coming from extrinsic sources such as EST, MS/MS, protein alignments and syntenic genomic alignments. Since version 3.0 AUGUSTUS can also predict the genes simultaneously in several aligned genomes.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nAugustus v3.5.0 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself via docker or singularity follow the instructions on the github page. If you lack the tools or necessary permission, it is also possible to setup Augustus via conda:\n\n# Change the directory to where you want to install augustus\ncd &lt;path_to_augustus_folder&gt;/\n\n# Clone the augustus git repository\ngit clone https://github.com/Gaius-Augustus/Augustus.git\n\n# Setup an augustus conda environment\nconda create -n augustus -c conda-forge -c bioconda -y\n\n# Install the required dependencies\nmamba install -c conda-forge -c bioconda gcc_linux-64 gxx_linux-64 wget git autoconf make gsl boost libiconv suitesparse lp_solve sqlite mysql-connector-cpp boost zlib bamtools samtools htslib cdbtools diamond perl-file-which perl-parallel-forkmanager perl-yaml perl-dbd-mysql biopython mysql\n\n# If you encounter the error: fatal error: mysql++/mysql++.h: No such file or directory\n# switch off MySQL usage by setting MYSQL = false in common.mk (found in the Augustus folder, you downloaded with git)\n\n# If you encounter the error: lp_lib.h: No such file or directory\n# switch off lp_solve usage by setting COMPGENEPRED = false in common.mk\n\n# Run make to install augustus itself\ncd Augustus\nmake augustus\n\n# Test if everything runs ok \n&lt;path_to_augustus_folder&gt;/Augustus/bin/augustus -h\n\n\n\nUsage\nAUGUSTUS has 2 mandatory arguments.\n\nThe query file, which contains the DNA input sequence and must be in uncompressed (multiple) fasta format\nThe species. AUGUSTUS has currently been trained on species specific training sets to predict genes in a list of species. To find the most appropriate species for your analysis, you can view a full list by running /zfs/omics/projects/bioinformatics/software/Augustus/bin/augustus --species=help\n\nInstructions for fasta headers:\n\nMost problems when running AUGUSTUS are caused by fasta headers in the sequence files. Some of the tools in our pipeline will truncate fasta headers if they are too long or contain spaces, or contain special characters. It is therefore strongly recommend that you adhere to the following rules for fasta headers:\n\nno whitespaces in the headers\nno special characters in the headers (e.g. !#@&|;)\nmake the headers as short as possible\nlet headers not start with a number but with a letter\nlet headers contain letters and numbers, only (and possibly underscores)\n\n\n\n# Activate the augustus conda environment\nconda activate augustus \n\n# Example for cleaning the fasta headers of the input sequence (adjust as needed for your purposes)\n# 1. Remove everything after a space in the fasta header \ncut -f1 -d \" \"   genome_with_wrong_headers.fasta  &gt;  genome.fasta \n\n# 2. Replace dots with underscores\nsed -i '/^&gt;/s/\\./_/g'  genome.fasta \n\n# Run Augustus\n/zfs/omics/projects/bioinformatics/software/Augustus/bin/augustus \\\n    --protein=on \\\n    --codingseq=on \\\n    --species=amphimedon \\\n    genome.fasta \\\n    &gt; output.gff\n\n# Extract CDS (output.codingseq) and proteins (output.aa)\n# Note, this only works if augustus is run with `--protein=on` and ` --species=amphimedon`\nperl /zfs/omics/projects/bioinformatics/software/Augustus/scripts/getAnnoFasta.pl output.gff\n\n# Exist the augustus conda environment\nconda deactivate\n\nUseful options, for a full list, go here:\n\n--strand=both, --strand=forward or --strand=backward: report predicted genes on both strands, just the forward or just the backward strand. default is ‘both’\n--genemodel=partial, --genemodel=intronless, --genemodel=complete, --genemodel=atleastone or --genemodel=exactlyone:\npartial : allow prediction of incomplete genes at the sequence boundaries (default)\nintronless : only predict single-exon genes like in prokaryotes and some eukaryotes\ncomplete : only predict complete genes\natleastone : predict at least one complete gene\nexactlyone : predict exactly one complete gene\n--singlestrand=true: predict genes independently on each strand, allow overlapping genes on opposite strands. This option is turned off by default.\n--hintsfile=hintsfilename: When this option is used the prediction considering hints (extrinsic information) is turned on. hintsfilename contains the hints in gff format.\n--extrinsicCfgFile=cfgfilename: Optional. This file contains the list of used sources for the hints and their boni and mali. If not specified the file “extrinsic.cfg” in the config directory $AUGUSTUS_CONFIG_PATH is used.\n--maxDNAPieceSize=n: This value specifies the maximal length of the pieces that the sequence is cut into for the core algorithm (Viterbi) to be run. Default is –maxDNAPieceSize=200000. AUGUSTUS tries to place the boundaries of these pieces in the intergenic region, which is inferred by a preliminary prediction. GC-content dependent parameters are chosen for each piece of DNA if /Constant/decomp_num_steps &gt; 1 for that species. This is why this value should not be set very large, even if you have plenty of memory.\n--protein=on/off\n--codingseq=on/off\n--introns=on/off\n--start=on/off\n--stop=on/off\n--cds=on/off\n--exonnames=on/off: Output options. Output predicted amino acid sequences or coding sequences. Or toggle the display of the GFF features/lines of type intron, start codon, stop codon, CDS or ‘initial’, ‘internal’, ‘terminal’ and ‘single’ exon type names. The CDS excludes the stop codon (unless stopCodonExcludedFromCDS=false) whereas the terminal and single exon include the stop codon.\n--AUGUSTUS_CONFIG_PATH=path: path to config directory (if not specified as environment variable)\n--alternatives-from-evidence=true/false: report alternative transcripts when they are suggested by hints\n--alternatives-from-sampling=true/false: report alternative transcripts generated through probabilistic sampling\n--gff3=on/off: output in gff3 format\n--UTR=on/off: predict the untranslated regions in addition to the coding sequence. This currently works only for human, galdieria, toxoplasma and caenorhabditis.\n--outfile=filename: print output to filename instead to standard output. This is useful for computing environments, e.g. parasol jobs, which do not allow shell redirection.\n--noInFrameStop=true/false: Don’t report transcripts with in-frame stop codons. Otherwise, intron-spanning stop codons could occur. Default: false\n--noprediction=true/false: If true and input is in genbank format, no prediction is made. Useful for getting the annotated protein sequences.\n--contentmodels=on/off: If ‘off’ the content models are disabled (all emissions uniformly 1/4). The content models are; coding region Markov chain(emiprobs), initial k-mers in coding region (Pls), intron and intergenic regin Markov chain. This option is intended for special applications that require judging gene structures from the signal models only, e.g. for predicting the effect of SNPs or mutations on splicing. For all typical gene predictions, this should be true. Default: on",
    "crumbs": [
      "Sequence data analyses",
      "Eukaryotic genomics",
      "Augustus"
    ]
  },
  {
    "objectID": "source/databases/readme.html",
    "href": "source/databases/readme.html",
    "title": "Bioinformatics guidance page",
    "section": "",
    "text": "tba"
  },
  {
    "objectID": "source/core_tools/tigrfam_db.html#tigrfam",
    "href": "source/core_tools/tigrfam_db.html#tigrfam",
    "title": "Bioinformatics guidance page",
    "section": "TIGRFAM",
    "text": "TIGRFAM\n\nIntroduction\nThe original TIGRFAMs database was a research project of The Institute for Genomic Research (TIGR) and its successor, the J. Craig Venter Institute (JCVI) (Haft, Selengut, and White 2003). TIGRFAMs is a collection of manually curated protein families focusing primarily on prokaryotic sequences. It consists of hidden Markov models (HMMs), multiple sequence alignments, Gene Ontology (GO) terminology, Enzyme Commission (EC) numbers, gene symbols, protein family names, descriptive text, cross-references to related models in TIGRFAMs and other databases, and pointers to literature.\nThe TIGRFAMs database was transferred in April 2018 to the National Center for Biotechnology Information (NCBI), which now holds the creative commons license to this data and is responsible for maintaining and distributing this intellectual property. The database is used in NCBI’s Prokaryotic Genome Annotation Pipeline for GenBank and RefSeq sequence annotation, and curators continue to revise existing models.\nNotice: Release 15.0 (January 2013) was the last full release of TIGRFAMs from JCVI and newer versions are maintained by NCBI. The up-to-date versions of all TIGRFAM models are available for download by FTP as a component of the current release of PGAP HMMs. They are recognizable by an accession number beginning with “TIGR”, or by the source designation “JCVI”\n\n\nInstallation\nAvailable on crunchomics: Yes,\n\nTIGRFAM is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\n\nThe TIGRFAMs database can be found here:\n\n/zfs/omics/projects/bioinformatics/databases/tigrfam/release_15.0.\n\nIf you want to download the database yourself, you can do:\n\nwget https://ftp.ncbi.nlm.nih.gov/hmm/TIGRFAMs/release_15.0/TIGRFAMs_15.0_HMM.LIB.gz\ngzip -d TIGRFAMs_15.0_HMM.LIB.gz\n\nTo get a linking of TIGRFAM ID to function, use this page",
    "crumbs": [
      "Bioinformatic databases",
      "TIGRFAM"
    ]
  },
  {
    "objectID": "source/core_tools/samtools.html#samtools",
    "href": "source/core_tools/samtools.html#samtools",
    "title": "Bioinformatics guidance page",
    "section": "Samtools",
    "text": "Samtools\n\nIntroduction\nSAMtools is a toolkit for manipulating alignments in SAM/BAM format, including sorting, merging, indexing and generating alignments in a per-position format (Danecek et al. 2021). The SAM format is a standard format for storing large nucleotide sequence alignments and is generated by many sequence alignment tools such as Bowtie or BWA. The BAM format is the binary form from SAM.\nFor more detailed information, please visit the manual.\n\n\nInstallation\nInstalled on crunchomics: Yes, samtools v1.9 is installed.\nIf you want to install it yourself, you can run:\n\nmamba create -n samtools_1.19.2\nmamba install -n samtools_1.19.2 -c bioconda samtools=1.19.2\nmamba activate samtools_1.19.2\n\n\n\nUsage\nThe basic usage of SAMtools is:\nsamtools COMMAND [options]\nThe following commands are available:\n\nview: SAM/BAM and BAM/SAM conversion\nsort: sort alignment file\nmpileup: multi-way pileup\ndepth: compute the depth\nfaidx: index/extract FASTA\ntview: text alignment viewer\nindex: index the alignment\nidxstats: generate BAM index stats\nfixmate: fix mate information\nflagstat: simple stats\ncalmd: recalculate MD/NM tags and = bases\nmerge: merge sorted alignments\nrmdup: remove PCR duplicates\nreheader: replace BAM header\ncat: concatenate BAMs\nbedcov: read depth per BED region\ntargetcut: cut fosmid regions\nphase: phase heterozygotes\nbamshuf: shuffle and group alignments by name\n\nFor detailed description and more information on a specific command, just type:\nsamtools COMMAND\nBelow, you find examples on how to run some of the most common samtools commands. For this, we start with the example of a researcher, who aligned Illumina reads from a sample to a reference genomes and generated a SAM file using Bowtie 2.\n\nWhat are SAM files\nThe SAM file is a tab-delimited text file that contains information for each individual read and its alignment to the reference.\nThe file begins with an optional header. The header describes the source of data, reference sequence, method of alignment,and might look slightly different depending on the aligner being used. Each section begins with character @ followed by a two-letter record type code. These are followed by two-letter tags and values.\nAfterwards, you see the alignment section. Each line corresponds to the alignment information for a single read. Each alignment line has 11 mandatory fields for essential mapping information and a variable number of other fields for aligner-specific information. This looks something like this:\n\nThe individual fields are:\n\nQNAME: Query name or read name, the same read name present in the header of the FASTQ file\nFLAG: numerical value providing information about read mapping and whether the read is part of a pair. To translate these flags into a more meaningful description, go here. In our example:\n\nThe 16 flag means that the short sequence maps on the reverse strand of the reference genome.\nThe 0 flag means that none of the bit-wise flags you see in the link are set. This means that your reads with flag 0 are unpaired (because the first flag, 0x1, is not set), successfully mapped to the reference (because 0x4 is not set) and mapped to the forward strand (because 0x10 is not set)\n\nRNAME: the reference sequence name\nPOS: refers to the 1-based leftmost position of the alignment\nMAPQ: the alignment quality, the scale of which will depend on the aligner being used. The maximum MAPQ value that Bowtie 2 generates is 42. In contrast, the maximum MAPQ value that BWA will generate is 37. The better the score the better the alignment quality\nCIGAR: a sequence of letters and numbers that represent the operations that are required to match the read to the reference. The letters are operations that are used to indicate which bases align to the reference (i.e. match, mismatch, deletion, insertion), and the numbers indicate the associated base lengths. For example 129M1I31M means that the first 129 bases match, then we have 1 insertion followed by 31 matches\nMRNM: the mate reference name\nMPOS: the mate position (1-based, leftmost)\nISIZE: the inferred insert size\nSEQ: the raw sequence\nQUAL: the associated quality values for each position in the read\n\n\n\nSamtools View\nWhile the SAM alignment file from Bowtie 2 is human readable, we need a BAM alignment file for downstream analysis. A BAM file is a binary equivalent version of the SAM file, i.e. the same file in a compressed format.\nWe can use the samtools view command to convert our SAM file into its binary compressed version (BAM) and save it to file. Once we generate the BAM file, we don’t need to retain the SAM file anymore, we can delete it to save space.\n\nsamtools view -h -S -b \\\n  -o SRR6344904_mapped.bam \\\n  SRR6344904_mapped.sam\n\nWe can adjust this command to ask very specific questions about our data as well. For example, we might ask how many insertions and deletions we have in our mapped reads. For this, we can use samtools view followed by some extra commands that we add with a pipe to:\n\nExtract only the mapped reads by removing the unmapped reads (-F 4)\nExtract the 6th field with the CIGAR string that contains information about insertions, deletions, etc (cut -f 6)\nExtract only alignments that have insertion or deletions (grep -P '[ID]')\nCount how many alignments have insertion or deletions (wc -l)\n\n\n#count total alignments\nSRR6344904_mapped_sorted.bam | wc -l  \n\n#count mapped alignments with ID events\nsamtools view -F 4 SRR6344904_mapped_sorted.bam | \\\n     cut -f 6 | \\\n     grep -P '[ID]' | \\\n     wc -l}\n\nUsed options:\n\n-h: include header in output\n-S: input is in SAM format\n-b: output BAM format\n-o: /path/to/output/file\n-F 4: exclude reads with the flag 4 (i.e. unmapped reads)\n-f 4: only keep reads with the flag 4 (i.e. unmapped reads)\n\n\n\nSamtools Sort\nSorting BAM files is recommended for most down-stream analyses and is done as follows:\n\nsamtools sort \\\n  SRR6344904_mapped.bam \\\n  -o SRR6344904_mapped_sorted.bam\n\n\n\nSamtools Index\nThe samtools index command creates a new index file that allows fast look-up of the data in a sorted SAM or BAM file.\n\nsamtools index SRR6344904_mapped_sorted.bam\n\n\n\nSamtools Idxstats\nThe samtools idxstats command prints stats for the BAM index file but it requires an index to run.\nThe output is TAB delimited with each line consisting of reference sequence name, sequence length, number of mapped reads and number of unmapped reads.\n\nsamtools idxstats SRR6344904_mapped_sorted.bam\n\n\n\nSamtools depth\nSamtools depth computes the read depth at each position or region\n\n#calculate the depth per position\n#header: the name of the contig or chromosome, the position, the number of reads aligned at that position\nsamtools depth  SRR6344904_mapped_sorted.bam | head\n\n#calculate the average coverage for all covered regions\nsamtools depth  SRR6344904_mapped_sorted.bam |  awk '{sum+=$3} END { print \"Average = \",sum/NR}'\n\n#calculate the average coverage for all regions\nsamtools depth -a SRR6344904_mapped_sorted.bam |  awk '{sum+=$3} END { print \"Average = \",sum/NR}'\n\nNotice:\n\nIf you want to calculate the average X coverage for your genome, then you would need to divide by the total size of your genome, instead of dividing by NR in the command above.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "Samtools"
    ]
  },
  {
    "objectID": "source/core_tools/prokka.html#prokka",
    "href": "source/core_tools/prokka.html#prokka",
    "title": "Bioinformatics guidance page",
    "section": "Prokka",
    "text": "Prokka\n\nIntroduction\nProkka is a software tool to annotate bacterial, archaeal and viral (but NOT eukaryotic) genomes quickly and produce standards-compliant output files (Seemann 2014). Whole genome annotation is the process of identifying features of interest in a set of genomic DNA sequences, and labeling them with useful information. Briefly, Prokka uses a two-step process for the annotation of protein coding regions:\n\nprotein coding regions on the genome are identified using Prodigal\nthe function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. These databases are ISfinder, NCBI Bacterial Antimicrobial Resistance Reference Gene Database and UniProtKB. You can also add your own databases, such as the TIGRFAM or Pfam databases.\n\nVisit the manual for more information.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nProkka v1.14.6 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -n prokka -c conda-forge -c bioconda -c defaults prokka\n\n#setup extra databases (optional, but useful for better functional assignments)\n#to do this:\n#cd into whatever folder prokka gets installed followed by db/hmm, \n#this might look something like this:\ncd /zfs/omics/projects/bioinformatics/software/miniconda3/envs/prokka_1.14.6/db/hmm\n\nwget https://ftp.ncbi.nlm.nih.gov/hmm/TIGRFAMs/release_15.0/TIGRFAMs_15.0_HMM.LIB.gz\ngzip -d TIGRFAMs_15.0_HMM.LIB.gz\nmv TIGRFAMs_15.0_HMM.LIB 1-TIGRFAMs_15.0.hmm\n\nwget ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz\ngzip -d Pfam-A.hmm.gz \nmv Pfam-A.hmm 2-Pfam-A.hmm\n\nmv HAMAP.hmm 3-HAMAP.hmm\n\n#ensure that the prokka databases are setup correctly\nconda activate prokka\n\nprokka --setupdb\n\nconda deactivate",
    "crumbs": [
      "Sequence data analyses",
      "Core tools",
      "Prokka"
    ]
  },
  {
    "objectID": "source/core_tools/prokka.html#usage",
    "href": "source/core_tools/prokka.html#usage",
    "title": "Bioinformatics guidance page",
    "section": "Usage",
    "text": "Usage\nLet’s assume you downloaded a genome from NCBI and want to annotate the genome or if you have a set of MAGs and you want to identify the coding regions.\nRequired input: The contigs from a genome of interest\nGenerated output:\n\n.gff This is the master annotation in GFF3 format, containing both sequences and annotations. It can be viewed directly in Artemis or IGV.\n.gbk This is a standard Genbank file derived from the master .gff. If the input to prokka was a multi-FASTA, then this will be a multi-Genbank, with one record for each sequence.\n.fna Nucleotide FASTA file of the input contig sequences.\n.faa Protein FASTA file of the translated CDS sequences.\n.ffn Nucleotide FASTA file of all the prediction transcripts (CDS, rRNA, tRNA, tmRNA, misc_RNA)\n.sqn An ASN1 format “Sequin” file for submission to Genbank. It needs to be edited to set the correct taxonomy, authors, related publication etc.\n.fsa Nucleotide FASTA file of the input contig sequences, used by “tbl2asn” to create the .sqn file. It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines.\n.tbl Feature Table file, used by “tbl2asn” to create the .sqn file.\n.err Unacceptable annotations - the NCBI discrepancy report.\n.log Contains all the output that Prokka produced during its run. This is a record of what settings you used, even if the –quiet option was enabled.\n.txt Statistics relating to the annotated features found.\n.tsv Tab-separated file of all features: locus_tag,ftype,len_bp,gene,EC_number,COG,product\n\n\nmkdir data \n\n#download genome \nwget -O - https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz | gzip -d &gt; data/GCF_000005845.2_ASM584v2_genomic.fna\n\nconda activate prokka_1.14.6 \n\nprokka \\\n    --outdir data/prokka \\\n    --prefix GCF_000005845 \\\n    data/GCF_000005845.2_ASM584v2_genomic.fna \\\n    --cpus 4 --kingdom Bacteria\n\nconda deactivate\n\nFor a list of all options, visit the manual. Some key options are:\n\n--outdir [X] Output folder [auto] (default ’’)\n--force Force overwriting existing output folder (default OFF)\n--prefix [X] Filename output prefix [auto] (default ’’)\n--addgenes Add ‘gene’ features for each ‘CDS’ feature (default OFF)\n--locustag [X] Locus tag prefix (default ‘PROKKA’)\n--increment [N] Locus tag counter increment (default ‘1’)\n--kingdom [X] Annotation mode: Archaea|Bacteria|Mitochondria|Viruses (default ‘Bacteria’)\n--gcode [N] Genetic code / Translation table (set if –kingdom is set) (default ‘0’)\n--metagenome Improve gene predictions for highly fragmented genomes (default OFF)\n--norrna Don’t run rRNA search (default OFF)\n--notrna Don’t run tRNA search (default OFF)\n--compliant: Force Genbank/ENA/DDJB compliance: –addgenes –mincontiglen 200 –centre XXX (default OFF). This setting can be useful if you need a specific format for downstream analyses.\n\nUnsure what genetic code/translation table to use? Check out wikipedia. You mainly need to watch out for this when working with mitochondria, mycoplasma and spiroplasma.",
    "crumbs": [
      "Sequence data analyses",
      "Core tools",
      "Prokka"
    ]
  },
  {
    "objectID": "source/core_tools/pfam_db.html#pfam-database",
    "href": "source/core_tools/pfam_db.html#pfam-database",
    "title": "Bioinformatics guidance page",
    "section": "Pfam database",
    "text": "Pfam database\n\nGeneral info\nPfam is a collection of protein family alignments which were constructed semi-automatically using hidden Markov models (HMMs) (Bateman et al. 2004). Sequences that are not covered by Pfam are clustered, aligned and automatically and released as Pfam-B. Pfam-B used to be integrated into the Pfam website, in addition to being available as a flatfile. It was discontinued from Pfam 28.0 to Pfam 33.0. As of Pfam 33.1,Pfam-B entries are available as a tar archive of alignments. Pfam families have permanent accession numbers and contain functional annotation and cross-references to other databases, while Pfam-B families are re-generated at each release and are unannotated.\nPfam is available on the web at:\nhttps://www.ebi.ac.uk/interpro/\n\n\nInstallation\nAvailable on crunchomics: Yes,\n\nPfam is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\n\nThe Pfam database can be found here:\n\n/zfs/omics/projects/bioinformatics/databases/pfam/release_36.0/.\n\nIf you want to download the database yourself, you can do:\n\nwget ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz\nwget https://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.clans.tsv.gz\n\ngzip -d Pfam-A.hmm.gz \ngzip -d Pfam-A.clans.tsv.gz\n\n#cleanup mapping file \nsed -i \"s/ /_/g\" Pfam-A.clans.tsv",
    "crumbs": [
      "Bioinformatic databases",
      "Pfam database"
    ]
  },
  {
    "objectID": "source/core_tools/ncbi.html#downloading-data-from-ncbi",
    "href": "source/core_tools/ncbi.html#downloading-data-from-ncbi",
    "title": "Bioinformatics guidance page",
    "section": "Downloading data from NCBI",
    "text": "Downloading data from NCBI\n\nIntroduction\nOn this page you will get introduced to a set of tools that are useful to download genomes as well as sequencing data from NCBI:\n\nsra-tools: a collection of tools and libraries for using data in the INSDC Sequence Read Archives\nncbi-datasets-cli: NCBI Datasets is a new resource that lets you easily gather data from across NCBI databases\nparallel-fastq-dump: is a tool that tool up the process of downloading sra data and is a multi-thread alternative to sra-tools\n\n\n\nInstallation\nSince we work with a set of programs that fulfill a similar purpose we install them in one single conda environment as follows:\n\nmamba create -n ncbi_tools\n\nmamba install -n ncbi_tools \\\n    -c conda-forge -c bioconda \\\n    sra-tools parallel-fastq-dump ncbi-datasets-cli\n\nmamba activate ncbi_tools",
    "crumbs": [
      "Sequence data analyses",
      "Core tools",
      "Downloading data from NCBI"
    ]
  },
  {
    "objectID": "source/core_tools/ncbi.html#usage",
    "href": "source/core_tools/ncbi.html#usage",
    "title": "Bioinformatics guidance page",
    "section": "Usage",
    "text": "Usage\n\nDownloading a single SRA archive using sra-tools\nLet’s assume we want to download some sequencing data, for example some paired-end Illumina sequencing data from which we know the SRA accession, for example SRR27829729.\n\nmkdir data\n\n#download sra data \nprefetch SRR27829729 -O data\n\n#convert to fastq.gz\n#the space you need during the conversion is approximately 17 times the size of the accession\nfasterq-dump SRR27829729 -O data\n\n#compress files to reduce file size \ngzip data/*fastq\n\nFor paired-end data, fasterq-dump will automatically generate separate files for the forward and reverse reads.\n\n\nDownloading several SRA archives using sra-tools\nLet’s now assume we want to download two (or more) SRA archives. We can do this by first preparing a list with all the SRAs we want to download. I am doing this with echo, but you can easily prepare a list in excel.\n\n#prepare a list of SRAs we want to work with \necho -e \"SRR27829729\\nSRR27829749\" &gt; sra_list\n\n#get sra \nprefetch $(&lt;sra_list) -O data\n\n#convert to fastq\nfasterq-dump $(&lt;sra_list) -O data\n\n#compress files to reduce file size \ngzip data/*fastq\n\n\n\nDownloading several SRA archives using parallel-fastq-dump\nIf you have few data files to download, the sra-tools are good to use. However, with more data you generate a lot of intermediate files that might need a lot of space. parallel-fastq-dump allows to combine all steps in once and make use of several threads (if needed). Assuming we need a lot of space, we can also redirect the temporary directory, which the tool uses to store the sra files, elsewhere.\n\nmkdir temp \n\nparallel-fastq-dump \\\n    --sra-id $(&lt;sra_list) \\\n    --split-files \\\n    --threads 4 --gzip -T temp \\\n    --outdir data \n\nOptions:\nparallel-fastq-dump is a parallel version of fast-dump, the predecessor of fasterq-dump. As such you can add any option you see when typing parallel-fastq-dump -h as well as fastq-dump -h. In the example above, we use the option --split-files in order to split the reads into separate files for the reverse and forward reads.\n\n\nDownloading a genome from NCBI\nWe can use the datasets command from the ncbi-datasets-cli software to easily download a genome from NCBI as follows:\n\ndatasets download genome accession GCF_000385215.1 \\\n    --include gff3,protein,genome \\\n    --filename GCF_000385215.zip\n\n#unzip the data directory\nunzip GCF_000385215.zip\n\n#cleanup\nrm GCF_000385215.zip\n\n\n\nDownloading multiple genomes from NCBI\n\nmkdir data\n\n#prepare a list of accessions we want to work with \necho -e \"GCF_000385215.1\\nGCA_000774145.1\" &gt; genome_list\n\n#download data\nfor i in `cat genome_list`; do\n    datasets download genome accession ${i} \\\n        --include gff3,protein,genome \\\n        --filename ${i}.zip\ndone \n\n#unzip the data directories\nfor i in `cat genome_list`; do\n    unzip -j ${i}.zip -d data/${i}\ndone \n\n#rename the generic protein and gff file names by adding the accession (works if more than one genome is downloaded)\nfor file in data/*/{protein.faa,genomic.gff}\ndo\n    directory_name=$(dirname $file)\n    accession=$(basename $directory_name)\n    mv \"${file}\" \"${directory_name}/${accession}_$(basename $file)\"\ndone\n\n\n#cleanup\nrm *.zip\n\nFor more functionality of ncbi-datasets-cli visit this NCBI website.",
    "crumbs": [
      "Sequence data analyses",
      "Core tools",
      "Downloading data from NCBI"
    ]
  },
  {
    "objectID": "source/core_tools/hmmer.html#hmmer",
    "href": "source/core_tools/hmmer.html#hmmer",
    "title": "Bioinformatics guidance page",
    "section": "HMMER",
    "text": "HMMER\n\nIntroduction\nHMMER is used for searching sequence databases for sequence homologs, and for making sequence alignments (Eddy 2011). It implements methods using probabilistic models called profile hidden Markov models (profile HMMs).\nHMMER is often used together with a profile database, such as Pfam or many of the databases that participate in Interpro. But HMMER can also work with query sequences, not just profiles, just like BLAST. For example, you can search a protein query sequence against a database with phmmer, or do an iterative search with jackhmmer.\nHMMER is designed to detect remote homologs as sensitively as possible, relying on the strength of its underlying probability models. In the past, this strength came at significant computational expense, but as of the new HMMER3 project, HMMER is now essentially as fast as BLAST.\nFor more information, please visit the official website and have a look at the User’s guide.\n\n\nInstallation\nInstalled on crunchomics: Yes,HMMER 3.3.2 is installed\n\n\nUsage\nHMMER is a software suite with a lot of programs and possibilities, for a full list have a look at the User’s guide. Some key programs are used to:\n\nBuild models and align sequences (DNA or protein)\n\nhmmbuild - Build a profile HMM from an input multiple alignment.\nhmmalign - Make a multiple alignment of many sequences to a common profile HMM\n\nSearch protein queries against protein database\n\nphmmer - Search a single protein sequence against a protein sequence database. (BLASTP-like)\njackhmmer - Iteratively search a protein sequence against a protein sequence database. (PSIBLAST-like)\nhmmsearch - Search a protein profile HMM against a protein sequence database.\nhmmscan - Search a protein sequence against a protein profile HMM database.\nhmmpgmd - Search daemon used for hmmer.org website.\n\n\nNotice, that either hmmsearch or hmmscan can compare a set of profiles to a set of sequences. Due to disk access patterns of the two tools, it is usually more efficient to use hmmsearch, unless the number of profiles greatly exceeds the number of sequences.\nExamples:\n\nGenerate your own hmm for searches\nIn some cases a good hmm might not exist but you can easily create your own. For this, collect all proteins for your orthologue of interest. For example, we might want to build a custom profile for a methyl coenzyme M reductase (mcrA). To do this, search for protein sequences in databases, such as NCBI and generate a protein fasta file. Next, you can generate a multiple sequence (MSA) alignment with tools such as MAFFT, let’s call this file mcrA.aln. For good alignments, consider to manually inspect it and discard sequences that don’t align well.\nThen you can run:\n\n#generate a hmm profile\nhmmbuild mcrA.hmm mcrA.aln\n\n#we can then use this profile to against a protein database,\n#my_proteins.faa could be proteins from a genome for which we want to check for the presence of mcrA proteins\nhmmsearch mcrA.hmm my_proteins.faa &gt; mcrA.out\n\n\n\nCompare proteins of interest against a hmm database\n\nhmmsearch \\\n    --tblout results/sequence_results.txt \\\n    --domtblout results/domain_results.txt \\\n    --notextw \\\n    --cpu 10 \\\n    KO_db.hmm \\\n    results/prokka/bin5.faa\n\nOutput:\n\nThe --tblout output option produces the target hits table. The target hits table consists of one line for each different query/target comparison that met the reporting thresholds, ranked by decreasing statistical significance (increasing E-value). Page 67 of the User’s guide explains each column in more detail.\nIn protein search programs, the –domtblout option produces the domain hits table. There is one line for each domain. There may be more than one domain per sequence. Page 70 of the User’s guide explains each column in more detail.\n--notextw: Unlimits the length of each line in the main output. The default is a limit of 120 characters per line, which helps in displaying the output cleanly on terminals and in editors, but can truncate target profile description lines.\n\nFor more options, check the manual or use hmmsearch -h\n\n\nCompare proteins of interest against a hmm database (advanced)\nLet’s do this in an advanced mode that goes into preparing the input files well and parsing the hmmsearch results afterwards. This is an advanced mode since we will use a lot of bash commands to format the outputs into what we want. You can easily do this in R or python but this code was added here to show an example for how flexible the command line is.\nIts recommended to view the file after each step with head or nano in order to understand what the step does. This is the best way to know what the code does and adjust it, if needed, for your ow analyses.\n\nPreparing the input files\nWe start with downloading proteins from two genomes and do some cleaning, here we:\n\nMake sure the file header is concise and does not have ANY spaces and that ideally uses a ‘-’ (or any other unique delimiter) to separate the genome ID from the protein ID. Also avoid any unusual symbols, such as |, (, ), {, }…\nAdd not only the protein ID but also the genome ID sequence header\nIf you have a concise header + the bin ID in the header, it is easy to concatenate the protein sequences of your genomes into one single file and still easily know from what genome the sequence originally came from\n\n\nmkdir -p results/faa/renamed\nmkdir results/kos/\n\n#download some example genomes from NCBI\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/008/085/GCA_000008085.1_ASM808v1/GCA_000008085.1_ASM808v1_protein.faa.gz  -P results/faa/\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/017/945/GCA_000017945.1_ASM1794v1/GCA_000017945.1_ASM1794v1_protein.faa.gz  -P results/faa/\ngzip -d results/faa/*gz\n\n#view the header of one file\n#we see the header looks like this: &gt;ABU81185.1 translation initiation factor aIF-2 [Ignicoccus hospitalis KIN4/I]\n#this is too long and the extra characters, i.e. spaces, can disrupt downstream analysis, so let's fix that first\nhead results/faa/GCA_000017945.1_ASM1794v1_protein.faa \n\n#remove everything after the space in the fasta headers\nfor i in  results/faa/*faa; do \nfilename=$(basename $i .faa)\nsed -i '/^&gt;/ s/ .*$//g' $i\ndone\n\n#afterwards, the header looks like this; &gt;ABU81185.1\nhead results/faa/GCA_000017945.1_ASM1794v1_protein.faa \n\n#next, we add the filename into the fasta header and store them in our new folder\n#this allows us to combine all genomes into 1 file has the benefit that you only need to run hmmsearch 1x\n#for this to work proberly its a good practice to add the genomeID into the fasta header so that you can easily distinguish from where each protein originally came\nfor i in  results/faa/*faa; do \nfilename=$(basename $i .faa | cut -f1 -d \".\")\nawk -v fname=\"$filename\" '/&gt;/{sub(\"&gt;\",\"&\"fname\"-\")}1' $i &gt; results/faa/renamed/$filename.faa\ndone\n\n#afterwards, the header looks like this; GCA_000017945-ABU81186.1\nhead results/faa/renamed/GCA_000017945.faa \n\n#after ensuring our files have good headers, we can combine the individual faa files \ncat results/faa/renamed/*.faa &gt; results/faa/combined.faa\n\n\n\n\n\n\n\nTip: How does the sed command work\n\n\n\n\n\nLet’s look into how this code works:\n\nfor i in  results/faa/*faa; do \nfilename=$(basename $i .faa)\nsed -i '/^&gt;/ s/ .*$//g' $i\ndone\n\n\nfor i in results/faa/*faa; do: This line starts a loop that iterates over each file in the directory results/faa/ that ends with the extension .faa. The loop variable i will hold the path of each file in turn during each iteration.\nfilename=$(basename $i .faa): Inside the loop, this line extracts the base name of the current file ($i) by removing the directory path and the .faa extension. It assigns this base name to the variable filename.\nsed -i '/^&gt;/ s/ .*$//g' $i: This line uses the sed command to edit the contents of the current file ($i). Let’s break down the sed command:\n\n-i: This flag tells sed to edit the file in-place.\n'/^&gt;/ s/ .*$//g': This is a regular expression pattern that sed will use to find and replace text in the file.\n\n^&gt;: This part of the pattern matches lines that start with &gt;.\ns/ .*$//g: This part of the pattern replaces any space character and everything after it on lines that match the pattern with nothing (i.e., it removes everything after the first space). Here, the s stands for substitution, and the .*$ matches any character (.) until the end of the line ($).\n\n$i: This is the file that sed will operate on, which is the current file in the loop.\n\ndone: This marks the end of the loop. So, in summary, this code loops through each file in the directory results/faa/ with the .faa extension, extracts the base name of each file, and then edits each file in-place to remove any text after the first space on lines that start with &gt;.\n\n\n\n\n\n\n\n\n\n\nTip: How does the awk command work\n\n\n\n\n\nLet’s look into how this code works:\n\nfor i in  results/faa/*faa; do \nfilename=$(basename $i .faa | cut -f1 -d \".\")\nawk -v fname=\"$filename\" '/&gt;/{sub(\"&gt;\",\"&\"fname\"-\")}1' $i &gt; results/faa/renamed/$filename.faa\ndone\n\n\nfor i in results/faa/*faa; do: This line initiates a loop that iterates over each file in the directory results/faa/ with the .faa extension. During each iteration, the variable i holds the path of the current file.\nfilename=$(basename $i .faa | cut -f1 -d \".\"): Inside the loop, this line extracts the base name of the current file ($i) by removing the directory path and the .faa extension using basename, and then it uses cut to split the filename at the “.” character and extracts the first part. The extracted base name is stored in the variable filename.\nawk -v fname=\"$filename\" '/&gt;/{sub(\"&gt;\",\"&\"fname\"-\")}1' $i &gt; results/faa/renamed/$filename.faa: This line utilizes the awk command to process the contents of the current file ($i). Let’s break down the awk command:\n\n-v fname=\"$filename\": This option passes the value of the shell variable filename to awk as an awk variable named fname.\n/&gt;/: This part of the pattern matches lines that contain &gt;.\n{sub(\"&gt;\",\"&\"fname\"-\")}: This action performs a substitution on lines that match the pattern:\n\nsub(\"&gt;\",\"&\"fname\"-\"): This substitutes the &gt; character with itself (&) followed by the value of fname and a hyphen (-). So, it’s essentially appending the value of fname followed by a hyphen after the &gt; character.\n\n1: This is a condition that always evaluates to true, triggering the default action of awk, which is to print the current line.\n$i: This is the file that awk will operate on.\n&gt; results/faa/renamed/$filename.faa: This redirects the output of awk to a new file with the same base name as the original file ($filename.faa), but located in the results/faa/renamed/ directory.\n\n\nSo, in summary, this code loops through each file in the directory results/faa/ with the .faa extension, extracts the base name of each file, and then uses awk to modify each file’s contents. It appends the base name followed by a hyphen after any line that starts with &gt;, and saves the modified content to a new file in the results/faa/renamed/ directory with the same base name as the original file.\n\n\n\n\n\nPrepare a mapping file (optional)\nNext, we prepare a mapping file that is a two-column file with the genome and the protein name. This mapping file is useful to merge with the results of the hmmsearch since it allows us to (a) keep the proteins in the same order as they occur on the contig and (b) note proteins for which we didn’t find a match during the hmmsearch. If you don’t care about the order or want to list proteins without a KO hit, then you can omit the file.\n\n#get list of protein accession numbers\ngrep \"^&gt;\" results/faa/combined.faa  &gt; temp\n\n#remove the ``&gt;``\nsed 's/&gt;//g' temp &gt; temp2\n\n#Modify protein list to add in a column with genome ID\nawk -F'\\t' -v OFS='\\t' '{split($1,a,\"-\"); print $1, a[1]}' temp2 &gt; mapping_file.txt\n\n#check file content: we now have the accession and the binID\nhead mapping_file.txt\n\n\n\n\n\n\n\nTip: How does the awk command work\n\n\n\n\n\n\nawk -F'\\t' -v OFS='\\t' '{split($1,a,\"-\"); print $1, a[1]}' temp2 &gt; mapping_file.txt\n\n\nawk: This invokes the AWK programming language interpreter.\n-F'\\t': This option sets the field separator to tab (\\t). It tells AWK to split each input line into fields based on tabs.\n-v OFS='\\t': This sets the Output Field Separator (OFS) to tab (\\t). It specifies how AWK should separate fields when printing output. Here, it’s set to tab as well.\n'{split($1,a,\"-\"); print $1, a[1]}': This is the AWK script enclosed within single quotes. Let’s break it down:\n\nsplit($1,a,\"-\"): This function splits the first field ($1) of each input line into an array a, using - as the delimiter. After this operation, the array a will contain the substrings of $1 separated by -.\nprint $1, a[1]: This statement prints the first field ($1) of the input line, followed by the first element of the array a. It essentially prints the original first field and the first part of it obtained after splitting by -.\n\ntemp2: This is the input file for AWK. It represents the file that AWK will process.\n&gt; mapping_file.txt: This part redirects the output of AWK to a new file named mapping_file.txt. It creates or overwrites this file with the content generated by AWK.\n\nSo, in summary, this AWK command reads each line from the file temp2, assumes that fields are separated by tabs, splits the first field of each line by the - character, and then prints the original first field along with the first part obtained after splitting. The output is saved to a new file named mapping_file.txt.\n\n\n\n\n\nRunning the hmmsearch\nThe next thing is easy, we run hmmsearch:\n\n#now we can run the hmmsearch\nhmmsearch --tblout results/kos/sequence_results.txt \\\n  --domtblout results/kos/domain_results.txt \\\n  --notextw --cpu 20 \\\n  /zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04/KO_db.hmm \\\n  results/faa/combined.faa\n\n\n\nParsing the hmmsearch results\nNext, we want to parse the hmmsearch results and:\n\nClean the table by removing rows starting with a # (these are usually comments we don’t need) and ensure that we work with a tab-separated file\nRemove hits with insufficient e-values\nEnsure that for each protein we retain only one hit to the database\nAdd some description for the database identifier, here the KO ID.\nCombine the data with our mapping file\nAdd a header\n\nNotice: For this code to work on other databases, i.e. the Pfam database, you need to ensure that you act on the right columns. Below are some tips but read through the detailed code explanations to make this work for other databases.\n\nThe first sed command extract the relevant columns, i.e. protein-id, ko-id, e-value, bitscore, by specifying the right fields. For the KO results, these are columns $1, $3, $6, $5\nThe second sort command sorts on the right columns. For the KOs column 3 is the e-value and column 4 the bitscore\nThe join command joins the right fields from the KO mapping file. Ensure that\n\nThe ID to merge the two dataframes is in the second column in your results (-1 2) and in the first column in the ko_list (-2 1)\nYou add the right columns from the mapping file from the database. I.e. with 2.2 and 2.12 we exact a custom cutoff and a description from the ko_list.\nThe sort part acts on the right columns. I.e. We sort ko_list on column 1 (-k1) because that is the column that contains the KO ID that we want to use for merging\n\n\n\n#1.format the full table by:\n#remove hash symbols and replacing spaces with a tab\n#retain only the columns with the protein-id, ko-id, e-value, bitscore\n#and only select hits above a certain e-value\nsed 's/ \\+ /\\t/g' results/kos/sequence_results.txt | sed '/^#/d'| sed 's/ /\\t/g'| awk -F'\\t' -v OFS='\\t' '{print $1, $3, $6, $5}' | awk -F'\\t' -v OFS='\\t' '($4 + 0) &lt;= 1E-3'  &gt; results/kos/sequence_results_red_e_cutoff.txt\n\n#example for a sanity check:\n#we see less columns and hits with an insufficient e-value disappear\ngrep ABU82216 results/kos/sequence_results.txt\ngrep ABU82216 results/kos/sequence_results_red_e_cutoff.txt\n\n#2.get best hit/protein based on bit score, and e-value\n#i.e. one protein might hit toward more than 1 HMM, we only want to retain one hit\nsort -t$'\\t' -k3,3gr -k4,4g results/kos/sequence_results_red_e_cutoff.txt | sort -t$'\\t' --stable -u -k1,1  | sort -t$'\\t' -k3,3gr -k4,4g &gt;  results/kos/sequence_results_red_e_cutoff_best_hit.txt\n\n#sanity check: We see that only one hit, the one with the best scores, remains\ngrep ABU82216 results/kos/sequence_results_red_e_cutoff_best_hit.txt\n\n#3.merge with KO mapping file \n#ensure that the KOs are in column 2 and 1 of the results and the mapping file\n#-o decides what columns are kept after the merge from table 1 and 2. I.e. from the mapping file only columns 2 and 12 are used\nLC_ALL=C join -a1 -1 2 -2 1 -e'-' -t $'\\t' -o1.1,1.2,1.4,1.3,2.2,2.12 &lt;(LC_ALL=C sort -k2 results/kos/sequence_results_red_e_cutoff_best_hit.txt) &lt;(LC_ALL=C sort -k1  /zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04/ko_list) | LC_ALL=C  sort &gt;  results/kos/temp1\n\n#sanity check: We see a description for the KO id and now know better at what function we look at \ngrep ABU82216 results/kos/temp1\n\n#4.add in an extra column that lists whether hits have a high confidence score\nawk  -v OFS='\\t' '{ if ($4 &gt; $5){ $7=\"high_score\" }else{ $7=\"-\" } print } ' results/kos/temp1 &gt; results/kos/temp2\n\n#sanity check: This hit is very likely true because the bitscore is sufficiently high\ngrep ABU82216 results/kos/temp2\n\n#5.merge with protein mapping file\nLC_ALL=C join -a1  -j1 -e'-' -t $'\\t' -o 0,1.2,2.2,2.3,2.4,2.5,2.6,2.7 &lt;(LC_ALL=C sort mapping_file.txt) &lt;(LC_ALL=C sort results/kos/temp2) | LC_ALL=C sort  &gt; results/kos/temp3\n\n#6.add header\necho -e \"accession\\tBinID\\tKO_hmm\\tKO_e_value\\tKO_bit_score\\tKO_bit_score_cutoff\\tKO_Definition\\tKO_confidence\" | cat - results/kos/temp3 &gt; results/kos/KO_hmm.tsv\n\n#sanity check\ngrep ABU82216 results/kos/KO_hmm.tsv\n\n#cleanup \nrm results/kos/temp*  \n\n\n\n\n\n\n\nTip: What does the 1st command do\n\n\n\n\n\n\nsed 's/ \\+ /\\t/g' results/kos/sequence_results.txt | sed '/^#/d'| sed 's/ /\\t/g'| awk -F'\\t' -v OFS='\\t' '{print $1, $3, $6, $5}' | awk -F'\\t' -v OFS='\\t' '($4 + 0) &lt;= 1E-3'  &gt; results/kos/sequence_results_red_e_cutoff.txt\n\nHere, we use a pipe to combine multiple smaller commands into one:\n\nsed 's/ \\+ /\\t/g' results/kos/sequence_results.txt: This command replaces one or more spaces with a single tab character in the file sequence_results.txt located in the results/kos/ directory.\n| sed '/^#/d': This command pipes the output of the previous sed command to another sed command, which deletes lines that start with #. This is commonly used to remove comments from files.\n| sed 's/ /\\t/g': This command pipes the output of the previous sed command to another sed command, which replaces all spaces with tab characters.\n| awk -F'\\t' -v OFS='\\t' '{print $1, $3, $6, $5}': This command pipes the output of the previous sed command to an AWK command. Let’s break it down:\n\nawk -F'\\t': This sets the input field separator to tab.\n-v OFS='\\t': This sets the output field separator to tab.\n'{print $1, $3, $6, $5}': This prints the first, third, sixth, and fifth fields of each line, separated by tabs. This selects specific columns from the data.\n\n| awk -F'\\t' -v OFS='\\t' '($4 + 0) &lt;= 1E-3': This command pipes the output of the previous AWK command to another AWK command. Let’s break it down:\n\nawk -F'\\t': This sets the input field separator to tab.\n-v OFS='\\t': This sets the output field separator to tab.\n'($4 + 0) &lt;= 1E-3': This is a condition that filters the output. It checks if the numerical value of the fourth field ($4) is less than or equal to 1E-3 (0.001). If true, the line is printed.\n\n\nSo, in summary, this command sequence manipulates and filters the content of the sequence_results.txt file, and the filtered output is saved to a new file named sequence_results_red_e_cutoff.txt.\n\n\n\n\n\n\n\n\n\nTip: What does the 2nd command do\n\n\n\n\n\n\nsort -t$'\\t' -k3,3gr -k4,4g results/kos/sequence_results_red_e_cutoff.txt | sort -t$'\\t' --stable -u -k1,1  | sort -t$'\\t' -k3,3gr -k4,4g &gt;  results/kos/sequence_results_red_e_cutoff_best_hit.txt\n\nThis command sequence performs several sorting operations on the file sequence_results_red_e_cutoff.txt in the results/kos/ directory.\n\nsort -t$'\\t' -k3,3gr -k4,4g results/kos/sequence_results_red_e_cutoff.txt: This command sorts the content of the file sequence_results_red_e_cutoff.txt based on multiple fields:\n\n-t$'\\t': Specifies that the field separator is a tab character.\n-k3,3gr: Sorts based on the third field (column) in descending numerical order (-r flag indicates reverse order).\n-k4,4g: If values in the third field are equal, it sorts based on the fourth field in ascending numerical order (g flag indicates general numerical sorting).\n\n| sort -t$'\\t' --stable -u -k1,1: This command pipes the output of the previous sort command to another sort command, which performs the following operations:\n\n-t$'\\t': Specifies that the field separator is a tab character.\n--stable: Ensures that the original order of records with equal keys is preserved.\n-u: Specifies unique mode, keeping only the first occurrence of lines with identical keys.\n-k1,1: Sorts based on the first field (column) only.\n\n| sort -t$'\\t' -k3,3gr -k4,4g: This command pipes the output of the previous sort command to another sort command, which performs the same sorting operations as the first command:\n\n-t$'\\t': Specifies that the field separator is a tab character.\n-k3,3gr: Sorts based on the third field (column) in descending numerical order (-r flag indicates reverse order).\n-k4,4g: If values in the third field are equal, it sorts based on the fourth field in ascending numerical order (g flag indicates general numerical sorting).\n\n\nSo, in summary, this command sequence sorts the content of the file sequence_results_red_e_cutoff.txt based on specific fields and criteria, removes duplicate records based on the first field, and then performs another sorting based on different fields before saving the final sorted output to a new file named sequence_results_red_e_cutoff_best_hit.txt.\n\n\n\n\n\n\n\n\n\nTip: What does the 3rd command do\n\n\n\n\n\n\nLC_ALL=C join -a1 -1 2 -2 1 -e'-' -t $'\\t' -o1.1,1.2,1.4,1.3,2.2,2.12 &lt;(LC_ALL=C sort -k2 results/kos/sequence_results_red_e_cutoff_best_hit.txt) &lt;(LC_ALL=C sort -k1  /zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04/ko_list) | LC_ALL=C  sort &gt;  results/kos/temp1\n\n\nLC_ALL=C sort -k2 results/kos/sequence_results_red_e_cutoff_best_hit.txt: This command sorts the content of the file sequence_results_red_e_cutoff_best_hit.txt located in the results/kos/ directory based on the second column (-k2) using the sort command. The LC_ALL=C part ensures that the sorting is done based on byte values, which is useful when dealing with non-English characters or locales.\nLC_ALL=C sort -k1 /zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04/ko_list: This command sorts the content of the file /zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04/ko_list based on the first column (-k1) using the sort command. Similar to the previous sort command, LC_ALL=C ensures byte-based sorting.\njoin -a1 -1 2 -2 1 -e'-' -t $'\\t' -o1.1,1.2,1.4,1.3,2.2,2.12: This command performs a join operation on the sorted files. Let’s break down the options:\n\n-a1: Specifies to output unpairable lines from the first file.\n-1 2: Specifies that the join field in the first file is the second column.\n-2 1: Specifies that the join field in the second file is the first column.\n-e'-': Specifies the string to replace missing input fields with.\n-t $'\\t': Specifies the field separator as a tab character.\n-o1.1,1.2,1.4,1.3,2.2,2.12: Specifies the output format. It selects specific columns from both files to be included in the output.\n\n&lt;(...): This is process substitution. It allows the output of a command to be used as the input to another command.\n&gt; results/kos/temp1: This part redirects the output of the entire command sequence to a new file named temp1 in the results/kos/ directory.\nLC_ALL=C sort: Finally, the output of the entire command sequence is piped to another sort command to ensure the final output is sorted. The LC_ALL=C part ensures byte-based sorting.\n\nSo, in summary, this command sequence sorts two files, performs a join operation based on specific columns, and then sorts the joined output again before saving it to a new file named temp1 in the results/kos/ directory.\n\n\n\n\n\n\n\n\n\nTip: What does the 4th command do\n\n\n\n\n\n\nawk  -v OFS='\\t' '{ if ($4 &gt; $5){ $7=\"high_score\" }else{ $7=\"-\" } print } ' results/kos/temp1 &gt; results/kos/temp2\n\n\nawk -v OFS='\\t': This invokes the AWK programming language interpreter and sets the Output Field Separator (OFS) to tab (\\t).\n'{ if ($4 &gt; $5){ $7=\"high_score\" }else{ $7=\"-\" } print }': This is the AWK script enclosed within single quotes. Let’s break it down:\n\nif ($4 &gt; $5): This is an if condition that checks if the value in the fourth field ($4) is greater than the value in the fifth field ($5).\n{ $7=\"high_score\" }: If the condition is true, it sets the value of the seventh field ($7) to “high_score”.\nelse { $7=\"-\" }: If the condition is false, it sets the value of the seventh field ($7) to “-”.\nprint: This command prints the modified line.\n\n'results/kos/temp1 &gt; results/kos/temp2': This redirects the output of the AWK command to a new file named temp2 in the results/kos/ directory. So, in summary, this AWK command processes each line of the file temp1. If the value in the fourth field is greater than the value in the fifth field, it sets the seventh field to “high_score”; otherwise, it sets the seventh field to “-”. The modified content is then saved to a new file named temp2 in the results/kos/ directory.\n\n\n\n\n\n\n\n\n\n\nTip: What does the 5th command do\n\n\n\n\n\n\nLC_ALL=C join -a1  -j1 -e'-' -t $'\\t' -o 0,1.1,2.2,2.3,2.4,2.5,2.6,2.7 &lt;(LC_ALL=C sort mapping_file.txt) &lt;(LC_ALL=C sort results/kos/temp2) | LC_ALL=C sort  &gt; results/kos/temp3\n\n\nLC_ALL=C join -a1 -j1 -e'-' -t $'\\t' -o 0,1.1,2.2,2.3,2.4,2.5,2.6,2.7: This part executes the join command with the following options:\n\n-a1: Specifies to output unpairable lines from the first file.\n-j1: Specifies the join field for the first file is the first field.\n-e'-': Specifies the string to replace missing input fields with.\n-t $'\\t': Specifies the field separator as a tab character.\n-o 0,1.1,2.2,2.3,2.4,2.5,2.6,2.7: Specifies the output format. It selects specific columns from both files to be included in the output. Here, 0 represents the join field, 1.1 represents the first field from the first file, and 2.2 to 2.7 represent the second to seventh fields from the second file.\n\n&lt;(...): This is process substitution. It allows the output of a command to be used as the input to another command. In this case, it is used to sort the contents of mapping_file.txt and temp2 before passing them to join.\n| LC_ALL=C sort: This part pipes the output of the join command to another sort command. LC_ALL=C ensures byte-based sorting.\n&gt; results/kos/temp3: This part redirects the output of the entire command sequence to a new file named temp3 in the results/kos/ directory.\n\nSo, in summary, this command sequence sorts two files, performs a join operation based on the first column of the first file, and then sorts the joined output before saving it to a new file named temp3.\n\n\n\n\n\n\n\n\n\nTip: What does the 6th command do\n\n\n\n\n\n\necho -e \"accession\\tBinID\\tKO_hmm\\tKO_e_value\\tKO_bit_score\\tKO_bit_score_cutoff\\tKO_Definition\\tKO_confidence\" | cat - results/kos/temp3 &gt; results/kos/KO_hmm.tsv\n\nThis command concatenates the specified header line with the contents of the file temp2 located in the results/kos/ directory and saves the combined output to a new file named KO_hmm.tsv in the same directory. Let’s break it down:\n\necho -e \"accession\\tKO_hmm\\tKO_e_value\\tKO_bit_score\\tKO_bit_score_cutoff\\tKO_Definition\\tKO_confidence\": This part prints the specified header line to the standard output. The -e option allows interpretation of backslash escapes, and \\t represents a tab character, effectively creating a tab-separated header line.\n| cat - results/kos/temp2: This pipes the output of the echo command (the header line) and the content of the file temp2 into the cat command for concatenation.\n&gt; results/kos/KO_hmm.tsv: This redirects the concatenated output to a new file named KO_hmm.tsv in the results/kos/ directory.\n\nSo, in summary, this command generates a TSV (tab-separated values) file named KO_hmm.tsv with a header line and the contents of temp2, effectively creating a table with the specified column headers and the data from temp2.\n\n\n\n\n\nCreating count tables\nIf we work with many genomes, scanning the hmmsearch tables will take a lot of time. We speed things up by summarizing our data a bit. Here, we will\n\nCreate a table that counts how often each KO is found in each genome.\nCombine the count table with the pathway and module metadata tables. This allows us to more quickly check whether, for example, all the genes for Glycolysis are present in our genomes.\n\nThe step below require the pandas library. If not available you can install it via conda install pandas. To start python, type python.\n\nimport pandas as pd \n\n#read in data\ndf = pd.read_csv('results/kos/KO_hmm.tsv', sep = '\\t')\npathways = pd.read_csv('/zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04/pathway_to_kegg.tsv', sep = '\\t')\nmodules = pd.read_csv('/zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04/modules_to_kegg.tsv', sep = '\\t')\n\n#summarize data per bin\ncounts = df.groupby(['BinID','KO_hmm']).KO_hmm.agg('count').to_frame('count').reset_index()\ncounts_wide =  counts.pivot_table(index='KO_hmm', columns='BinID', values='count', fill_value=0).reset_index()\ncounts_wide.rename(columns={'KO_hmm': 'KO_id'}, inplace=True)\n\n#add metadata \npathway_df = pathways.merge(counts_wide, how = 'left', left_on = 'KO_id', right_on = 'KO_id').fillna(0)\nmodule_df = modules.merge(counts_wide, how = 'left', left_on = 'KO_id', right_on = 'KO_id').fillna(0)\n\n#print\npathway_df.to_csv('results/kos/KO_to_pathway.txt', sep =',', index = False)\nmodule_df.to_csv('results/kos/KO_to_modules.txt', sep =',', index = False)\n\nexit()\n\nA word about modules:\nIn KEGG, modules are are predefined sets of genes or proteins that represent functional units within biological systems. If we for example look at Glycolysis, we see that it consists of multiple pathway modules.\nOne such module, M00001, belongs to the Glycolysis module. We also see different KEGG Orthology (KO) entries that can fulfill each step. For example, for step 2, various KOs might perform this step. For step 6, our genome needs to either possess the KOs K00134 or K00150 and K00927 to convert a substrate in a 2-step reaction. Alternatively, genomes might possess K11389 to convert the substrate in a 1-step reaction.",
    "crumbs": [
      "Sequence data analyses",
      "Core tools",
      "HMMER"
    ]
  },
  {
    "objectID": "source/core_tools/diamond.html#diamond",
    "href": "source/core_tools/diamond.html#diamond",
    "title": "Bioinformatics guidance page",
    "section": "Diamond",
    "text": "Diamond\n\nIntroduction\nDIAMOND is a sequence aligner for protein and translated DNA searches, designed for high performance analysis of big sequence data (Buchfink, Reuter, and Drost 2021). The key features are:\n\nPairwise alignment of proteins and translated DNA at 100x-10,000x speed of BLAST.\nProtein clustering of up to tens of billions of proteins\nFrameshift alignments for long read analysis.\nLow resource requirements and suitable for running on standard desktops or laptops.\nVarious output formats, including BLAST pairwise, tabular and XML, as well as taxonomic classification.\n\nFor a full description, please visit the tools website.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nDiamond v 2.1.9 is installed by default on the Crunchomics HPC\n\nIf you want to install the latest version yourself, you can run:\n\nmamba create -n diamond -c bioconda -c conda-forge diamond\n\n\n\nUsage\n\nGenerating your own diamond database\nTo generate your own database, feel free to follow the instructions found here. Additionally, if you want to build a database using NCBI BLAST database and want to include taxonomy information in your searches, you can follow an example to prepare a diamond database from NCBI nr here.\n\n\nRunning a diamond search\nWith diamond you can run a BlastP as well as BlastX search against a diamond (dmnd) database.\n\n# Basic usage using a protein file as query and use nr as database\ndiamond blastp -q proteins.faa -d nr -o out.tsv --very-sensitive\n\n# Running a search against a NCBI database \ndiamond blastp -q proteins.faa \\\n    --more-sensitive --evalue 1e-3 --threads 20 --include-lineage --max-target-seqs 50 \\\n    --db /zfs/omics/projects/bioinformatics/databases/ncbi_nr/diamond/nr \\\n    --outfmt 6 qseqid qtitle qlen sseqid salltitles slen qstart qend sstart send evalue bitscore length pident staxids sphylums \\\n    --out results.txt\n\nFor a full set of options, please visit the manual.\nGeneral options:\n\n--threads/-p: Number of CPU threads. By default, the program will auto-detect and use all available virtual cores on the machine\n\nInput options:\n\n--db/-d &lt;file&gt; : Path to the DIAMOND database file. Since v2.0.8, a BLAST database can also be used here. Specify the base path of the database without file extensions. Since v2.0.10, BLAST databases have to be prepared using the prepdb command. Note that for self-made BLAST databases, makeblastdb should be used with the -parse_seqids option.\n--query/-q &lt;file&gt;: Path to the query input file in FASTA or FASTQ format (may be gzip compressed, or zstd compressed if compiled with zstd support). If this parameter is omitted, the input will be read from stdin.\n--taxonlist &lt;list&gt;: Comma-separated list of NCBI taxonomic IDs to filter the database by. Any taxonomic rank can be used, and only reference sequences matching one of the specified taxon ids will be searched against. Using this option requires setting the --taxonmap and --taxonnodes parameters for makedb.\n--taxon-exclude &lt;list&gt;: Comma-separated list of NCBI taxonomic IDs to exclude from the database. Using this option requires setting the --taxonmap and --taxonnodes parameters for makedb.\n--seqidlist &lt;filename&gt;: Filter the database by a list of accessions provided as a text file. Only supported when using a BLAST database.\n--query-gencode #: Genetic code used for translation of query in BLASTX mode. A list of possible values can be found at the NCBI website. By default, the Standard Code is used. Note: changing the genetic code is currently not fully supported for the DAA format.\n--strand {both, plus, minus}: Set strand of query to align for translated searches. By default both strands are searched.\n--min-orf/-l # : Ignore translated sequences that do not contain an open reading frame of at least this length. By default this feature is disabled for sequences of length below 30, set to 20 for sequences of length below 100, and set to 40 otherwise. Setting this option to 1 will disable this feature.\n\nSensitivity modes:\nWithout using any sensitivity option, the default mode will run which is designed for finding hits of &gt;60% identity and short read alignment.\n\n--fast: Enable the fast sensitivity mode, which runs faster than default and is designed for finding hits of &gt;90% identity. Option supported since v2.0.10\n--mid-sensitive: Enable the mid-sensitive mode which is between the default mode and the sensitive mode in sensitivity. Option supported since v2.0.3\n--sensitive: Enable the sensitive mode designed for full sensitivity for hits of &gt;40% identity.\n--more-sensitive: This mode is equivalent to the --sensitive mode except for soft-masking of certain motifs being disabled (same as setting --motif-masking 0).\n--very-sensitive: Enable the very-sensitive mode designed for best sensitivity including the twilight zone range of &lt;40% identity.\n--ultra-sensitive: Enable the ultra-sensitive mode which is yet more sensitive than the --very-sensitive mode. Available since version 2.0.0.\n\nOutput options:\n\n--out/-o &lt;file&gt;: Path to the output file. If this parameter is omitted, the results will be written to the standard output and all other program output will be suppressed.\n--outfmt/-f #: Format of the output file. The following values are accepted:\n\n0: BLAST pairwise format.\n5: BLAST XML format.\n6: BLAST tabular format (default). This format can be customized, the 6 may be followed by a space-separated list of the following keywords, each specifying a field of the output. N.B.: these additional arguments should not be quoted as is often required for other tools, e.g. use diamond --outfmt 6 qseqid sseqid, not diamond --outfmt '6 qseqid sseqid'\n\nqseqid: Query Seq - id\nqlen: Query sequence length\nsseqid: Subject Seq - id\nsallseqid: All subject Seq - id(s), separated by a ’;’\nslen: Subject sequence length\nqstart: Start of alignment in query*\nqend: End of alignment in query*\nsstart: Start of alignment in subject*\nsend: End of alignment in subject*\nqseq: Aligned part of query sequence*\nqseq_translated: Aligned part of query sequence (translated)* Supported since v2.0.7.\nfull_qseq: Full query sequence\nfull_qseq_mate: Query sequence of the mate (requires two files for --query) Supported since v2.0.7.\nsseq: Aligned part of subject sequence*\nfull_sseq: Full subject sequence\nevalue: Expect value\nbitscore: Bit score\nscore: Raw score\nlength: Alignment length*\npident: Percentage of identical positions*\nnident: Number of identical matches*\nmismatch: Number of mismatches*\npositive: Number of positive - scoring matches*\ngapopen: Number of gap openings*\ngaps: Total number of gaps*\nppos: Percentage of positive - scoring matches*\nqframe: Query frame\nbtop: Blast traceback operations(BTOP)*\ncigar: CIGAR string*\nstaxids: Unique Subject Taxonomy ID(s), separated by a ’;’ (in numerical order). This field requires setting the --taxonmap parameter for makedb.\nsscinames: Unique Subject Scientific Name(s), separated by a ‘;’.\nsskingdoms: Unique Subject Super Kingdom(s), separated by a ‘;’.\nskingdoms: Unique Subject Kingdom(s), separated by a ‘;’.\nsphylums: Unique Subject Phylums(s), separated by a ‘;’.\nstitle: Subject Title\nsalltitles: All Subject Title(s), separated by a ’&lt;&gt;’\nqcovhsp: Query Coverage Per HSP*\nscovhsp: Subject Coverage Per HSP*\nqtitle: Query title\n\nqqual: Query quality values for the aligned part of the query*\nfull_qqual: Query quality values\nqstrand: Query strand\n\n\n--salltitles: Include full length subject titles into the DAA format. By default, DAA files contain only the shortened sequence id (up to the first blank character).\n--sallseqid: Include all subject ids into the DAA file. By default only the first id of each subject is included. As the subject ids are much shorter than the full titles this option will save space compared to the --salltitles option.\n--compress (0,1,zstd): Enable compression of the output file. 0 (default) means no compression, 1 means gzip compression, zstd means zstd compression (executable is required to have been compiled with zstd support).\n--max-target-seqs/-k # : The maximum number of target sequences per query to reportalignments for (default=25). Setting this to -k0 will report all targets for which alignments were found. Note that this parameter does not only affect the reporting, but also the algorithm as it is taken into account for heuristics that eliminate hits prior to full gapped extension.\n--top #: Report alignments within the given percentage range of the top alignment score for a query (overrides --max-target-seqs option). For example, setting --top 10 will report all alignments whose score is at most 10% lower than the best alignment score for a query. Using this option will cause targets to be sorted by bit score instead of e-evalue in the output.\n--max-hsps #: The maximum number of HSPs (High-Scoring Segment Pairs) per target sequence to report for each query. The default policy is to report only the highest-scoring HSP for each target, while disregarding alternative, lower-scoring HSPs that are contained in the same target.This is not to be confused with the --max-target-seqs option.\n--range-culling: Enable hit culling with respect to the query range. This feature is designed for long query DNA sequences that may span several genes. In these cases, reporting the overall top N hits can cause hits to a lower-scoring gene to be superseded by a higher-scoring gene.Using this option, hit culling will be performed locally with respect to a hit’s query range, thus reporting the locally top N hits while allowing more hits that span a different region of the query. Using this feature along with -k 25 (default), a hit will only be deleted if at least 50% of its query range is spanned by at least 25 higher or equal scoring hits.\n--evalue/-e #: Maximum expected value to report an alignment (default=0.001).\n--min-score #: Minimum bit score to report an alignment. Setting this option will override the --evalue parameter.\n--id #: Report only alignments above the given percentage of sequence identity.Note that using this option reduces performance.\n--query-cover #: Report only alignments above the given percentage of query cover.Note that using this option reduces performance.\n--subject-cover #: Report only alignments above the given percentage of subject cover. Note that using this option reduces performance.\n--unal (0,1): Report unaligned queries (0=no, 1=yes). By default, unaligned queries are reported for the BLAST pairwise, BLAST XML and SAM format.\n--no-self-hits: Suppress reporting of identical self-hits between sequences. The FASTA sequence identifiers as well as the sequences of query and target need to be identical for a hit to be deleted.",
    "crumbs": [
      "Sequence data analyses",
      "Core tools",
      "Diamond"
    ]
  },
  {
    "objectID": "source/core_tools/busco.html#busco",
    "href": "source/core_tools/busco.html#busco",
    "title": "Bioinformatics guidance page",
    "section": "BUSCO",
    "text": "BUSCO\n\nIntroduction\nBUSCO can be used to evaluated the quality of different data types that can range from genome assemblies of single isolates and assembled transcriptomes and annotated gene sets to metagenome-assembled genomes where the taxonomic origin of the species is unknown (Manni et al. 2021).\nBUSCO provides a quantitative assessment of the completeness in terms of expected gene content of a genome assembly, transcriptome, or annotated gene set. The results are simplified into categories of Complete and single-copy, Complete and duplicated, Fragmented, or Missing BUSCOs, where “BUSCOs” is shorthand for “BUSCO marker genes”.\nFor more information, check out the User guide and for updates check the tools Gitlab.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nBUSCO v5.8.0 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -n busco_5.8.0 -c bioconda -c conda-forge busco=5.8.0\n\n\n\nUsage\n\n# List available lineages\n# Note, BUSCO will automatically download the requested dataset if it is not already present\nbusco --list-datasets\n\n# Run BUSCO and automatically select the closest lineage on a eukaryotic genome using 20 cores\nbusco -i genome.fasta -m genome \\\n    -c 20 --out results/busco/ \\\n    --auto-lineage-euk\n\n    # Generate a plot with your results\ngenerate_plot.py -wd results/busco/\n\nPlease note:\nBUSCO completeness results make sense only in the context of the biology of your organism. You have to understand whether missing or duplicated genes are of biological or technical origin. For instance, a high level of duplication may be explained by a recent whole duplication event (biological) or a chimeric assembly of haplotypes (technical). Transcriptomes and protein sets that are not filtered for isoforms will lead to a high proportion of duplicates. Therefore, you should filter them before a BUSCO analysis. Finally, focusing on specific tissues or specific life stages and conditions in a transcriptomic experiment is unlikely to produce a BUSCO-complete transcriptome. In this case, consistency across your samples is what you will be aiming for.\nMandatory parameters:\n\n-i SEQUENCE_FILE, --in SEQUENCE_FILE Input sequence file in FASTA format. Can be an assembled genome or transcriptome (DNA), or protein sequences from an annotated gene set. Also possible to use a path to a directory containing multiple input files.\n-o OUTPUT, --out OUTPUT Give your analysis run a recognisable short name. Output folders and files will be labelled with this name. The path to the output folder is set with –out_path.\n-m MODE, --mode MODE Specify which BUSCO analysis mode to run. There are three valid modes:\n\ngeno or genome, for genome assemblies (DNA)\ntran or transcriptome, for transcriptome assemblies (DNA)\nprot or proteins, for annotated gene sets (protein)\n\n\nRecommended parameters:\n\n-l LINEAGE, --lineage_dataset LINEAGE Specify the name of the BUSCO lineage to be used, e.g. hymenoptera_odb10. A full list of available datasets can be viewed by entering busco --list-datasets. You should always select the dataset that is most closely related to the assembly or gene set you are assessing. If you are unsure, you can use the --auto-lineage option to automatically select the most appropriate dataset. BUSCO will automatically download the requested dataset if it is not already present in the download folder. You can optionally provide a path to a local dataset instead of a name, e.g. -l /path/to/my/dataset.\n-c N, --cpu N Specify the number (N=integer) of threads/cores to use.\n\nOptional parameters\n\n--augustus Use augustus gene predictor for eukaryote runs\n--augustus_parameters \"--PARAM1=VALUE1,--PARAM2=VALUE2\" Pass additional arguments to Augustus. All arguments should be contained within a single string with no white space, with each argument separated by a comma.\n--augustus_species AUGUSTUS_SPECIES Specify a species for Augustus training.\n--auto-lineage Run auto-lineage to find optimum lineage path\n--auto-lineage-euk Run auto-placement just on eukaryote tree to find optimum lineage path\n--auto-lineage-prok Run auto-lineage just on non-eukaryote trees to find optimum lineage path\n--contig_break n Number of contiguous Ns to signify a break between contigs. Default is n=10.\n--download [dataset ...] Download dataset. Possible values are a specific dataset name, “all”, “prokaryota”, “eukaryota”, or “virus”. If used together with other command line arguments, make sure to place this last.\n--download_base_url DOWNLOAD_BASE_URL Set the url to the remote BUSCO dataset location\n--download_path DOWNLOAD_PATH Specify local filepath for storing BUSCO dataset downloads\n-e N, --evalue N E-value cutoff for BLAST searches. Allowed formats, 0.001 or 1e-03 (Default: 1e-03)\n-f, --force Force rewriting of existing files. Must be used when output files with the provided name already exist.\n--limit N How many candidate regions (contig or transcript) to consider per BUSCO (default: 3)\n--list-datasets Print the list of available BUSCO datasets\n--long Optimization Augustus self-training mode (Default: Off); adds considerably to the run time, but can improve results for some non-model organisms\n--metaeuk Use Metaeuk gene predictor\n--metaeuk_parameters \"--PARAM1=VALUE1,--PARAM2=VALUE2\" Pass additional arguments to Metaeuk for the first run. All arguments should be contained within a single string with no white space, with each argument separated by a comma.\n--metaeuk_rerun_parameters “–PARAM1=VALUE1,–PARAM2=VALUE2” Pass additional arguments to Metaeuk for the second run. All arguments should be contained within a single string with no white space, with each argument separated by a comma.\n--miniprot Use Miniprot gene predictor\n--skip_bbtools Skip BBTools for assembly statistics\n--offline To indicate that BUSCO cannot attempt to download files\n--opt-out-run-stats Opt out of data collection. Information on the data collected is available in the user guide.\n-q, --quiet Disable the info logs, displays only errors\n-r, --restart Continue a run that had already partially completed.\n--scaffold_composition Writes ACGTN content per scaffold to a file scaffold_composition.txt\n--tar Compress some subdirectories with many files to save space",
    "crumbs": [
      "Sequence data analyses",
      "Core tools",
      "BUSCO"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#what-are-we-talking-about",
    "href": "source/core_tools/bash-for-loops.html#what-are-we-talking-about",
    "title": "For-loops in bash",
    "section": "What are we talking about?",
    "text": "What are we talking about?\nA for loop in Bash is a control structure that allows you to execute a sequence of commands repeatedly. It’s particularly useful when you need to perform an operation a fixed number of times or iterate over a list of items.\nIn its simplest form, a for loop in Bash has the following syntax:\nfor item in list\ndo\n    # commands to execute for each item\ndone\nHere’s what each part of the for loop means:\n\nfor: This keyword indicates the start of the loop.\nitem: This is a variable that will hold each item from the list in each iteration of the loop.\nin: This keyword separates the variable name from the list of items.\nlist: This is a list of items over which the loop will iterate. It can be specified explicitly (e.g., 1 2 3) or through a command substitution (e.g., $(ls) to loop over files in a directory).\ndo: This keyword indicates the start of the block of commands to execute in each iteration.\ndone: This keyword marks the end of the loop.\n\nDuring each iteration of the loop, the variable item takes on the value of the next item in the list, and the commands within the loop are executed with this value. The loop continues until all items in the list have been processed.",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#a-for-loop-to-get-started",
    "href": "source/core_tools/bash-for-loops.html#a-for-loop-to-get-started",
    "title": "For-loops in bash",
    "section": "A for loop to get started",
    "text": "A for loop to get started\nLet’s delve into a basic example to kickstart your understanding of for loops in Bash. Imagine the task is to print numbers from 01 to 10, each on a separate line. Here’s how you can achieve this with a for loop:\n\nfor i in {01..10};\ndo\necho $i\ndone\n\nHere:\n\n{01..10} is a Bash brace expansion. Brace expansion is a feature in Bash that generates strings based on a specified pattern. In the case of {01..10}, it expands to 01 02 03 04 05 06 07 08 09 10. The for loop itself then iterates over each of these values.\necho $i: This command prints the value of the loop variable i to the terminal.\n\nWe can add some spaces to enhance readability. Although this formatting is optional, it’s a good practice to improve code clarity. Personally, I prefer using a tab (or 4 spaces) for indentation, but feel free to adopt your preferred style.\n\nfor i in {01..10};\ndo\n    echo $i\ndone\n\nYou might also see something like this:\n\nfor i in {01..10};  do  echo $i; done\n\nThis is the same code as above condensed into a single line. For more complex loops its useful to display the code in multiple lines for readability but you might encounter this syntax in the wild as well.",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#preparing-some-example-files",
    "href": "source/core_tools/bash-for-loops.html#preparing-some-example-files",
    "title": "For-loops in bash",
    "section": "Preparing some example files",
    "text": "Preparing some example files\nLet’s explore a more practical example that you might encounter in biology, such as working with genome files and modifying their headers. First, let’s download some example genome files from the NCBI database:\n\nmkdir data \n\n#download some example genomes from NCBI\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/008/085/GCA_000008085.1_ASM808v1/GCA_000008085.1_ASM808v1_protein.faa.gz  -P data\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/017/945/GCA_000017945.1_ASM1794v1/GCA_000017945.1_ASM1794v1_protein.faa.gz  -P data\ngzip -d data/*gz\n\n#view the header of one file\nhead data/GCA_000017945.1_ASM1794v1_protein.faa \n\nUpon opening the file, you’ll notice that the header looks something like this: &gt;ABU81185.1 translation initiation factor aIF-2 [Ignicoccus hospitalis KIN4/I]. The header contains unnecessary characters, such as spaces, which can potentially disrupt downstream analyses. We can remedy this using the cut command.\n\ncut -f1 -d \" \" data/GCA_000017945.1_ASM1794v1_protein.faa | head\n\nRunning this command shortens the header to &gt;ABU81185.1.\nIn bioinformatics, it might be useful to modify your files that way since downstream processes, such as functional annotation, might not run successfully if there are special symbols (i.e. spaces) in the fasta header or if the fasta header is too long. Generally, its recommended to:\n\nMake sure the file header is concise and does not have ANY spaces and that ideally uses a ‘-’ (or any other unique delimiter) to separate the genome ID from the protein ID. Also avoid any unusual symbols, such as |, (, ), {, }…\nIf you work with more than one genome, it might be useful to not only have the protein ID but also the genome ID in the sequence header\nIf you have a concise header + the bin ID in the header, it is easy to concatenate the protein sequences of your genomes into one single file and still easily know from what genome the sequence originally came from\n\nNow, let’s learn how to apply this operation to multiple files, ranging from 2 to 100 (or more), using a loop.",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#building-the-core-structure",
    "href": "source/core_tools/bash-for-loops.html#building-the-core-structure",
    "title": "For-loops in bash",
    "section": "Building the core structure",
    "text": "Building the core structure\nLet’s lay the groundwork for our loop by starting with the most basic command: printing a list for the files that we want to iterate over. Once we confirm that this works, we’ll gradually build up the loop step by step.\n\nfor i in data/*faa;\ndo\necho $i\ndone\n\nWhen we execute this code snippet, we observe that both files, along with their relative paths, are printed:\ndata/GCA_000008085.1_ASM808v1_protein.faa\ndata/GCA_000017945.1_ASM1794v1_protein.faa\nThis output confirms that the loop is correctly identifying the files we want to process.",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#adding-the-first-variable",
    "href": "source/core_tools/bash-for-loops.html#adding-the-first-variable",
    "title": "For-loops in bash",
    "section": "Adding the first variable",
    "text": "Adding the first variable\nTo ensure flexibility in naming the output files, let’s extract the file names using the basename command and storing the output in a variable. We will see why this really is useful once we made it to the final command:\n\nfor i in data/*faa;\ndo\nbasename=$(basename $i)\necho $basename\ndone\n\nExecuting this code snippet should yield the following names without the relative path. Making this change is extremely useful to later store our output files somewhere else than the data folder:\nGCA_000008085.1_ASM808v1_protein.faa\nGCA_000017945.1_ASM1794v1_protein.faa\nLet’s delve into some new syntax we have used in this section:\n\nIn bash a variable acts as temporary storage for a string or a number. Here, we want to store a file name in the variable called basename.\nThe $(..) syntax represents command substitution, where Bash executes a command in a subshell environment and then replaces the command substitution with the standard output of the command. In other words, whatever the basename command prints gets stored in the basename variable.\nDefining variables and the use of quotes:\n\nvariable_name=“xxx”: This assigns the string “xxx” to the variable variable_name.\nvariable_name=xxx: This assigns the value of the expression xxx to the variable variable_name. If xxx is a command, the result of that command will be assigned to variable_name. If xxx is a variable or literal string, its value will be assigned to variable_name.\nIn our example, we use command substitution to assign the result of the basename command to the basename variable, which is why no quotes are used.\nIt’s important NOT to add spaces between the variable name, =, and variable assignment, as it’s part of the syntax that Bash uses for variables.",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#removing-the-extension-from-the-basename",
    "href": "source/core_tools/bash-for-loops.html#removing-the-extension-from-the-basename",
    "title": "For-loops in bash",
    "section": "Removing the extension from the basename",
    "text": "Removing the extension from the basename\nTo ensure flexibility in naming the output files, let’s extract the file names AND exclude the extension by using the basename command with a slight modification:\n\nfor i in data/*faa;\ndo\nbasename=$(basename $i .faa)\necho $basename\ndone\n\nExecuting this code snippet should yield the following names:\nGCA_000008085.1_ASM808v1_protein \nGCA_000017945.1_ASM1794v1_protein",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#simplifying-the-basename",
    "href": "source/core_tools/bash-for-loops.html#simplifying-the-basename",
    "title": "For-loops in bash",
    "section": "Simplifying the basename",
    "text": "Simplifying the basename\nNow the basename looks cleaner, however, I don’t like that it is so long. When naming files its generatelly good to keep the filenames short and precise. We can do this by combining the basename command with a pipe.\nPut simply, a pipe allows us to chain the output of the basename command into the cut command to further modify the filename. Here, we use cut to shorten the basename by cutting everything off after the second underscore using cut.\n\nfor i in data/*faa;\ndo\nbasename=$(basename $i .faa | cut -f1-2 -d \"_\")\necho $basename\ndone\n\nExecuting this code snippet should yield the following names:\nGCA_000008085.1 \nGCA_000017945.1\nAdditionally, let’s delve into some new syntax:\n\nThe cut command is used to extract sections from each line of input (or from files) and write the result to standard output.\nIn our example, -f1-2 specifies that we want to retain fields 1 to 2 from each line, and -d “_” specifies that the field delimiter is underscore .\nSo, the cut command ensures that only the part of the basename before the second underscore is retained, effectively shortening the basename.\nYou would need to adjust this for your own file names, but hopefully this gives you an idea about how flexible the command line is",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#adding-a-variable-for-the-output-file",
    "href": "source/core_tools/bash-for-loops.html#adding-a-variable-for-the-output-file",
    "title": "For-loops in bash",
    "section": "Adding a variable for the output file",
    "text": "Adding a variable for the output file\nTo facilitate the creation of output file names, let’s introduce a variable named outfile_name. We’ll use this variable to specify the path and name for the modified files:\n\nmkdir data/renamed \n\nfor i in data/*faa;\ndo\nbasename=$(basename $i .faa | cut -f1-2 -d \"_\")\noutfile_name=\"data/renamed/${basename}.faa\"\necho $outfile_name\ndone\n\nThis should print:\ndata/renamed/GCA_000008085.1.faa \ndata/renamed/GCA_000017945.1.faa\nNow, we can finally see how this whole exercise becomes useful. We were able to (a) simplify the file name itself by making it shorter and (b) ensure that we can store the modified file in a new directory.\nAlso, notice how we now use quotes to assign a string to the variable outfile_name?\nLet’s also discuss a new syntax: using brackets {} around our variable:\n\nWe enclose variables in {} whenever they are part of a string that includes other text, as demonstrated in the line with the outfile_name variable. This is especially important when the variable is adjacent to other characters that might be confused as part of the variable name.\nHowever, if the variable is not surrounded by other characters, as in the echo command, we don’t need to use the brackets.",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#finalizing-the-loop",
    "href": "source/core_tools/bash-for-loops.html#finalizing-the-loop",
    "title": "For-loops in bash",
    "section": "Finalizing the loop",
    "text": "Finalizing the loop\nLet’s complete the loop by implementing the desired modifications to the files.\n\nfor i in data/*faa;\ndo\nbasename=$(basename $i .faa | cut -f1-2 -d \"_\")\noutfile_name=\"data/renamed/${basename}.faa\"\ncut -f1 -d \" \" $i &gt; $outfile_name\ndone\n\n#check if that worked \nls data/renamed/\nhead data/renamed/GCA_000008085.1.faa\n\nAfter running this code, you should observe two new files in the data/renamed folder, each with a shortened file name compared to the original:\nGCA_000008085.1.faa\nGCA_000017945.1.faa\nFurthermore, you’ll notice that the fasta headers have been shortened:\n&gt;AAR38856.1\nMRLLLELKALNSIDKKQLSNYLIQGFIYNILKNTEYSWLHNWKKEKYFNFTLIPKKDIIENKRYYLIISSPDKRFIEVLH\nNKIKDLDIITIGLAQFQLRKTKKFDPKLRFPWVTITPIVLREGKIVILKGDKYYKVFVKRLEELKKYNLIKKKEPILEEP\nIEISLNQIKDGWKIIDVKDRYYDFRNKSFSAFSNWLRDLKEQSLRKYNNFCGKNFYFEEAIFEGFTFYKTVSIRIRINRG\nEAVYIGTLWKELNVYRKLDKEEREFYKFLYDCGLGSLNSMGFGFVNTKKNSAR\n&gt;AAR38857.1\nMKKPQPYKDEEIYSILEEPVKQWFKEKYKTFTPPQRYAIMEIHKRNNVLISSPTGSGKTLAAFLAIINELIKLSHKGKLE\nNRVYAIYVSPLRSLNNDVKKNLETPLKEIKEKAKELNYYIGDIRIAVRTSDTKESEKAKMLKQPPHILITTPESLAIILS\nTKKFREHIKKVEFVVVDEIHALAESKRGTHLALSLERLNYLTNFVRIGLSATIHPLEEVAKFLFGYENGKPREGYIIDVS\nFEKPIEIQVYSPVDDIIYSSQEELMRNLYKFIGEQLKKYRTILIFTNTRHGAESVAYHLKKAFPDLEKYIAVHHSSLSRE\nDo you see how we elegantly use $outfile_name to create a new file with a cleaner name in a different folder compared to the imput file? This is a very good example for how flexible you can be with your files with some small bash knowledge.\n\nUsing Loops with Files\nIn addition to iterating over files directly within a directory using wildcard patterns (e.g., *.txt), Bash also allows you to iterate over a list of filenames stored in a file. This can be achieved by using command substitution to capture the output of a command that generates the list of filenames. Here’s how you can use this approach:\n\n# Assume 'list' is a file containing a list of filenames, with each filename on a separate line\n# Iterate over each filename in the list\nfor filename in $(cat list); do\n    # Perform operations on each filename\n    echo \"Processing file: $filename\"\n    # Add more commands here\ndone\n\nIn this example, cat list is used within command substitution $(...) to generate the list of filenames stored in the file named list. The loop then iterates over each filename in the list, performing operations as needed.\nWhy This Might Be Useful:\nUsing loops with files allows for greater flexibility and automation in handling sets of files that may not follow a consistent naming pattern or reside in different directories. This approach is particularly useful in scenarios where:\n\nDynamic File Lists: The list of files to process may vary over time or depend on external factors. By storing filenames in a separate file, you can easily update the list without modifying the loop structure.\nComplex Filtering: You need to filter files based on specific criteria or conditions that cannot be expressed using wildcard patterns alone. For example, you may need to select files based on metadata stored in another file or database. Imagine that part of your genomes are eukaryotic, the other are bacteria. Maybe you need to process them separately because eukaryotes require the use of different options compared to bacteria.\nIntegration with External Tools: The filenames need to be processed or transformed before being used in the loop. You can leverage command-line tools or scripting languages to generate the list of filenames dynamically based on certain criteria or conditions.\nImproved Readability and Maintainability: Storing filenames in a separate file can make your scripts more readable and maintainable, especially when dealing with large sets of files or complex directory structures. It also allows for easier collaboration and sharing of scripts among team members.\n\nBy leveraging loops with files, you can streamline your workflow and efficiently process large sets of files with ease and flexibility.",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#addon-alternative-syntax",
    "href": "source/core_tools/bash-for-loops.html#addon-alternative-syntax",
    "title": "For-loops in bash",
    "section": "Addon: Alternative syntax",
    "text": "Addon: Alternative syntax\nFor more complex code, it might be useful to break things up for readability. Consider this command (the details are not important here, however, see how we use exactly the same loop structure to run a completely different command?):\n\nfor i in results/prokka/*/bin*/*.gbk; do\n    base_name=$(basename \"$i\" .gbk)\n    outdir=\"results/pseudofinder/${base_name}\"\n    pseudofinder.py annotate -g $i -op $outdir -db /zfs/omics/projects/bioinformatics/databases/uniprot/uniprot_sprot_070524.fasta -t 20\ndone\n\nThe line that runs the tool pseudofinder.py is becoming longer, and for readability, we might want to break this command down and display each option on an individual line. We can do this as follows:\n\nfor i in results/prokka/*/bin*/*.gbk; do\n    base_name=$(basename \"$i\" .gbk)\n    outdir=\"results/pseudofinder/${base_name}\"\n    pseudofinder.py annotate \\\n      -g $i \\\n      -op $outdir \\\n      -db /zfs/omics/projects/bioinformatics/databases/uniprot/uniprot_sprot_070524.fasta \\\n      -t 20\ndone\n\nHere, we use the \\ to split a long line into two shorter lines, without signifying an end of the statement. If you use this, be careful with the syntax, as \\ needs to be immediately followed by a new line. So don’t add any spaces afterwards.",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/core_tools/bash-for-loops.html#error-handling-or-why-my-loop-does-not-work",
    "href": "source/core_tools/bash-for-loops.html#error-handling-or-why-my-loop-does-not-work",
    "title": "For-loops in bash",
    "section": "Error handling or why my loop does not work?",
    "text": "Error handling or why my loop does not work?\nWhile writing and executing loops in Bash can greatly streamline your workflow, it’s essential to anticipate and handle potential errors that may arise during script execution. Here are some common issues you might encounter and strategies for troubleshooting them:\n\n0. General recommendations\nWhen getting started with loops, I recommend starting as simple as possible and building your way up, similar to the approach used in this workflow. Here’s a suggested workflow to follow:\n\nBegin by running your command or software on a single file. Explore the outputs generated and observe how they are named. Understanding these outputs will help you determine how to structure your loop effectively.\nBuild the most basic loop by simply printing the names of the files you want to iterate over. This step allows you to verify that your loop is correctly identifying the files you intend to process.\nExtend the loop structure step-by-step, adding functionality incrementally. This iterative approach enables you to catch mistakes early, as it’s easier to debug a simple command compared to a complex loop.\n\nAdditionally, keep in mind that syntax matters a lot in Bash scripting. Common syntax-related mistakes to avoid include:\n\nUsing straight quotes (“), as opposed to curly quotes (“), which may not be recognized by the shell. Depending on your text editor, you might inadvertently use curly quotes, causing syntax errors. Be mindful of this difference and adjust your editor settings if necessary. For example, for Mac users, check out the 3rd comment in this post\nWhen defining variables ensure to use proper syntax and not use spaces between the variable name and variable content. I.e. variable=\"hello\".\nWhitespace and Indentation: Ensure consistent whitespace and indentation in your Bash scripts to enhance readability and maintainability. Pay attention to spaces around operators and arguments to commands within your script to avoid syntax errors or unexpected behavior. Also ensure that you always use spaces after command names and command options.\nForgetting to Quote Variables or Use Brackets Around Variables: Properly quote variables, especially when dealing with filenames or strings containing spaces or special characters, to ensure their values are interpreted correctly by the shell. Additionally, use brackets {} around variables when they are part of a string with other text to avoid ambiguity and potential issues.\nFailure to Handle Empty or Missing Files: Be vigilant about handling scenarios where files might be empty or missing in your loops. Implement checks within your script to gracefully handle such situations, either by skipping empty or missing files or by providing appropriate error messages or default values.\n\n\n\n1. File Not Found\nIf your loop is not working as expected, one common issue could be that the specified files are not found in the designated directory. This could occur due to incorrect file paths or missing files. To address this:\n\nDouble-check the file paths specified in your loop to ensure they are accurate and point to the correct directory.\nSimplify your command and use echo to see if the correct filename is printed and\nVerify that the files you intend to process actually exist in the specified directory. You can do this using the ls command or by manually inspecting the directory with nano or head.\n\n\n\n2. Incorrect File Format\nAnother potential issue is if the files you’re attempting to process are not in the expected format. For example, if your loop expects fasta files but encounters files in a different format, it may not function correctly. To handle this:\n\nConfirm that the files in the specified directory are in the expected format. You can examine file contents using commands like head or cat to verify their format.\nImplement checks within your loop to validate file formats before processing them. This can help prevent errors and ensure that only compatible files are processed.\n\n\n\n3. Permissions\nPermissions issues can also prevent your loop from executing properly, particularly if you’re attempting to write output to a directory where you don’t have sufficient permissions. To resolve this:\n\nEnsure that you have the necessary permissions to read input files and write output files to the designated directories. You can use the ls -l command to view file permissions and ownership.\nIf necessary, adjust file permissions using the chmod command to grant yourself the required access.\n\n\n\n4. Syntax Errors\nSyntax errors in your loop script can also cause it to fail. Common syntax errors include missing semicolons, incorrect variable assignments, or mismatched quotes. To address syntax errors:\n\nCarefully review your loop script for any syntax errors or typos. Pay close attention to variable assignments, loop syntax, usage of spaces and command usage.\n\n\n\n5. Handling DOS File Format Issues\nIn some cases, you may encounter issues related to the file format when working with loops in Bash, particularly if your files originate from a Windows environment. DOS (or Windows) text files use different line-ending characters compared to Unix-based systems, which can lead to unexpected behavior when processing files in Bash.\nDOS text files typically use a combination of carriage return ( and line feed () characters to denote line endings, whereas Unix-based systems use only a line feed character. When processing DOS files in Bash, the presence of extra carriage return characters can cause issues, such as unexpected line breaks or syntax errors.\nTo handle DOS line endings:\n\nUse utilities like dos2unix to convert DOS-formatted text files to Unix format before processing them in your loop. This command removes any extraneous carriage return characters, ensuring consistent line endings.\nIf dos2unix is not available, you can use tr (translate or delete characters) to remove carriage return characters from DOS-formatted files. Here’s how you can accomplish this:\n\n  #Remove carriage return characters from DOS-formatted files\n  for file in *.txt; do\n  tr -d '\\r' &lt; \"$file\" &gt; \"$file.tmp\" && mv \"$file.tmp\" \"$file\"\n  done\n\nThis loop iterates over all .txt files in the current directory, removes carriage return characters ( using tr, and then overwrites the original files with the processed versions. This approach achieves the same result as dos2unix, but using built-in Bash functionality instead.\nYou can modify the file extension (*.txt) to match the specific file types you’re working with in your environment.",
    "crumbs": [
      "Bioinformatics toolbox",
      "For-loops in bash"
    ]
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#introductory-notes",
    "href": "source/Qiime/qiime_cmi_tutorial.html#introductory-notes",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Introductory notes",
    "text": "Introductory notes\n\nTerminology\n\nAll files generated by QIIME 2 are either .qza or .qzv files, and these are simply zip files that store your data alongside some QIIME 2-specific metadata. You can unzip them with unzip xxx.qza\nThe .qza file extension is an abbreviation for QIIME Zipped Artifact, and the .qzv file extension is an abbreviation for QIIME Zipped Visualization. .qza files (which are often simply referred to as artifacts) are intermediary files in a QIIME 2 analysis, usually containing raw data of some sort.\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files (generally .qza and .qzv files) without requiring a QIIME installation.\nPlugins are software packages that can be developed by anyone and define actions, which are steps in an analysis workflow. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses.\nQIIME actions:\n\nA method, i.e.alpha-phylogenetic, accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts (qza file) as output.\nA visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input and generate qzv files. In contrast to a method, a visualizer produces exactly one visualization as output. An example of a QIIME 2 visualizer is the beta-group-significance action in the q2-diversity plugin.\nA pipeline can generate one or more .qza and/or .qzv as output. Pipelines are special in that they’re a type of action that can call other actions. They are often used by developers to define simplify common workflows so they can be run by users in a single step. For example, the core-metrics-phylogenetic action in the q2-diversity plugin is a Pipeline that runs both alpha-phylogenetic and beta-phylogenetic, as well as several other actions, in a single command.\n\nTypes used in QIIME 2:\n\nFile type: the format of a file used to store some data. For example, newick is a file type that is used for storing phylogenetic trees.\nData type: refer to how data is represented in a computer’s memory (i.e., RAM) while it’s actively in use.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. This is a representation of the meaning of the data. For example, two semantic types used in QIIME 2 are Phylogeny[Rooted] and Phylogeny[Unrooted], which are used to represent rooted and unrooted trees, respectively. QIIME 2 methods will describe what semantic types they take as input(s), and what semantic types they generate as output(s)."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#metadata",
    "href": "source/Qiime/qiime_cmi_tutorial.html#metadata",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Metadata",
    "text": "Metadata\nFind out more also here.\n\nMetadata:\n\nSample metadata may include technical details, such as the DNA barcodes that were used for each sample in a multiplexed sequencing run, or descriptions of the samples, such as which subject, time point, and body site each sample came from\nFeature metadata is often a feature annotation, such as the taxonomy assigned to an amplicon sequence variant (ASV).\nQIIME 2 does not place restrictions on what types of metadata are expected to be present; there are no enforced “metadata standards”.\nthe MIxS and MIMARKS standards [1] provide recommendations for microbiome studies and may be helpful in determining what information to collect in your study. If you plan to deposit your data in a data archive (e.g. ENA or Qiita), it is also important to determine the types of metadata expected by that resource.\nIn QIIME 2, we always refer to these files as metadata files, but they are conceptually the same thing as QIIME 1 mapping files. QIIME 2 metadata files are backwards-compatible with QIIME 1 mapping files, meaning that you can use existing QIIME 1 mapping files in QIIME 2 without needing to make modifications to the file.\n\n\n\nFormatting requirements\n\nUsually provided as TSV file (.tsv or .txt file extension) that provides data in form of rows and columns\nFirst row = non-comment, non-empty column headers containing a unique identifier for each metadata entry\nRows whose first cell begins with the pound sign (#) are interpreted as comments and may appear anywhere in the file. Comment rows are ignored by QIIME 2 and are for informational purposes only. Inline comments (i.e., comments that begin part-way through a row or at the end of a row) are not supported.\nEmpty rows (e.g. blank lines or rows consisting solely of empty cells) may appear anywhere in the file and are ignored.\nColumn 1: identifier (ID) column. This column defines the sample or feature IDs associated with your study. It is not recommended to mix sample and feature IDs in a single metadata file; keep sample and feature metadata stored in separate files. The ID column name (also referred to as the ID column header) must be:\n\nCase-insenitive (i.e., uppercase or lowercase, or a mixing of the two, is allowed): id, sampleid, sample id, sample-id, featureid feature id, feature-id\nIDs may consist of any Unicode characters, with the exception that IDs must not start with the pound sign (#), as those rows would be interpreted as comments and ignored.\nIDs cannot be empty (i.e. they must consist of at least one character).\nIDs must be unique (exact string matching is performed to detect duplicates).\nAt least one ID must be present in the file.\nIDs cannot be any of the reserved ID headers listed above.\n\nThe ID column is the first column in the metadata file, and can optionally be followed by additional columns defining metadata associated with each sample or feature ID. Metadata files are not required to have additional metadata columns, so a file containing only an ID column is a valid QIIME 2 metadata file.\nThe contents of a metadata file following the ID column and header row (excluding comments and empty lines) are referred to as the metadata values. A single metadata value, defined by an (ID, column) pair, is referred to as a cell. The following rules apply to metadata values and cells:\n\nMay consist of any Unicode characters.\nEmpty cells represent missing data. Other values such as NA are not interpreted as missing data; only the empty cell is recognized as “missing”. Note that cells consisting solely of whitespace characters are also interpreted as missing data\nIf any cell in the metadata contains leading or trailing whitespace characters (e.g. spaces, tabs), those characters will be ignored when the file is loaded.\n\n\nRecommendations for identifiers:\n\nIdentifiers should be 36 characters long or less.\nIdentifiers should contain only ASCII alphanumeric characters (i.e. in the range of [a-z], [A-Z], or [0-9]), the period (.) character, or the dash (-) character.\nNote that some bioinformatics tools may have more restrictive requirements on identifiers than the recommendations that are outlined here. For example, Illumina sample sheet identifiers cannot have . characters, while we do include those in our set of recommended characters\n[cual-id] (https://github.com/johnchase/cual-id) can be used to help create identifiers and the associated paper also includes a discussion on how to choose identifiers [2].\n\nColumn types:\n\nQIIME 2 currently supports categorical and numeric metadata columns and will automatically attempt to infer the type of each metadata column\nQIIME 2 supports an optional comment directive to allow users to explicitly state a column’s type.\nThe comment directive must appear directly below the header row. The value in the ID column in this row must be #q2:types to indicate the row is a comment directive. Subsequent cells in this row may contain the values categorical or numeric (both case-insensitive). The empty cell is also supported if you do not wish to assign a type to a column\n\nMetadata validation:\n\nQIIME 2 will automatically validate a metadata file anytime it is used. Loading your metadata in QIIME 2 will typically present only a single error at a time, which can make identifying and resolving validation issues cumbersome, especially if there are many issues with the metadata.\nSample and feature metadata files stored in Google Sheets can additionally be validated using Keemei [3]\n\n\n\nThe study\n\nThis tutorial focuses on data reused a Compilation of longitudinal microbiota data and hospitalome from hematopoietic cell transplantation patients [4]\n\n\n\nGetting help\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#setup",
    "href": "source/Qiime/qiime_cmi_tutorial.html#setup",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Setup",
    "text": "Setup\n\nQIIME installation\nRun this if needed. If you are not using mamba yet, replace mamba with conda to use this as an alternative installer.\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n#mamba activate qiime2-2023.7\n\n\n\nSet the working environment\n\n#set wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/QIIME/qiime_cmi_tutorial\"\ncd $wdir \n\n#activate QIIME environment \nmamba activate qiime2-2023.7"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#download-data",
    "href": "source/Qiime/qiime_cmi_tutorial.html#download-data",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Download data",
    "text": "Download data\n\nmkdir data\nmkdir visualizations\n\n#download metadata table\nwget \\\n  -O 'sample-metadata.tsv' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/020-metadata/sample-metadata.tsv'\n\n#prepare metadata for qiime view \nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv \\\n  --o-visualization visualizations/metadata-summ-1.qzv\n\n#download sequencing data (already demultiplexed)\nwget \\\n  -O 'data_to_import.zip' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/020-tutorial-upstream/030-importing/data_to_import.zip'\n\nunzip -d data_to_import data_to_import.zip\n\nrm data_to_import.zip"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-import-import-data-into-qiime",
    "href": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-import-import-data-into-qiime",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "qiime tools import: Import data into QIIME",
    "text": "qiime tools import: Import data into QIIME\n\n#import data into qiime\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-format CasavaOneEightSingleLanePerSampleDirFmt \\\n  --input-path data_to_import \\\n  --output-path demultiplexed-sequences.qza\n\n#generate a summary of the imported data\nqiime demux summarize \\\n  --i-data demultiplexed-sequences.qza \\\n  --o-visualization visualizations/demultiplexed-sequences-summ.qzv\n\nqiime demux: supports demultiplexing of single-end and paired-end sequence reads and visualization of sequence quality information."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-peek-check-artifact-format",
    "href": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-peek-check-artifact-format",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "qiime tools peek: check artifact format",
    "text": "qiime tools peek: check artifact format\n\nqiime tools peek  demultiplexed-sequences.qza"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#demultiplexing",
    "href": "source/Qiime/qiime_cmi_tutorial.html#demultiplexing",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Demultiplexing",
    "text": "Demultiplexing\nIf you have reads from multiple samples in the same file, you’ll need to demultiplex your sequences.\nIf your barcodes are still in your sequences, you can use functions from the cutadapt plugin. The cutadapt demux-single method looks for barcode sequences at the beginning of your reads (5’ end) with a certain error tolerance, removes them, and returns sequence data separated by each sample. The QIIME 2 forum has a tutorial on various functions available in cutadapt, including demultiplexing. You can learn more about how cutadapt works under the hood by reading their documentation.\nNote: Currently q2-demux and q2-cutadapt do not support demultiplexing dual-barcoded paired-end sequences, but only can demultiplex with barcodes in the forward reads. So for the time being, this type of demultiplexing needs to be done outside of QIIME 2 using other tools, for example bcl2fastq.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here.\nThere are two plugins to check out:\n\nq2-demux\ncutadapt"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#merging-reads",
    "href": "source/Qiime/qiime_cmi_tutorial.html#merging-reads",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Merging reads",
    "text": "Merging reads\nWhether or not you need to merge reads depends on how you plan to cluster or denoise your sequences into amplicon sequence variants (ASVs) or operational taxonomic units (OTUs). If you plan to use deblur or OTU clustering methods next, join your sequences now. If you plan to use dada2 to denoise your sequences, do not merge — dada2 performs read merging automatically after denoising each sequence.\nIf you need to merge your reads, you can use the QIIME 2 q2-vsearch plugin with the merge-pairs method.\nNotice: This is not applicable for this tutorial and you only will find some relevant notes here."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#removing-non-biological-sequences",
    "href": "source/Qiime/qiime_cmi_tutorial.html#removing-non-biological-sequences",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Removing non-biological sequences",
    "text": "Removing non-biological sequences\nIf your data contains any non-biological sequences (e.g. primers, sequencing adapters, PCR spacers, etc), you should remove these.\nThe q2-cutadapt plugin has comprehensive methods for removing non-biological sequences from paired-end or single-end data.\nIf you’re going to use DADA2 to denoise your sequences, you can remove biological sequences at the same time as you call the denoising function. All of DADA2’s denoise fuctions have some sort of –p-trim parameter you can specify to remove base pairs from the 5’ end of your reads. (Deblur does not have this functionality yet.)"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#denoising-and-clustering",
    "href": "source/Qiime/qiime_cmi_tutorial.html#denoising-and-clustering",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Denoising and clustering",
    "text": "Denoising and clustering\nThe names for these steps are very descriptive:\n\nWe denoise our sequences to remove and/or correct noisy reads.\nWe dereplicate our sequences to reduce repetition and file size/memory requirements in downstream steps.\nWe cluster sequences to collapse similar sequences (e.g., those that are ≥ 97% similar to each other) into single replicate sequences. This process, also known as OTU picking, was once a common procedure, used to simultaneously dereplicate but also perform a sort of quick-and-dirty denoising procedure (to capture stochastic sequencing and PCR errors, which should be rare and similar to more abundant centroid sequences). Use denoising methods instead if you can.\n\nThe denoising methods currently available in QIIME 2 include DADA2 and Deblur. Note that deblur (and also vsearch dereplicate-sequences) should be preceded by basic quality-score-based filtering, but this is unnecessary for dada2. Both Deblur and DADA2 contain internal chimera checking methods and abundance filtering, so additional filtering should not be necessary following these methods.\nTo put it simply, these methods filter out noisy sequences, correct errors in marginal sequences (in the case of DADA2), remove chimeric sequences, remove singletons, join denoised paired-end reads (in the case of DADA2), and then dereplicate those sequences.\n\nThe feature table\nThe final products of all denoising and clustering methods/workflows are a FeatureTable[Frequency] (feature table) artifact and a FeatureData[Sequence] (representative sequences) artifact. These are two of the most important artifacts in an amplicon sequencing workflow, and are used for many downstream analyses.\nMany operations on these tables can be performed with q2-feature-table.\nWant to see which sequences are associated with each feature ID? Use qiime metadata tabulate with your FeatureData[Sequence] artifact as input.\n\n\nDada 2 workflow\nQuality control or denoising of the sequence data will be performed with DADA2 [5]. DADA2 is an model-based approach for correcting amplicon errors without constructing OTUs. Can be applied to every gene of interest and is not limited to the 16S.\nThe DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).\nAlso check out this tutorial for using DADA2. One important parameter to check is truncLen to trim low quality read ends. Also check out this paper discussing quality filtering [6].\nIn QIIME the denoise_paired action in the q2-dada2 plugin. This performs quality filtering, chimera checking, and paired- end read joining.\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots and identify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively. If you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively. Your reads must still overlap after truncation in order to merge them later!\nSome thoughts on this:\n\nReviewing the data we notice that the twenty-fifth percentile quality score drops below 30 at position 204 in the forward reads and 205 in the reverse reads. We chose to use those values for the required truncation lengths. This truncates the 3’/5’ end of the of the input sequences. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads.\nSince the first base of the reverse reads is slightly lower than those that follow, I choose to trim that first base in the reverse reads, but apply no trimming to the forward reads. This trimming is probably unnecessary here, but is useful here for illustrating how this works.\nFigaro is a tool to automatically choose the trunc_len\nChimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2\n\nDesired overlap length\n\nOften between 10 and 20 for most applications.\nDADA2 often recommends 20\nRemember: longer overlaps means more 3’ end must be kept at a cost of decreased overall sequence quality\n\nOther parameters:\n\n--p-max-ee-f/r {number}: Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]. If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads (eg. maxEE=c(2,5))\n--p-trunc-q {integeer}: Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-for trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap {INTEGER}: The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method {TEXT Choices(‘independent’, ‘pseudo’)}: he method used to pool samples for denoising. By default, the dada function processes each sample independently. However, pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method {TEXT Choices(‘consensus’, ‘none’, ‘pooled’)}: The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-n-threads {INTEGER}: default 1\n\nOutputs:\n\nThe feature table describes which amplicon sequence variants (ASVs) were observed in which samples, and how many times each ASV was observed in each sample.\nThe feature data in this case is the sequence that defines each ASV. Generate and explore the summaries of each of these files.\nSanity check: in stats-dada2 check that outside of filtering, there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nSequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the asv table.\n\n\n#denoise data\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs demultiplexed-sequences.qza \\\n  --p-trunc-len-f 204 \\\n  --p-trim-left-r 1 \\\n  --p-trunc-len-r 205 \\\n  --o-representative-sequences asv-sequences-0.qza \\\n  --o-table feature-table-0.qza \\\n  --o-denoising-stats dada2-stats.qza\n\n#generate summaries\nqiime feature-table summarize \\\n  --i-table feature-table-0.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-0-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data asv-sequences-0.qza \\\n  --o-visualization visualizations/asv-sequences-0-summ.qzv\n\nqiime metadata tabulate \\\n  --m-input-file dada2-stats.qza \\\n  --o-visualization visualizations/stats-dada2.qzv\n\nOutputs:\n\nASV table,\nthe representative sequences,\nstatistics on the procedure"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#filtering-the-feature-table",
    "href": "source/Qiime/qiime_cmi_tutorial.html#filtering-the-feature-table",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Filtering the feature table",
    "text": "Filtering the feature table\nWe’ll next obtain a much larger feature table representing all of the samples included in the study dataset. These would take too much time to denoise in this course, so we’ll start with the feature table, sequences, and metadata provided by the authors and filter to samples that we’ll use for our analyses.\nA full description can be foun on the QIIME website\n\n#cleanup first dataset\nmkdir upstream_tutorial\nmv *qza upstream_tutorial/\n\n#get the data\nwget \\\n  -O 'feature-table.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/feature-table.qza'\n\nwget \\\n  -O 'rep-seqs.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/010-filtering/rep-seqs.qza'\n\n#summarize table\nqiime feature-table summarize \\\n  --i-table feature-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/feature-table-summ.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data rep-seqs.qza \\\n  --o-visualization visualizations/rep-seqs-summ.qzv\n\nOutput explanation:\n\nA feature is essentially any unit of observation, e.g., an OTU, a sequence variant, a gene, a metabolite, etc, and a feature table is a matrix of sample X feature abundances (the number of times each feature was observed in each sample).\nminimum frequency is 5342 - if you click the “Interactive Sample Detail” tab and scroll to the bottom, you will see that the sample with the lowest feature frequency has 5342 observations. Similarly, the sample with the highest frequency of features has 193491 total observations - meaning that many/most features were seen more than once.\nmean frequency of features (881)? Does it mean that one feature is found in average 881 times throughout all the samples\n\n\nDownsample the feature table\n\n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table autofmt-table.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/autofmt-table-summ.qzv\n\n#filter time window\nqiime feature-table filter-samples \\\n  --i-table autofmt-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'DayRelativeToNearestHCT BETWEEN -10 AND 70' \\\n  --o-filtered-table filtered-table-1.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\n#done to reduce runtime (so optional)\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#summarize\nqiime feature-table summarize \\\n  --i-table filtered-table-2.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-2-summ.qzv\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\nWe can also also filter tables by lists:\n\n#create imaginary list of ids to keep\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\n#filter \nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nSome examples using where:\n\n–p-where “[subject]=‘subject-1’”\n\n–p-where “[body-site] IN (‘left palm’, ‘right palm’)”\n\n–p-where “[subject]=‘subject-1’ AND [body-site]=‘gut’”\n\n–p-where “[body-site]=‘gut’ OR [reported-antibiotic-usage]=‘Yes’”\n\n–p-where “[subject]=‘subject-1’ AND NOT [body-site]=‘gut’”\n\n\n\n\nFilter features from sequence data\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#taxonomic-annotations",
    "href": "source/Qiime/qiime_cmi_tutorial.html#taxonomic-annotations",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Taxonomic annotations",
    "text": "Taxonomic annotations\nTo identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods: - classify-consensus-blast and classify-consensus-vsearch are both alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained - Machine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! - Most users do not even need to follow that tutorial and perform that training step, because the lovely QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods we’ve tested based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nIn the example below we use a pre-trained Naive Bayes taxonomic classifier. This particular classifier was trained on the Greengenes 13-8 database, where sequences were trimmed to represent only the region between the 515F / 806R primers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [7].\nCheck out this study discussing reproducible sequence taxonomy reference database management [8].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [9].\n\n\nLink ASV to taxonomy\n\n#get classifier\nwget \\\n  -O 'gg-13-8-99-nb-classifier.qza' \\\n  'https://docs.qiime2.org/jupyterbooks/cancer-microbiome-intervention-tutorial/data/030-tutorial-downstream/020-taxonomy/gg-13-8-99-nb-classifier.qza'\n\n#assign taxonomic info to ASV sequences\nqiime feature-classifier classify-sklearn \\\n  --i-classifier gg-13-8-99-nb-classifier.qza \\\n  --i-reads filtered-sequences-1.qza \\\n  --o-classification taxonomy.qza\n\n#generate summary\nqiime metadata tabulate \\\n  --m-input-file taxonomy.qza \\\n  --o-visualization visualizations/taxonomy.qzv\n\n\n\nFilter based on taxonomy\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza\n\n\n\nFiltering samples with low sequence counts\nYou may have noticed when looking at feature table summaries earlier that some of the samples contained very few ASV sequences. These often represent samples which didn’t amplify or sequence well, and when we start visualizing our data low numbers of sequences can cause misleading results, because the observed composition of the sample may not be reflective of the sample’s actual composition. For this reason it can be helpful to exclude samples with low ASV sequence counts from our samples. Here, we’ll filter out samples from which we have obtained fewer than 10,000 sequences.\n\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#summarize data\nqiime feature-table summarize \\\n  --i-table filtered-table-4.qza \\\n  --m-sample-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/filtered-table-4-summ.qzv\n\n\n\nFiltering our sequence representatives\n\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-4.qza \\\n  --o-filtered-data filtered-sequences-2.qza"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#generate-taxonomic-composition-barplots",
    "href": "source/Qiime/qiime_cmi_tutorial.html#generate-taxonomic-composition-barplots",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Generate taxonomic composition barplots",
    "text": "Generate taxonomic composition barplots\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/taxa-bar-plots-1.qzv\n\nNotice: We can also generate heatmaps with feature-table heatmap"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#phylogenetic-tree-construction",
    "href": "source/Qiime/qiime_cmi_tutorial.html#phylogenetic-tree-construction",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Phylogenetic tree construction",
    "text": "Phylogenetic tree construction\n\nBasics\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\nRuning the Qiime pipeline\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\nRun things step by step\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose"
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "href": "source/Qiime/qiime_cmi_tutorial.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Identifying an even sampling depth for use in diversity metrics",
    "text": "Identifying an even sampling depth for use in diversity metrics\n\nNotes on rarefication\nNotice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [10]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [11].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [12]\nNormalization and microbial differential abundance strategies depend upon data characteristics [11]\n\n\n\nAlpha rarefaction plots\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#computing-diversity-metrics",
    "href": "source/Qiime/qiime_cmi_tutorial.html#computing-diversity-metrics",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Computing diversity metrics",
    "text": "Computing diversity metrics\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\nAlpha diversity\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)\n\n\nStatistics\nIf continuous sample metadata columns (e.g., days-since-experiment-start) are correlated with alpha diversity, we can test for those associations here. If you’re interested in performing those tests (for this data set, or for others), you can use the qiime diversity alpha-correlation command.\nWe can also analyze sample composition in the context of categorical metadata using PERMANOVA using the beta-group-significance command. The code below was taken from another test but would allow to test whether distances between samples within a group, such as samples from the same body site (e.g., gut), are more similar to each other then they are to samples from the other groups (e.g., tongue, left palm, and right palm). If you call this command with the --p-pairwise parameter, as we’ll do here, it will also perform pairwise tests that will allow you to determine which specific pairs of groups (e.g., tongue and gut) differ from one another, if any. This command can be slow to run, especially when passing --p-pairwise, since it is based on permutation tests. So, unlike the previous commands, we’ll run beta-group-significance on specific columns of metadata that we’re interested in exploring, rather than all metadata columns to which it is applicable. Here we’ll apply this to our unweighted UniFrac distances, using two sample metadata columns, as follows.\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column body-site \\\n  --o-visualization core-metrics-results/unweighted-unifrac-body-site-significance.qzv \\\n  --p-pairwise\n\nqiime diversity beta-group-significance \\\n  --i-distance-matrix core-metrics-results/unweighted_unifrac_distance_matrix.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --m-metadata-column subject \\\n  --o-visualization core-metrics-results/unweighted-unifrac-subject-group-significance.qzv \\\n  --p-pairwise\n\nCheck this link for an example output.\nIf continuous sample metadata are correlated with sample composition, we can use the qiime metadata distance-matrix in combination with qiime diversity mantel and qiime diversity bioenvcommands.\n\n\nLinear Mixed Effects\nIn order to manage the repeated measures, we will use a linear mixed-effects model. In a mixed-effects model, we combine fixed-effects (your typical linear regression coefficients) with random-effects. These random effects are some (ostensibly random) per-group coefficient which minimizes the error within that group. In our situation, we would want our random effect to be the PatientID as we can see each subject has a different baseline for richness (and we have multiple measures for each patient). By making that a random effect, we can more accurately ascribe associations to the fixed effects as we treat each sample as a “draw” from a per-group distribution.\nIt is called a random effect because the cause of the per-subject deviation from the population intercept is not known. Its source is “random” in the statistical sense. That is randomness is not introduced to the model, but is instead tolerated by it.\nThere are several ways to create a linear model with random effects, but we will be using a random-intercept, which allows for the per-subject intercept to take on a different average from the population intercept (modeling what we saw in the group-significance plot above).\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column DayRelativeToNearestHCT \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-HCT.qzv\n\nHere we see a significant association between richness and the bone marrow transplant.\nOptions:\n\n–p-state-column TEXT Metadata column containing state (time) variable information.\n–p-individual-id-column TEXT Metadata column containing IDs for individual subjects.\n–p-metric TEXT Dependent variable column name. Must be a column name located in the metadata or feature table files.\n\nWe may also be interested in the effect of the auto fecal microbiota transplant. It should be known that these are generally correlated, so choosing one model over the other will require external knowledge.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-FMT.qzv\n\nWe also see a downward trend from the FMT. Since the goal of the FMT was to ameliorate the impact of the bone marrow transplant protocol (which involves an extreme course of antibiotics) on gut health, and the timing of the FMT is related to the timing of the marrow transplant, we might deduce that the negative coefficient is primarily related to the bone marrow transplant procedure. (We can’t prove this with statistics alone however, in this case, we are using abductive reasoning).\nLooking at the log-likelihood, we also note that the HCT result is slightly better than the FMT in accounting for the loss of richness. But only slightly, if we were to perform model testing it may not prove significant.\nIn any case, we can ask a more targeted question to identify if the FMT was useful in recovering richness.\nBy adding the autoFmtGroup to our linear model, we can see if there are different slopes for the two groups, based on an interaction term.\n\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric observed_features \\\n  --o-visualization visualizations/lme-obs-features-treatmentVScontrol.qzv\n\nHere we see that the autoFmtGroup is not on its own a significant predictor of richness, but its interaction term with Q(‘day-relative-to-fmt’) is. This implies that there are different slopes between these groups, and we note that given the coding of Q(‘day-relative-to-fmt’):autoFmtGroup[T.treatment] we have a positive coefficient which counteracts (to a degree) the negative coefficient of Q(‘day-relative-to-fmt’).\nNotice: The slopes of the regression scatterplot in this visualization are not the same fit as our mixed-effects model in the table. They are a naive OLS fit to give a sense of the situation. A proper visualization would be a partial regression plot which can condition on other terms to show some effect in “isolation”. This is not currently implemented in QIIME 2.\n\n\nTest the above models with a different diversity index, such as Faith’s Phylogenetic Diversity\n\n#test group significance \nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-faith-pd.qzv\n\n#lme\nqiime longitudinal linear-mixed-effects \\\n  --m-metadata-file sample-metadata.tsv diversity-core-metrics-phylogenetic/faith_pd_vector.qza \\\n  --p-state-column day-relative-to-fmt \\\n  --p-group-columns autoFmtGroup \\\n  --p-individual-id-column PatientID \\\n  --p-metric faith_pd \\\n  --o-visualization visualizations/lme-faith-pd-treatmentVScontrol.qzv\n\n\n\nBeta diversity\nBeta diversity is between sample diversity. This is useful for answering the question, how different are these microbial communities?\nList of available indices:\n\nJaccard distance (a qualitative measure of community dissimilarity)\nBray-Curtis distance (a quantitative measure of community dissimilarity)\nunweighted UniFrac distance (a qualitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\nweighted UniFrac distance (a quantitative measure of community dissimilarity that incorporates phylogenetic relationships between the features)\n\n\nqiime diversity beta-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metric braycurtis \\\n  --p-clustering-method nj \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/braycurtis-rarefaction-plot.qzv\n\n\numap is an ordination method that can be used in place of PCoA and has been shown to better resolve differences between microbiome samples in ordination plots [13].\nLike PCoA, umap operates on distance matrices. We’ll compute this on our weighted and unweighted UniFrac distance matrices.\n\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/unweighted_unifrac_distance_matrix.qza \\\n  --o-umap uu-umap.qza\n\nqiime diversity umap \\\n  --i-distance-matrix diversity-core-metrics-phylogenetic/weighted_unifrac_distance_matrix.qza \\\n  --o-umap wu-umap.qza\n\nIn the next few steps, we’ll integrate our unweighted UniFrac umap axis 1 values, and our Faith PD, evenness, and Shannon diversity values, as metadata in visualizations. This will provide a few different ways of interpreting these values.\n\nqiime metadata tabulate \\\n  --m-input-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization expanded-metadata-summ.qzv\n\nTo see how this information can be used, let’s generate another version of our taxonomy barplots that includes these new metadata values.\n\nqiime taxa barplot \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --o-visualization visualizations/taxa-bar-plots-2.qzv\n\nWe’ll start by integrating these values as metadata in our ordination plots. We’ll also customize these plots in another way: in addition to plotting the ordination axes, we’ll add an explicit time axis to these plots. This is often useful for visualization patterns in ordination plots in time series studies. We’ll add an axis for week-relative-to-hct.\n\nqiime emperor plot \\\n  --i-pcoa uu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa wu-umap.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-umap-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/unweighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/uu-pcoa-emperor-w-time.qzv\n\nqiime emperor plot \\\n  --i-pcoa diversity-core-metrics-phylogenetic/weighted_unifrac_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-custom-axes week-relative-to-hct \\\n  --o-visualization visualizations/wu-pcoa-emperor-w-time.qzv\n\nTip:\nQIIME 2’s q2-diversity plugin provides visualizations for assessing whether microbiome composition differs across groups of independent samples (for example, individuals with a certain disease state and healthy controls) and for assessing whether differences in microbiome composition are correlated with differences in a continuous variable (for example, subjects’ body mass index). These tools assume that all samples are independent of one another, and therefore aren’t applicable to the data used in this tutorial where multiple samples are obtained from the same individual. We therefore don’t illustrate the use of these visualizations on this data, but you can learn about these approaches and view examples in the Moving Pictures tutorial. The Moving Pictures tutorial contains example data and commands, like this tutorial does, so you can experiment with generating these visualizations on your own."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#differential-abundance-testing",
    "href": "source/Qiime/qiime_cmi_tutorial.html#differential-abundance-testing",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Differential abundance testing",
    "text": "Differential abundance testing\nANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [14].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#longitudinal-microbiome-analysis",
    "href": "source/Qiime/qiime_cmi_tutorial.html#longitudinal-microbiome-analysis",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "Longitudinal microbiome analysis",
    "text": "Longitudinal microbiome analysis\n\nData transformation\nBefore applying these analyses, we’re going to perform some additional operations on the feature table that will make these analyses run quicker and make the results more interpretable.\nFirst, we are going to use the taxonomic information that we generated earlier to redefine our features as microbial genera. To do this, we group (or collapse) ASV features based on their taxonomic assignments through the genus level. This is achieved using the q2-taxa plugin’s collapse action.\n\nqiime taxa collapse \\\n  --i-table filtered-table-4.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-level 6 \\\n  --o-collapsed-table genus-table.qza\n\nThen, to focus on the genera that are likely to display the most interesting patterns over time (and to reduce the runtime of the steps that come next), we will perform even more filtering. This time we’ll apply prevalence and abudnance based filtering. Specifically, we’ll require that a genus’s abundance is at least 1% in at least 10% of the samples.\nNote: The prevalence-based filtering applied here is fairly stringent. In your own analyses you may want to experiment with relaxed settings of these parameters. Because we want the commands below to run quickly, stringent filtering is helpful for the tutorial.\n\nqiime feature-table filter-features-conditionally \\\n  --i-table genus-table.qza \\\n  --p-prevalence 0.1 \\\n  --p-abundance 0.01 \\\n  --o-filtered-table filtered-genus-table.qza\n\nFinally, we’ll convert the counts in our feature table to relative frequencies. This is required for some of the analyses that we’re about to perform.\n\nqiime feature-table relative-frequency \\\n  --i-table filtered-genus-table.qza \\\n  --o-relative-frequency-table genus-rf-table.qza\n\n\n\nVolatility plots\nThe first plots we’ll generate are volatility plots. We’ll generate these using two different time variables. First, we’ll plot based on week-relative-to-hct.\nThe volatility visualizer generates interactive line plots that allow us to assess how volatile a dependent variable is over a continuous, independent variable (e.g., time) in one or more groups. See also the QIIME notes\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-hct \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-1.qzv\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal volatility \\\n  --i-table genus-rf-table.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-individual-id-column PatientID \\\n  --p-default-group-column autoFmtGroup \\\n  --o-visualization visualizations/volatility-plot-2.qzv\n\n\n\nFeature volatility\nThe last plots we’ll generate in this section will come from a QIIME 2 pipeline called feature-volatility. These use supervised regression to identify features that are most associated with changes over time, and add plotting of those features to a volatility control chart.\nAgain, we’ll generate the same plots but using two different time variables on the x-axes. First, we’ll plot based on week-relative-to-hct.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-hct \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility\n\nNext, we’ll plot based on week-relative-to-fmt.\n\nqiime longitudinal feature-volatility \\\n  --i-table filtered-genus-table.qza \\\n  --m-metadata-file sample-metadata.tsv uu-umap.qza diversity-core-metrics-phylogenetic/faith_pd_vector.qza diversity-core-metrics-phylogenetic/evenness_vector.qza diversity-core-metrics-phylogenetic/shannon_vector.qza \\\n  --p-state-column week-relative-to-fmt \\\n  --p-individual-id-column PatientID \\\n  --output-dir visualizations/longitudinal-feature-volatility-2\n\nOutputs:\n\nvolatility-plot contains an interactive feature volatility plot. This is very similar to the plots produced by the volatility visualizer described above, with a couple key differences. First, only features are viewable as “metrics” (plotted on the y-axis). Second, feature metadata (feature importances and descriptive statistics) are plotted as bar charts below the volatility plot. The relative frequencies of different features can be plotted in the volatility chart by either selecting the “metric” selection tool, or by clicking on one of the bars in the bar plot. This makes it convenient to select features for viewing based on importance or other feature metadata. By default, the most important feature is plotted in the volatility plot when the visualization is viewed. Different feature metadata can be selected and sorted using the control panel to the right of the bar charts. Most of these should be self-explanatory, except for “cumulative average change” (the cumulative magnitude of change, both positive and negative, across states, and averaged across samples at each state), and “net average change” (positive and negative “cumulative average change” is summed to determine whether a feature increased or decreased in abundance between baseline and end of study).\naccuracy-results display the predictive accuracy of the regression model. This is important to view, as important features are meaningless if the model is inaccurate. See the sample classifier tutorial for more description of regressor accuracy results.\nfeature-importance contains the importance scores of all features. This is viewable in the feature volatility plot, but this artifact is nonetheless output for convenience. See the sample classifier tutorial for more description of feature importance scores.\nfiltered-table is a FeatureTable[RelativeFrequency] artifact containing only important features. This is output for convenience.\nsample-estimator contains the trained sample regressor. This is output for convenience, just in case you plan to regress additional samples. See the sample classifier tutorial for more description of the SampleEstimator type."
  },
  {
    "objectID": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-export-export-data",
    "href": "source/Qiime/qiime_cmi_tutorial.html#qiime-tools-export-export-data",
    "title": "Notes to follow the QIIME CMI tutorial",
    "section": "qiime tools export: Export data",
    "text": "qiime tools export: Export data\nIf you’re a veteran microbiome scientist and don’t want to use QIIME 2 for your analyses, you can extract your feature table and sequences from the artifact using the export tool. While export only outputs the data, the extract tool allows you to also extract other metadata such as the citations, provenance etc.\nNote that this places generically named files (e.g. feature-table.txt) into the output directory, so you may want to immediately rename the files to something more information (or somehow ensure that they stay in their original directory)!\nYou can also use the handy qiime2R package to import QIIME 2 artifacts directly into R."
  },
  {
    "objectID": "source/conda/conda.html#installing-software",
    "href": "source/conda/conda.html#installing-software",
    "title": "Bioinformatics guidance page",
    "section": "Installing software",
    "text": "Installing software\nA lot of bioinformatic workflows start with installing software. Since this often means not only installing the software but several dependencies, we recommend the use of a package management system, such as conda or mamba. These tools allow you to find and install packages in their own environment without administrator privileges.\nThis is especially useful if you require different software versions, such as python3.6 versus python3.10, for different workflows. With package management systems you can easily setup different python versions in different environments.\n\nInstalling mamba\nA lot of system already come with conda installed, however, if possible we recommend working with mamba instead of conda. mamba is a drop-in replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\nTo install mamba, follow the instructions here. This should look something like this for mac and linux-systems. If you are on windows, the easiest is to setup up Windows Subsystem for Linux (WSL) first and then use the code below.\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\nWhen running the bash command, you get asked a few questions:\n\nRead the license and use arrow down to scroll down. Don’t scroll too fast, so that you see the next question\nDecide were conda gets installed. You can say yes, with the default location in your home but don’t forget that for Crunchomics your home only has 25G of space. You could also install the miniforge/mambaforge folder in your personal folder instead.\nSay yes, when you get asked whether conda should be initialized during start up\nRestart the shell (exit the terminal and use ssh to log back in) for the changes to take effect\nCheck if conda is accessible by running conda -h\n\n\n\n\n\n\n\nConda for Mobaxterm users\n\n\n\n\n\nTo setup conda/mamba on Mobaxterm we need to do some extra steps first.\nMobaXterm is primarily designed as a terminal application for Windows, providing a Unix-like environment through the use of Cygwin or a Windows Subsystem for Linux (WSL) integration. While it offers many Linux-like features and utilities, there are some differences and limitations compared to a native Linux environment. In order to deal with these limitations, we can install WSL in order to get access to a native Linux environment.\nIf you want to install and use conda/mamba you need to install WSL first.\n\nTo install WSL, follow these instructions\nOnce you have that installed, you can download and install MobaXterm if you have not done that already: https://mobaxterm.mobatek.net\nInside MobaXterm, you will probably will see that your WSL is already listed on the left panel as an available connection. Just double-click it and you will be accessing it via MobaXterm. If you don’t see it you can also run WSL directly.\nBy default this home directory is in a temporary folder that gets deleted every time you exit Mobaxterm, To give this folder a persistent home, do the following:\n\nSettings –&gt; Configuration –&gt; General\nIn General set Persistent home directory to a folder of your choice\n\nFollow the installation instructions for conda as listed above\n\n\n\n\n\n\nSetting up an environment\nLet’s assume we want to install a tool, ITSx, into an environment called fungal_genomics. If you only have conda installed that is completely fine, just replace mamba with conda in the code below.\nWe can do this as follows:\n\n#check if the tool is installed (should return command not found)\nITSx -h\n\n#create an empty environment and name it fungal_genomics\nmamba create -n fungal_genomics\n\n#install some software, i.e. itsx, into the fungal_genomics environment\nmamba install -n fungal_genomics -c bioconda itsx\n\n#to run the tool activate the environment\nconda activate fungal_genomics\n\n#check if tool is installed\nITSx -h\n\n#leave the environment\nconda deactivate\n\nA full set of mamba/conda commands can be found here\n\n\nAdding existing environments\nOn crunchomics other people might have already installed environments that might be useful for your work. One example is the amplicomics share, which comes with several QIIME 2 installations. To use this, first ask for access to the amplicomics share by contacting n.dombrowski@uva.nl with your uva net id. After you got access, you can add conda environments in the amplicomics share with:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/",
    "crumbs": [
      "Getting Started",
      "Installing software"
    ]
  },
  {
    "objectID": "source/core_tools/bowtie.html#bowtie2",
    "href": "source/core_tools/bowtie.html#bowtie2",
    "title": "Bioinformatics guidance page",
    "section": "Bowtie2",
    "text": "Bowtie2\n\nIntroduction\nBowtie 2 is an ultra-fast and memory-efficient tool for aligning sequencing reads to long reference sequences, for example a genome (Langmead and Salzberg 2012). It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long genomes.\nFor more detailed information, please visit the manual.\n\n\nInstallation\nInstalled on crunchomics: Yes, Bowtie v2.4.1 is installed. If you want to install it yourself, you can run:\n\nmamba create -n bowtie_2.5.3\nmamba install -n bowtie_2.5.3 -c bioconda bowtie2=2.5.3\nmamba activate bowtie_2.5.3\n\n\n\nUsage\n\nIndexing\nTo perform the Bowtie2 alignment, an index is required. The index is analogous to the index in a book. By indexing the reference sequence, we organize it in a manner that allows for an efficient search and retrieval of matches of the query (sequence read) to the reference sequences.\nThe basic syntax is for building an index for a genome called GCF_000385215.fna is as follows:\n\nbowtie2-build GCF_000385215.fna GCF_000385215\n\nIn the command above GCF_000385215.fna is the input file of sequence reads in fasta format, and GCF_000385215 is the prefix of the generated index files.\n\n\nRead alignment\nAfter generating the index, we can align some short Illumina reads against our genome index. Notice, that Bowtie 2 does not generate log summary files and this information gets printed to screen. To save this output in a file we use the 2&gt; operator.\nRequired inputs:\n\nSingle-end and paired-end files in fasta or fastq format (can be compressed)\n\nGenerated output:\n\nThe output from the Bowtie2 is an unsorted SAM file (i.e. Sequence Alignment/Map format)). The SAM file is a tab-delimited text file that contains information for each individual read and its alignment to the genome. To learn how to work with this file format, view the page about samtools.\n\nIf you have a single-end file, you can run:\n\nbowtie2 -p 2 -q \\\n    -x GCF_000385215 \\\n    -U sample1.fastq.gz \\\n    -S sample1_mapped.sam 2&gt; bowtie2.log\n\nFor paired-end data you can do:\n\nbowtie2 -p 2 -q \\\n    -x GCF_000385215 \\\n    -1 sample1_R1.fastq.gz \\\n    -2 sample1_R2.fastq.gz \\\n    -S sample1_mapped.sam 2&gt; bowtie2.log\n\nBasic options, for a fool list, go here:\n\n-p: number of processors/cores\n-q: reads are in FASTQ format\n-x: /path/to/genome_indices_directory\n-U: /path/to/FASTQ_file\n-S: /path/to/output/SAM_file",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "Bowtie2"
    ]
  },
  {
    "objectID": "source/core_tools/cog.html#cog-database",
    "href": "source/core_tools/cog.html#cog-database",
    "title": "Bioinformatics guidance page",
    "section": "COG database",
    "text": "COG database\n\nIntroduction\nThe Clusters of Orthologous Genes (COG) database provides a comprehensive functional annotation of widespread bacterial and archaeal genes by clustering their protein products by sequence similarity reflecting their common evolutionary origin (Tatusov, Koonin, and Lipman 1997; Galperin et al. 2024) . The current versions was generated based on genomes from 2103 bacteria and 193 archaea, in most cases, with a single representative genome per genus.\nFor more information, please also have a look at the NCBI COG website.\n\n\nInstallation\nAvailable on crunchomics: Yes,\n\nA COG HMM database is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share you can send an email with your Uva netID to Nina Dombrowski.\nHMMER is installed on Crunchomics by default\n\nThe COG database can be found here:\n\n/zfs/omics/projects/bioinformatics/databases/cog/2024_release\nThe original data can be found here.\nIf you want to generate your own HMM database, instructions can be found at /zfs/omics/projects/bioinformatics/databases/cog/scripts/generate_COG_hmms.md\n\n\n\nExample usage\nThe COG database can be used against a query of proteins using HMMER. For example, if you have protein-coding genes of a genome of interest (GCF_003697165_2.faa) you could do the following:\n\n# Define location of COG HMM and mapping file \ncog_mapping=\"/zfs/omics/projects/bioinformatics/databases/cog/2024_release/cog-24.def.tab\" \ncog_hmmdb=\"/zfs/omics/projects/bioinformatics/databases/cog/2024_release/hmm/NCBI_COGs_Nov2024.hmm\"\n\n# Generate output folders\nmkdir -p results/cog/\n\n# Run hmmsearch against all COGs \nhmmsearch \\\n    --tblout results/cog/sequence_results.txt \\\n    --domtblout results/cog/domain_results.txt \\\n    --notextw --cpu 20 \\\n    $cog_hmmdb \\\n    data/GCF_003697165_2.faa\n\nThe resulting table of per-sequence hits (tblout) and table of per-domain hits (domtblout) contain several hits per query with no e-value or bit-score cutoff. You can further parse the output using your favorite coding language. A brief bash example, that makes also use of a COG to description mapping file is outlined below:\n\n# Format the full table and only select hits above a certain e-value\nsed 's/ \\+ /\\t/g' results/cog/sequence_results.txt | \\\n    sed '/^#/d'| sed 's/ /\\t/g'| \\\n    awk -F'\\t' -v OFS='\\t' '{print $1, $3, $6, $5}' | \\\n    awk -F'\\t' -v OFS='\\t' '($4 + 0) &lt;= 1E-3'  &gt; results/cog/sequence_results_red_e_cutoff.txt\n\n# Get best hit/protein based on bit score, and e-value\nsort -t$'\\t' -k3,3gr -k4,4g  results/cog/sequence_results_red_e_cutoff.txt | \\\n    sort -t$'\\t' --stable -u -k1,1  | \\\n    sort -t$'\\t' -k3,3gr -k4,4g &gt;  results/cog/temp1\n\n# Merge with COG mapping file \nLC_ALL=C join -a1 -1 2 -2 1 -e'-' -t $'\\t' -o1.1,0,2.3,2.2,2.5,1.4,1.3 &lt;(LC_ALL=C sort -k2 results/cog/temp1) &lt;(LC_ALL=C sort -k1 $cog_mapping) | LC_ALL=C  sort &gt; results/cog/temp2\n\n# Add a header\necho -e \"accession\\tCOG\\tCOG_Description\\tCOG_PathwayID\\tCOG_Pathway\\tCOG_evalue\\tCOG_bitscore\" | \\\n    cat - results/cog/temp2 &gt; results/cog/NCBI_COGs2024.tsv\n\n# Cleanup\nrm results/cog/temp*",
    "crumbs": [
      "Bioinformatic databases",
      "COG database"
    ]
  },
  {
    "objectID": "source/core_tools/featurecounts.html#featurecounts",
    "href": "source/core_tools/featurecounts.html#featurecounts",
    "title": "Bioinformatics guidance page",
    "section": "FeatureCounts",
    "text": "FeatureCounts\n\nIntroduction\nFeatureCounts is part of the Subread software package, a tool kit for processing next-gen sequencing data (Liao, Smyth, and Shi 2013). It includes Subread aligner, Subjunc exon-exon junction detector and the featureCounts read summarization program.\nFeatureCounts is a program that counts how many reads map to features, such as genes, exon, promoter and genomic bins. Therefore, it is useful to use after you, for example, aligned sequences (from a genome, metagenome, transcriptome) to reference sequences and want to generate a count table.\nA detailed documentation can be downloaded from here.\n\n\nInstallation\nInstalled on crunchomics: No\nIf you want to install it yourself, you can run:\n\nmamba create -n subread_2.0.6\nmamba install -n subread_2.0.6 -c bioconda subread=2.0.6\nmamba activate subread_2.0.6\n\n\n\nUsage\nFeatureCounts takes as input a annotation file in gtf or gff format and a sorted bam file.\nIt outputs a text file with the counts for each feature (in our example CDS) per sample. Notice, how you can use a wildcard to generate a counts table for multiple bam files at the same time.\n\nfeatureCounts -T 5 -t CDS -g gene_id -M \\\n    -a data/genome/genomic.gtf \\\n    -o  results/featurecounts/ncbi_gtf/counts.txt \\\n    results/bowtie/*_mapped_sorted.bam\n\nUseful options:\n\n-a  Name of an annotation file. GTF/GFF format by default. See -F option for more format information. Inbuilt annotations (SAF format) is available in ‘annotation’ directory of the package. Gzipped file is also accepted.\n-o  Name of output file including read counts. A separate file including summary statistics of counting results is also included in the output (‘.summary’). Both files are in tab delimited format.\n-t  Specify feature type(s) in a GTF annotation. If multiple types are provided, they should be separated by ‘,’ with no space in between. ‘exon’ by default. Rows in the annotation with a matched feature will be extracted and used for read mapping.\n-g  Specify attribute type in GTF annotation. ‘gene_id’ by default. Meta-features used for read counting will be extracted from annotation using the provided value.\n-M Multi-mapping reads will also be counted. For a multi- mapping read, all its reported alignments will be counted. The ‘NH’ tag in BAM/SAM input is used to detect multi-mapping reads.\n-L Count long reads such as Nanopore and PacBio reads. Long read counting can only run in one thread and only reads (not read-pairs) can be counted. There is no limitation on the number of ‘M’ operations allowed in a CIGAR string in long read counting.\n--maxMOp  Maximum number of ‘M’ operations allowed in a CIGAR string. 10 by default. Both ‘X’ and ‘=’ are treated as ‘M’ and adjacent ‘M’ operations are merged in the CIGAR string.\n-p If specified, libraries are assumed to contain paired-end reads. For any library that contains paired-end reads, the ‘countReadPairs’ parameter controls if read pairs or reads should be counted.\n-s  Perform strand-specific read counting. A single integer value (applied to all input files) or a string of comma- separated values (applied to each corresponding input file) should be provided. Possible values include: 0 (unstranded), 1 (stranded) and 2 (reversely stranded). Default value is 0 (ie. unstranded read counting carried out for all input files).",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "FeatureCounts"
    ]
  },
  {
    "objectID": "source/core_tools/kegg_db.html#kofam-database",
    "href": "source/core_tools/kegg_db.html#kofam-database",
    "title": "Bioinformatics guidance page",
    "section": "KOfam database",
    "text": "KOfam database\n\nIntroduction\nKyoto Encyclopedia of Genes and Genomes (KEGG) is a widely used reference knowledge base, which helps investigate genomic functions by linking genes to biological knowledge such as metabolic pathways and molecular networks (Aramaki et al. 2020). In KEGG, the KEGG Orthology (KO) database—a manually curated large collection of protein families (i.e. KO families)—serves as a baseline reference to link genes with other KEGG resources such as metabolic maps through K number identifiers.\nKOfam is a profile hidden Markov model (HMM) database of KEGG Orthology (KO). Profiles are built from sequences in KO database using CD-HIT, MAFFT and HMMER. Each profile has its own HMMER score threshold, with which the KO is assigned to a sequence.\nPersonal note: The HMMER score threshold is a good starting point to confidently assign functions, however, for novel organisms that are underrepresented in the KEGG database, it might be useful to explore hits with lower scores, for example when working with DPANN or CPR.\n\n\nInstallation\nAvailable on crunchomics: Yes,\n\nThe KEGG database is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\n\nThe database can be found here:\n\n/zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04.\n\nIf you want to download the database yourself, you can do:\n\ncd database_folder\n\nwget https://www.genome.jp/ftp/db/kofam/profiles.tar.gz\nwget https://www.genome.jp/ftp/db/kofam/ko_list.gz\n\ntar -xzvf profiles.tar.gz\n\n#create combined db: Pressed and indexed 26206 HMMs\ncat profiles/*hmm &gt; KO_db.hmm \nhmmpress KO_db.hmm \n\n#prepare mapping file\ngzip -d ko_list.gz\nsed -i 's/ /_/g' ko_list\n\n#cleanup \nrm -r profiles\nrm -r profiles.tar.gz\n\nThis directory contains the following files.\n\nKO_db.hmm: a combined profile HMMs of all KO groups.\nko_list: Tab separated file containing the following information:\n\nknum … K number\nthreshold … score threshold\nscore_type … score type used for the KO (full or domain)\nprofile_type … Poorly aligned sequences are removed (trim) or not (all) when building the profile\nF-measure … F-measure when calculating threshold\nnseq … number of sequences\nnseq_used … number of sequences used to build the profile\nalen … alignment length\nmlen … length of consensus positions\neff_nseq … effective number of sequences\nre/pos … relative entropy per position\ndefinition … KO definition\n\n\n\n\nKO mapping files\nNext, to the ko_list we provide the following mapping files:\n/zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04/pathway_to_kegg.tsv:\ncontains a mapping for each KO to a pathway and looks like this:\n\n\n\n\n\n\n\n\n\npathway_desc\npathway_map\nKO_id\nKO_desc\n\n\n\n\nMineral absorption\nmap04978\nK00510\nHMOX1; heme oxygenase 1 [EC:1.14.14.18]\n\n\nMineral absorption\nmap04978\nK00522\nFTH1; ferritin heavy chain [EC:1.16.3.1]\n\n\nMineral absorption\nmap04978\nK01539\nATP1A; sodium/potassium-transporting ATPase subunit alpha [EC:7.2.2.13]\n\n\nMineral absorption\nmap04978\nK05849\nSLC8A, NCX; solute carrier family 8 (sodium/calcium exchanger)\n\n\nMineral absorption\nmap04978\nK05850\nATP2B; P-type Ca2+ transporter type 2B [EC:7.2.2.10]\n\n\nMineral absorption\nmap04978\nK07213\nATOX1, ATX1, copZ, golB; copper chaperone\n\n\nMineral absorption\nmap04978\nK08370\nCYBRD1, Dcytb; plasma membrane ascorbate-dependent reductase [EC:7.2.1.3]\n\n\nMineral absorption\nmap04978\nK14738\nSTEAP2; metalloreductase STEAP2 [EC:1.16.1.-]\n\n\nMineral absorption\nmap04978\nK14739\nMT1_2; metallothionein 1/2\n\n\nMineral absorption\nmap04978\nK17686\ncopA, ctpA, ATP7; P-type Cu+ transporter [EC:7.2.2.8]\n\n\nMineral absorption\nmap04978\nK21398\nSLC11A2, DMT1, NRAMP2; natural resistance-associated macrophage protein 2\n\n\nMineral absorption\nmap04978\nK21418\nHMOX2; heme oxygenase 2 [EC:1.14.14.18]\n\n\nFatty acid biosynthesis\nmap00061\nK00059\nfabG, OAR1; 3-oxoacyl-[acyl-carrier protein] reductase [EC:1.1.1.100]\n\n\nFatty acid biosynthesis\nmap00061\nK00208\nfabI; enoyl-[acyl-carrier protein] reductase I [EC:1.3.1.9 1.3.1.10]\n\n\nFatty acid biosynthesis\nmap00061\nK00645\nfabD, MCAT, MCT1; [acyl-carrier-protein] S-malonyltransferase [EC:2.3.1.39]\n\n\nFatty acid biosynthesis\nmap00061\nK00647\nfabB; 3-oxoacyl-[acyl-carrier-protein] synthase I [EC:2.3.1.41]\n\n\nFatty acid biosynthesis\nmap00061\nK00648\nfabH; 3-oxoacyl-[acyl-carrier-protein] synthase III [EC:2.3.1.180]\n\n\nFatty acid biosynthesis\nmap00061\nK00665\nFASN; fatty acid synthase, animal type [EC:2.3.1.85]\n\n\nFatty acid biosynthesis\nmap00061\nK00667\nFAS2; fatty acid synthase subunit alpha, fungi type [EC:2.3.1.86]\n\n\nFatty acid biosynthesis\nmap00061\nK00668\nFAS1; fatty acid synthase subunit beta, fungi type [EC:2.3.1.86]\n\n\nFatty acid biosynthesis\nmap00061\nK01071\nMCH; medium-chain acyl-[acyl-carrier-protein] hydrolase [EC:3.1.2.21]\n\n\n\n…\n/zfs/omics/projects/bioinformatics/databases/kegg/release_2024_26-04/modules_to_kegg.tsv\ncontains all genes that are part of a KEGG module in the order they occur within the module:\n\n\n\n\n\n\n\n\n\n\nModule\nmodule description\nKO id\nOrder\nKO desc\n\n\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK00844\n1\nHK; hexokinase [EC:2.7.1.1]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK12407\n1\nGCK; glucokinase [EC:2.7.1.2]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK00845\n1\nglk; glucokinase [EC:2.7.1.2]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK25026\n1\nglk; glucokinase [EC:2.7.1.2]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK00886\n1\nppgK; polyphosphate glucokinase [EC:2.7.1.63]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK08074\n1\nADPGK; ADP-dependent glucokinase [EC:2.7.1.147]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK00918\n1\npfkC; ADP-dependent phosphofructokinase/glucokinase [EC:2.7.1.146 2.7.1.147]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK01810\n2\nGPI, pgi; glucose-6-phosphate isomerase [EC:5.3.1.9]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK06859\n2\npgi1; glucose-6-phosphate isomerase, archaeal [EC:5.3.1.9]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK13810\n2\ntal-pgi; transaldolase / glucose-6-phosphate isomerase [EC:2.2.1.2 5.3.1.9]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK15916\n2\npgi-pmi; glucose/mannose-6-phosphate isomerase [EC:5.3.1.9 5.3.1.8]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK00850\n3\npfkA, PFK; 6-phosphofructokinase 1 [EC:2.7.1.11]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK16370\n3\npfkB; 6-phosphofructokinase 2 [EC:2.7.1.11]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK21071\n3\npfk, pfp; ATP-dependent phosphofructokinase / diphosphate-dependent phosphofructokinase [EC:2.7.1.11 2.7.1.90]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK00918\n3\npfkC; ADP-dependent phosphofructokinase/glucokinase [EC:2.7.1.146 2.7.1.147]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK01623\n4\nALDO; fructose-bisphosphate aldolase, class I [EC:4.1.2.13]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK01624\n4\nFBA, fbaA; fructose-bisphosphate aldolase, class II [EC:4.1.2.13]\n\n\nM00001\nGlycolysis (Embden-Meyerhof pathway), glucose =&gt; pyruvate\nK11645\n4\nfbaB; fructose-bisphosphate aldolase, class I [EC:4.1.2.13]",
    "crumbs": [
      "Bioinformatic databases",
      "KOfam database"
    ]
  },
  {
    "objectID": "source/core_tools/ncbi_nr.html#ncbi-nr",
    "href": "source/core_tools/ncbi_nr.html#ncbi-nr",
    "title": "Bioinformatics guidance page",
    "section": "NCBI nr",
    "text": "NCBI nr\n\nIntroduction\nNCBI has several databases that can be used for BLAST searches. One of those is the the non-redundant (nr) protein database. Non-redundant means that identical sequences are represented by a single entry in the database. In the case of protein sequences, sometimes hundreds of sequences may be collapsed into a single entry.\nThe default protein database nr contains nearly all protein sequences available at NCBI.\n\nno patent sequences\nno wgs metagenomes and no transcriptome shotgun assembly proteins\nincludes proteins from outside protein-only sources that are also available as separate databases.\n\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nThe NCBI nr database was downloaded on 30th of August 2024 and converted to a diamond database which can be found on the bioinformatics share.\nThe database can be found here: /zfs/omics/projects/bioinformatics/databases/ncbi_nr/diamond/\nTaxonomy files that link the NCBI taxonomy ID to a taxonomy string can be found at /zfs/omics/projects/bioinformatics/databases/ncbi_tax\n\nIf you want to generate the database yourself you can do the following:\n\nGet the NCBI nr database\nComments:\n\nPlease note, the database is quite large (~450 Gb as of Oktober 2024)\nThe tool update_blastdb.pl is part of NCBI’s BLAST® Command Line Applications (Wang et al. 2003). Installation instructions can be found here. Additionally, BLAST as well as diamond can also be installed via mamba\n\n\n# Download the nr database\nupdate_blastdb.pl --decompress nr\n\n# Download taxonomy information\nwget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz.md5\nwget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz\nwget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip\nunzip taxdmp.zip\n\n# Prepare NR for diamond\ndiamond prepdb --db nr\n\n# Extract protein sequences (required to convert BLAST to diamond format)\n# While you can use a BLAST database directly with diamond it is so far not possible to extract the taxonomy information\n# If you require taxonomy information, convert to dmd format as follows\nblastdbcmd -entry 'all' -db nr &gt;nr.faa\n\n# Generate diamond database (the output generated here is called nr.dmnd)\ndiamond makedb --in nr.faa --db nr \\\n    --taxonmap prot.accession2taxid.gz \\\n    --taxonnodes nodes.dmp \\\n    --taxonnames names.dmp\n\n\n\nParse the taxonomy files\nThe code below outlines how to generate a two-column text file that lists the NCBI taxonomy ID with a taxonomy string. If you use the ncbitax2lin tool, please refer to this github page in your methods.\n\n# Install ncbitax2lin\nmamba create -n ncbitax2lin_2.3.2 -c bioconda ncbitax2lin==2.3.2\n\n# Run ncbitax2lin on the files in the taxdmp folder we unzipped earlier\nconda activate ncbitax2lin_2.3.2\n\nncbitax2lin --nodes-file nodes.dmp --names-file names.dmp --output ncbi_lineages_31102024.csv.gz \n\nconda deactivate\n\n# Parse the results\ngzip -d ncbi_lineages_31102024.csv.gz\n\n# Replace space with underscore and only print tax level until species\nsed 's/ /_/g' ncbi_lineages_31102024.csv | awk -F',' -v OFS=\"\\t\" '{print $1,$2,\n$3,$4,$5,$6,$7,$8}' &gt; temp1\n\n# Add in “none” whenever a tax level is emtpy\nawk 'BEGIN { FS = OFS = \"\\t\" } { for(i=1; i&lt;=NF; i++) if($i ~ /^ *$/) $i=\"none\"}; 1' temp1 &gt; temp2\n\n# Merge columns 2-8\nawk ' BEGIN { FS = OFS = \"\\t\" } {print $1,$2\";\"$3\";\"$4\";\"$5\";\"$6\";\"$7\";\"$8}' temp2 | sed '1d' &gt; ncbi_tax_31102024.tsv\n\n# Cleanup \ngzip ncbi_lineages_31102024.csv\nrm temp*\n\n\n\n\nUsage\nThe code below gives an example for using the NCBI-nr database with diamond (Buchfink, Reuter, and Drost 2021):\n\n# Run diamond search\ndiamond blastp -q proteins.faa \\\n    --more-sensitive --evalue 1e-3 --threads 20 --include-lineage --max-target-seqs 50 \\\n    --db /zfs/omics/projects/bioinformatics/databases/ncbi_nr/diamond/nr \\\n    --outfmt 6 qseqid qtitle qlen sseqid salltitles slen qstart qend sstart send evalue bitscore length pident staxids sphylums \\\n    --out results.txt\n\nFor a full documentation and list of all available options, please go here.\nIf you want to do some parsing and, for example, find the single-best hit per protein and integrate the taxonomy string, you could do the following:\n\n# Select columns of interest in diamond output file\nawk -F'\\t' -v OFS=\"\\t\" '{ print $1, $5, $11, $12, $14, $15, $16 }'  results.txt | sed 's/ /_/g' &gt; temp1\n\n# Get single best hit based on bit score, and then e-value\nsort -t$'\\t' -k4,4gr -k3,3g temp1 | sort -t$'\\t' --stable -u -k1,1  | sort -t$'\\t' -k4,4gr -k3,3g &gt;  temp2\n\n# Add an '-' into empty columns or columns without tax assignment\nawk -F\"\\t\" '{for(i=1;i&lt;=NF;i++) {if($i ~ /^[[:blank:]]*$/) $i=\"_\"; else gsub(/[[:blank:]]/,\"_\",$i); if($i==\"N/A\") $i=\"-\"}}1' OFS=\"\\t\" temp2 &gt; temp3\n\n# In column 2 remove everything after &lt; (otherwise the name can get too long)\nawk -F'\\t' -v OFS='\\t' '{split($2,a,\"&lt;\"); print $1, a[1], $3, $4, $5, $6, $7}' temp3 &gt; temp4\n\n# Merge with taxon names\nLC_ALL=C join -a1 -1 6 -2 1 -e'-' -t $'\\t'  -o1.1,1.2,1.3,1.4,1.5,1.6,1.7,2.2  &lt;(LC_ALL=C sort -k6  temp4) &lt;(LC_ALL=C sort -k1 /zfs/omics/projects/bioinformatics/databases/ncbi_tax/ncbi_tax_31102024.tsv) | LC_ALL=C  sort &gt; temp5\n\n# Add in header\necho -e \"accession\\tTopHit\\te_value\\tbitscore\\tperc_id\\ttax_id\\tphylum\\tncbi_tax\" | cat - temp5 &gt; results_parsed.txt\n\n# Cleanup \nrm temp*",
    "crumbs": [
      "Bioinformatic databases",
      "NCBI nr"
    ]
  },
  {
    "objectID": "source/core_tools/pgap_db.html#pgap",
    "href": "source/core_tools/pgap_db.html#pgap",
    "title": "Bioinformatics guidance page",
    "section": "PGAP",
    "text": "PGAP\n\nIntroduction\nThe PGAP database is a database originally based on the TIGRFAMs database (Li et al. 2021).\nThe original TIGRFAMs database was a research project of The Institute for Genomic Research (TIGR) and its successor, the J. Craig Venter Institute (JCVI) (Haft, Selengut, and White 2003). TIGRFAMs is a collection of manually curated protein families focusing primarily on prokaryotic sequences. It consists of hidden Markov models (HMMs), multiple sequence alignments, Gene Ontology (GO) terminology, Enzyme Commission (EC) numbers, gene symbols, protein family names, descriptive text, cross-references to related models in TIGRFAMs and other databases, and pointers to literature.\nThe TIGRFAMs database was transferred in April 2018 to the National Center for Biotechnology Information (NCBI), which now holds the creative commons license to this data and is responsible for maintaining and distributing this intellectual property. The database is used in NCBI’s Prokaryotic Genome Annotation Pipeline for GenBank and RefSeq sequence annotation, and curators continue to revise existing models.\nNotice: Release 15.0 (January 2013) was the last full release of TIGRFAMs from JCVI and newer versions are maintained by NCBI. The up-to-date versions of all TIGRFAM models are available for download by FTP as a component of the current release of PGAP HMMs. They are recognizable by an accession number beginning with “TIGR”, or by the source designation “JCVI”\n\n\nInstallation\nAvailable on crunchomics: Yes,\n\nPGAP is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\n\nThe PGAP database can be found here:\n\n/zfs/omics/projects/bioinformatics/databases/pgap/release_15.0.\n\nIf you want to download the database yourself, you can do:\n\nwget https://ftp.ncbi.nlm.nih.gov/hmm/current/hmm_PGAP.tsv\nwget https://ftp.ncbi.nlm.nih.gov/hmm/current/hmm_PGAP.LIB\n\n#cleanup mapping file \nsed -i \"s/ /_/g\" hmm_PGAP.tsv",
    "crumbs": [
      "Bioinformatic databases",
      "PGAP"
    ]
  },
  {
    "objectID": "source/core_tools/rsem.html#rsem",
    "href": "source/core_tools/rsem.html#rsem",
    "title": "Bioinformatics guidance page",
    "section": "RSEM",
    "text": "RSEM\n\nIntroduction\nRSEM is a software package for estimating gene and isoform expression levels from RNA-Seq data (Li and Dewey 2011). The RSEM package supports threads for parallel computation of the EM algorithm, single-end and paired-end read data, quality scores, variable-length reads and RSPD estimation. In addition, it provides posterior mean and 95% credibility interval estimates for expression levels. For visualization, it can generate BAM and Wiggle files in both transcript-coordinate and genomic-coordinate. Genomic-coordinate files can be visualized by both UCSC Genome browser and Broad Institute’s Integrative Genomics Viewer (IGV). Transcript-coordinate files can be visualized by IGV. RSEM also has its own scripts to generate transcript read depth plots in pdf format.\nFor a full list of options, we recommend that the users visits the documentation and the tutorial.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nRSEM v1.3.3 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nIndividual RSEM modules can be found in this folder /zfs/omics/projects/bioinformatics/software/RSEM_1.3.3/bin/\n\nIf you want to install it yourself, you can run:\n\ncd &lt;path_to_software_folder&gt;\ngit clone https://github.com/deweylab/RSEM.git\n\nmv RSEM/ RSEM_1.3.3\ncd RSEM_1.3.3\nmake\nmake install prefix=/&lt;path_to_software_folder&gt;/RSEM_1.3.3\n\n\n\nUsage\nThere are many different options how to use RSEM as indicted in the figure below:\n\n\n\n\n\nFor a full list of options, we recommend that the users visits the documentation and the tutorial.\nBelow, you find a very brief example on how to use RSEM to estimate gene and isoform expression with BAM mapping files that were generated with STAR using --quantMode TranscriptomeSAM. Notice, you don’t need to run the mapping with an aligner separately, as RSEM supports read mapping with Bowtie2, STAR and HISAT2.\n\n#prepare folders\nmkdir -p data/genome_files/rsem_ref\nmkdir -p results/quantification/rsem\n\n#prepare the reference files\n#for this step we need the fasta and gtf files from a reference assembly (the same assembly that was also used when running STAR)\n/zfs/omics/projects/bioinformatics/software/RSEM_1.3.3/bin/rsem-prepare-reference \\\n    --gtf data/genome_files/assembly.gtf \\\n    data/genome_files/assembly.fasta \\\n    data/genome_files/rsem_ref\n\n#caculate the expression \n#for this we need the bam files generated using STAR\n/zfs/omics/projects/bioinformatics/software/RSEM_1.3.3/bin/rsem-calculate-expression \\\n    --bam --no-bam-output -p 12 --paired-end --forward-prob 0.5 \\\n    results/star_output/sample1_Aligned.toTranscriptome.out.bam \\\n    data/genome_files/rsem_ref \\\n    results/quantification/rsem/sample1 &gt;& results/quantification/rsem/sample1.log\n\n#if you ran star and rsem on more than one sample via a for-loop, then you can use the code below to\n#combine rsem results for multiple samples \npython3 /zfs/omics/projects/bioinformatics/scripts/combine_rsem.py \\\n    -i 06_mapping/quantification/rsem \\\n    -o 06_mapping/quantification/rsem/rsem_genes.tsv \\\n    -t genes\n\npython3 /zfs/omics/projects/bioinformatics/scripts/combine_rsem.py \\\n    -i 06_mapping/quantification/rsem \\\n    -o 06_mapping/quantification/rsem/rsem_transcripts.tsv \\\n    -t isoforms\n\nOptions used in the example (please read the manual for a full set of options!):\n\n--paired-end is applicable to paired stranded RNA-seq data\n--forward-prob NUM. Here:\n\n0 is for a strand-specific protocol where all (upstream) read are derived from the reverse strand\n1 is for a strand-specific protocol where all (upstream) reads are derived from the forward strand\n0.5 is for a non-strand specific protocol\n\n\nA note on combining results from different samples\nWe provide a small python script o combine the rsem outputs from multiple files. This script can be found on Crunchomics but you are free to combine the files yourself in your favorite computational language. The script combine_rsem.py requires that the gene and isoform files are located in a specific folder. Namely, each sample should be have a folder that has the same name as the sample. In the example above, the script expects a folder called sample1 in the target folder specified in the script, i.e. results/quantification/rsem/. In the sample1 folder, the script will look for two files, rsem.genes.results and rsem.isoforms.results.\nIf you want to use DESeq2, then you can import the individual tables into R with tximport.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "RSEM"
    ]
  },
  {
    "objectID": "source/core_tools/seqkit.html#seqkit",
    "href": "source/core_tools/seqkit.html#seqkit",
    "title": "Bioinformatics guidance page",
    "section": "Seqkit",
    "text": "Seqkit\n\nIntroduction\nSeqKit is a tool for FASTA/Q File Manipulation (Shen et al. 2016). As such it comes with a range of abilities and, among others, can:\n\nTransform sequences\nGenerate statistics\nSub-select sequences\nConvert Fasta/Fastq files\nRemove duplicates\nSplit sequences into multiple files\nEdit the content of fasta files\n…\n\nFor a full range of what Seqkit can do and some examples, please visit the manual. Below, you only find example for some, but not all, usages for Seqkit.\n\n\nInstallation\nInstalled on crunchomics: Yes, seqkit v2.7.0 is installed.\nIf desired, you can install seqkit yourself with:\n\nmamba create -n seqkit\n\nmamba install -n seqkit -c bioconda seqkit\n\nmamba activate seqkit",
    "crumbs": [
      "Sequence data analyses",
      "Core tools",
      "Seqkit"
    ]
  },
  {
    "objectID": "source/core_tools/seqkit.html#usage",
    "href": "source/core_tools/seqkit.html#usage",
    "title": "Bioinformatics guidance page",
    "section": "Usage",
    "text": "Usage\nSeqKit comes with a range of options and it is outside of the scope of this page to go into all aspects. Below you will just find a few examples on how to use the tool.\nRequired inputs: Fasta or Fastq (compressed and uncompressed)\nTo run some example, we downloaded a genome from NCBI for testing first.\n\nmkdir data \nmkdir seqkit\n\n#download a genome for testing\nwget -O - https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/003/662/865/GCA_003662865.1_ASM366286v1/GCA_003662865.1_ASM366286v1_genomic.fna.gz | gzip -d &gt; data/GCA_003662865.1_ASM366286v1_genomic.fna\n\n\nGet statistics\nYou can get summary statistics for a single file a as follows:\n\n#get summary statistics (total length, total read number, average phred score, ...)\nseqkit stats -a -To seqkit/seqkit_stats.tsv data/GCA_003662865.1_ASM366286v1_genomic.fna \n\nYou can easily get statistics for several files at once, for example, if you have multiple fastq.gz files in a common folder you can do:\n\nseqkit stats -a -To seqkit/seqkit_stats.tsv data/*fastq.gz --threads 10\n\n\n\nGrepping sequences by name\nWe can use seqkit to extract sequences based on a list with names to extract.\n\n#make a list with potential sequences to extract\necho -e \"QMYW01000303.1\\nQMYW01000213.1\" &gt; contigs_to_screen\n\nseqkit grep -f contigs_to_screen \\\n    data/GCA_003662865.1_ASM366286v1_genomic.fna \\\n    -o data/extracted_clean.fna\n\n#sanity check: \n#we went from 309 to 2 sequences \ngrep -c \"&gt;\" data/GCA_003662865.1_ASM366286v1_genomic.fna\ngrep -c \"&gt;\" data/extracted_clean.fna\n\nWe can also do the reverse: Remove the two sequences from the genome file using -v, something you would for example do when removing contaminants:\n\nseqkit grep -f contigs_to_screen -v \\\n    data/GCA_003662865.1_ASM366286v1_genomic.fna \\\n    -o data/extracted_clean.fna\n\n#sanity check: \n#we went from 309 to 307 sequences \ngrep -c \"&gt;\" data/GCA_003662865.1_ASM366286v1_genomic.fna\ngrep -c \"&gt;\" data/extracted_clean.fna\n\n\n\nIdentify duplicated sequences\n\nseqkit rmdup --by-seq --ignore-case  data/GCA_003662865.1_ASM366286v1_genomic.fna \\\n     -o data/GCA_003662865_uniq.fasta \\\n     --dup-seqs-file data/GCA_003662865_dup.fasta --dup-num-file data/GCA_003662865_dup.text\n\n\n\nSplit sequences\nSplitting a sequence is useful if you perform large database searches. I.e. you have 1 million proteins that you want to compare against the Uniprot database? You can parallelize this by splitting the proteins first into several files and comparing them in parallel against a database.\n\n#split file into parts with at most 100 sequences \nseqkit split2 data/GCA_003662865.1_ASM366286v1_genomic.fna \\\n    -s 100 \\\n    -O data/split",
    "crumbs": [
      "Sequence data analyses",
      "Core tools",
      "Seqkit"
    ]
  },
  {
    "objectID": "source/core_tools/toolbox-readme.html#bioinformatic-toolbox",
    "href": "source/core_tools/toolbox-readme.html#bioinformatic-toolbox",
    "title": "Bioinformatics guidance page",
    "section": "Bioinformatic toolbox",
    "text": "Bioinformatic toolbox\nIn this section, you will find a collection of tutorials and resources designed to equip you with essential tools and techniques for bioinformatics analysis. The aim of these tutorials is offer step-by-step guidance and practical insights to enhance your proficiency in bioinformatics.\nPlease note that this is a growing resources and will be extended over time.",
    "crumbs": [
      "Bioinformatics toolbox"
    ]
  },
  {
    "objectID": "source/metagenomics/atlas.html#atlas",
    "href": "source/metagenomics/atlas.html#atlas",
    "title": "Bioinformatics guidance page",
    "section": "Atlas",
    "text": "Atlas\n\nIntroduction\nMetagenome-atlas is a easy-to-use metagenomic pipeline based on snakemake. It handles all steps from QC, Assembly, Binning, to Annotation (Kieser et al. 2019).\nThis workflow is best used for short-read Illumina data. Hybrid assembly of long and short reads is supported with spades and metaSpades. However metaSpades needs a paired-end short-read library.\nFor more information, visit the tools github page and manual\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nAtlas v2.18.1 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nNotice: This share only includes the atlas software but NOT the databases. Due to the size of the databases you will need to download these yourself\nAfter getting added to the bioinformatics share you can access the software by running the following command (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install Atlas yourself, you can run:\n\nmamba create --name atlas -c bioconda -c conda-forge metagenome-atlas\n\n\n\nUsage\nTo test the software, we provide some example data and a minimal example describing how to run Atlas. For more details on running Atlas with specific settings, please consult the manual.\nTo run Atlas, you will need:\n\nenough memory. It is recommended to use a minimum of ~50GB but an assembly usually requires up to 250GB of memory.\nenough space. Atlas will download some larger databases. If you don’t have the space, you can also omit some steps by editing the config.yaml file (more on that below). The databases downloaded are the following:\n\nSoftware needed by Atlas to run: 8G\nCheckM2: 3G\nGUNC database: 13G\nDRAM database: 66G\nGTDB r214 database: 85G\n\n\n\nDownload example data\nTo run Atlas, you will need a folder with your raw reads, if you don’t have any available then you can download some example data as outlined below.\n\ncd &lt;path_to_analysis_folder&gt;\n\n#download example data\nwget https://zenodo.org/records/6518160/files/0_assembly_and_reads.tar.gz\ntar -xzvf 0_assembly_and_reads.tar.gz\n\n#remove unneccessary files\nrm 0_assembly_and_reads.tar.gz\nrm 0_assembly_and_reads/1_reads/*unpaired*\nrm -r 0_assembly_and_reads/2_assembly/\n\n\n\nAtlas initialization\nThe first step when running Atlas is to initialize the environment, by telling Atlas the following:\n\nWhere we want to install the databases needed to run Atlas (here: databases). This is the directory in which all databases are installed so choose it wisely. (here: databases)\nWhere the reads are located (here: 0_assembly_and_reads/1_reads/)\n\n\nconda activate atlas_2.18.1\n\n#prepare database from the raw reads \natlas init --db-dir databases 0_assembly_and_reads/1_reads/\n\nThis command parses the folder for fastq files (extension .fastq(.gz) or .fq(.gz) , gzipped or not). fastq files can be arranged in subfolders, in which case the subfolder name will be used as a sample name. If you have paired-end reads the files are usually distinguishable by _R1/_R2 or simple _1/_2 in the file names. Atlas searches for these patterns and lists the paired-end files for each sample.\nThe command creates a samples.tsv and a config.yaml in the working directory.\nHave a look at samples.tsv and check if the samples names are inferred correctly. The sample names are used for the naming of contigs, genes, and genomes. Therefore, the sample names should consist only form digits and letters and start with a letter (Even though one - is allowed). Atlas tries to simplify the file name to obtain unique sample names, if it doesn’t succeed it simply puts S1, S2, … as sample names.\nThe BinGroup parameter is used during the genomic binning. In short: If you have between 5 and 150 samples the default (putting everything in one group) is fine. If you have less than 5 samples, put every sample in an individual BinGroup and use metabat as final binner. If you have more samples see the co-binning section for more details.\nYou should also check the config.yaml file, especially:\n\nYou may want to add ad host genomes to be removed.\nYou may want to change the resources configuration, depending on the system you run atlas on\nIn order to decrease the runtime and space requirements, you can omit the taxonomy assignment (and thus not downloading the GTDB database) or DRAM functional assignment by removing the relevant lines in the section # Annotations section.\n\nYou can run atlas init with the following options:\n\n-d, --db-dir PATH location to store databases (need ~50GB)\n-w, --working-dir PATH location to run atlas\n--assembler megahit|spades assembler [default: spades]\n--data-type metagenome|metatranscriptome: sample data type [default: metagenome]\n--interleaved-fastq : fastq files are paired-end in one files: (interleaved)\n--threads INTEGER : number of threads to use per multi-threaded job\n--skip-qc : Skip QC, if reads are already pre-processed\n-h, --help: Show this message and exit.\n\n\n\nInstalling the required software\nAtlas will first download all required software via conda. Since v2.18.1 there is a small issue with one of the software scripts. To circumvent running into an error further down the line we will first install all required software without running Atlas further and then editing the problematic script:\n\n#set conda channels to strict (recommended by snakemake)\nconda config --set channel_priority strict\n\n#setip the required conda environments\natlas run all --use-conda --conda-create-envs-only\n\n#set conda channel priority back to the default\nconda config --set channel_priority flexible\n\nNext, we need to edit one of the scripts. In the command below change databases to the location you used in the atlas init command after the --db-dir option.\nnano databases/conda_envs/*/lib/python3.11/site-packages/mag_annotator/database_processing.py\nPress Ctrl+w and search for this line of text:\nmerge_files(glob(path.join(hmm_dir, 'VOG*.hmm')), vog_hmms)\nchange this line to (don’t change the syntax while doing this, i.e. you still want to keep 4 spaces in front of merge_files ):\nmerge_files(glob(path.join(hmm_dir, 'hmm', 'VOG*.hmm')), vog_hmms)\nThen:\n\nPress Ctr+x\nType Y to save\nPress enter to save the changes without changing the file name\n\n\n\nRunning Atlas\nNext, we start the actual pipeline which will do multiple steps, including quality-control, assembly binning and annotation.\n\n#run atlas \nsrun --cpus-per-task 20 --mem=100G atlas run all -j 20 --max-mem 100\n\nconda deactivate\n\nThe output files are in more detail described here.\nYou can run atlas run all with the following options:\n\n-w, --working-dir PATH location to run atlas\n-c, --config-file PATH config-file generated with ‘atlas init’\n-j, --jobs INTEGER use at most this many jobs in parallel (see the manual for mor details). [default: 64]\n--max-mem FLOAT Specify maximum virtual memory to use by atlas.\n--profile TEXT: snakemake profile e.g. for cluster execution\n-n, --dryrun Test execution.\n-h, --help: Show this message and exit.\n\n\n\n\nCommon Issues and Solutions\n\nIssue 1: I am running out of memory/space\n\nSolution 1: Some steps are quite memory intensive or might need large (~80GB) databases. You can edit the config.yaml file for example using nano to omit these steps. To do this, go to the # Annotations section and remove the lines starting with gtdb to omit the taxonomy annotation. You can also delete the line with dram if you run out of space as this requires a larger database",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Atlas"
    ]
  },
  {
    "objectID": "source/metagenomics/bakta.html#bakta",
    "href": "source/metagenomics/bakta.html#bakta",
    "title": "Bioinformatics guidance page",
    "section": "Bakta",
    "text": "Bakta\n\nIntroduction\nBakta is a tool for the rapid & standardized annotation of bacterial genomes and plasmids from both isolates and MAGs (Schwengers et al. 2021). It provides dbxref-rich, sORF-including and taxon-independent annotations in machine-readable JSON & bioinformatics standard file formats for automated downstream analysis. The Bakta workflow looks as follows:\n\nPlease, note that Bacta was designed and developed to annotate isolated bacterial genomes. In particular, the database comprises bacterial protein coding genes, only. Also, there are currently no options/parameters to surpass archaeal taxonomic information to 3rd party tools of the workflow, e.g. tRNAScan-SE. Therefore, the identification of CDS and protein sequences can be still used but for functional annotation it is recommended to use archaeal-specific or domain-unspecific databases.\nThere also is a web-server on which you can upload your genomes or visualize bakta json files that you have already generated.\nFor more information, please visit the manual.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nBakta v1.9.4 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\n# Install bakta\nmamba create -p /zfs/omics/projects/bioinformatics/software/miniconda3/envs/bakta_1.9.4 -c conda-forge -c bioconda bakta\n\n# Setup database\nmamba activate bakta_1.9.4\n\nbakta_db download --output /zfs/omics/projects/bioinformatics/databases/bakta --type full\n\nmamba deactivate\n\n# To update an existing database you can use the following code:\n#bakta_db update --db &lt;existing-db-path&gt; [--tmp-dir &lt;tmp-directory&gt;]\n\n\n\nUsage\n\nmkdir -p results/bakta \n\nconda activate bakta_1.9.4\n\nbakta --db /zfs/omics/projects/bioinformatics/databases/bakta/db \\\n    --verbose \\\n    --output results/bakta \\\n    --prefix strainX \\\n    --locus-tag strainX \\\n    --threads 8 \\\n    --force \\\n    strainX.fasta\n\nconda deactivate\n\nFor a complete description, please visit the manual.\n\nGenerated output\nAnnotation results are provided in standard bioinformatics file formats:\n\n&lt;prefix&gt;.tsv: annotations as simple human readable TSV\n&lt;prefix&gt;.gff3: annotations & sequences in GFF3 format\n&lt;prefix&gt;.gbff: annotations & sequences in (multi) GenBank format\n&lt;prefix&gt;.embl: annotations & sequences in (multi) EMBL format\n&lt;prefix&gt;.fna: replicon/contig DNA sequences as FASTA\n&lt;prefix&gt;.ffn: feature nucleotide sequences as FASTA\n&lt;prefix&gt;.faa: CDS/sORF amino acid sequences as FASTA\n&lt;prefix&gt;.hypotheticals.tsv: further information on hypothetical protein CDS as simple human readable tab separated values\n&lt;prefix&gt;.hypotheticals.faa: hypothetical protein CDS amino acid sequences as FASTA\n&lt;prefix&gt;.json: all (internal) annotation & sequence information as JSON\n&lt;prefix&gt;.txt: summary as TXT\n&lt;prefix&gt;.png: circular genome annotation plot as PNG\n&lt;prefix&gt;.svg: circular genome annotation plot as SVG\n\nThe &lt;prefix&gt; can be set via --prefix &lt;prefix&gt;. If no prefix is set, Bakta uses the input file prefix.\n\nList of Bakta options\nusage: bakta [--db DB] [--min-contig-length MIN_CONTIG_LENGTH] [--prefix PREFIX] [--output OUTPUT]\n             [--genus GENUS] [--species SPECIES] [--strain STRAIN] [--plasmid PLASMID]\n             [--complete] [--prodigal-tf PRODIGAL_TF] [--translation-table {11,4}] [--gram {+,-,?}] [--locus LOCUS]\n             [--locus-tag LOCUS_TAG] [--keep-contig-headers] [--replicons REPLICONS] [--compliant] [--replicons REPLICONS] [--regions REGIONS] [--proteins PROTEINS] [--meta]\n             [--skip-trna] [--skip-tmrna] [--skip-rrna] [--skip-ncrna] [--skip-ncrna-region]\n             [--skip-crispr] [--skip-cds] [--skip-pseudo] [--skip-sorf] [--skip-gap] [--skip-ori] [--skip-plot]\n             [--help] [--verbose] [--debug] [--threads THREADS] [--tmp-dir TMP_DIR] [--version]\n             &lt;genome&gt;\n\nRapid & standardized annotation of bacterial genomes, MAGs & plasmids\n\npositional arguments:\n  &lt;genome&gt;              Genome sequences in (zipped) fasta format\n\nInput / Output:\n  --db DB, -d DB        Database path (default = &lt;bakta_path&gt;/db). Can also be provided as BAKTA_DB environment variable.\n  --min-contig-length MIN_CONTIG_LENGTH, -m MIN_CONTIG_LENGTH\n                        Minimum contig size (default = 1; 200 in compliant mode)\n  --prefix PREFIX, -p PREFIX\n                        Prefix for output files\n  --output OUTPUT, -o OUTPUT\n                        Output directory (default = current working directory)\n  --force, -f           Force overwriting existing output folder (except for current working directory)\n\nOrganism:\n  --genus GENUS         Genus name\n  --species SPECIES     Species name\n  --strain STRAIN       Strain name\n  --plasmid PLASMID     Plasmid name\n\nAnnotation:\n  --complete            All sequences are complete replicons (chromosome/plasmid[s])\n  --prodigal-tf PRODIGAL_TF\n                        Path to existing Prodigal training file to use for CDS prediction\n  --translation-table {11,4}\n                        Translation table: 11/4 (default = 11)\n  --gram {+,-,?}        Gram type for signal peptide predictions: +/-/? (default = ?)\n  --locus LOCUS         Locus prefix (default = 'contig')\n  --locus-tag LOCUS_TAG\n                        Locus tag prefix (default = autogenerated)\n  --keep-contig-headers\n                        Keep original contig headers\n  --compliant           Force Genbank/ENA/DDJB compliance\n  --replicons REPLICONS, -r REPLICONS\n                        Replicon information table (tsv/csv)\n  --regions REGIONS     Path to pre-annotated regions in GFF3 or Genbank format (regions only, no functional annotations).\n  --proteins PROTEINS   Fasta file of trusted protein sequences for CDS annotation\n  --meta                Run in metagenome mode. This only affects CDS prediction.\n\nWorkflow:\n  --skip-trna           Skip tRNA detection & annotation\n  --skip-tmrna          Skip tmRNA detection & annotation\n  --skip-rrna           Skip rRNA detection & annotation\n  --skip-ncrna          Skip ncRNA detection & annotation\n  --skip-ncrna-region   Skip ncRNA region detection & annotation\n  --skip-crispr         Skip CRISPR array detection & annotation\n  --skip-cds            Skip CDS detection & annotation\n  --skip-pseudo         Skip pseudogene detection & annotation\n  --skip-sorf           Skip sORF detection & annotation\n  --skip-gap            Skip gap detection & annotation\n  --skip-ori            Skip oriC/oriT detection & annotation\n  --skip-plot           Skip generation of circular genome plots\n\nGeneral:\n  --help, -h            Show this help message and exit\n  --verbose, -v         Print verbose information\n  --debug               Run Bakta in debug mode. Temp data will not be removed.\n  --threads THREADS, -t THREADS\n                        Number of threads to use (default = number of available CPUs)\n  --tmp-dir TMP_DIR     Location for temporary files (default = system dependent auto detection)\n  --version             show program's version number and exit",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Bakta"
    ]
  },
  {
    "objectID": "source/metagenomics/checkm2.html#checkm2",
    "href": "source/metagenomics/checkm2.html#checkm2",
    "title": "Bioinformatics guidance page",
    "section": "CheckM2",
    "text": "CheckM2\nCheckM2 is a software to assess the quality (completeness, contamination, coding density, etc.) of a genome assembly (Chklovski et al., n.d.). Unlike CheckM1, CheckM2 has universally trained machine learning models it applies regardless of taxonomic lineage to predict the completeness and contamination of genomic bins. This allows it to incorporate many lineages in its training set that have few - or even just one - high-quality genomic representatives, by putting it in the context of all other organisms in the training set. As a result of this machine learning framework, CheckM2 is also highly accurate on organisms with reduced genomes or unusual biology, such as the Nanoarchaeota or Patescibacteria.\nFor more information, check out the tools github page.\n\nIntroduction\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nCheckM2 v1.0.1 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski. Afterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -p /zfs/omics/projects/bioinformatics/software/miniconda3/envs/checkm2_1.0.1 -c bioconda -c conda-forge checkm2=1.0.1\n\nconda activate checkm2_1.0.1 \n\n#install right python version, otherwise you get a class error\nmamba install python=3.8 \n\n#install database \ncheckm2 database --download --path /zfs/omics/projects/bioinformatics/databases/checkm\n\n#do test run \ncheckm2 testrun\n\n\n\nUsage\n\n#run checkm2\nconda activate checkm2_1.0.1\n\ncheckm2 predict --threads 30 \\\n  --input  folder_with_genomes_to_analyse/  \\\n  -x fasta \\\n  --output-directory results/checkm2 \n\nconda deactivate\n\nAfter running this, you fill find all relevant information in the quality_report.tsv file in the output folder.\nUseful options (for a full list, use the help function):\n\n--genes : Treat input files as protein files. [Default: False]\n-x EXTENSION, --extension EXTENSION: Extension of input files. [Default: .fna]\n--tmpdir TMPDIR : specify an alternative directory for temporary files\n--force: overwrite output directory [default: not set]\n--resume: Reuse Prodigal and DIAMOND results found in output directory [default: not set]\n--threads num_threads, -t num_threads: number of CPUS to use [default: 1]\n--ttable ttable: Provide a specific prodigal translation table for bins [default: automatically determine either 11 or 4]\n\nFor more information, please visit the manual.\n\n\nCommon Issues and Solutions\n\nIssue 1: Running out of memory\n\nSolution 1: If you are running CheckM2 on a device with limited RAM, you can use the --lowmem option to reduce DIAMOND RAM use by half at the expense of longer runtime.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "CheckM2"
    ]
  },
  {
    "objectID": "source/metagenomics/deeploc.html#deeploc",
    "href": "source/metagenomics/deeploc.html#deeploc",
    "title": "Bioinformatics guidance page",
    "section": "DeepLoc",
    "text": "DeepLoc\n\nIntroduction\nDeepLoc 2.0 predicts the subcellular localization(s) of eukaryotic proteins (Thumuluri et al. 2022). DeepLoc 2.0 is a multi-label predictor, which means that is able to predict one or more localizations for any given protein. It can differentiate between 10 different localizations: Nucleus, Cytoplasm, Extracellular, Mitochondrion, Cell membrane, Endoplasmic reticulum, Chloroplast, Golgi apparatus, Lysosome/Vacuole and Peroxisome. Additionally, DeepLoc 2.0 can predict the presence of the sorting signal(s) that had an influence on the prediction of the subcellular localization(s).\nProkaryotic proteins: To predict the locations of proteins in prokaryotes, use DeepLocPro. RNA: To predict the locations of RNA, use DeepLocRNA.\nDeepLoc can be used via a webserver, or, for larger datasets, the software can be installed as well.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nDeepLoc v2.0.0 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can:\n\nFill out form here\nWait for the mail and download software\n\n\ncd /zfs/omics/projects/bioinformatics/software\n\n# Download\nwget https://services.healthtech.dtu.dk/download/b8447188-64d9-44f6-a4e3-e4bbd41e6904/deeploc-2.0.All.tar.gz\n\n# Decompress\ntar -xzvf deeploc-2.0.All.tar.gz\n\n# Setup all required dependencies via a conda environment\nmamba create -p /zfs/omics/projects/bioinformatics/software/miniconda3/envs/deeploc_2.0 python=3.6\n\nconda activate deeploc_2.0\n\ncd deeploc2_package/\npip install .\n\n# Test installation \ndeeploc2 -h\n\n\n\nUsage\nNote when using this with slurm\n\nDeeploc does not have an option to set the number of CPUs used. Instead it will use all CPUs available on the system\nTherefore, when running deeplpc with srun or sbatch, ensure that you set the number of CPUs with --cpus-per-task=n with n being the number of CPUs you desire.\n\n\nconda activate deeploc_2.0\n\ndeeploc2 \\\n  -f 03_data/annotations/Ochro1393_1_4_GeneCatalog.faa \\\n  -o 03_data/annotations/manual/deeploc \\\n  -d cpu\n\nconda deactivate\n\nDeepLoc can be run with 4 possible arguments:\n\n-f, --fasta. Input in fasta format of the proteins.\n-o, --output. Output folder name.\n-m, --model. High-quality (Accurate) model or high-throughput (Fast) model. Default: Fast.\n-p, --plot. Plot and save attention values for each individual protein.\n\nThe output is a tabular file with the following format:\n\n1st column: Protein ID.\n2nd column: Predicted localization(s).\n3rd column: Predicted sorting signal(s).\n4th-13th column: Probability for each of the individual localizations.\n\nIf –plot is defined, a plot and a text file with the sorting signal importance for each protein will be generated.",
    "crumbs": [
      "Sequence data analyses",
      "Eukaryotic genomics",
      "DeepLoc"
    ]
  },
  {
    "objectID": "source/metagenomics/fama_readme.html",
    "href": "source/metagenomics/fama_readme.html",
    "title": "FAMA",
    "section": "",
    "text": "FAMA\n\nIntroduction\nFama is a fast pipeline for functional and taxonomic analysis of shotgun metagenomic sequences.\n\n\nInstallation\nIf you want to install FAMA on your own, follow these instructions in the FAMA manual.\nAvailable on Crunchomics: Yes, via the metatools share. For access contact Anna Heintz Buschart a.u.s.heintzbuschart@uva.nl\n\n\nInstructions\n\n#activate environment\nconda activate /zfs/omics/projects/metatools/TOOLS/miniconda3/envs/FAMA\n\n#get template and add \n#(if you downloaded FAMA by yourself then the ini file can be found in the folder you downloaded)\ncp /zfs/omics/projects/metatools/DB/fama_new/project.ini.sample Annotations/Fama/my.project.config\n\n#run FAMA \npython3 /zfs/omics/projects/metatools/DB/fama_new/py/fama.py -c /zfs/omics/projects/metatools/DB/fama_new/config.ini -p my.project.config\n\nExample content of Annotations/Fama/my.project.config:\n[DEFAULT]\nproject_name = 'Test sample sulfur'\ncollection = test_sulfur_v1\nref_output_name = ref_tabular_output.txt\nbackground_output_name = bgr_tabular_output.txt\nref_hits_list_name = ref_hits.txt\nref_hits_fastq_name = ref_hits.fq\nreads_fastq_name = reads.fq\npe_reads_fastq_name = reads_pe.fq\noutput_subdir = out_sulfur\nreport_name = report.txt\nxml_name = krona.xml\nhtml_name = functional_profile.html\nreads_json_name = reads.json\nassembly_subdir = assembly\nwork_dir = /zfs/omics/projects/thiopac-mgx/nina_wdir/Annotations/Fama\n\n[test_sample]\nsample_id = TPS_fame_sulfur\nfastq_pe1 = /zfs/omics/projects/thiopac-mgx/nina_wdir/faa/All_genomes.faa\nsample_dir = /zfs/omics/projects/thiopac-mgx/nina_wdir/Annotations/Fama\nreplicate = 0\n#fastq_pe1_readcount = 30\n#fastq_pe1_basecount = 4509\n#fastq_pe2_readcount = 30\n#fastq_pe2_basecount = 4346\n#rpkg_scaling = 0.18962061540779365\n#insert_size = 233.4950097660472\n\n\nAdding more databases to FAMA\nIt is possible to add new databases to FAMA. Below is an example on how to add a custom sulfur database.\nSulfur database installation instructions:\n\nDownload and unpack the archive into a separate directory:https://iseq.lbl.gov/mydocs/fama_downloads/fama_sulfur_dataset.tar.gz\nCreate diamond databases:\n\n\ndiamond makedb --in classification_database.faa --db classification_database\ndiamond makedb --in selection_database_clustered.faa --db selection_database\n\n\nAppend your config.ini file with a new section and replace “” with real paths:\n\n\n[test_sulfur_v1]\nfunctions_file = &lt;path to db files&gt;/collection_functions.txt\ntaxonomy_file = &lt;path to db files&gt;/collection_taxonomy.txt\nreference_diamond_db = &lt;path to db files&gt;/selection_database.dmnd\nbackground_diamond_db = &lt;path to db files&gt;/classification_database.dmnd\nreference_db_size = 6990766\nbackground_db_size = 99120132",
    "crumbs": [
      "Sequence data analyses",
      "Functional annotation",
      "FAMA"
    ]
  },
  {
    "objectID": "source/metagenomics/flye.html#flye",
    "href": "source/metagenomics/flye.html#flye",
    "title": "Bioinformatics guidance page",
    "section": "Flye",
    "text": "Flye\n\nIntroduction\nFlye is a de novo assembler for single-molecule sequencing reads, such as those produced by PacBio and Oxford Nanopore Technologies (Kolmogorov et al. 2019). It is designed for a wide range of datasets, from small bacterial projects to large mammalian-scale assemblies. The package represents a complete pipeline: it takes raw PacBio / ONT reads as input and outputs polished contigs. Flye also has a special mode for metagenome assembly.\nFor more information, please visit this page.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nFlye v2.9.3 is installed as part of the bioinformatics share as part of the trycycler_0.5.5 conda environment. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -n flye_v2.9.3\n\nmamba activate flye_v2.9.3\nmamba install -c bioconda flye\nmamba deactivate\n\n\n\nUsage\n\nconda activate trycycler_0.5.5\n\n#run flye on long read nanopore data\nflye --nano-raw  my_data.fastq.gz \\\n  --iterations 2 \\\n  -o results/assembly/flye_v1 \\\n  -t 30\n\nconda deactivate\n\nNotice:\n\nThere are different options, that you can use for different long-read datasets\n\n--nano-raw is suitable for data generated with the R9 technology\n\nUse the --meta option if you work with metagenomes\n\nFor a full set of options, visit the manual and also have a look at the FAQ for commonly asked questions.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Flye"
    ]
  },
  {
    "objectID": "source/metagenomics/homopolish.html#homopolish",
    "href": "source/metagenomics/homopolish.html#homopolish",
    "title": "Bioinformatics guidance page",
    "section": "Homopolish",
    "text": "Homopolish\n\nIntroduction\nHomopolish is a genome polisher originally developed for Nanopore and subsequently extended for PacBio CLR (Huang, Liu, and Shih 2021). It generates a high-quality genome (&gt;Q50) for viruses, bacteria, and fungi. Nanopore/PacBio systematic errors can be corrected by retrieving homologs from closely-related genomes and polished by an support vector machine (SVM). When paired with Racon and Medaka, the genome quality can reach Q50-90 (&gt;99.999%) on Nanopore R9.4/10.3 flowcells (Guppy &gt;3.4). For PacBio CLR, Homopolish also improves the majority of Flye-assembled genomes to Q90\nFor more information, please visit the tools github page.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nHomopolish v0.4.1 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\n# Download git folder\ncd software_folder \ngit clone https://github.com/ythuang0522/homopolish.git\ncd homopolish\n\n# Install dependencies \nmamba env create -f environment.yml\n\n# Homopolish retrieves homologous sequences by scanning microbial genomes compressed in (Mash) sketches. \n# Three sketches, bacteria (3.3Gb) , virus (74Mb), and fungi (74Mb) can be downloaded\nwget http://bioinfo.cs.ccu.edu.tw/bioinfo/downloads/Homopolish_Sketch/bacteria.msh.gz\n#wget http://bioinfo.cs.ccu.edu.tw/bioinfo/downloads/Homopolish_Sketch/virus.msh.gz \n#wget http://bioinfo.cs.ccu.edu.tw/bioinfo/downloads/Homopolish_Sketch/fungi.msh.gz\n\n# Extract data \ngzip -d bacteria.msh.gz\n\n\n\nUsage\nHomopolish should be run with a pre-trained model (R9.4.pkl/R10.3.pkl for Nanopore and pb.pkl for PacBio CLR) and one sketch (virus, bacteria, or fungi). For Nanopore sequencing, Homopolish should be run after the Racon-Medaka pipeline as it only removes indel errors. For PacBio CLR sequencing, it can be invoked directly after Flye assembly.\n\nconda activate homopolish\n\n# Polish a genome (data sequenced with a R9.4 flowcell)\npython3 /zfs/omics/projects/bioinformatics/software/homopolish/homopolish.py polish \\\n  -a my_genome.fasta \\\n  -m R9.4.pkl \\\n  -o results \\\n  -s /zfs/omics/projects/bioinformatics/software/homopolish/data/bacteria.msh \\\n  -t 20\n\nHomopolish will automatically search the most closely related strains to your genome. For cases where this fails you can specify a genus with -g genusname_speciesname\nKeep in mind that polishing can, but does not have to, improve a genome assembly. Therefore it is advised to compare the original and polished genome to a close reference genome, or, if not available, look at assembly parameters, number of proteins or number of pseudogenes.\nUseful options:\n  -h, --help            show this help message and exit\n  -m MODEL_PATH, --model_path MODEL_PATH\n                        [REQUIRED] Path to a trained model (pkl file). Please\n                        see our github page to see options.\n  -a ASSEMBLY, --assembly ASSEMBLY\n                        [REQUIRED] Path to a assembly genome.\n  -s SKETCH_PATH, --sketch_path SKETCH_PATH\n                        Path to a mash sketch file.\n  -g GENUS, --genus GENUS\n                        Genus name\n  -l LOCAL_DB_PATH, --local_DB_path LOCAL_DB_PATH\n                        Path to your local DB (ex: cat closely-related_genomes1.fasta closely-related_genomes2.fasta&gt; DB.fasta)\n  -t THREADS, --threads THREADS\n                        Number of threads to use. [1]\n  -o OUTPUT_DIR, --output_dir OUTPUT_DIR\n                        Path to the output directory. [output]\n  --minimap_args MINIMAP_ARGS\n                        Minimap2 -x argument. [asm5]\n  --mash_threshold MASH_THRESHOLD\n                        Mash output threshold. [0.95]\n  --ani                 Ani identity [99%]\n  --download_contig_nums DOWNLOAD_CONTIG_NUMS\n                        How much contig to download from NCBI. [20]\n  -d, --debug           Keep the information of every contig after mash, such\n                        as homologous sequences and its identity infomation.\n                        [no]\n  --mash_screen         Use mash screen. [mash dist]",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Homopolish"
    ]
  },
  {
    "objectID": "source/metagenomics/medaka.html#medaka",
    "href": "source/metagenomics/medaka.html#medaka",
    "title": "Bioinformatics guidance page",
    "section": "Medaka",
    "text": "Medaka\n\nIntroduction\nMedaka is a tool to create consensus sequences and variant calls from nanopore sequencing data (Nanoporetech/Medaka 2024). This task is performed using neural networks applied a pileup of individual sequencing reads against a reference sequence, mostly commonly either a draft assembly or a database reference sequence. It provides state-of-the-art results outperforming sequence-graph based methods and signal-based methods, whilst also being faster.\nNote: Medaka has been trained to correct draft sequences output from the Flye assembler. Processing a draft sequence from alternative sources (e.g. the output of canu or wtdbg2) may lead to different results.\nFor a full set of options, please visit the tools github page.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nMedaka v2.0.1 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\n# Create empty conda environment and use PyPI/pip for installation\n# Python versions that are supported:  python 3.6-3.9 \nmamba create -p /zfs/omics/projects/bioinformatics/software/miniconda3/envs/medaka_2.0.1 -c conda-forge python=3.9\n\n# Install CPU version (use the other option, if you have a GPU)\nconda activate medaka_2.0.1\n#pip install medaka\npip install medaka-cpu --extra-index-url https://download.pytorch.org/whl/cpu\n\n# Optional: Solve an issue with a missing dependency\n# Run this if you encounter the error \"error while loading shared libraries: libcrypto.so.10\"\nmamba install -c bioconda pyabpoa\nmamba install -c conda-forge -c bioconda htslib==1.16\n\n# Optional: Update any outdated dependency\n# When running medaka for the first time, it might notify you that tools are missing or outdated, if that is the case, you can add them with\nmamba install -c bioconda samtools=1.11\n\nconda deactivate\n\n\n\nUsage\n\nconda activate medaka_2.0.1 \n\n# For best results it is important to specify the correct inference model, according to the basecaller used\n# You can list all available models \nmedaka tools list\\_models\n\n# The command medaka inference will attempt to automatically determine a correct model by inspecting its BAM input file. The helper scripts medaka_consensus and medaka_variant will make similar attempts from their FASTQ input.\n# Note: If your model is not listed then  users are encouraged to rebasecall their data with a more recent basecaller version \nmedaka tools resolve_model --auto_model consensus data/sample.fastq.gz\n\n# Often, you also can find the model in the sequence header\nzcat data/sample.fastq.gz | head -n1\n\n# Run medaka \nmedaka_consensus -i data/sample.fastq.gz \\\n    -d data/initial_assembly.fasta \\\n    -o results \\\n    -m r941_min_fast_g507 \\\n    -t 20 \n\nconda deactivate\n\nUseful options:\n\nFor native data with bacterial modifications, such as bacterial isolates, metagenomic samples, or plasmids expressed in bacteria, there is a research model that shows improved consensus accuracy. This model is compatible with several basecaller versions for the R10 chemistries. By adding the flag --bacteria the bacterial model will be selected if it is compatible with the input basecallers\nFor a full overview of all possible options, please visit the tools github page\n\nHow to assess the output\nPolishing tools can, but do not have to, improve an assembly. Therefore, it is important to evaluate the results. While, there is no perfect answer on how to do you can consider to compare the genome to a close reference (if available). Alternatively, you can look at genome statistics such as mean length of predicted proteins. Finally, you can also visualize the medaka results (original genome and calls_to_draft.bam) in genome browsers, such as IGV.\nFor more information, also have a look at this blog post from Ryan Wick.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Medaka"
    ]
  },
  {
    "objectID": "source/metagenomics/metacerberus.html#metacerberus",
    "href": "source/metagenomics/metacerberus.html#metacerberus",
    "title": "Bioinformatics guidance page",
    "section": "MetaCerberus",
    "text": "MetaCerberus\n\nIntroduction\nMetaCerberus transforms raw sequencing (i.e. genomic, transcriptomics, metagenomics, metatranscriptomic) data into knowledge (Figueroa III et al. 2024). It is a start to finish python code for versatile analysis of the Functional Ontology Assignments for Metagenomes (FOAM), KEGG, CAZy/dbCAN, VOG, pVOG, PHROG, COG, and a variety of other databases including user customized databases via Hidden Markov Models (HMM) for functional annotation for complete metabolic analysis across the tree of life (i.e., bacteria, archaea, phage, viruses, eukaryotes, and whole ecosystems). MetaCerberus also provides automatic differential statistics using DESeq2/EdgeR, pathway enrichments with GAGE, and pathway visualization with Pathview R.\nFor more information, please visit the software’s github page.\nThis software contains the following databases as of August 2024:\n\n\n\nDatabase\nLast Update\nVersion\nPublication\nMetaCerberus Update Version\n\n\n\n\nKEGG/KOfams\n2024-01-01\nJan24\nAramaki et al. 2020\nbeta\n\n\nFOAM/KOfams\n2017\n1\nPrestat et al. 2014\nbeta\n\n\nCOG\n2020\n2020\nGalperin et al. 2020\nbeta\n\n\ndbCAN/CAZy\n2023-08-02\n12\nYin et al., 2012\nbeta\n\n\nVOG\n2017-03-03\n80\nWebsite\nbeta\n\n\npVOG\n2016\n2016\nGrazziotin et al. 2017\n1.2\n\n\nPHROG\n2022-06-15\n4\nTerizan et al., 2021\n1.2\n\n\nPFAM\n2023-09-12\n36\nMistry et al. 2020\n1.3\n\n\nTIGRfams\n2018-06-19\n15\nHaft et al. 2003\n1.3\n\n\nPGAPfams\n2023-12-21\n14\nTatusova et al. 2016\n1.3\n\n\nAMRFinder-fams\n2024-02-05\n2024-02-05\nFeldgarden et al. 2021\n1.3\n\n\nNFixDB\n2024-01-22\n2\nBellanger et al. 2024\n1.3\n\n\nGVDB\n2021\n1\nAylward et al. 2021\n1.3\n\n\nPads Arsenal\n2019-09-09\n1\nZhang et al. 2020\nComing soon\n\n\nefam-XC\n2021-05-21\n1\nZayed et al. 2021\nComing soon\n\n\nNMPFams\n2021\n1\nBaltoumas et al. 2024\nComing soon\n\n\nMEROPS\n2017\n1\nRawlings et al. 2018\nComing soon\n\n\nFESNov\n2024\n1\nRodríguez del Río et al. 2024\nComing soon\n\n\n\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nMetaCerberus v1.3.2 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -n metacerberus_1.3.2 -c conda-forge -c bioconda metacerberus\n\nconda activate metacerberus_1.3.2 \n\nmetacerberus.py --setup #Setting up FragGeneScanRS\nmetacerberus.py --download # Downloading required databases\n\nconda deactivate\n\n\n\nUsage\nGeneral usage notes:\n\nMetaCerberus can use three different input files:\n\nraw read data from any sequencing platform (Illumina, PacBio, or Oxford Nanopore),\nassembled contigs, as MAGs, vMAGs, isolate genomes, or a collection of contigs,\namino acid fasta (.faa), previously called pORFs.\n\nIn QC mode, raw reads are quality controlled via FastQC prior and post trim FastQC. Raw reads are then trimmed via data type; if the data is Illumina or PacBio, fastp is called, otherwise it assumes the data is Oxford Nanopore then Porechop is utilized PoreChop.\nIn the formatting and gene prediction stage, contigs and genomes are checked for N repeats. These N repeats are removed by default.\nContigs can be converted to pORFs using Prodigal, FragGeneScanRs, and Prodigal-gv as specified by user preference.\n\nExample usage on a folder with two protein fasta files:\n\nconda activate metacerberus_1.3.2 \n\nmkdir metacerberus\n\n# Run MetaCerberus on a folder with two protein files\nmetacerberus.py --protein faa/ --hmm ALL --dir_out metacerberus --cpus 20 \n\nconda deactivate\n\nWhen running this example, the results will be stored in a metacerberus folder. Inside this folder, the final folder will contain:\n\n\n\n\n\n\n\n\nFile Extension\nDescription Summary\nMetaCerberus Update Version\n\n\n\n\n.gff\nGeneral Feature Format\n1.3\n\n\n.gbk\nGenBank Format\n1.3\n\n\n.fna\nNucleotide FASTA file of the input contig sequences.\n1.3\n\n\n.faa\nProtein FASTA file of the translated CDS/ORFs sequences.\n1.3\n\n\n.ffn\nFASTA Feature Nucleotide file, the Nucleotide sequence of translated CDS/ORFs.\n1.3\n\n\n.html\nSummary statistics and/or visualizations, in step 10 folder\n1.3\n\n\n.txt\nStatistics relating to the annotated features found.\n1.3\n\n\nlevel.tsv\nVarious levels of hierachical steps that is tab-separated file from various databases\n1.3\n\n\nrollup.tsv\nAll levels of hierachical steps that is tab-separated file from various databases\n1.3\n\n\n.tsv\nFinal Annotation summary, Tab-separated file of all features from various databases\n1.3\n\n\n\nSince final_annotation_summary.tsv only provides the best hit across all databases, we provide two small scripts that you can run if you want to generate a table that allows you to compare the annotations across all databases for each protein. You can run this as follows (only tested on protein files so far):\n\n# Combine results from individual database folders for each protein file\nfor i in metacerberus/final/Protein_*_protein; do\n    python /zfs/omics/projects/bioinformatics/scripts/merge_metacerberus_individual_dbs.py -i ${i} -o merged_annotations.tsv\ndone\n\n# Concatenate the merged results into one document\npython /zfs/omics/projects/bioinformatics/scripts/combine_metacerberus_annotations.py -b metacerberus/final/ -o combined_annotations.tsv\n\nOptions:\n\nusage: metacerberus.py [--setup] [--update] [--list-db] [--download [DOWNLOAD ...]] [--uninstall] [-c CONFIG] [--prodigal PRODIGAL [PRODIGAL ...]]\n                       [--fraggenescan FRAGGENESCAN [FRAGGENESCAN ...]] [--super SUPER [SUPER ...]] [--prodigalgv PRODIGALGV [PRODIGALGV ...]]\n                       [--phanotate PHANOTATE [PHANOTATE ...]] [--protein PROTEIN [PROTEIN ...]] [--hmmer-tsv HMMER_TSV [HMMER_TSV ...]] [--class CLASS]\n                       [--illumina | --nanopore | --pacbio] [--dir-out DIR_OUT] [--replace] [--keep] [--tmpdir TMPDIR] [--hmm HMM [HMM ...]] [--db-path DB_PATH] [--meta]\n                       [--scaffolds] [--minscore MINSCORE] [--evalue EVALUE] [--skip-decon] [--skip-pca] [--cpus CPUS] [--chunker CHUNKER] [--grouped] [--version] [-h]\n                       [--adapters ADAPTERS] [--qc_seq QC_SEQ]\n\nSetup arguments:\n  --setup               Setup additional dependencies [False]\n  --update              Update downloaded databases [False]\n  --list-db             List available and downloaded databases [False]\n  --download [DOWNLOAD ...]\n                        Downloads selected HMMs. Use the option --list-db for a list of available databases, default is to download all available databases\n  --uninstall           Remove downloaded databases and FragGeneScan+ [False]\n\nInput files\nAt least one sequence is required.\n    accepted formats: [.fastq, .fq, .fasta, .fa, .fna, .ffn, .faa]\nExample:\n&gt; metacerberus.py --prodigal file1.fasta\n&gt; metacerberus.py --config file.config\n*Note: If a sequence is given in [.fastq, .fq] format, one of --nanopore, --illumina, or --pacbio is required.:\n  -c CONFIG, --config CONFIG\n                        Path to config file, command line takes priority\n  --prodigal PRODIGAL [PRODIGAL ...]\n                        Prokaryote nucleotide sequence (includes microbes, bacteriophage)\n  --fraggenescan FRAGGENESCAN [FRAGGENESCAN ...]\n                        Eukaryote nucleotide sequence (includes other viruses, works all around for everything)\n  --super SUPER [SUPER ...]\n                        Run sequence in both --prodigal and --fraggenescan modes\n  --prodigalgv PRODIGALGV [PRODIGALGV ...]\n                        Giant virus nucleotide sequence\n  --phanotate PHANOTATE [PHANOTATE ...]\n                        Phage sequence\n  --protein PROTEIN [PROTEIN ...], --amino PROTEIN [PROTEIN ...]\n                        Protein Amino Acid sequence\n  --hmmer-tsv HMMER_TSV [HMMER_TSV ...]\n                        Annotations tsv file from HMMER (experimental)\n  --class CLASS         path to a tsv file which has class information for the samples. If this file is included scripts will be included to run Pathview in R\n  --illumina            Specifies that the given FASTQ files are from Illumina\n  --nanopore            Specifies that the given FASTQ files are from Nanopore\n  --pacbio              Specifies that the given FASTQ files are from PacBio\n\nOutput options:\n  --dir-out DIR_OUT     path to output directory, defaults to \"results-metacerberus\" in current directory. [./results-metacerberus]\n  --replace             Flag to replace existing files. [False]\n  --keep                Flag to keep temporary files. [False]\n  --tmpdir TMPDIR       temp directory for RAY (experimental) [system tmp dir]\n\nDatabase options:\n  --hmm HMM [HMM ...]   A list of databases for HMMER. Use the option --list-db for a list of available databases [KOFam_all]\n  --db-path DB_PATH     Path to folder of databases [Default: under the library path of MetaCerberus]\n\noptional arguments:\n  --meta                Metagenomic nucleotide sequences (for prodigal) [False]\n  --scaffolds           Sequences are treated as scaffolds [False]\n  --minscore MINSCORE   Score cutoff for parsing HMMER results [60]\n  --evalue EVALUE       E-value cutoff for parsing HMMER results [1e-09]\n  --remove-n-repeats    Remove N repeats, splitting contigs [False]\n  --skip-decon          Skip decontamination step. [False]\n  --skip-pca            Skip PCA. [False]\n  --cpus CPUS           Number of CPUs to use per task. System will try to detect available CPUs if not specified [Auto Detect]\n  --chunker CHUNKER     Split files into smaller chunks, in Megabytes [Disabled by default]\n  --grouped             Group multiple fasta files into a single file before processing. When used with chunker can improve speed\n  --version, -v         show the version number and exit\n  -h, --help            show this help message and exit\n\n  --adapters ADAPTERS   FASTA File containing adapter sequences for trimming\n  --qc_seq QC_SEQ       FASTA File containing control sequences for decontamination\n\nArgs that start with '--' can also be set in a config file (specified via -c). Config file syntax allows: key=value, flag=true, stuff=[a,b,c] (for details, see syntax at\nhttps://goo.gl/R74nmi). In general, command-line values override config file values which override defaults.\n\n\n\nCommon Issues and Solutions\n\nIssue 1: : The files providing counts have an issue an only provide counts of 1 with v1.3.2\n\nSolution 1: For now do not use these files, the authors of the software are aware",
    "crumbs": [
      "Sequence data analyses",
      "Functional annotation",
      "MetaCerberus"
    ]
  },
  {
    "objectID": "source/metagenomics/multiqc.html#multiqc",
    "href": "source/metagenomics/multiqc.html#multiqc",
    "title": "Bioinformatics guidance page",
    "section": "MultiQC",
    "text": "MultiQC\n\nIntroduction\nMultiQC is a reporting tool that parses results and statistics from bioinformatics tool outputs, such as log files and console outputs (Ewels et al. 2016). It helps to summarise experiments containing multiple samples and multiple analysis steps. It’s designed to be placed at the end of pipelines or to be run manually when you’ve finished running your tools. For more information, visit the official website.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nMultiQC v1.22.1 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name multiqc_1.22.1 -c bioconda multiqc\n\n\n\nUsage\nThere isn’t much to running MultiQC really - just point it at the directory that contains your files and it will search recursively for anything it recognises. It, for example, can run on reports from FastQC, TrimGalore, Cutadapt, STAR and featureCounts.\n\nconda activate multiqc_1.22.1\nmultiqc fastqc_folder\n\nOnce MultiQC has finished, you should have a HTML report file called multiqc_report.html that you can open in any browser.\nYou can also convert some of the intermediate files into data frames in R for reports. If you are interested in that, have a look at TidyMultiqc",
    "crumbs": [
      "Sequence data analyses",
      "Quality control",
      "MultiQC"
    ]
  },
  {
    "objectID": "source/metagenomics/pycirclize.html#pycirclize",
    "href": "source/metagenomics/pycirclize.html#pycirclize",
    "title": "Bioinformatics guidance page",
    "section": "pycirclize",
    "text": "pycirclize\n\nIntroduction\npyCirclize is a circular visualization python package based on matplotlib (Shimoyama 2022). This package is developed for the purpose of easily and beautifully plotting circular figure such as Circos Plots and Chord Diagrams in Python. In addition, useful genome and phylogenetic tree visualization methods for the bioinformatics field are also implemented.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nThe pycirclize conda environment is part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -p pycirclize -c bioconda -c conda-forge python=3.10.12 pycirclize numpy matplotlib\n\n\n\nUsage\nYou can easily generate custom plots using pycirclize in python after activating the environment with conda activate pycirclize. To get started, you can view more instructions here.\nAdditionally, we provide a small python wrapper, that can take a Prokka gbk (possibly others, however, that was not tested yet) and generate a circos plot that will look something like this:\n\n\nconda activate pycirclize\n\npython /zfs/omics/projects/bioinformatics/scripts/generate_circos_plot.py \\\n    -i dnaapler.gbk \\\n    -o circos_plot.pdf \\\n    -g genes_of_interest.txt\n\nconda deactivate\n\nSettings:\n\n-h, --help show this help message and exit\n-i INPUT, --input INPUT Input GenBank file generated by Prokka (.gbk)\n-o OUTPUT, --output OUTPUT Output PDF file (.pdf)\n-g GENES, --genes GENES Text file containing prokka IDs of genes of interest that should be added to the plot (one gene per line). This setting is optional.\n--version show program’s version number and exit",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "pycirclize"
    ]
  },
  {
    "objectID": "source/metagenomics/quast.html#quast",
    "href": "source/metagenomics/quast.html#quast",
    "title": "Bioinformatics guidance page",
    "section": "Quast",
    "text": "Quast\n\nIntroduction\nQUAST stands for QUality ASsessment Tool (Gurevich et al. 2013). The tool evaluates genome assemblies by computing various metrics. This tool has different mpdules including the general QUAST tool for genome assemblies, MetaQUAST, the extension for metagenomic datasets, QUAST-LG, the extension for large genomes (e.g., mammalians), and Icarus, the interactive visualizer for these tools.\nFor more information, visit the official website as well as the manual.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nQuast v5.2.0 is installed as part of the bioinformatics share. If you have access to crunchomics you can be added to the bioinformatics share by sending an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name quast_5.2.0 -c bioconda quast=5.2.0\n\n#for quast to run, you need to downgreate minimap from 2.28 to 2.4\n#2.28 gives an error that says that the miniconda2 version is not suited for this installation \n#chose 2.24 based on these notes for quast 5.2.0, https://quast.sourceforge.net/docs/CHANGES.txt\nconda activate quast_5.2.0\nmamba install -c bioconda minimap2=2.24\n\n\n\nUsage\n\nconda activate quast_5.2.0\n\n#running quast with a reference genome, which was downloaded from ncbi\n#and gff file, which was generated using prokka\nquast.py \\\n  assembly.fasta \\\n  -r  db/ncbi_ref/GCF_016756315.1_genomic.fna \\\n  -g assembly.gff \\\n  --nanopore data/raw_sequence_data.fastq.gz \\\n  -o results/quast_report \\\n  -t 20\n\n#running quast on several assemblies, i.e. several assemblies polished with medaka\nsrun --cpus-per-task 20 --mem=50G quast.py \\\n  -l 'r1,r2,r3,r4,r5,r6,r7,r8,r9,r10' \\\n  results/polishing/medaka/r1/consensus.fasta \\\n  results/polishing/medaka/r2/consensus.fasta \\\n  results/polishing/medaka/r3/consensus.fasta \\\n  results/polishing/medaka/r4/consensus.fasta \\\n  results/polishing/medaka/r5/consensus.fasta \\\n  results/polishing/medaka/r6/consensus.fasta \\\n  results/polishing/medaka/r7/consensus.fasta \\\n  results/polishing/medaka/r8/consensus.fasta \\\n  results/polishing/medaka/r9/consensus.fasta \\\n  results/polishing/medaka/r10/consensus.fasta \\\n  --nanopore data/raw_sequence_data.fastq.gz \\\n  -r  db/ncbi_ref/GCF_016756315.1_genomic.fna \\\n  -o results/polishing/quast_report \\\n  -t 20\n\nconda deactivate \n\nFor more information about the different modules, please visit the manual.\n\n\nCommon Issues and Solutions\n\nIssue 1: Running out of memory\n\nSolution 1: Especially when analysing more than one assembly, it might be good to increase the memory via slurm",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Quast"
    ]
  },
  {
    "objectID": "source/metagenomics/signalp6.html#signalp-6.0",
    "href": "source/metagenomics/signalp6.html#signalp-6.0",
    "title": "Bioinformatics guidance page",
    "section": "SignalP 6.0",
    "text": "SignalP 6.0\n\nIntroduction\nSignalP 6.0 predicts the presence of signal peptides and the location of their cleavage sites in proteins from Archaea, Gram-positive Bacteria, Gram-negative Bacteria and Eukarya (Teufel et al. 2022). In Bacteria and Archaea, SignalP 6.0 can discriminate between five types of signal peptides:\n\nSec/SPI: “standard” secretory signal peptides transported by the Sec translocon and cleaved by Signal Peptidase I (Lep)\nSec/SPII: lipoprotein signal peptides transported by the Sec translocon and cleaved by Signal Peptidase II (Lsp)\nTat/SPI: Tat signal peptides transported by the Tat translocon and cleaved by Signal Peptidase I (Lep)\nTat/SPII: Tat lipoprotein signal peptides transported by the Tat translocon and cleaved by Signal Peptidase II (Lsp)\nSec/SPIII: Pilin and pilin-like signal peptides transported by the Sec translocon and cleaved by Signal Peptidase III (PilD/PibD)\n\nAdditionally, SignalP 6.0 predicts the regions of signal peptides. Depending on the type, the positions of n-, h- and c-regions as well as of other distinctive features are predicted.\nSignalP 6.0 can be run via the webserver and, for larger sets of proteins, the software can be installed as well.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nSignalP v6.0h is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you need to first request the software by filling out this page. Afterwards, you get an email with a link leading you to the software and installation instructions and you can install things via:\n\n# Download software\nwget &lt;link_to_tar.gz&gt;\n\n# Decompress the folder you downloaded\n# If downloading a different version, change the name of the tar.gz folder accordingly \ntar -xzvf signalp-6.0h.fast.tar.gz\n\n# Go into the folder that was just decompressed \n# If downloading a different version, change the folder name accordingly\ncd signalp6_fast\n\n# Setup a conda environment in which we install the required dependencies and run the software setup \nmamba create -n signalP6 python=3.6\n\n# Install everything needed in the new conda environment\nconda activate signalP6\npip install signalp-6-package\n\n# Copy the model files to the location at which the signalP module got installed\nSIGNALP_DIR=$(python3 -c \"import signalp; import os; print(os.path.dirname(signalp.__file__))\" )\n\ncp -r signalp-6-package/models/* $SIGNALP_DIR/model_weights/\n\n# Check if the general setup is ok\nsignalp6 -h\n\n# Note, in some version there is a bug resulting in the error \"RuntimeError: set_num_threads expects an int, but got str\"\n# To solve this find out the path of the conda environment things are setup \ncd &lt;path_to_signalp6_conda_env&gt;/lib/python3.6/site-packages/signalp\n\n# Open the predict.py script and exchange `torch.set_num_threads(args.torch_num_threads)` with `torch.set_num_threads(int(args.torch_num_threads))`\n\n\n\nUsage\n\nconda activate signalP6\n\nsignalp6 --fastafile  my_organism.faa \\\n  --organism eukarya \\\n  --output_dir signalP_output \\\n  --write_procs 10 \\\n  --torch_num_threads 10 \\\n  --format txt --mode fast\n\nconda deactivate\n\nRequired options:\n\n--fastafile, -ff, specifies the fasta file with the sequences to be predicted. To prevent invalid file paths, non-alphanumeric characters in fasta headers are replaced with “_” for saving the individual sequence output files.\n--output_dir, -od, speicifies the directory in which to save the outputs. If it does not exist, it will be created. Note that repeated calls with the same --output_dir will overwrite previous prediction results.\n\nOther useful options:\n\n--organism, -org, is either other or eukarya. Specifying eukarya triggers post-processing of the SP predictions to prevent spurious results (only predicts type Sec/SPI).\nDefaults to other.\n--format, -fmt, can take the values txt, png, eps, all, none. It defines what output files are created for individual sequences. txt produces a tabular .gff file with the per-position predictions for each sequence. png, eps, all additionally produce probability plots in the requested format. none only writes the summary prediction files. For larger prediction jobs, plotting will slow down the processing speed significantly.\nDefaults to txt.\n--mode, -m, is either fast, slow or slow-sequential. Default is fast, which uses a smaller model that approximates the performance of the full model, requiring a fraction of the resources and being significantly faster. slow runs the full model in parallel, which requires more than 14GB of RAM to be available. slow-sequential runs the full model sequentially, taking the same amount of RAM as fast but being 6 times slower. If the specified model is not installed, SignalP will abort with an error.\nDefaults to fast.\nmodel_dir, -md allows you to specify an alternative directory containing the SignalP 6.0 model weight files. Defaults to the location that is used by the installation commands. Does not need to be specified when following the default installation instructions.\n--bsize, -bs is the integer batch size used for prediction. When running on GPU, this should be adjusted to maximize usage of the available memory. On CPU, the choice usually has only a limited effect on performance. Defaults to 10.\n--torch_num_threads, -tt is the number of threads used by PyTorch. Defaults to 8.\n--write_procs, -wp is the integer number of parallel processes launched for writing output files. Using multiple processes significantly speeds up writing the outputs for prediction jobs with many sequences. However, due to the way multiprocessing works in Python, this leads to increased memory usage. By setting to 1, no additional processes are started. Defaults to the number of available CPUs with 8 processes maximum.\n\nThe script will require the following outputs:\n\nprediction_results.txt: A tab delimited file with one line per prediction. This file has the following columns:\n\nID: the sequence ID parsed from the fasta input.\nPrediction: The predicted type. One of [OTHER (No SP), SP (Sec/SPI), LIPO (Sec/SPII), TAT (Tat/SPI), TATLIPO (Tat/SPII), PILIN (Sec/SPIII)].\nOne column for each possible type with the model’s probability.\nCS Position: The cleavage site. The sequence positions between which the SPase cleaves and its predicted probability.\n\nprocessed_entries.fasta: Predicted mature proteins, i.e. sequences with their signal peptides removed.\noutput.gff3: The start and end positions of all predicted signal peptides in GFF3 format.\nregion_output.gff3: The start and end positions of all predicted signal peptide regions in GFF3 format.\noutput.json: The prediction results in JSON format, together with details on the run parameters and paths to the generated output files. Useful for integrating SignalP 6.0 in pipelines.",
    "crumbs": [
      "Sequence data analyses",
      "Functional annotation",
      "SignalP 6.0"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#deseq2",
    "href": "source/metatranscriptomics/DSEq2.html#deseq2",
    "title": "Bioinformatics guidance page",
    "section": "DeSeq2",
    "text": "DeSeq2\nThe DESeq2 package provides methods to test for differential expression in RNA-seq data by use of negative binomial generalized linear models; the estimates of dispersion and logarithmic fold changes incorporate data-driven prior distributions (Love, Huber, and Anders 2014).\nIn this workflow we look for differentially expressed genes among 6 RNA-seq single-read sequencing libraries that were collected from from 2 growth treatments: Thioalkalivibrio thiocyanoxidans grown on thiocyanate and thiosulfate (3 replicates each). To generate a count table, the sequence reads were first processed with FastP and Sortmerna and the remaining reads mapped against the T. thiocyanoxidans genome using Bowtie2. Afterwards, count tables were generated using FeatureCounts. Here, we will analyse these count tables using DeSeq2.\nA Gbk file as well as annotations were downloaded from NCBI in order to have annotations for each gene available.\nOn those pages you will find information about how to:\n\nRead the data into R and prepare the count table to be used in DeSeq2\nInvestigate the count tables and look at the quality of the different replicates\nRun DeSeq2 to identify differentially expressed genes\nGenerate summary tables\nGenerate visuals\n\nIf you want to follow this code on your own, feel free to download the required files from here",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#load-libraries",
    "href": "source/metatranscriptomics/DSEq2.html#load-libraries",
    "title": "Bioinformatics guidance page",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(\"tidyverse\")\nlibrary(\"DESeq2\")\nlibrary(\"apeglm\")\nlibrary(\"pheatmap\")\nlibrary(\"ggrepel\")\nlibrary(\"RColorBrewer\")\nlibrary(\"EnhancedVolcano\")\n\nThe exact versions used to write this workflow are:\n\nR version 4.2.2\nDESeq2_1.38.3\napeglm_1.20.0\ntidyverse_2.0.0\nEnhancedVolcano_1.16.0 (just for visuals, not essential)\nRColorBrewer_1.1-3 (just for visuals, not essential)\nggrepel_0.9.5 (just for visuals, not essential)\npheatmap_1.0.12 (just for visuals, not essential)",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#load-data",
    "href": "source/metatranscriptomics/DSEq2.html#load-data",
    "title": "Bioinformatics guidance page",
    "section": "Load data",
    "text": "Load data\nThe data we work with are:\n\nThe count table generated with FeatureCounts\nA mapping file linking the sample IDs with the treatment conditions\nA gene annotation table\n\n\ncount_table &lt;- read.csv(\"../../data/metatranscriptomics/counts.txt\", header = TRUE, sep = \"\\t\", row.names = 1)\nmetadata_table &lt;- read.csv(\"../../data/metatranscriptomics/mapping.txt\", header = TRUE, sep = \"\\t\", row.names = 1)\nannotation_table &lt;- read.csv(\"../../data/metatranscriptomics/ncbi_dataset.tsv\", header = TRUE, sep = \"\\t\")\n\nNotice:\n\nTo be able to use DeSeq2, the counts in the table should not be normalized\nDeSeq2 will correct for library size and therefore you do not need to correct values yourself\nThe columns of the count matrix and the rows of the metadata table should be in the same order (we will check if that is the case in the step below)\nOur final count_table will have the geneIDs as row.names, which is why we use row.names=1 and other than the count information should have no other columns. Since FeatureCounts provides colums with some other information, such as the gene length, we perform some cleaning steps in the code below and you might need to adjust your own data depending on whether or not you have any other columns other than the counts",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#prepare-data",
    "href": "source/metatranscriptomics/DSEq2.html#prepare-data",
    "title": "Bioinformatics guidance page",
    "section": "Prepare data",
    "text": "Prepare data\nHere we will:\n\nEnsure that our annotation table has as many rows (i.e. genes) as our count data table\nCreate a minimal metadata file for plotting\nEnsure that our count table only has the count data as columns and not any other information. I.e. we want the row names be our gene IDs and the columns be the count data for each sample\nWe want to ensure that the columns in our count data is in the same order as in the metadata file. If this is not the case then we will fix this\n\n\n#subset the gtf table from ncbi to ensure that we work with the same number of genes\n#adjust this depending on where your annotations come from\nannotation_table &lt;- annotation_table |&gt; \n  filter(Gene.Type %in% c(\"protein-coding\", \"pseudogene\"))\n\n# Subsetting the dataframe based on the \"treatment\" column to create a minimal metadata file\nsubset_metadata &lt;- metadata_table[, c(\"treatment\"), drop = FALSE]\n\n#remove unneccessary columns from the count table\n#adjust this step if you work with a different count table\n#the goal is to have the rownames be the gene names and the columns only contain the count data from our sequencing libraries\ncount_table &lt;-\n  count_table |&gt; \n    select(!(Chr:Length))\n\n#ensure that the sampleIDs in the count data are the same as in the metadata\nall(rownames(metadata_table) %in% colnames(count_table))\n\n[1] TRUE\n\n#ensure that names between the count and metadata tables are in the same order\nall(rownames(metadata_table) == colnames(count_table))\n\n[1] FALSE\n\n#reorder data if the tables above are not in the same order\ncount_table &lt;- count_table[, rownames(metadata_table)]\n\n#confirm that this worked\nall(rownames(metadata_table) == colnames(count_table))\n\n[1] TRUE\n\n#convert count data to long format \ncount_table_long &lt;- gather(count_table, key = \"Sample\", value = \"RawCount\")",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#explore-raw-data",
    "href": "source/metatranscriptomics/DSEq2.html#explore-raw-data",
    "title": "Bioinformatics guidance page",
    "section": "Explore raw data",
    "text": "Explore raw data\nExploring the count table first is important to find any potential issues with the data, for example spot replicates that behave unexpectedly.\n\nGenerate histograms\n\nggplot(count_table_long) +\n  geom_histogram(aes(x = RawCount), stat = \"bin\", bins = 200) +\n  xlab(\"Raw expression counts\") +\n  ylab(\"Number of genes\") +\n  facet_wrap(~Sample, scales = \"free\")\n\n\n\n\n\n\n\n\nIn this plot, we see some common features of RNA-seq count data:\n\nour data will not have a fixed range, i.e. some samples have have a range rom 0 - 150 000 counts, others have more counts\nour data is not normal distributed:\n\na low number of counts associated with a large proportion of genes\na long right tail due to the lack of any upper limit for expression\na large dynamic range\n\n\nThis tells us that our RNA-seq data is not normally distributed, which is quite normal for this kind of data. Statistical models used in down-stream analyses need to take these characteristics into account (DeSeq2 does this for us in order to make predictions (i.e. model) about our data.\nIf we can not use a normal distribution to model our data, then we could work with a Poisson distribution instead. Poisson can be used for data where the number of cases are large and the probability of an event happening is low. Since in RNA-Seq data, we have millions of reads being sequences, the probability of a read mapping to a gene is low.\nHowever, we first have to test whether our data fulfills the criteria to use the Poisson distribution. Poisson has only one parameter, lamba, and Poisson expects that lamba = mean = variance. We can easily check if that is true for our data.\n\n\nPlot the mean vs variance\n\nMean: average of a given set of data, the mean across all the samples for each individual gene\nVariance: average of the squared differences from the mean, it therefore measures the spread of a data set.\n\nComparing these two values tells us more about the assumptions we can make about our data, specifically if the mean == variance we could model our data using a poisson distribution.\n\n#The second argument '1' of 'apply' function indicates the function being applied to rows. Use '2' if applied to columns\nmean_counts &lt;- apply(count_table[,4:6], 1, mean)         \nvariance_counts &lt;- apply(count_table[,4:6], 1, var)\ndf &lt;- data.frame(mean_counts, variance_counts)\n\nggplot(df) +\n        geom_point(aes(x=mean_counts, y=variance_counts)) + \n        scale_y_log10(limits = c(1,1e9)) +\n        scale_x_log10(limits = c(1,1e9)) +\n        geom_abline(intercept = 0, slope = 1, color=\"red\")\n\n\n\n\n\n\n\n\nWe see:\n\nthat the mean is not equal to the variance since data points do not fall on the diagonal\nthat for the genes with high mean expression, the variance across replicates tends to be greater than the mean, since the points fall above the red line. So variance &gt; mean\nfor the genes with low mean expression we see more scattering, i.e.“heteroscedasticity”. This tells us that lowly expressed genes show a lot of variability in the variance values\n\nAll of this tells us that RNA-Seq data does not fit the criteria if we wanted to model the count data with a Poisson distribution (where mean == variance). If we would increase the samples (&gt;20) we might be able to use Poisson, however, a large number of replicates can be either hard to obtain (depending on how samples are obtained) and/or may not be affordable.\n\nThe distribution that fits RNA-seq data best, given this type of variability between replicates, is the negative binomial, which can be used for data where the mean &lt; variance. DeSeq2 is one tool that allows us to do this.\nThe negative binomial has two parameters,\n\nthe fitted mean, which is composed of the sample specific size factor and the expected true concentration of fragments for each sample\nthe dispersion parameter (which accounts for the extra variability for lowly expressed genes).\n\nNotice, that if you work with a lot of samples, DeSeq2 might perform poorly because the data does not work well anymore with the negative binomial distribution. For such cases, a Wilcoxon rank-sum test can be used. A sample size &gt; 8 has been shown to be useful for switching to Wilcoxon. For more information (and a code example) go here. Notice , the example uses edgeR but we can easily use the DeSeq normalized count table we extract below.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#generate-a-deseqdataset-object",
    "href": "source/metatranscriptomics/DSEq2.html#generate-a-deseqdataset-object",
    "title": "Bioinformatics guidance page",
    "section": "Generate a DESEQDataSet object",
    "text": "Generate a DESEQDataSet object\nThe R function DESeqDataSetFromMatrix is used to store our count table and metadata in a DESeqDataSet object. Additionally, we provide a formula which specifies the design of our experiment, in our case we want to compare thiocyanate versus thiosulfate treatments. Information about these treatments we find in the treatment column in our metadata_table.\n\n#construct a DESEQDataSet Object\ndds &lt;- DESeqDataSetFromMatrix(countData = count_table,\n                              colData = metadata_table,\n                              design = ~treatment,\n                              tidy = FALSE)\n\n#view the structure of this new object\ndds\n\nclass: DESeqDataSet \ndim: 2606 6 \nmetadata(1): version\nassays(1): counts\nrownames(2606): G372_RS0100005 G372_RS0100015 ... G372_RS0113390\n  G372_RS0113395\nrowData names(0):\ncolnames(6): SRR6344906 SRR6344904 ... SRR6344913 SRR6344910\ncolData names(3): srx_id treatment description\n\n#you can view the original data with the counts function\nhead(counts(dds))\n\n               SRR6344906 SRR6344904 SRR6344905 SRR6344912 SRR6344913\nG372_RS0100005        365        369        384        196        132\nG372_RS0100015        549        636       1428        220        106\nG372_RS13400          408        515        910       5338        479\nG372_RS0100025        733        759        626        341        504\nG372_RS0100030        222        165        148         66        109\nG372_RS0100040       1443       1674       1967       1614        786\n               SRR6344910\nG372_RS0100005        221\nG372_RS0100015        162\nG372_RS13400         2702\nG372_RS0100025        474\nG372_RS0100030        127\nG372_RS0100040       1435\n\n\nData inputs\nThere are different ways to read in count data depending on how such tables were generated. In our example we read in a simple count matrix. There are some other import options as well:\n\nIf you have performed transcript quantification (with Salmon, kallisto, RSEM, etc.) you could import the data with tximport, which produces a list, and then you can use DESeqDataSetFromTximport() .\nIf you imported quantification data with tximeta, which produces a SummarizedExperiment with additional metadata, you can then use DESeqDataSet() .\nIf you have htseq-count files, you can use DESeqDataSetFromHTSeq() .\n\nThe design argument\n\nThe design formula, i.e. design = ~treatment in the code above, expresses the values we want to use for modeling and is written as a tilde followed by the variables (with plus signs in between them if we work with multiple variables). In our example, we want to investigate the data based on the treatment column. If we work with multiple variables we should put the variable of interest at the end of the formula and put the control variable as the first level.\nPrior to performing the differential expression analysis, it is a good idea to know what sources of variation are present in your data. If you know major sources of variation, you can remove them prior to analysis or control for them in the statistical model by including them in the design formula\nIf you want to examine the expression differences between treatments, and you know that major sources of variation include, for example, day and age, then your design formula would be design = ~ day + age + treatment\nThe factors included in the design formula need to match the column names in the metadata. It is best practice to list the variable that is your main effect in the last position of your design formula.\nWe can explore interactions or ‘the difference of differences’ by specifying for it in the design formula. For example, if you wanted to explore the effect of age on the treatment effect, you could specify for it in the design formula as follows: design = ~ day + age + treatment + age:treatment\n\n\nPre-filter data\nWhile it is not necessary to pre-filter low count genes before running the DESeq2 functions, it can be useful to do:\n\nby removing rows with few reads, we reduce the memory size of the dds data object, and increase the speed of count modeling within DESeq2\nThis can also improve visualizations, as features with no information for differential expression are not plotted in dispersion plots or MA-plots\n\nHere, we perform pre-filtering to keep only rows that have a count of at least 5 for a minimal number of samples. A recommendation for the minimal number of samples is to specify the smallest group size, e.g. here there are 3 treated samples.\nOne can also omit this step entirely and just rely on the independent filtering procedures available in the results() function (see below).\n\nsmallestGroupSize &lt;- 3\nkeep &lt;- rowSums(counts(dds) &gt;= 5) &gt;= smallestGroupSize\ndds &lt;- dds[keep,]\ndds\n\nclass: DESeqDataSet \ndim: 2592 6 \nmetadata(1): version\nassays(1): counts\nrownames(2592): G372_RS0100005 G372_RS0100015 ... G372_RS0113390\n  G372_RS0113395\nrowData names(0):\ncolnames(6): SRR6344906 SRR6344904 ... SRR6344913 SRR6344910\ncolData names(3): srx_id treatment description\n\n\nWhen viewing our dds object, we should see that we have less rows in the dim category, since some genes will have been removed during this step.\n\n\nOrganize factor levels\nBy default, R will choose a reference level for factors based on alphabetical order. If you don’t tell DESeq2, which level you want to compare against (e.g. which level represents the control group), the comparisons will be based on the alphabetical order of the levels. There are two solutions:\n\nyou can either explicitly tell results which comparison to make using the contrast argument (this will be shown later)\nyou can explicitly set the factors levels. In order to see the change of reference levels reflected in the results\n\nBelow, we manually will sort the factors and ensure that we compare thiosulfate versus thiocyanate (our reference level), i.e. the last factor is our factor of interest.\n\ndds$treatment &lt;- relevel(dds$treatment, ref = \"thiocyanate\")\ndds$treatment\n\n[1] thiosulfate thiosulfate thiosulfate thiocyanate thiocyanate thiocyanate\nLevels: thiocyanate thiosulfate",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#quality-assessment-of-dseq-data",
    "href": "source/metatranscriptomics/DSEq2.html#quality-assessment-of-dseq-data",
    "title": "Bioinformatics guidance page",
    "section": "Quality assessment of DSEq data",
    "text": "Quality assessment of DSEq data\nNext, let’s assess the quality of our sequencing data by visualizing the amount of variation across replicates.\nTo stabilize the variance across the mean DeSeq2 has two methods:\n\nthe variance stabilizing transformation (VST) for negative binomial data with a dispersion-mean trend implemented in the vst function\nthe regularized-logarithm transformation implemented in the rlog function\nrlog tends to work well on small datasets (n &lt; 30), potentially outperforming the VST when there is a wide range of sequencing depth across samples (an order of magnitude difference).\nvst should be used for medium-to-large datasets (n &gt; 30)\nblind: whether to blind the transformation to the experimental design.\n\nblind=TRUE should be used for comparing samples in an manner unbiased by prior information on samples, for example to perform sample quality assurance\nblind=FALSE should be used for transforming data for downstream analysis, where the full use of the design information should be made. blind=FALSE will skip re-estimation of the dispersion trend, if this has already been calculated. If many of genes have large differences in counts due to the experimental design, it is important to set blind=FALSE for downstream analysis.\n\n\n\n### Transform counts for data visualization\n#we use blind=TRUE because we first want to do some quality controls\nrld &lt;- rlog(dds, blind=TRUE)\nvsd &lt;- vst(dds, blind=TRUE)    \n\nIn the down-stream analyses we will work with rld data, since we work with few samples only.\n\nPCA\nThe plotPCA function is part of DeSeq2 and let’s us check whether our replicates cluster together.\n\n### Plot PCA \nplotPCA(rld, intgroup=\"treatment\")\n\n\n\n\n\n\n\n\nBy default plotPCA() uses the top 500 most variable genes. You can change this by adding the ntop= argument and specifying how many of the genes you want the function to consider.\nWe can also use ggplot to better modify the graph:\n\npcaData &lt;- plotPCA(rld, intgroup=\"treatment\", returnData = TRUE)\npercentVar &lt;- round(100 * attr(pcaData, \"percentVar\"))\n\nggplot(pcaData, aes(x = PC1, y = PC2, color = treatment)) +\n  geom_point(size =3) +\n  xlab(paste0(\"PC1: \", percentVar[1], \"% variance\")) +\n  ylab(paste0(\"PC2: \", percentVar[2], \"% variance\")) +\n  coord_fixed() +\n  ggtitle(\"PCA with rlog data\")\n\n\n\n\n\n\n\n\nWe see that our samples cluster by replicate and we do not see any clear outliers that we would need to take care of.\n\n\nHierarchical clustering\nWe can also compute pairwise correlations for our samples as follows:\n\n### Extract the rlog matrix from the object\nrld_mat &lt;- assay(rld)   \n\n### Compute pairwise correlation values\nrld_cor &lt;- cor(rld_mat)    \n\n### Plot heatmap using the correlation matrix and the metadata object\nheat.colors &lt;- brewer.pal(6, \"Blues\")\npheatmap(rld_cor, annotation = subset_metadata,\n         color = heat.colors)\n\n\n\n\n\n\n\n\nOverall, we observe pretty high correlations ( &gt; 0.97) suggesting no outlying sample(s). Also, similar to the PCA plot you see the samples clustering together by treatment. Since the majority of genes will not be differentially expressed, samples generally have high correlations with each other (values higher than 0.80). Samples below 0.80 may indicate an outlier in your data and/or sample contamination.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#run-deseq",
    "href": "source/metatranscriptomics/DSEq2.html#run-deseq",
    "title": "Bioinformatics guidance page",
    "section": "Run DeSeq",
    "text": "Run DeSeq\nNext, we use the DESeq function to identify differentially expressed (DE) genes as follows:\n\ndds &lt;- DESeq(dds)\n\nestimating size factors\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n#check if level is set correctly\nresultsNames(dds)\n\n[1] \"Intercept\"                           \n[2] \"treatment_thiosulfate_vs_thiocyanate\"\n\n#perform normalization post Deseq\n### Transform counts for data visualization\nrld_downstream &lt;- rlog(dds, blind=FALSE)\nvsd_downstream &lt;- vst(dds, blind=FALSE)   \n\nNote about technical replicates\nIf you have technical replicates, DeSeq2 comes with a function to collapse these before running DESeq.\nDESeq2 does the following:\n\nit calculates the relative library depth of each sample to estimate a size factor\nit estimates the dispersion of counts for each gene\nit fits a curve to the gene-wide dispersion estimates\nit shrinks gene-wise dispersion estimates\nit uses a generalized linear model (GLM) of the NB family to fit the data\nit estimates coefficients for each sample group along with their standard error. The coefficents are the estimated for the log2 foldchanges, and will be used as input for hypothesis testing\nit uses the nbinomWaldTest to calculate the significance of coefficients in a Negative Binomial GLM using the size and dispersion outputs\n\nDetails on size factor estimation\nThere are multiple biases we need to account for with our data: library size, genes length, RNA population composition for each condition and genes GC composition. We can discard differences with gene length and GC composition if we ompare genes among conditions only.\n\nLibrary size: the total number of reads is not always equivalent between samples\nCompositional bias: Our counts are not absolute values butdescribed as proportions or probabilities. Also have a look at Fig1 for a visual example.\n\nTo account for this, DeSeq2 calculates a size factor for each sample in the counts matrix via the median ratios method. This size factor is used to normalize the counts.\nDetails on estimating dispersion\n\nThis step is done to account for variability between replicates\nDispersion is a measure of spread/variability in the data\nThe higher the dispersion, the higher the variability is, i.e.a dispersion of 0.01 == 10% of variability between samples\nWe have seen in the plot above that low mean counts have much larger variance in the data and therefore the dispersion estimates will differ more between genes with small means\nIf we have a small number of replicates, 3-7, calculation of dispersion is not as reliable but DeSeq2 tries to solve this problem by looking at variability across eachg ene. I.e. DESeq2 assumes that genes with similar expression levels should have similar dispersion.\nDeSeq2 does the following:\n\nGet dispersion estimates for each gene\nFit a curve to the gene-wise dispersion estimates\nShrink the gene-wise dispersion estimates towards values predicted by the curve\n\n\nWe can explore the behavior of each gene across these steps as follows:\n\n#view dispersion plot\nplotDispEsts(dds)\n\n\n\n\n\n\n\n\nHere, black dot in the plot represents the dispersion for one gene. The red line is fit to the data, then the dispersions are squeezed toward the red line, resulting in the final (blue) dispersion estimates. Usually, the dispersion is highest at the low counts and levels off at higher counts. Notice that some genes will not be shrunk towards the curve (black dots, with a blue circle) because the variability is so high that DeSeq will not assume that these genes will follow the models assumptions.\nThis is a good plot to examine to ensure your data is a good fit for the DESeq2 model. Evaluate the plot to see if:\n\nThe data to generally scatter around the curve, with the dispersion decreasing with increasing mean expression levels.\nHow much shrinkage you get across the whole range of means in your data. For any experiment with low degrees of freedom, you will expect to see more shrinkage.\nThe data scatter in a cloud or different shapes. If that is the case then you might want to explore your data more to see if you have contamination (mitochondrial, etc.) or outlier samples.\nIf you do not see a good fit, you can consider exploring other methods, such as EdgeR or limma.\n\nDetails on fitting the linear model:\n\nDeSeq2 will fit a generalized linear model for each gene\nIt will model the counts using a negative binominal distribution\nIt will calculate the log2-fold differences between our treatments that we provide in the design argument in the DeSeq formula when using DESeqDataSetFromMatrix\n\nDetails on the hypothesis testing:\n\nThe first step is to set up a null hypothesis for each gene. DeSeq2’s null hypothesis is that there is no differential expression across the two sample groups (i.e. the log-fold change (LFC) == 0)\nIn DESeq2, the Wald test is the default test used for hypothesis testing when comparing two groups. The Wald test only estimates one model per gene and evaluates the null hypothesis that LFC == 0.\nDESeq2 also offers the Likelihood Ratio Test (LRT) as an alternative hypothesis test for when we are comparing more than two sample classes. Rather than evaluating whether a gene’s expression is up- or down-regulated in one class compared to another, the LRT identifies genes that are changing in expression in any direction across the different sample classes.\n\nDetails about p-values:\n\nFor a gene with a significance cut-off of p &lt; 0.05, this means there is a 5% chance it is a false positive. If we test 20,000 genes for differential expression, at p &lt; 0.05 we would expect to find 1,000 genes by chance.\nEach p-value is the result of a single test (single gene). The more genes we test, the more we inflate the false positive rate. We can correct the p-value for multiple testing. The most common approaches are:\n\nBonferroni: The adjusted p-value is calculated by: p-value * m (m = total number of tests). This is a very conservative approach with a high probability of false negatives and is generally recommended.\nFDR/Benjamini-Hochberg: The False Discovery Rate (FDR) is an algorithm to control the expected FDR below a specified level given a list of independent p-values. For example, if gene X has a q-value of 0.013 it means that 1.3% of genes that show p-values at least as small as gene X are false positives.\n\nDESeq2 helps reduce the number of genes tested by removing genes unlikely to be significantly DEs prior to testing, such as those with low number of counts and outlier samples. Additionally, multiple test correction is implemented to reduce the False Discovery Rate using an interpretation of the Benjamini-Hochberg procedure.\nBy setting the FDR cutoff to &lt; 0.05, we’re saying that the proportion of false positives we expect among the DE genes is 5%. For example, if you call 500 genes as differentially expressed with an FDR cutoff of 0.05, you expect 25 of them to be false positives.\n\n\nGenerate a results table\nIn our dataset, we have a single variable in our design formula and we can make two possible pairwise comparisons:\n\nthiocyanate vs. thiosulfate\nthiosulfate vs. thiocyanate\n\nWhen we initially created our dds object we had provided ~ treatment as our design formula, indicating that treatment is our main factor of interest.\nTo indicate, which classes we are interested in comparing, we need to specify contrasts. The contrasts are used as input to the DESeq2 results() function to extract the desired results. If we run the results() function without specifying contrast or name, it will return the comparison of the last level of the last variable in the design formula over the first level of this variable. If the order of levels are not specified, they are ordered alphabetically by DESeq2.\nContrasts can be specified in different ways:\n\nContrasts can be supplied as a character vector with exactly three elements: the name of the factor (of interest) in the design formula, the name of the two factors levels to compare. The factor level given last is the base level for the comparison.\nContrasts can be given as a list of 2 character vectors: the names of the fold changes for the level of interest, and the names of the fold changes for the base level. These names should match identically to the elements of resultsNames(object). This method can be useful for combining interaction terms and main effects.\nAlternatively, if we have only two factor levels we could do nothing and not worry about specifying contrasts (i.e. results(dds)). In this case, DESeq2 will choose what your base factor level based on alphabetical order of the levels.\n\nDeciding what level is the base level will determine how to interpret the fold change that is reported. So for example, if we observe a log2 fold change of -2 this would mean the gene expression is lower in factor level of interest relative to the base level.\n\nres_thios &lt;- results(dds, alpha = 0.05, contrast =  c(\"treatment\", \"thiosulfate\", \"thiocyanate\"))\n\n#view data, sorted by p-value \nres_thios &lt;- res_thios[order(res_thios$padj),]\nhead(res_thios)\n\nlog2 fold change (MLE): treatment thiosulfate vs thiocyanate \nWald test p-value: treatment thiosulfate vs thiocyanate \nDataFrame with 6 rows and 6 columns\n                baseMean log2FoldChange     lfcSE      stat       pvalue\n               &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt;\nG372_RS0106330   9825.08       -7.45847  0.191514  -38.9448  0.00000e+00\nG372_RS0106335  14327.34       -6.70146  0.166838  -40.1675  0.00000e+00\nG372_RS0106340   6323.94       -7.34020  0.220270  -33.3237 1.75382e-243\nG372_RS0106310   6594.09       -5.97008  0.221860  -26.9093 1.71058e-159\nG372_RS0106305   1554.12       -6.24349  0.240691  -25.9398 2.36822e-148\nG372_RS0106290   8768.28       -5.51185  0.214228  -25.7289 5.55410e-146\n                       padj\n                  &lt;numeric&gt;\nG372_RS0106330  0.00000e+00\nG372_RS0106335  0.00000e+00\nG372_RS0106340 1.51471e-240\nG372_RS0106310 1.10803e-156\nG372_RS0106305 1.22721e-145\nG372_RS0106290 2.39845e-143\n\n\nThe results function :\n\nextracts a results table with log2 fold changes, p-values and adjusted p values. With no additional arguments to results, the log2 fold change and Wald test p-value will be for the last variable in the design formula, and if this is a factor, the comparison will be the last level of this variable over the reference level\nautomatically performs independent filtering based on the mean of normalized counts for each gene, optimizing the number of genes which will have an adjusted p value below a given FDR cutoff, i.e. alpha. By default the argument alpha is set to 0.1.\nUseful arguments\n\nalpha: the significance cutoff used for optimizing the independent filtering (by default 0.1). If the adjusted p-value cutoff (FDR) will be a value other than 0.1, alpha should be set to that value\nlfcThreshold: a non-negative value which specifies a log2 fold change threshold. The default value is 0, corresponding to a test that the log2 fold changes are equal to zero.\n\n\nInformation about the generated output:\n\nbaseMean = the average of the normalized counts taken over all samples\nlog2FoldChange = log2 fold change between the treated versus untreated conditions (thiosulfate versus thiocyanate in our case). Here, a value of 2 means that the expression has increased 4-fold in the thiosulfate sample\nlfcSE = standard error of the log2FoldChange estimate\nstat = Wald statistic\npvalue = Wald test p-value.\npadj = Benjamini-Hochberg adjusted p-value. The p-value adjusted for multiple testing, and is the most important column of the results. Typically, a threshold such as padj &lt; 0.05 is a good starting point for identifying significant genes.\n\nDeSeq2 filtering steps:\n\nIf we scroll through the results table, we notice that for selected genes there are NA values in the pvalue and padj columns.The missing values represent genes that have undergone filtering as part of the DESeq() function. Prior to differential expression analyses it is beneficial to omit genes that have little or no chance of being detected as differentially expressed. This will increase the power to detect differentially expressed genes. DESeq2 does not physically remove any genes from the original counts matrix, and so all genes will be present in your results table. The genes omitted by DESeq2 meet one of the three filtering criteria:\n\nGenes with zero counts in all samples\nGenes with an extreme count outlier. The DESeq() function calculates, for every gene and for every sample, a diagnostic test for outliers called Cook’s distance. Genes which contain a Cook’s distance above a threshold are flagged, however at least 3 replicates are required for flagging, as it is difficult to judge which sample might be an outlier with only 2 replicates. We can turn off this filtering by using the cooksCutoff argument in the results() function.\nGenes with a low mean normalized counts: DESeq2 defines a low mean threshold, that is empirically determined from your data, in which the fraction of significant genes can be increased by reducing the number of genes that are considered for multiple testing. This is based on the notion that genes with very low counts are not likely to see significant differences typically due to high dispersion.\n\nDESeq2 will perform this filtering by default; however other DE tools, such as EdgeR will not. Filtering is a necessary step, even if you are using limma-voom and/or edgeR’s quasi-likelihood methods. Be sure to follow pre-filtering steps when using other tools, as outlined in their user guides found on Bioconductor as they generally perform much better.\n\nWe can check for the number of filtered genes by the different categories as follows:\n\n#filter genes by zero expression\nres_thios[which(res_thios$baseMean == 0),] \n\nlog2 fold change (MLE): treatment thiosulfate vs thiocyanate \nWald test p-value: treatment thiosulfate vs thiocyanate \nDataFrame with 0 rows and 6 columns\n\n#filter genes that have an extreme outlier\nres_thios[which(is.na(res_thios$pvalue) & \n          is.na(res_thios$padj) &\n          res_thios$baseMean &gt; 0),]\n\nlog2 fold change (MLE): treatment thiosulfate vs thiocyanate \nWald test p-value: treatment thiosulfate vs thiocyanate \nDataFrame with 1 row and 6 columns\n                baseMean log2FoldChange     lfcSE      stat    pvalue      padj\n               &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nG372_RS0104780   2059.77      -0.818111     1.006 -0.813233        NA        NA\n\n#filter genes below the low mean threshold\nres_thios[which(!is.na(res_thios$pvalue) & \n           is.na(res_thios$padj) & \n           res_thios$baseMean &gt; 0),] \n\nlog2 fold change (MLE): treatment thiosulfate vs thiocyanate \nWald test p-value: treatment thiosulfate vs thiocyanate \nDataFrame with 0 rows and 6 columns",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#log-fold-change-shrinkage-for-visualization-and-ranking",
    "href": "source/metatranscriptomics/DSEq2.html#log-fold-change-shrinkage-for-visualization-and-ranking",
    "title": "Bioinformatics guidance page",
    "section": "Log fold change shrinkage for visualization and ranking",
    "text": "Log fold change shrinkage for visualization and ranking\nTo generate more accurate log2 foldchange (LFC) estimates, DESeq2 allows for the shrinkage of the LFC estimates toward zero when the information for a gene is low, which could include:\n\nLow counts\nHigh dispersion values\n\nLFC shrinkage uses information from all genes to generate more accurate estimates. Specifically, the distribution of LFC estimates for all genes is used (as a prior) to shrink the LFC estimates of genes with little information or high dispersion toward more likely (lower) LFC estimates.\nFor a gene with low within group variation, the unshrunken LFC estimate will be very similar to the shrunken LFC estimate. However, LFC estimates a gene with a quite high dispersion will be very different.\nShrinking the log2 fold changes will not change the total number of genes that are identified as differentially expressed but it will help with downstream assessment of results. Notice, that fcShrink doesn’t change padj in the results table, it just adds a new log2FoldChange column.\n\n#extract the name of the coefficient we want to shrink\nresultsNames(dds)\n\n[1] \"Intercept\"                           \n[2] \"treatment_thiosulfate_vs_thiocyanate\"\n\n#save unshrunken results in a separate object\nres_thios_unshrunken &lt;- res_thios\n\n#shrink log2 fold changes, this adds column lfcSE (shrunken log2 fold changes (LFC))\nres_thios &lt;- lfcShrink(dds, coef=\"treatment_thiosulfate_vs_thiocyanate\", type=\"apeglm\")\n\nNote that the stat column is no longer present in the results table. If you run type=“apeglm” or type=“ashr” you do not get a stat column because these don’t really make sense in the context of the posterior quantities that are returned.\nThe plotMA function we use below generates a so-called “MA-plot”, i.e. a scatter plot of log2 fold changes (on the y-axis) versus the mean of normalized counts (on the x-axis).\n\n# MA plot using unshrunken fold changes\nplotMA(res_thios_unshrunken, ylim=c(min(res_thios_unshrunken$log2FoldChange),\n                                    max(res_thios_unshrunken$log2FoldChange)))\n\n\n\n\n\n\n\n\nFor most genes, we make the assumption is that most of the genes would not see any change in their expression; therefore, the majority of the points on the y-axis would be located at 0, since log(1) is 0. If a gene is colored in blue then this indicates a significant data point across the tested conditions (alpha=0.005).\n\n# MA plot using shrunken fold changes\nplotMA(res_thios, ylim=c(min(res_thios_unshrunken$log2FoldChange),\n                                    max(res_thios_unshrunken$log2FoldChange)))\n\n\n\n\n\n\n\n\nWe can see that genes with low mean counts have a smaller estimate after shrinkage. We can also use this plot to evaluate the magnitude of fold changes and how they are distributed relative to mean expression. Generally, we would expect to see significant genes across the full range of expression levels.",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#generate-a-summary-of-the-differential-gene-expression",
    "href": "source/metatranscriptomics/DSEq2.html#generate-a-summary-of-the-differential-gene-expression",
    "title": "Bioinformatics guidance page",
    "section": "Generate a summary of the differential gene expression",
    "text": "Generate a summary of the differential gene expression\n\nThe summary function\nThe DESeq summary function can be used with a DESeq results table as input and will summarize the results using a default threshold of padj &lt; 0.1. However, since we had set the alpha argument to 0.05 when creating our results table we should adjust it accordingly.\n\nsummary(res_thios, alpha = 0.05)\n\n\nout of 2592 with nonzero total read count\nadjusted p-value &lt; 0.05\nLFC &gt; 0 (up)       : 389, 15%\nLFC &lt; 0 (down)     : 445, 17%\noutliers [1]       : 1, 0.039%\nlow counts [2]     : 0, 0%\n(mean count &lt; 5)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n\n\n\n\nExtract results as dataframe\nNext, lets make convert our DeSeq2 results into a table (which you can filter or export as needed).\n\n#define thresholds to identify DE genes\npadj.cutoff &lt;- 0.05\nlfc.cutoff &lt;- 1\n\n#convert deseq object to a dataframe\nres_thios_tb &lt;- res_thios |&gt;\n  data.frame() |&gt;\n  rownames_to_column(var=\"gene\") |&gt; \n  as_tibble()\n\n#deal with pvalues == 0 (those are lower than the most negative number R can display and can do weird things when visualizing our data without transforming these 0s first)\n#here, we exchange 0 with the smallest number R can represent, which is stored in `Machine$double.xmin`\nres_thios_tb &lt;- res_thios_tb |&gt;\n  mutate(padj = ifelse(padj == 0, .Machine$double.xmin, padj))\n\n## add column where TRUE values denote padj values &lt; 0.05 and fold change &gt; 1.5 in either direction\nres_thios_tb &lt;- res_thios_tb |&gt; \n                  dplyr::mutate(threshold_OE = padj &lt; padj.cutoff & abs(log2FoldChange) &gt;= lfc.cutoff)\n\n## Add all the gene symbols as a column to our table using merge\nres_thios_tb &lt;- merge(res_thios_tb, annotation_table[,c(\"Symbol\", \"Locus.tag\")],\n                by.x = \"gene\", by.y = \"Locus.tag\", all.x = TRUE)\n\n\n\nExtract DE genes\nWe can extract significantly DE genes based on our cutoffs as follows:\n\n#extract significant DE genes \nsigOE &lt;- res_thios_tb |&gt;\n        filter(padj &lt; padj.cutoff & abs(log2FoldChange) &gt; lfc.cutoff)\n\n#count number of DE genes\ndim(sigOE)\n\n[1] 326   8",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/DSEq2.html#data-exploration",
    "href": "source/metatranscriptomics/DSEq2.html#data-exploration",
    "title": "Bioinformatics guidance page",
    "section": "Data exploration",
    "text": "Data exploration\n\nPrepare data frames\nNext, we export a normalized count table from our DeSeq object in order to generate some plots.\n\n#convert metadata matrix to tibble\ntibble_meta &lt;- metadata_table |&gt; \n              rownames_to_column(var=\"samplename\") |&gt; \n              as_tibble()\n\n#convert normalized DeSeq data to a data frame and transfer the row names to a new column\nnormalized_counts &lt;- counts(dds, normalized=T) |&gt; \n                     data.frame() |&gt;\n                     rownames_to_column(var=\"gene\") \n\n#add gene description from the annotation table\nnormalized_counts &lt;- merge(normalized_counts , annotation_table[,c(\"Locus.tag\", \"Symbol\")],\n              by.x = \"gene\", by.y = \"Locus.tag\")\n\n#create a tibble for the normalized counts\nnormalized_counts &lt;- normalized_counts |&gt;\n                     as_tibble()\n\nHere, counts:\n\nextracts the count data as a matrix of non-negative integer count values, one row for each observational unit (gene or the like), and one column for each sample\nnormalized indicates whether or not to divide the counts by the size factors or normalization factors\n\n\n\nPlot a single DE gene\nWe can start investigating genes by looking for their gene description, i.e. we might want to look at a single gene, aprA.\n\n# Find the gene id of a single gene of interest\ngene_of_interest &lt;- annotation_table[annotation_table$Symbol == \"aprA\", \"Locus.tag\"]\n\n# Plot expression for single gene\nplotCounts(dds, gene = gene_of_interest, intgroup=\"treatment\") \n\n\n\n\n\n\n\n\nTo customize this plot better, we can save the output of plotCounts() to a variable specifying the returnData=TRUE argument, then use ggplot():\n\n# Save plotcounts to a data frame object\nd &lt;- plotCounts(dds, gene = gene_of_interest, intgroup=\"treatment\", returnData = TRUE) \n\n# Plot\nggplot(d, aes(x = treatment, y = count, color = treatment)) + \n    geom_point(position=position_jitter(w = 0.1,h = 0)) +\n    geom_text_repel(aes(label = rownames(d))) + \n    ylab(\"normalized count\") +\n    theme_bw() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\nPlot several DE genes\nWe can also plot several genes, for example, we might investigate the 20 genes with the most significant p-values. Notice that you can easily change this by providing a vector that lists your genes of interest:\n\n#extract a vector with the Top20 genes (based on p-value)\ntop20_sigOE_genes &lt;- res_thios_tb |&gt; \n        arrange(padj) |&gt; \n        pull(gene) |&gt;       \n        head(n=20)  \n\n#extract the normalized counts for the genes of interest\ntop20_sigOE_norm &lt;- normalized_counts |&gt;\n        filter(gene %in% top20_sigOE_genes)\n\n#gather the columns to have normalized counts to a single column\ngathered_top20_sigOE &lt;- top20_sigOE_norm |&gt;\n  gather(colnames(top20_sigOE_norm)[2:7], key = \"samplename\", value = \"normalized_counts\")\n\n#add metadata \ngathered_top20_sigOE &lt;- inner_join(tibble_meta, gathered_top20_sigOE)\n\nJoining with `by = join_by(samplename)`\n\n## plot using ggplot2\nggplot(gathered_top20_sigOE) +\n        geom_point(aes(x = gene, y = normalized_counts, color = treatment)) +\n        scale_y_log10() +\n        xlab(\"Genes\") +\n        ylab(\"log10 Normalized Counts\") +\n        ggtitle(\"Top 20 Significant DE Genes\") +\n        theme_bw() +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n        theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\nHeatmap\nTo generate a graph showing the Z-scores for all significant genes we defined earlier we can run the code below. Z-scores are used to compare expression levels between samples and are calculated for each row in our dataframe (i.e. for each gene) by subtracting the mean and then dividing by the standard deviation. Thus these values allow us to compare a genes expression level in a given sample to the expression level of that gene across all samples.\n\n### Extract normalized expression for significant genes from the OE and control samples\nmat  &lt;- assay(rld_downstream)[sigOE$gene, ]\n\n### Set a color palette\nheat_colors &lt;- brewer.pal(6, \"YlOrRd\")\n\n### Run pheatmap using the metadata data frame for the annotation\npheatmap(mat, \n    #color = heat_colors, \n    cluster_rows = T, \n    show_rownames = F,\n    annotation = subset_metadata, \n    border_color = NA, \n    fontsize = 10, \n    scale = \"row\", \n    fontsize_row = 10, \n    height = 20)\n\n\n\n\n\n\n\n\nNOTE: There are several additional arguments that are included for aesthetics. One important one is scale=\"row\". For the pheatmap package, scale means that Z-scores are computed on a gene-by-gene basis by subtracting the mean (centering) and then dividing by the standard deviation (scaling). Without this we might not see much due to some of the very highly expressed genes.\nWe can also plot the amount by which each gene deviates in a specific sample from the gene’s average across all samples. Therefore, we can center each genes’ values across samples as follows:\n\nmat  &lt;- assay(rld_downstream)[ sigOE$gene, ]\nmat  &lt;- mat - rowMeans(mat)\n\npheatmap(mat, \n         annotation_col = subset_metadata,\n          show_rownames = F,\n          border_color = NA, \n          fontsize = 10, \n          fontsize_row = 10, \n          height = 20)\n\n\n\n\n\n\n\n\n\n\nVulcano plot\nAnother plot that helps view the data is a Vulcano plot, in which log transformed adjusted p-values are plotted on the y-axis and log2 fold change values on the x-axis as follows:\n\n## Volcano plot\nggplot(res_thios_tb) +\n    geom_point(aes(x = log2FoldChange, y = -log10(padj), colour = threshold_OE)) +\n    ggtitle(\"Thiosulfate expression\") +\n    xlab(\"log2 fold change\") + \n    ylab(\"-log10 adjusted p-value\") +\n    #scale_y_continuous(limits = c(0,3500)) +\n    theme(legend.position = \"none\",\n          plot.title = element_text(size = rel(1.5), hjust = 0.5),\n          axis.title = element_text(size = rel(1.25)))  \n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nA vulcano plot is a great way to get an overall picture of what is going on, but what if we also wanted to know where the top 20 genes (lowest padj) in our DE list are located on this plot? We could label those dots with the gene name on the Volcano plot using geom_text_repel().\nNotice, that the code below only adds a label for those genes that actually are annotated.\n\n## Create an empty column to indicate which genes to label\nres_thios_tb &lt;- res_thios_tb |&gt; dplyr::mutate(genelabels = \"\")\n\n## Sort by padj values \nres_thios_tb &lt;- res_thios_tb |&gt; dplyr::arrange(padj)\n\n## Populate the genelabels column with contents of the gene symbols column for the first 50 rows, i.e. the top 50 most significantly expressed genes\nres_thios_tb$genelabels[1:20] &lt;- as.character(res_thios_tb$Symbol[1:20])\n\n\n#plot\nggplot(res_thios_tb, aes(x = log2FoldChange, y = -log10(padj))) +\n    geom_point(aes(colour = threshold_OE)) +\n    geom_text_repel(aes(label = genelabels)) +\n    ggtitle(\"Thiosulfate versus thiocyanate expression\") +\n    xlab(\"log2 fold change\") + \n    ylab(\"-log10 adjusted p-value\") +\n    theme(legend.position = \"none\",\n          plot.title = element_text(size = rel(1.5), hjust = 0.5),\n          axis.title = element_text(size = rel(1.25))) \n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text_repel()`).\n\n\nWarning: ggrepel: 1 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\nAn alternative way to generate these plots is by using the EnhancedVolcano library:\n\nEnhancedVolcano(res_thios_tb,\n    lab = as.character(res_thios_tb$Symbol),\n    #lab = res_thios_tb$gene,\n    x = 'log2FoldChange',\n    y = 'pvalue',\n    title = 'thios versus thioc',\n    FCcutoff = 2,\n    pCutoff = 1e-05,\n    pointSize = 3.0,\n    labSize = 4.0,\n    colAlpha = 1,\n    #legendLabels=c('Not sig.','Log (base 2) FC','p-value',\n    #  'p-value & Log (base 2) FC'),\n    legendPosition = 'right',\n    legendLabSize = 16,\n    legendIconSize = 5.0)\n\nWarning: One or more p-values is 0. Converting to 10^-1 * current lowest\nnon-zero p-value...\n\n\n\n\n\n\n\n\n\nWe can label specific genes as follows:\n\nres_thios_tb &lt;- res_thios_tb |&gt;\n  mutate(symbol = ifelse(Symbol == \"\", \" \", Symbol))\n\ngenes_to_label_neg &lt;- res_thios_tb |&gt; \n        filter(log2FoldChange &lt; -0.5) |&gt;    \n        filter(symbol != \"\" ) |&gt; \n        filter(padj &lt;= 0.01) |&gt; \n        arrange(padj ) |&gt; \n        pull(symbol)  |&gt; \n        head(n=20)  \n\ngenes_to_label_pos &lt;- res_thios_tb |&gt; \n        filter(log2FoldChange &gt; 0.5) |&gt;     \n        filter(symbol != \"\" ) |&gt; \n        filter(padj &lt;= 0.01) |&gt; \n        arrange(padj ) |&gt; \n        pull(symbol)  |&gt; \n        head(n=20)  \n\ngenes_to_label &lt;- append(genes_to_label_neg,genes_to_label_pos)\n\np1 &lt;- EnhancedVolcano(res_thios_tb,\n    lab = res_thios_tb$Symbol,\n    x = 'log2FoldChange',\n    y = 'padj',\n    title = \"thios versus thioc\",\n    selectLab = genes_to_label,\n    FCcutoff = 2,\n    pCutoff = 1e-05,\n    pointSize = 3.0,\n    labSize = 4.0,\n    col=c('black', 'yellow', 'orange', 'red3'),\n    colAlpha = 1,\n    legendLabels=c('Not sig.','Log (base 2) FC','p-value',\n      'p-value & Log (base 2) FC'),\n    legendPosition = 'right',\n    legendLabSize = 16,\n    legendIconSize = 5.0) +\n  coord_flip()\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\np1\n\n\n\n\n\n\n\n#print\npdf(file = \"../../data/metatranscriptomics/MyPlot.pdf\", width = 20, height = 20)\np1\ndev.off()\n\npng \n  2",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "DeSeq2"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/ribodetector.html#ribodetector",
    "href": "source/metatranscriptomics/ribodetector.html#ribodetector",
    "title": "Bioinformatics guidance page",
    "section": "Ribodetector",
    "text": "Ribodetector\n\nIntroduction\nRiboDetector is a software developed to accurately yet rapidly detect and remove rRNA sequences from metagenomic, metatranscriptomic, and ncRNA sequencing data (Deng et al. 2022). It was developed based on LSTMs and optimized for both GPU and CPU usage to achieve a 10 times on CPU and 50 times on a consumer GPU faster runtime compared to the current state-of-the-art software. Moreover, it is very accurate, with ~10 times fewer false classifications. Finally, it has a low level of bias towards any GO functional groups.\nFor more information, check out the manual\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nRibodetector v0.3.1 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -p /zfs/omics/projects/bioinformatics/software/miniconda3/envs/ribodetector_0.3.1 -c bioconda ribodetector python=3.8\n\n\n\nUsage\nIn the example below, we use the CPU version of ribodetector. If you have access to GPUs, check out the manual for more information.\nTo run ribodetector, you need to provide it with the sequencing length, You can set the -l parameter to the mean read length if you have reads with variable length. The mean read length can be computed with seqkit stats.\n\nconda activate ribodetector_0.3.1\n\nribodetector_cpu -t 20 \\\n  -i sample_R1_trim.fastq.gz sampleR2_trim.fastq.gz \\\n  -l 138 \\\n  -e rrna \\\n  --chunk_size 256 \\\n  -o sampleR1_nonrrna.fastq.gz sample_R2_nonrrna.fastq.gz \\\n  --log sample.log\n\nconda deactivate\n\noptional arguments:\n\n-h, --help show this help message and exit\n-lLEN, --len LEN Sequencing read length (mean length). Note: the accuracy reduces for reads shorter than 40.\n-i [INPUT [INPUT …]],--input [INPUT [INPUT …]] Path of input sequence files (fasta and fastq), the second file will be considered as second end if two files given.\n-o [OUTPUT [OUTPUT …]], --output [OUTPUT [OUTPUT …]] Path of the output sequence files after rRNAs removal (same number of files as input). (Note: 2 times slower to write gz files)\n-r [RRNA [RRNA …]], --rrna [RRNA [RRNA …]] Path of the output sequence file of detected rRNAs (same number of files as input)\n-e {rrna,norrna,both,none}, --ensure {rrna,norrna,both,none} Ensure which classificaion has high confidence for paired end reads. norrna: output only high confident non-rRNAs, the rest are clasified as rRNAs; rrna: vice versa, only high confident rRNAs are classified as rRNA and the rest output as non-rRNAs; both: both non-rRNA and rRNA prediction with high confidence; none: give label based on the mean probability of read pair. (Only applicable for paired end reads, discard the read pair when their predicitons are discordant)\n-t THREADS, --threads THREADS number of threads to use. (default: 20)\n--chunk_size CHUNK_SIZE chunk_size * 1024 reads to load each time. When chunk_size=1000 and threads=20, consumming ~20G memory, better to be multiples of the number of threads..\n--log LOG Log file name\n-v, --version Show program’s version number and exit",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "Ribodetector"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/sortmerna.html#sortmerna",
    "href": "source/metatranscriptomics/sortmerna.html#sortmerna",
    "title": "Bioinformatics guidance page",
    "section": "SortMeRNA",
    "text": "SortMeRNA\n\nIntroduction\nSortMeRNA is a tool for the fast and accurate filtering of ribosomal RNAs in metatranscriptomic data.\n\nManual\nPaper: Please do not forget to cite the paper whenever you use the software (Kopylova, Noé, and Touzet 2012).\n\nAvailable on Crunchomics: No\n\n\nInstallation\nSortMeRNA can be easily installed with conda/mamba:\n\nmamba create -n sortmerna\nmamba install -n sortmerna -c conda-forge -c bioconda -c defaults sortmerna \n\n\n\nUsage\nSortMeRNA has many different options, so its best to also have a look at the manual. Below you find a quick example to try out.\n\nRequired input:\n\nA reference database\nSingle-end or paired-end FastQ files in the following formats: FASTA/FASTQ/FASTA.GZ/FASTQ.GZ\n\nSortmerna generates multiple output folders:\n\nkvdb/ key-value datastore for alignment results\nidx/ index database\nout/ output files: Here you find the fastq (or fastq.gz) files that aligned or not-aligned (depending in the used options) to the reference database.\n\nUseful arguments (not extensive, check manual for all arguments as well as use sortmerna -h as not all options are listed in the manual):\n\n-ref{PATH}: Reference file (FASTA) absolute or relative path. Can be used multiple times, once per a reference file, if working with more than one reference\n-reads {PATH}: Raw reads file. Use twice for files with paired reads\n-wordir {PATH}: Working directory for storing the Reference index, Key-value database, Output. Default location is USRDIR/sortmerna/run/\n-fastx: Output the reads that aligned to the reference database into FASTA/FASTQ file\n-other: Create output file for the non-aligned reads output file. Must be used with fastx\n--paired_out: Flags the paired-end reads as Non-aligned, when either of them is non-aligned\n--out2: Output paired reads into separate files. Must be used with fastx. If a single reads file is provided, this options implies interleaved paired reads\n--sout: Separate paired and singleton aligned reads. Must be used with fastx and Cannot be used with ‘paired_in’ | ‘paired_out’\n…\n\n\n\nExample code\nIn the example below we start with downloading a database provided by sortmerna into a db folder. When running this on your own, ensure that there are no newer releases or other databases that might be of interest to you. For us, this downloads the following databases:\n\nsmr_v4.3_default_db.fasta -&gt; bac-16S 90%, 5S & 5.8S seeds, rest 95% (benchmark accuracy: 99.899%)\nsmr_v4.3_sensitive_db.fasta -&gt; all 97% (benchmark accuracy: 99.907%) and thus the most complete database\nsmr_v4.3_sensitive_db_rfam_seeds.fasta -&gt; all 97%, except RFAM database which includes the full seed database sequences\n\nAfterwards, we run Sortmerna on an imaginary metatranscriptomic sample1 from which we have paired-end sequencing data that we beforehand cleaned with other tools, such as FastP. Since our goal is in this example to remove rRNA reads from metatranscriptomic data, we would continue working with the unaligned files. However, at the same time sortmerna can be used to extract and work further with rRNA reads if desired.\n\n#generate some folder to better structure the data \nmkdir dbs\nmkdir -p sortmerna/sample1\n\n#get rRNA db\nwget https://github.com/biocore/sortmerna/releases/download/v4.3.4/database.tar.gz\ntar -xvf database.tar.gz -C dbs/\nrm database.tar.gz \n\n#run sortmerna \nsortmerna \\\n  --ref dbs/smr_v4.3_sensitive_db.fasta \\\n  --reads data/sample1_forward_filtered.fq.gz \\\n  --reads data/sample1_reverse_filtered.fq.gz \\\n  --fastx --other --out2 --paired_out \\\n  --workdir sortmerna/sample1 \\\n  --threads 20 -v \n\n\n\n\nReferences",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "SortMeRNA"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/stringtie.html#stringtie",
    "href": "source/metatranscriptomics/stringtie.html#stringtie",
    "title": "Bioinformatics guidance page",
    "section": "StringTie",
    "text": "StringTie\n\nIntroduction\nStringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts (Pertea et al. 2015). It uses a novel network flow algorithm as well as an optional de novo assembly step to assemble and quantitate full-length transcripts representing multiple splice variants for each gene locus. Its input can include not only alignments of short reads that can also be used by other transcript assemblers, but also alignments of longer sequences that have been assembled from those reads. In order to identify differentially expressed genes between experiments, StringTie’s output can be processed by specialized software like Ballgown, Cuffdiff or other programs (DESeq2, edgeR, etc.).\nFor a full set of options, please visit the manual.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nStringTie v3.0.0 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name stringtie_3.0.0 -c bioconda -c conda-forge stringtie=3.0.0\n\n\n\nUsage\nMain input:\n\na SAM, BAM or CRAM file with RNA-Seq read alignments sorted by their genomic location. For unsorted data, you can run samtools sort -o alnst.sorted.bam alns.sam\n\nAny SAM record with a spliced alignment (i.e. having a read alignment across at least one junction) should have the XS tag (or the ts tag, see below) which indicates the transcription strand, the genomic strand from which the RNA that produced the read originated. TopHat and HISAT2 alignments already include this tag, but for other read mappers one should check that this tag is also included for spliced alignment records. For example the STAR aligner should be run with the option --outSAMstrandField intronMotif in order to generate this tag.\nThe XS tags are not necessary in the case of long RNA-seq reads aligned with minimap2 using the -ax splice option. minimap2 adds the ts tags to spliced alignments to indicate the transcription strand (albeit in a different manner than the XS tag) and StringTie can make use of the ts tag as well if the XS tag is missing.\n\nOptionally, you can also provide a reference annotation file (in GTF or GFF3 format) to guide the assembly process. The output will include expressed reference transcripts as well as any novel transcripts that are assembled. This option is required by options -B, -b, -e and -C\n\n\nExample usage: Assemble with StringTie\n\nconda activate stringtie_3.0.0\n\n# Assemble transcripts with a reference gtf (useful when investigating alternative splicing)\n# When working with multiple samples, run this for each sample, for example in a for loop.\nstringtie sample1_sorted.bam \\\n    -G my_genome.gff3 \\\n    --rf -p 8 -v --conservative \\\n    -o results/${sample_id}_stringtie_assembly.gtf\n\nconda deactivate\n\nUsed settings (for a full list of settings, visit the manual):\n\n-G &lt;ref_ann.gff&gt;: Use a reference annotation file (in GTF or GFF3 format) to guide the assembly process.\n--rf: Assumes a stranded library fr-firststrand/RF/reverse stranded. For secondstrand libraries use --fr. Omit for unstranded data\n-p &lt;int&gt;: Specify the number of processing threads (CPUs) to use for transcript assembly. The default is 1.\n-v: Turns on verbose mode, printing bundle processing details.\n--conservative: Assembles transcripts in a conservative mode. Same as -t -c 1.5 -f 0.05\n\n-t: This parameter disables trimming at the ends of the assembled transcripts. By default StringTie adjusts the predicted transcript’s start and/or stop coordinates based on sudden drops in coverage of the assembled transcript.\n-c &lt;float&gt;: Sets the minimum read coverage allowed for the predicted transcripts. A transcript with a lower coverage than this value is not shown in the output. Default: 1\n-f &lt;0.0-1.0&gt;: Sets the minimum isoform abundance of the predicted transcripts as a fraction of the most abundant transcript assembled at a given locus. Lower abundance transcripts are often artifacts of incompletely spliced precursors of processed transcripts. Default: 0.01\n\n\nMain output:\n\na GTF file containing the structural definitions of the transcripts assembled by StringTie from the read alignment data\n\n\n\nExample usage: Merge transcripts with StringTie\nTranscript merge mode is a special usage mode of StringTie, distinct from the assembly usage mode described above. In the merge mode, StringTie takes as input a list of GTF/GFF files and merges/assembles these transcripts into a non-redundant set of transcripts. This mode is used in the new differential analysis pipeline to generate a global, unified set of transcripts (isoforms) across multiple RNA-Seq samples.\n\nconda activate stringtie_3.0.0\n\n# Generate a list of all generated gtf files\nls results/*gtf &gt; gtf_list.txt\nwc -l results/gtf_list.txt\n\n# Merge individual gtf files \nstringtie --merge -p 8 -g 100 -f 0.05 -G my_genome.gff3 \\\n    -o results/merged_transcripts.gtf \\\n    results/gtf_list.txt\n\nconda deactivate\n\nSettings:\n\n-G &lt;guide_gff&gt;: reference annotation to include in the merging (GTF/GFF3)\n-o &lt;out_gtf&gt;: output file name for the merged transcripts GTF (default: stdout)\n-m &lt;min_len&gt;: minimum input transcript length to include in the merge (default: 50)\n-c &lt;min_cov&gt;: minimum input transcript coverage to include in the merge (default: 0)\n-F &lt;min_fpkm&gt;: minimum input transcript FPKM to include in the merge (default: 0)\n-T &lt;min_tpm&gt;: minimum input transcript TPM to include in the merge (default: 0)\n-f &lt;min_iso&gt;: minimum isoform fraction (default: 0.01)\n-i: keep merged transcripts with retained introns (default: these are not kept unless there is strong evidence for them)\n-l &lt;label&gt;: name prefix for output transcripts (default: MSTRG)",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "StringTie"
    ]
  },
  {
    "objectID": "source/metatranscriptomics/trinity.html#trinity",
    "href": "source/metatranscriptomics/trinity.html#trinity",
    "title": "Bioinformatics guidance page",
    "section": "Trinity",
    "text": "Trinity\n\nIntroduction\nTrinity is a tool to assemble transcript sequences from Illumina RNA-Seq data de novo (without a reference genome). Additionally, it comes with several scripts that can be used to compare replicates, identify differentially expressed genes or functional annotation.\n\nManual\nPaper: Please do not forget to cite the paper whenever you use the software (Grabherr et al. 2011).\n\nAvailable on Crunchomics: No\n\n\nInstallation\nTrinity can be easily installed with conda/mamba:\n\nmamba create -n trinity\nmamba install -n trinity -c bioconda trinity \n\n\n\nUsage\nTrinity can not only be used to assemble reads but also do many down-stream analyses. Since its not the scope of this page to give an in-depth overview about these functionalities, we recommend that an in-depth look at the manual.\n\nRequired input:\n\nPaired-reads (fa or fq)\nSingle-reads (fa or fq)\nNotice: Trinity performs best with strand-specific data, in which case sense and antisense transcripts can be resolved\n\nGenerated outputs:\n\nTrinity.fasta: An assembled transcriptome\n\n\nRecommendations:\n\nA basic recommendation is to have 1G of RAM per 1M pairs of Illumina reads in order to run some of the steps in the workflow\nThe entire process can require ~1 hour per million pairs of reads\nMost (not all) parts of Trinity are parallelized. It therefore, makes sense to use most available CPUs and also due to high mem requirement to run such a job on a complete node\n\nPossible settings to adjust when dealing with deeply sequenced data:\n\n--min_kmer_cov 2 (singleton K-mers will not be included in initial Inchworm contigs)\nPerform K-mer based insilico read set normalization (–-normalize_max_read_cov 50), this is adjusted compared to the default of 200 , see figure 4 for some benchmarking. As you see applying this coverage thresholds can easily result in ~70% of read reduction. Notice, that some genes may be missed when reads are removed\n--JM allows the user to control the amount of RAM used during Jellyfish kmer counting\n\n\n\nExample code\nTrinity can do a lot of things and the goal of this page is not to go through everyone of them, for this, visit the manual. However, below you find the key commands to generate the assembly, get some quality assessment scores and identify differentially expressed genes\n\nAssemble the reads\nIn a first example, we work with a single sample and have paired-end reads.\n\nTrinity --seqType fq --max_memory 50G \\\n         --left sample1_f.fq.gz  --right sample1_r.fq.gz --CPU 6\n\nMore often you however will work with multiple samples, for example you might work with 6 samples: 3 replicates for control conditions and three replicates for sulfur-treatment. Trinity can work with multiple samples by using a text file that gives the tool all relevant information. For example in our case samples.txt provides the condition, the replicates, the location of the forward reads and the location of the reverse reads in a tab-delimited file:\nC   C_rep1  sortmerna/C_rep1/other_fwd.fq.gz    sortmerna/C_rep1/other_rev.fq.gz\nC   C_rep2  sortmerna/C_rep2/other_fwd.fq.gz    sortmerna/C_rep2/other_rev.fq.gz\nC   C_rep3  sortmerna/C_rep3/other_fwd.fq.gz    sortmerna/C_rep3/other_rev.fq.gz\nS   S_rep1  sortmerna/S_rep3/other_fwd.fq.gz    sortmerna/S_rep1/other_rev.fq.gz\nS   S_rep2  sortmerna/S-3C/out/other_fwd.fq.gz  sortmerna/S_rep1//other_rev.fq.gz\nS   S_rep3  sortmerna/S-5C/out/other_fwd.fq.gz  sortmerna/S_rep3/other_rev.fq.gz\nOnce we have this file, we can run the assembly as follows:\n\n#make folders for better file organization\nmkdir trinity_output \n\n#generate an assembly\n#using min_kmer_cov and normalize_max_read_cov can be useful for large assemblies\nTrinity \\\n  --seqType fq \\\n  --samples_file samples.txt \\\n  --max_memory 50G \\\n  --CPU 6 \\\n  --output trinity_output\n\n\n\nAssess the assembly quality\nTo assess the quality, we can use a script that comes with Trinity as follows:\n\nTrinityStats.pl \\\n   trinity_output/Trinity.fasta \\\n  &gt; trinity_output/trinity_stats.txt\n\nThe output of trinity_output/trinity_stats.txt might look something like this:\n- Total trinity 'genes':  931,787\n- Total trinity transcripts:      1,130,657\n- Percent GC: 48.25\n\n- Stats based on ALL transcript contigs::\n  - Contig N10: 3008\n  - Contig N20: 1492\n  - Contig N30: 1065\n  - Contig N40: 799\n  - Contig N50: 607 ***might be a bit short\n  - Median contig length: 302\n  - Average contig: 491.91\n  - Total assembled bases: 556178948\n\n- Stats based on ONLY LONGEST ISOFORM per 'GENE:\n  - Contig N10: 1480\n  - Contig N20: 1000\n  - Contig N30: 738\n  - Contig N40: 561\n  - Contig N50: 434\n  - Median contig length: 276\n  - Average contig: 406.06\n  - Total assembled bases: 378358462\nIf the N50 statistics falls in the right area expected for a gene (about 1000-1,500), then N50 can be used as a rough check on overall “sanity” of the transcriptome assembly.\n\n\nAlign reads back to the transcriptome\nWe will next use some code to ask how many of our reads can be mapped back to our transcriptome. The results can be used to construct an expression matrix as well as estimate how many reads map to our assembly.\nThere are three options we can use to align our reads to the de novo assembly: bowtie, kallisto and salmon. Due to its speed we will use salmon here but feel free to have a look at the other methods as well by checking out the manual.\nThe command below will print some useful information to the screen. In order to capture this in a file instead we redirect this output by using &&gt; logs/salmon.info.\n\n#get a folder for better organization \nmkdir logs\n\n#run the pseudo-alignment with salmon\nalign_and_estimate_abundance.pl \\\n  --transcripts trinity_output/Trinity.fasta \\\n  --seqType fq \\\n  --thread_count 10 \\\n  --samples_file samples.txt \\\n  --est_method salmon \\\n  --trinity_mode --prep_reference \\\n  --output_dir trinity_output/salmon &&gt; logs/salmon.info\n\nIf you check logs/salmon.info you will see how many of our reads mapped back to the transcriptome. A typical “good” assembly has ~80% reads mapping to the assembly and ~80% are properly paired.\n\n\nBuild expression matrix\nNext, we use the results from aligning our reads to the transcriptome to build an expression matrix.\nNotice, that we talk about isoforms when we talk about transcripts and genes when talking about genes. Depending on what you want to look at you can most of the following steps on either the transcripts or genes. The examples below will only be looking at the transcripts.\n\n#make files containing a list of all the target files\nls trinity_output/salmon/*/quant.sf &gt; isoform-file-paths.txt\nls trinity_output/salmon/*/quant.sf.genes &gt; gene-file-paths.txt\n\n#make matrices for transcripts\nabundance_estimates_to_matrix.pl \\\n            --est_method salmon \\\n            --out_prefix trinity_output/salmon \\\n            --gene_trans_map trinity_output/trinity.Trinity.fasta.gene_trans_map \\\n            --name_sample_by_basedir \\\n            --quant_files isoform-file-paths.txt\n\nThe abundance_estimates_to_matrix script generates the following files:\n\nsalmon.[gene|isoform].counts.matrix: the estimated RNA-Seq fragment counts (raw counts). This file is used for downstream analyses of differential expression.\nsalmon.[gene|isoform].TMM.EXPR.matrix: a matrix of TPM expression values (not cross-sample normalized). This file is used as the gene expression matrix in most other analyses\nsalmon.[gene|isoform].TPM.not_cross_norm : a matrix of TMM-normalized expression values\n\n\n\nQuality check your samples and biological replicates\nThe PtR script can be used to check the variation across samples and replicates. This is useful to do to ensure that your replicates are more similar to each other compared to any treatments.\n\n\n\n\n\n\nWarning\n\n\n\nThe ptr script might run into an error “‘length = 2’ in coercion to ‘logical(1)’”, this has to do with an incompatibility with a script with a newer R version, if that happens, you need to make the following changes in one of the script. The script will be found in the location trinity will be installed with and should be something like \\$HOME/personal/mambaforge/envs/metatranscriptomics/opt/trinity-2.15.1/Analysis/DifferentialExpression/R/heatmap.3.R. You can edit this scripts according to these instructions.\nAlternatively, you can install R v4.2.2 in metatranscriptomic environment (not tested myself).\n\n\nLet’s first see how to compare replicates:\n\nPtR --matrix trinity_output/salmon.gene.counts.matrix \\\n    --samples &lt;(awk '{print $1\"\\t\"$2}' samples.txt) \\\n    --log2 --CPM \\\n    --min_rowSums 10 \\\n    --compare_replicates\n\n#view the files\n#since the files are large they likely will not open, so best transfer to your own computer\ndisplay N.rep_compare.pdf\ndisplay S.rep_compare.pdf\n\nWe can also compare replicates across all samples. Run PtR to generate a correlation matrix for all sample replicates like so:\n\nPtR \\\n  --matrix trinity_output/salmon.isoform.counts.matrix \\\n  --min_rowSums 10 \\\n  -s &lt;(awk '{print $1\"\\t\"$2}' samples.txt) \\\n  --log2 --CPM --sample_cor_matrix \n\nAs before you can use display to view the files. Ideally, we want to see that replicates are more highly correlated within samples than between samples.\nAnother important analysis method to explore relationships among the sample replicates is Principal Component Analysis (PCA). You can generate a PCA plot showing the first 3 principal components like so:\n\nPtR \\\n    --matrix trinity_output/salmon.isoform.counts.matrix \\\n    -s &lt;(awk '{print $1\"\\t\"$2}' samples.txt) \\\n    --min_rowSums 10 --log2 \\\n    --CPM --center_rows \\\n    --prin_comp 3 \n\nTo keep things organized, lets move these pdfs into a new folder:\n\nmkdir trinity_output/plots\nmv *pdf trinity_output/plots\nrm salmon*\n\n\n\nDifferential Expression analysis\nThe script below will perform pairwise comparisons among each of your sample types. To analyze transcripts, use the ‘transcripts.counts.matrix’ (or isoform.counts.matrix in later software versions) file. To perform an analysis at the ‘gene’ level, use the ‘genes.counts.matrix’.\nTrinity comes with three methods for this analysis: edgeR, DESeq2 and voom. View the manual for more information about each of these approaches.\n\n#look at differential expression  for transcripts\nrun_DE_analysis.pl \\\n        --matrix trinity_output/salmon.isoform.counts.matrix \\\n        --method edgeR \\\n        --samples_file samples.txt \\\n        --output trinity_output/edgeR-transcript\n\n#view vulcano plots\ndisplay trinity_output/edgeR-transcript/salmon.gene.counts.matrix.N_vs_S.edgeR.DE_results.MA_n_Volcano.pdf\n\ndisplay trinity_output/edgeR-transcript/salmon.isoform.counts.matrix.N_vs_S.edgeR.DE_results.MA_n_Volcano.pdf\n\nThis will generate some information including differentially expressed genes. The output will look something like this:\n\n${prefix}.sampleA_vs_sampleB.${method}.DE_results : the DE analysis results,\nincluding log fold change and statistical significance (see FDR column).\n${prefix}.sampleA_vs_sampleB.${method}.MA_n_Volcano.pdf : MA and Volcano plots\nfeatures found DE at FDR &lt;0.05 will be colored red. Plots are shown\nwith large (top) or small (bottom) points only for choice of aesthetics.\n${prefix}.sampleA_vs_sampleB.${method}.Rscript : the R-script executed to perform the DE analysis.\n\nNow that we did this, we can compare our samples and identify differentially expressed features as follows:\n\n#go into the edge R output folder \ncd trinity_output/edgeR-transcript\n\n#extract differentially expressed genes\nanalyze_diff_expr.pl \\\n    --matrix ../salmon.gene.counts.matrix  \\\n    --samples ../../samples.txt \\\n    -P 0.001 \\\n    -C 2\n\n#view results \ndisplay diffExpr.P0.001_C2.matrix.log2.centered.sample_cor_matrix.pdf\ndisplay diffExpr.P0.001_C2.matrix.log2.centered.genes_vs_samples_heatmap.pdf\n\nOptions:\n\n-P &lt;float&gt;: p-value cutoff for FDR (default: 0.001)\n-C &lt;float&gt;: min abs(log2(a/b)) fold change (default: 2 (meaning 2^(2) or 4-fold)).\n\nBy default, each pairwise sample comparison will be performed. If you want to restrict the pairwise comparisons, provide the list of the comparisons to perform to the --contrasts parameter.\nIn this output directory, you’ll find the following files for each of the pairwise comparisons performed:\n\n${prefix}.sampleA_vs_sampleB.voom.DE_results.P0.001_C2.sampleA-UP.subset : the expression matrix subset\nfor features up-regulated in sampleA\n${prefix}.sampleA_vs_sampleB.voom.DE_results.P0.001_C2.sampleB-UP.subset : the expression matrix subset\nfor features up-regulated in sampleB\ndiffExpr.P0.001_C2.matrix.log2.dat : All features found DE in any of these pairwise comparisons\nconsolidated into a single expression matrix:\ndiffExpr.P0.001_C2.matrix.log2.sample_cor.dat : A Pearson correlation matrix for pairwise sample comparisons\nbased on this set of DE features.\ndiffExpr.P0.001_C2.matrix.log2.sample_cor_matrix.pdf : clustered heatmap showing the above sample correlation matrix.\ndiffExpr.P0.001_C2.matrix.log2.centered.genes_vs_samples_heatmap.pdf : clustered heatmap of DE genes vs. sample replicates.\n\n\n\n\nReferences",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)transcriptomics",
      "Trinity"
    ]
  },
  {
    "objectID": "source/nanopore/chopper.html#chopper",
    "href": "source/nanopore/chopper.html#chopper",
    "title": "Bioinformatics guidance page",
    "section": "Chopper",
    "text": "Chopper\n\nIntroduction\nChopper is a tool for quality filtering of long read data. It is a Rust implementation of two other tools for long-read quality filtering, NanoFilt and NanoLyse, both originally written in Python. This tool, intended for long read sequencing such as PacBio or ONT, filters and trims a fastq file (De Coster and Rademakers 2023).\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nChopper v0.8 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name chopper -c bioconda chopper\n\n\n\nUsage\nRequired input:\n\nFASTQ files\n\nOutput:\n\nFASTQ files\n\nExample usage:\n\nconda activate chopper0.8.0\n\ngunzip -c results/porechop/my_reads.fastq.gz |\\\n    chopper -q 10 \\\n    --headcrop 0 --tailcrop 0  \\\n    -l 1000 \\\n    --threads 20 |\\\n    gzip &gt; results/chopper/my_reads_filtered1000.fastq.gz\n\nconda deactivate\n\nUseful arguments:\n\n--headcrop Trim N nucleotides from the start of a read [default: 0]\n--maxlength Sets a maximum read length [default: 2147483647]\n-l, --minlength Sets a minimum read length [default: 1]\n-q, --quality Sets a minimum Phred average quality score [default: 0]\n--tailcrop Trim N nucleotides from the end of a read [default: 0]\n--threads Number of parallel threads to use [default: 4]\n--contam Fasta file with reference to check potential contaminants against [default None]}\n\n\n\nReferences",
    "crumbs": [
      "Sequence data analyses",
      "Nanopore analyses",
      "Chopper"
    ]
  },
  {
    "objectID": "source/nanopore/nanoITS.html#nanoits",
    "href": "source/nanopore/nanoITS.html#nanoits",
    "title": "Bioinformatics guidance page",
    "section": "NanoITS",
    "text": "NanoITS\n\nIntroduction\nNanoITS is a classifier for long-read Oxford Nanopore data of the eukaryotic 18S/SSU-ITS1/2 operon.\nWhen giving the tool some nanopore long-read data it will:\n\nProvide a quality report of the raw reads\nCheck the reads for adapters and barcodes and if present trim the reads\nRemove low-quality and short reads\nProvide a quality report for the cleaned reads\nIdentify and separate the ITS1, ITS2 and 18S rRNA gene using the Uniprot and Silva database, respectively\nClassify the SSU, ITS2 and/or ITS1 gene using kraken2 and/or minimap2\nGenerate taxonomic barplots and OTU tables\n\nFor a more detailed explanation, check out the manual.\nBelow you can find the full workflow:\n\n\n\nQuick start\nTo run NanoITs, install snakemake via conda and the clone the directory from github via:\n\nmamba create --name snakemake_f_NanoITS -c conda-forge -c bioconda snakemake=7.32.4 python=3.11.6 tabulate=0.8\n\ncd &lt;path_to_install_software&gt;\ngit clone https://github.com/ndombrowski/NanoITS.git\nmv NanoITS/ NanoITS_0.4\n\nProvide your sample names and path to the samples as a comma-separated file, for example, a file looking similar as the one provided in example_files/mapping.csv. Sample names should be unique and consist of letter, numbers and - only. The barcode column can be left empty as it is not yet implemented. The path should contain the path to your demultiplexed, compressed fastq file.\nAdjust config/config.yaml to configure the location of your mapping file as well as specify the parameters used by NanoITs.\nNanoITs can then be run with:\n\nconda activate snakemake_f_NanoITS\n\nsnakemake --use-conda --cores &lt;nr_cores&gt; \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile \\\n  --configfile config/config.yaml \\\n  --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  \\\n  --rerun-incomplete --nolock \n\nFor a more detailed explanation, check out the manual.\n\n\nNanoITS on Crunchomics\nNanoITS is installed on the UvA crunchomics HPC. If you have access to crunchomics you can be added to the amplicomics share in which NanoITS is set up. To be added, please send an email with your Uva netID to Nina Dombrowski.\nTo be able to use software installed on the amplicomics share, you first need to ensure that conda is installed. If it is, then you can run the following command:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\nThis command will add pre-installed conda environments in the amplicomics share to your conda env list. After you run conda env list you should see several tools from amplicomics, including QIIME2 and Snakemake. Snakemake is what we need to run NanoITS.\nNext, you can setup your working environment. Change the path of the working directory to wherever you want to analyze your data:\n\n#define the directory in which you want to run your analyses\nwdir=\"/home/$USER/personal/my_folder\"\n\n#go into the working directory\ncd $wdir\n\n\nPreparing the configuration file\nGet the config.yaml file:\n\n#copy config yaml (comes with NanoITS) \ncp /zfs/omics/projects/amplicomics/bin/NanoITS_0.4/config/config.yaml .\n\nThe config.yaml tells NanoITS how you want to run the software and how to find the sequencing data. Start by opening the config.yaml with an editor, such as nano. There are several things you can modify:\nThe project name\nYou can provide the project name in project: \"run_v1\". Your results will be generated in the folder you start the snakemake workflow in and the results will be generated in results/&lt;project_name&gt; (results/run_v1 if you use the default settings). Your project name can contain letters, numbers, _ and -. Do not use other symbols, such as spaces or dots.\nThe mapping file\nHere, you need to provide the path to a comma-separated mapping file that describes the samples you want to analyse, i.e. samples_file: \"input/mapping.csv\". In this example, the mapping file should be located in the input folder. If the mapping file is in your working directory, use samples_file: \"mapping.csv\".\nThe classifiers to use\nYou can choose what classifiers you want to use in classifiers: [\"minimap2\", \"kraken2\"]. Currently, two classifiers are implemented: (a) the alignment-based classifier minimap2 and (b) the kmer-based classifier kraken2. You can use both or either of the two classifiers.\nThe markers to investigate\nYou can select what markers you want to analyse in markers: [\"SSU\", \"ITS1\", \"ITS2\"]. The workflow was developed for primers targeting both the SSU, ITS1 and ITS2 but the workflow will also run for either option selected and we plan to in the future extend the workflow to also accept the LSU.\nOther parameters\nFinally, you can change tool specific parameters: If desired, there are several parameters that can be changed by the user, such as the numbers of threads to use, the settings for the read filtering or the classification and so on. The configuration file provides more information on each parameter.\nTo save and close the nano screen:\n\nPress Ctr+x\nType Y to save\nPress enter to save the changes without changing the file name\n\n\n\nPreparing the mapping file\nNext, you need to write a mapping file with nano mapping.csv. This file gives each sample a unique name and also tells NanoITS were to find the sequencing data. Ensure that were you generate the mapping file is consistent with the location that you gave in the config.yaml.\nThe mapping file should look something like this and needs to contain the following columns:\n\nsample: The names of your sample. This id will be used to label all files created in subsequent steps. Your sample names should be unique and only contain letters, numbers and -. Do not use other symbols, such as spaces, dots or underscores in your sample names.\nbarcode: The barcode ID. Can be empty as it is not actively used in the workflow as of now\npath: Path to the fastq.gz files. You can provide the relative path (i.e. relative to the working directory you start the snakemake workflow in) or absolute path (i.e. the location of a file or directory from the root directory(/)). The workflow accepts one file per barcode, so if you have more than one file merge these files first, for example using the cat command.\n\nsample,barcode,path\nbc01,barcode01,data/barcode01.fastq.gz\nbc02,barcode02,data/barcode02.fastq.gz\nbc03,barcode03,data/barcode03.fastq.gz\nTo save and close the nano screen:\n\nPress Ctr+x\nType Y to save\nPress enter to save the changes without changing the file name\n\n\n\nPreparing a dry-run\nTo test whether the workflow is defined properly do a dry-run first. You don’t need to change anything if you use the Crunchomics install. If you have installed the software on your own system, ensure that you:\n\nProvide the path to where you installed NanoITS afer --s\nProvide the path to the edited config file after --configfile\nProvide the path to where you want snakemake to install all program dependencies after --conda-prefix. We recommend to install these into the folder in which you downloaded NanoITS but you can change this if desired\n\n\n#activate conda environment with your snakemake installation, i.e. \nconda activate snakemake_f_NanoITS\n\n#test if everything runs as extended (edit as described above)\nsnakemake --use-conda --cores 1 \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoITS_0.4/workflow/Snakefile \\\n  --configfile config.yaml \\\n  --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoITS_0.4/workflow/.snakemake/conda  \\\n  -np \n\nIf the above command works and you see This was a dry-run (flag -n). The order of jobs does not reflect the order of execution. all is good and you can submit things to a compute node.\n\n\nSubmit a sbatch job\nThe NanoITS installation comes with an example sbatch script that you can move over to where you do your analyses. This script allows you to submit the job to the compute nodes.\n\n#cp template batch script (comes with NanoITS) \ncp /zfs/omics/projects/amplicomics/bin/NanoITS_0.4/jobscript.sh .\n\nThe jobscript is setup to work with NanoITS in the amplicomics environment and you should not need to change anything if your config.yaml is set up correctly.\nThe script does the following:\n\nRequests 20 crunchomics cpus and 200G of memory. One can set this lower, but for testing purposes you might keep these values. The can be changed with #SBATCH --cpus-per-task=20 and #SBATCH --mem=200. Depending on the size of the dataset you can try increasing the number of CPUs.\nEnsures that conda is loaded properly by running source ~/.bashrc. The bashrc is a script executed once a user logs in and holds special configurations, such as telling the shell where conda is installed. If you run into issues that are related to conda/mamba not being found then open the this file with nano ~/.bashrc and check that you have a section with text similar to # &gt;&gt;&gt; conda initialize &gt;&gt;&gt;. If this is not present you might need to run conda init bash to add it.\nActivates the snakemake conda environment that is found in the amplicomics share with conda activate snakemake_f_NanoITS\nRuns NanoITS. When using the script as is, the script assumes that NanoITS is installed at this path /zfs/omics/projects/amplicomics/bin/NanoITS_0.4 and that the config.yaml is located in the folder from which you analyse the data. If that is not the case, change the paths accordingly\n\nNext, you can submit the script with:\n\nsbatch jobscript.sh\n\nThis will generate a log file that should something like this NanoITS-ndombro-50633-4294967294.log. You can check the progress with tail NanoITS-ndombro-50633-4294967294.log. If you see a comment that your job finished, then NanoITS finished successfully.\nIn the results folder, you will find different files generated for each step, the most useful ones are the otu_table.txt in the tables folder or the fasta files for the extracted SSU and ITS1 region. If you run into errors its worth inspecting the other files as well. A full list of generated output files can be found here.\nAdditionally, you can generate a small report as follows. To view the report, download the report.html to your own computer and view it in your favorite browser.\n\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoITS_0.4/workflow/Snakefile\n\n\n\n\nCommon Issues and Solutions\n\nIssue 1: My run stops with an error while running kraken or minimap2\n\nSolution 1: Check the output in the itsx folder for each sample and ensure that ITS1_final.fasta and SSU_final.fasta are not empty. The classifiers with crash if that is the case. If this happens you could try lowering the min_its_length in the Separate ITS1 and SSU or change markers:             [\"SSU\", \"ITS1\"] to only those markers that were successfully separated\n\nIssue 2: My run stops with an error when generating the generate_barplots.\n\nSolution 1: Check that the merged.outmat.tsv OTU tables in results/run_v1/classification/kraken2 and minimap2 are not empty.\n\nIssue 3: My run stops at the first step\n\nSolution 3: If NanoITS stops at the beginning and does not even read in the data that in most of the cases has to do with errors in the mapping or config file. Ensure that\n\nYou have unique IDs in the mapping file and that you don’t use any special symbol\nEnsure that the path to your sequencing data is correct (you can check the path with ls)\nEnsure that the format of the config file is still intact. I.e. do not remove any quotes or brackets around the parameters (i.e. project: \"run_v1\", classifiers:         [\"minimap2\", \"kraken2\"]) and keep the extra spaces in front of some parameters as for example in the filtering section. These syntax elements are important for the script to work",
    "crumbs": [
      "Sequence data analyses",
      "Fungal genomics",
      "NanoITS"
    ]
  },
  {
    "objectID": "source/nanopore/nanophase_how_to.html#introduction",
    "href": "source/nanopore/nanophase_how_to.html#introduction",
    "title": "Nanophase",
    "section": "Introduction",
    "text": "Introduction\nNanophase is an easy-to-use pipeline to generate reference-quality MAGs using only Nanopore long reads (long-read-only strategy) or both Nanopore long and Illumina short reads (hybrid strategy) from complex metagenomes (Liu et al. 2022). Since nanophase v0.2.0, it also supports to generate reference-quality genomes from bacterial/archaeal isolates (long-read-only or hybrid strategy). If nanophase is interrupted, it will resume from the last completed stage.\nNotice that nanophase does not allow for a separate assembly of multiple samples while at the same time making use of depth information from all of those samples. It is however possible to run nanophase independently on multiple samples or consider a co-assembly. However, “co-assembly of a large number of metagenomes that contain very closely related populations often hinders confident assignments of shared contigs into individual bins” and should be avoided for distinct samples (Shaiber and Eren 2019).\nSome more considerations on whether your samples can be used for a co-assembly or not can be found here and here.\nNanophase uses the following tools, please consider citing them as well when using this tool:\n\nflye\nmetabat2\nmaxbin2\nSemiBin\nmetawrap\ncheckm\nracon\nmedaka\npolypolish\nPOLCA\nbwa\nseqtk\nminimap2\nBBMap\nparallel\nperl\nsamtools\ngtdbtk\nfastANI\nblastp",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Nanophase"
    ]
  },
  {
    "objectID": "source/nanopore/nanophase_how_to.html#installation",
    "href": "source/nanopore/nanophase_how_to.html#installation",
    "title": "Nanophase",
    "section": "Installation",
    "text": "Installation\nNanophase is installed on Crunchomics amplicomics share. If you want access please send an email to n.dombrowski@uva.nl. After you got access, you can add the amplicomics conda environments (in which nanophase is installed) as follows:\n\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\nIf you want to install nanophase yourself, you can follow the steps below. Beware, that the GTDB database is quite large and requires ~80GB of space.\n\n#add necessary channels\nconda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda\n\n#install environment in the amplicomics share\nmamba create -p /zfs/omics/projects/amplicomics/miniconda3/envs/nanophase_0.2.3 -c nanophase nanophase -y\n\n## download database: May skip if you have done before or GTDB and PLSDB have been downloaded in the server\n#exhange the paths to where you want to download the data\ncd /path_to_gtdb_folder\nwget https://data.gtdb.ecogenomic.org/releases/latest/auxillary_files/gtdbtk_data.tar.gz && tar xvzf gtdbtk_data.tar.gz\n\ncd /path_to_plsdb_folder\nwget https://ccb-microbe.cs.uni-saarland.de/plsdb/plasmids/download/plsdb.fna.bz2 && bunzip2 plsdb.fna.bz2\n\n#activate env\nconda activate nanophase_0.2.3\n\n## setting locations for databases\n## change /path_to_gtdb_folder/release_xxx to the real location where you stored the GTDB\n## ensure that the release version number is changed to the release version number that was downloaded\necho \"export GTDBTK_DATA_PATH=/path_to_gtdb_folder/release_xxx\" &gt; $(dirname $(dirname `which nanophase`))/etc/conda/activate.d/np_db.sh\n\n## Change /path/to/plsdb.fna to the real location where you stored the PLSDB\necho \"export PLSDB_PATH=/path_to_plsdb_folder/plsdb.fna\" &gt;&gt; $(dirname $(dirname `which nanophase`))/etc/conda/activate.d/np_db.sh\n\n#confirm that all packages have been installed\nnanophase check\n\n##restart environment for it to recognize the changes \nconda deactivate && conda activate nanophase_0.2.3",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Nanophase"
    ]
  },
  {
    "objectID": "source/nanopore/nanophase_how_to.html#usage",
    "href": "source/nanopore/nanophase_how_to.html#usage",
    "title": "Nanophase",
    "section": "Usage",
    "text": "Usage\nInput:\n\nIllumina short reads and/or Nanopore long-reads in fastq, fastq.gz, fasta, fasta.qz or fq.gz format\n\nOutput:\n\n01-LongAssemblies sub-folder containing information of Nanopore long-read assemblies (assembler: metaFlye)\n02-LongBins sub-folder containing the initial bins with relatively low-accuracy quality\n03-Polishing sub-folder containing polished bins\n\n\nExample\nIn the example below, we download the ZymoBIOMICS Gut Microbiome Standard sequenced with Nanopore. This Standard is a mixture of 18 bacterial strains, 2 fungal strains, and 1 archaeal strain in staggered abundances to mimic a true gut microbiome.\nNotice:\n\nYou can also run the workflow if you have both Nanopore and Illumina data. If you have only Illumina-data other workflows that work with multiple samples might be more useful to explore\nThe analysis was successfully run on Crunchomics with 100 GB of memory and 30 threads on a dataset with 1,679,780 reads and an average length of 4300 bp. The most memory intensive step is running pplacer when you place your genomes with gtdb_tk (one of the last steps of the pipeline). If memory is an issue on your cluster, you can start the pipeline with less memory (i.e. 50GB) to get the assembly and MAGs. Afterwards, you can restart the pipeline with more memory to go through the pplacer step as the pipeline will resume from the last completed stage.\nIf your dataset is larger/smaller you might need to adjust the amount of resources but the numbers given should give you an idea on where to start\n\n\n#start environment \nconda activate nanophase_0.2.3\n\n#go to wdir (exchange path to where you want to analyse your data)\ncd /path/to/wdir\n\n#prepare folders for better organization \nmkdir logs\nmkdir scripts\nmkdir data\nmkdir -p results/seqkit\nmkdir -p results/nanophase\n\n#download the test-dataset \n#comes with 1,679,780 reads with an avg length of 4300 bp and avg quality of 16\nfastq-dump SRR17913199 -O data\n\n#get summary statistics for the dataset\nseqkit stats -a -To results/seqkit/stats.tsv data/SRR17913199.fastq \n\n#analyse data with only Nanopore reads \nnanophase meta -l data/SRR17913199.fastq -t 30 -o results\n\n#check memory usage for slurm job: 39147\nsacct -j 39147 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,ncpus\n\nUseful arguments:\n\n--long_read_only only Nanopore long reads were involved [default: on]\n--hybrid both short and long reads were required [Optional]\n-l, --long Nanopore reads: fasta/q file that basecalled by Guppy 5+ or using 20+ chemistry was recommended if only Nanopore reads were included [Mandatory]\n-1 Illumina short reads: fasta/q paired-end #1 file [Optional]\n-2 Illumina short reads: fasta/q paired-end #2 file [Optional]\n-m, --medaka_model medaka model used for medaka polishing [default: r1041_e82_400bps_sup_g615]\n-e, --environment Build-in model of SemiBin [default: wastewater]; detail see: SemiBin single_easy_bin -h. Other choices are: human_gut, dog_gut, ocean, soil, cat_gut, human_oral, mouse_gut, pig_gut, built_environment, wastewater, chicken_caecum, global\n-t, --threads number of threads that used for assembly and polishing [default: 16]\n-o, --out output directory [default: ./nanophase-out]",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "Nanophase"
    ]
  },
  {
    "objectID": "source/nanopore/nanoqc_readme.html#nanoqc",
    "href": "source/nanopore/nanoqc_readme.html#nanoqc",
    "title": "Bioinformatics guidance page",
    "section": "NanoQC",
    "text": "NanoQC\n\nIntroduction\nNanoQC is a quality control tool for long read sequencing data aiming to replicate some of the plots made by fastQC (De Coster et al. 2018).\nAvailable on Crunchomics: Not by default\n\n\nInstallation\nNanoQC is part of the Nanopack package and I would recommend installing this package to already have other useful tools installed. Therefore, we install a new conda environment called nanopack. If you already have an environment with tools for long-read analyses I suggest adding nanopack there instead.\n\n#setup new conda environment, which we name nanopack\nmamba create --name nanopack -c conda-forge -c bioconda python=3.6 pip\n\n#activate environment\nconda activate nanopack \n\n#install nanopack software tools using pip\n$HOME/personal/mambaforge/envs/nanopore/bin/pip3 install nanopack\n\n#close environment\nconda deactivate\n\n\n\nUsage\n\nInputs: Fastqc.gz file\nOutput: An HTML with quality information\n\nExample code:\n\n#start environment\nconda activate nanopack\n\n#run on a single file\nnanoQC myfile.fastq.gz -o outputfolder\n\nUseful arguments (for the full version, check the manual):\n\n-l, --minlen {int} Minimum length of reads to be included in the plots. This also controls the length plotted in the graphs from the beginning and end of reads (length plotted = minlen / 2)\n\n\n\nReferences",
    "crumbs": [
      "Sequence data analyses",
      "Nanopore analyses",
      "NanoQC"
    ]
  },
  {
    "objectID": "source/nanopore/porechop_readme.html#porechop",
    "href": "source/nanopore/porechop_readme.html#porechop",
    "title": "Bioinformatics guidance page",
    "section": "Porechop",
    "text": "Porechop\n\nIntroduction\nPorechop is a tool for finding and removing adapters from Oxford Nanopore reads. Adapters on the ends of reads are trimmed off, and when a read has an adapter in its middle, it is treated as chimeric and chopped into separate reads. Porechop performs thorough alignments to effectively find adapters, even at low sequence identity.\nNotice: From 2018 on, porechop is not actively maintained anymore. It runs perfectly fine, but that is something to keep in mind when running into bugs.\n\n\nInstallation\nInstalled on crunchomics: Yes,\n\nPorechop v0.2.4 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create --name porechop -c conda-forge -c bioconda porechop=0.2.4\n\n\n\nUsage\n\n\n\n\n\n\nImportant\n\n\n\nAdaptor trimming very much depends on how the sequencing library was generated. Therefore, I recommend to carefully read through the How it works section of the softwares manual to know what to expect and look out for.\nSimilarly, porechop works with both demultiplexed and non-demultiplexed sequences. Also here, the manual explains in more detail how to perform barcode demultiplexing.\n\n\nRequired input:\n\nFASTA/FASTQ of input reads or a directory which will be recursively searched for FASTQ files (required and can be fasta,fastq,fasta.gz,fastq.gz)\n\nOutput:\n\nFASTA or FASTQ of trimmed reads\n\nExample code:\n\nconda activate porechop_0.2.4\n\nporechop --input myfile.fastq.gz \\\n  --output outputfolder/myfile_filtered.fastq.gz \\\n  --threads 1 \\\n  --discard_middle\n\nconda deactivate\n\nUseful arguments (for the full version, check the manual):\n\n-b {BARCODE_DIR}, --barcode_dir {BARCODE_DIR}: Reads will be binned based on their barcode and saved to separate files in this directory (incompatible with –output)\n--barcode_threshold {BARCODE_THRESHOLD} A read must have at least this percent identity to a barcode to be binned (default: 75.0)\n--barcode_diff {BARCODE_DIFF} If the difference between a read’s best barcode identity and its second-best barcode identity is less than this value, it will not be put in a barcode bin (to exclude cases which are too close to call) (default: 5.0)\n--adapter_threshold {ADAPTER_THRESHOLD} An adapter set has to have at least this percent identity to be labelled as present and trimmed off (0 to 100) (default: 90.0)\n--check_reads{CHECK_READS} This many reads will be aligned to all possible adapters to determine which adapter sets are present (default: 10000)\n--no_split Skip splitting reads based on middle adapters (default: split reads when an adapter is found in the middle)\n--discard_middle Reads with middle adapters will be discarded (default: reads with middle adapters are split) (required for reads to be used with Nanopolish, this option is on by default when outputting reads into barcode bins)\n\n\n\nReferences",
    "crumbs": [
      "Sequence data analyses",
      "Nanopore analyses",
      "Porechop"
    ]
  },
  {
    "objectID": "source/order_a-z.html",
    "href": "source/order_a-z.html",
    "title": "Original",
    "section": "",
    "text": "Original\n\n\nNew\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nBioinformatic Tools A-Z\n\n\n\n\n\nBioinformatic Tools A-Z\n\n\nA-D\n\n\n\n\nATLAS: a metagenomic pipeline for QC, assembly binning and annotation\n\n\nBowtie2: A tool for aligning sequencing reads to genomes and other reference sequences\n\n\nCheckM2: A tool to assess the quality of a genome assembly\n\n\nChopper: A tool for quality filtering of long read data\n\n\nDeSeq2: Analyse gene expression data in R\n\n\nDnaapler: A tool to re-orient a genome, for example at dnaA\n\n\n\n\nE-H\n\n\n\n\nFAMA: A fast pipeline for functional and taxonomic analysis of metagenomic sequences\n\n\nFastP: A tool for fast all-in-one preprocessing of FastQ files\n\n\nFastQC: A quality control tool for read sequencing data\n\n\nFeatureCounts: A read summarization program that counts mapped reads for genomic features\n\n\nFiltlong: A tool for filtering long reads\n\n\nFlye: A de novo assembler for single-molecule sequencing reads\n\n\nGTDB_tk: A software toolkit for assigning objective taxonomic classifications to bacterial and archaeal genomes\n\n\nHMMER: A tool for searching sequence databases for sequence homologs, and for making sequence alignments\n\n\n\n\nI-L\n\n\n\n\nInterproscan: A tool to scan protein and nucleic sequences against InterPro signatures\n\n\nITSx: A tool to extract ITS1 and ITS2 subregions from ITS sequences\n\n\nKraken2: A taxonomic sequence classifier using kmers\n\n\n\n\nM-P\n\n\n\n\nMETABOLIC: A tool to predict functional trait profiles in genome datasets\n\n\nMOTUS: A tool to estimate microbial abundances in Illumina and Nanopore sequencing data\n\n\nMinimap2: A program to align DNA or mRNA sequences against a reference database\n\n\nMultiQC: A program to summarize analysis reports\n\n\nNanoClass2: A taxonomic meta-classifier for long-read 16S/18S rRNA gene sequencing data\n\n\nNanoITS: A taxonomic meta-classifier for long-read ITS operon sequencing data\n\n\nNanophase: A pipeline to generate MAGs using Nanopore long and Illumina short reads from metagenomes\n\n\nNanoPlot: Plotting tool for long read sequencing data\n\n\nNanoQC: A quality control tool for long read sequencing data\n\n\nPorechop: A tool for finding and removing adapters from Nanopore reads\n\n\nProkka: A tool to annotate bacterial, archaeal and viral genomes\n\n\nPseudofinder: A tool that detects pseudogene candidates from annotated genbank files of bacterial and archaeal genomes\n\n\n\n\nQ-T\n\n\n\n\nQUAST: A Quality Assessment Tool for Genome Assemblies\n\n\nRibodetector: Detect and remove rRNA sequences from metagenomic, metatranscriptomic, and ncRNA sequencing data\n\n\nSamtools: A tool to manipulating alignments in SAM/BAM format\n\n\nSeqKit: A tool for FASTA/Q file manipulation\n\n\nSortMerNa: A tool to filter ribosomal RNAs in metatranscriptomic data\n\n\nStar: An ultrafast universal RNA-seq aligner\n\n\nTrycyler: A tool for generating consensus long-read assemblies for bacterial genomes\n\n\nTrinity: A tool to assemble transcript sequences from Illumina RNA-Seq data\n\n\n\n\nU-Z"
  },
  {
    "objectID": "source/phylogenomics/iqtree2.html#iq-tree2",
    "href": "source/phylogenomics/iqtree2.html#iq-tree2",
    "title": "Bioinformatics guidance page",
    "section": "IQ-TREE2",
    "text": "IQ-TREE2\n\nIntroduction\nIQ-TREE was motivated by the rapid accumulation of phylogenomic data, leading to a need for efficient phylogenomic software that can handle a large amount of data and provide more complex models of sequence evolution (Nguyen et al. 2014; Minh et al. 2020). To this end, IQ-TREE can utilize multicore computers and distributed parallel computing to speed up the analysis. IQ-TREE automatically performs checkpointing to resume an interrupted analysis.\nFor more information, please visit the IQ-TREE website, which hosts a wealth of information about the software.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nIQ-TREE v2.3.5 is installed as part of the bioinformatics share. If you have access to crunchomics and have not yet access to the bioinformatics you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\nmamba create -n iqtree -c bioconda iqtree\n\n\n\nUsage\nIQ-TREE offers a wealth of different options and it is not the scope of this page to cover all of this. For a detailed instruction on how to use the software, please visit the IQ-TREE website, which also includes some great beginner tutorials.\nBelow you find a minimal example on how to generate a phylogenetic tree from an aligned and trimmed protein alignment.\n\nconda activate iqtree2.3.5\n\niqtree2 -s  my_alignment.aln \\\n    -pre output_folder/my_alignment \\\n    -m LG -T AUTO --threads-max 2 -B 1000 -bnni\n\nconda deactivate\n\nSome general comments:\n\nIQ-TREE is able to detect the input type by default and it accepts alignments in phylip, fasta (nucleotide and protein), nexus, clustal and msf alignment files.\nDifferent alignments run best with different number of threads (i.e. its hard to predict if it is better to use 5 or 20 threads for different alignments). Therefore, its usually best to use -T AUTO that let’s IQ-TREE determine the optimal number of threads. To not overload the system we can combine it with --threads-max. From personal experience single gene trees generally do not benefit from more threads and you can keep it at 2-5. In contrast, for concatenated alignments, using up to 20 threads can be beneficial.\nIf your run crashes, because it ran out of the SLURM time limit or memory, you can simply restart the run with the exact same command. IQ-TREE implements checkpoints that allow you to restart the analysis from the last checkpoint.\nIf you want to know more about the protein models to use, you can go here.\n\nIn our experience using LG together with a C-series mixture model (i.e. LG+C20) is a good start if you want to analyse some concatenated species tree.\n\nIf you are unsure what model to use, then you can make use of the model test implemented in IQ-TREE (-m MFP).\n\nTo speed up the search, we can define what model to test with different combinations of substitution models. I.e. if we work with nuclear proteins, there is no reason to also test for chloroplast protein models. If we just wanted to test LG and WAG, we could use -m MFP -mset LG,WAG\nNotice, that the model selection step does not automatically include mixture models as this would create too many combinations to test. From personal experience, it might be useful to look at the C-series and you could test C10 and C20 for shorter proteins and up to C60 for concatenated alignments with -m MFP -mset LG,WAG -madd LG+C10,LG+C10+G,LG+C10+R,LG+C10+F,LG+C10+R+F,LG+C10+G+F,LG+C20,LG+C20+G,LG+C20+F,LG+C20+G+F,LG+C20+R,LG+C20+R+F --score-diff all. Important: When you want to test mixture models, you have to add --score-diff all as otherwise the C-series gets not tested!!! Also, notice how we also have to add all model combinations that we want to check:\n\nHow to model the best rate heterogenetity across sites (+G for the discrete Gamma model, +I for the FreeRate model)\nHow to model different kinds of base frequencies (+F stands for empirical base frequencies. This is the default if the model has unequal base frequencies)\nIf you run a model selection step, you can open the log file to see the complete list of models tested\n\n\n\nUseful options (for a full list of options, visit the website or use the help function):\n\n-s FILE[,…,FILE] PHYLIP/FASTA/NEXUS/CLUSTAL/MSF alignment file(s)\n-m MODEL_NAME Substitution model to use. For a full list of models, go here.\n--prefix STRING Prefix for all output files (default: aln/partition). This also allows you to generate the output in the folder of your choice.\n-T NUM|AUTO No. cores/threads or AUTO-detect (default: 1).\n-B, --ufboot NUM Replicates for ultrafast bootstrap (&gt;=1000 recommended). Notice, that this argument was renamed in newer IQ-TREE versions and you might find documentation in which -bb is used for iqtree v1 versions.\n-alrt specifies the number of bootstrap replicates for SH-aLRT where 1000 is the minimum number recommended. This is an alternative method to the Ufboot method mentioned above\n--bnni Optimize UFBoot trees by NNI on bootstrap alignment. Useful option to as it reduce the risk of overestimating branch supports with UFBoot (-B) due to severe model violations\nLet IQ-TREE test for the best model to use\n\n-m TESTONLY Standard model selection (like jModelTest, ProtTest)\n-m TEST Standard model selection followed by tree inference\n-m MF Extended model selection with FreeRate heterogeneity\n-m MFP Extended model selection followed by tree inference\n--mset STR,… Comma-separated model list (e.g. -mset WAG,LG,JTT)\n-madd STR,… Comma-separated list of mixture models to consider\n\n\nIf you want to view the treefile, there are different options to do this, such as the ITol webserver or the figtree software which is also installed and can be accessed via the iqtree conda environment you activate above with figtree my_alignment.treefile",
    "crumbs": [
      "Sequence data analyses",
      "Phylogenomics",
      "IQ-TREE2"
    ]
  },
  {
    "objectID": "source/snakemake/tutorial.html#general",
    "href": "source/snakemake/tutorial.html#general",
    "title": "Snakemake tutorial",
    "section": "General",
    "text": "General\n\nPython-based workflow system\nCommand-line interface\nScheduling algorithm suitable to work on clusters and batch systems allowing scaling of workflows\nReproducibility:\n\nInstall required software and all dependencies in exact versions\nSoftware install from different sources\nNormalize installation via build script\nNo admin rights needed\nIsolated environments\n\nIntegrates with Conda package manager and the Singularity container engine\nGeneration of self-contained HTML reports\nWorkflows are defined in terms of rules that define how to create output files from input files. The are executed in three phases:\n\nInitialization (parsing)\nDAG phase (DAG is build)\nScheduling phase (execution of DAG)\n\nDependencies\n\nbetween the rules are determined automatically, creating a DAG (directed acyclic graph) of jobs that can be automatically parallelized\nDependencies are determined top-down, i.e. the rule on top which defines the input gets passed to a rule further down were files are for example sorted\n\nJob execution only if:\n\nOutput file is target and does notexist\nInput file newer than output file\nRule has been modified\nExecution is enforced\n\nEasy distribution of workflows via git repositories with standardized folder structure"
  },
  {
    "objectID": "source/snakemake/tutorial.html#best-practices",
    "href": "source/snakemake/tutorial.html#best-practices",
    "title": "Snakemake tutorial",
    "section": "Best practices",
    "text": "Best practices\n\nIt is best practice to have subsequent steps of a workflow in separate, unique, output folders. This keeps the working directory structured. Further, such unique prefixes allow Snakemake to quickly discard most rules in its search for rules that can provide the requested input. This accelerates the resolution of the rule dependencies in a workflow.\nSnakemake (&gt;=5.11) comes with a code quality checker (a so called linter), that analyzes your workflow and highlights issues that should be solved in order to follow best practices, achieve maximum readability, and reproducibility. The linter can be invoked with snakemake --lint\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nFurther, scripts should be stored in a subfolder workflow/scripts and notebooks in a subfolder workflow/notebooks\nConda environments (see Integrated Package Management) should be stored in a subfolder workflow/envs\nWhen publishing your workflow in a Github repository, it is a good idea to add some minimal test data and configure Github Actions for continuously testing the workflow on each new commit. For this purpose, we provide predefined Github actions for both running tests and linting here, as well as formatting here.\nConfiguration of a workflow should be handled via config files and, if needed, tabular configuration like sample sheets (either via Pandas or PEPs). Use such configuration for metadata and experiement information, not for runtime specific configuration like threads, resources and output folders. For those, just rely on Snakemake’s CLI arguments like –set-threads, –set-resources, –set-default-resources, and –directory. This makes workflows more readable, scalable, and portable.\nTry to keep filenames short (thus easier on the eye), but informative. Avoid mixing of too many special characters (e.g. decide whether to use _ or - as a separator and do that consistently throughout the workflow).\nTry to keep Python code like helper functions separate from rules (e.g. in a workflow/rules/common.smk file). This way, you help non-experts to read the workflow without needing to parse internals that are irrelevant for them. The helper function names should be chosen in a way that makes them sufficiently informative without looking at their content. Also avoid lambda expressions inside of rules.\nMake use of Snakemake wrappers whenever possible. Consider contributing to the wrapper repo whenever you have a rule that reoccurs in at least two of your workflows. More about wrappers can be found here.\n\nFollow this link to view the recommended structure to store a workflow in a git repository. A snakemake workflow template can be found here. Conda determines the priority of channels depending on their order of appearance in the channels list. For instance, the channel that comes first in the list gets the highest priority.\n├── .gitignore\n├── README.md\n├── LICENSE.md\n├── workflow\n│   ├── rules\n|   │   ├── module1.smk\n|   │   └── module2.smk\n│   ├── envs\n|   │   ├── tool1.yaml\n|   │   └── tool2.yaml\n│   ├── scripts\n|   │   ├── script1.py\n|   │   └── script2.R\n│   ├── notebooks\n|   │   ├── notebook1.py.ipynb\n|   │   └── notebook2.r.ipynb\n│   ├── report\n|   │   ├── plot1.rst\n|   │   └── plot2.rst\n|   └── Snakefile\n├── config\n│   ├── config.yaml\n│   └── some-sheet.tsv\n├── results\n└── resources"
  },
  {
    "objectID": "source/snakemake/tutorial.html#useful-options",
    "href": "source/snakemake/tutorial.html#useful-options",
    "title": "Snakemake tutorial",
    "section": "Useful options",
    "text": "Useful options\n\n-n --dry-run: Snakemake will only show the execution plan instead of actually performing the steps\n-p: print the resulting shell command for illustration.\n--cores {INTEGER}: The numbers of course to use. Must be part of every executed workflow\n--forcerun allows to repeat a run even if the output files already exist from a given rule\n--foreall enforces a complete re-execution of the workflow."
  },
  {
    "objectID": "source/snakemake/tutorial.html#description-of-example-workflow",
    "href": "source/snakemake/tutorial.html#description-of-example-workflow",
    "title": "Snakemake tutorial",
    "section": "Description of example workflow",
    "text": "Description of example workflow\nA Snakemake workflow is defined by specifying rules in a Snakefile. Rules decompose the workflow into small steps (for example, the application of a single tool) by specifying how to create sets of output files from sets of input files. Snakemake automatically determines the dependencies between the rules by matching file names.\nAll added syntactic structures begin with a keyword followed by a code block that is either in the same line or indented and consisting of multiple lines. The resulting syntax resembles that of original Python constructs.\nIn this workflow we perform a read mapping workflow."
  },
  {
    "objectID": "source/snakemake/tutorial.html#setup",
    "href": "source/snakemake/tutorial.html#setup",
    "title": "Snakemake tutorial",
    "section": "Setup",
    "text": "Setup\n\nInstalling mambaforge\nNotice:\n\nSetup done in WSL\nWe will use Conda to create an isolated environment with all the required software for this tutorial.\nMambaforge already installed, if instructions are needed go here\n\n\n\nPrepare working directory\n\n#define wdir\nwdir=\"/mnt/c/Users/ndmic/WorkingDir/UvA/Tutorials/Snakemake\"\ncd $wdir\n\n#activate conda enviornment (for this to work, see code cell below)\nconda activate snakemake-tutorial\n\n\n#download the example data\ncurl -L https://api.github.com/repos/snakemake/snakemake-tutorial-data/tarball -o snakemake-tutorial-data.tar.gz\n\n#extract the data\ntar --wildcards -xf snakemake-tutorial-data.tar.gz --strip 1 \"*/data\" \"*/environment.yaml\"\n\n#create conda env (/home/ndombrowski/mambaforge/envs/snakemake-tutorial)\nmamba env create --name snakemake-tutorial --file environment.yaml"
  },
  {
    "objectID": "source/snakemake/tutorial.html#basic-workflow",
    "href": "source/snakemake/tutorial.html#basic-workflow",
    "title": "Snakemake tutorial",
    "section": "Basic Workflow",
    "text": "Basic Workflow\n\nMapping reads\n\nBasics\nTo write our first rule, we create a new document and use it to map a sample to a reference genome using bwa mem.\nA snakemake rule:\n\nHas a name, i.e. bwa_map\nHas a number of directives (i.e. input, output, shell)\ninput can provide a list of files that are expected to be used. Important: have a comma between input/output items.\nHas a shell command, i.e. the shell command to be executed\nIf a rule has multiple input files, they will be concatenated and separated by a whitespace (i.e. we get data/genome.fa data/samples/A.fastq)\n\nBasics: - Snakemake applies the rules given in the Snakefile in a top-down way - The application of a rule to generate a set of output files is called job - Note that Snakemake automatically creates missing directories before jobs are executed. - Snakemake works backwards from requested output, and not from available input. Thus, it does not automatically infer all possible output from, for example, the fastq files in the data folder.\n\nnano Snakefile\n\nContent:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/A.fastq\"\n    output:\n        \"mapped_reads/A.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#perform dry-run workflow\nsnakemake -np mapped_reads/A.bam\n\n#execute workflow \nsnakemake --cores 1 mapped_reads/A.bam\n\nSince mapped_reads/A.bam now exists snakemake WILL NOT try to create this file again. Snakemake only re-runs jobs if one of the input files is newer than one of the output files or one of the input files will be updated by another job.\n\n\n\nGeneralized rules\nIf we want to work on more than one sample not only data/samples/A.fastq then we can generalize rules using named wildcards. Below, we for example replace the A with {sample}\nNote that you can have multiple wildcards in your file paths, however, to avoid conflicts with other jobs of the same rule, all output files of a rule have to contain exactly the same wildcards.\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\n\n#execute command (dry-run)\nsnakemake -np mapped_reads/B.bam\n\nIf we run this, we will see that bwa will use an input name that is based on the B.bam name, i.e. data/samples/B.fastq. You can see how the character B is propagaged through the input file names and the names in the shell command.\n\n#specify more than one target\nsnakemake -np mapped_reads/A.bam mapped_reads/B.bam\n\n#same command but more condensed using bash magic \nsnakemake -np mapped_reads/{A,B}.bam\n\nIn both cases, you will see that Snakemake only proposes to create the output file mapped_reads/B.bam. This is because you already executed the workflow before (see the previous step) and no input file is newer than the output file mapped_reads/A.bam.\n\n\nSorting alignments\nAdd a new rule:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam\"\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nComments:\n\nIn the shell command above we split the string into two lines, which are however automatically concatenated into one by Python. This is a handy pattern to avoid too long shell command lines. When using this, make sure to have a trailing whitespace in each line but the last, in order to avoid arguments to become not properly separated.\nWith sorted_reads/{wildcards.sample} we extract the value of the sample wildcard, i.e. we get sorted_reads/B\n\n\n#dry run\nsnakemake -np sorted_reads/B.bam\n\n\n\nIndexing the alignments and visualizing the DAG\nAdd a new rule:\n\nrule samtools_index:\n    input:\n        \"sorted_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam.bai\"\n    shell:\n        \"samtools index {input}\"\n\n\n#dry run \nsnakemake -np sorted_reads/B.bam.bai\n\n#create a DAG\nsnakemake --dag sorted_reads/{A,B}.bam.bai | dot -Tsvg &gt; dag.svg\n\nThe last command will create a visualization of the DAG using the dot command provided by Graphviz. For the given target files, Snakemake specifies the DAG in the dot language and pipes it into the dot command, which renders the definition into SVG format. The rendered DAG is piped into the file dag.svg.\nThe DAG contains a node for each job with the edges connecting them representing the dependencies. The frames of jobs that don’t need to be run (because their output is up-to-date) are dashed. For rules with wildcards, the value of the wildcard for the particular job is displayed in the job node.\n\n\nCall genomic variants and using expand\nNext, we will aggregate the mapped reads from all samples and jointly call genomic variants on them. Therefore, we will combine the two utilities samtools and bcftools. Snakemake provides a helper function for collecting input files that helps us to describe the aggregation in this step. With\nexpand(\"sorted_reads/{sample}.bam\", sample=SAMPLES)\nwe obtain a list of files where the given pattern “sorted_reads/{sample}.bam” was formatted with the values in a given list of samples SAMPLES, i.e.\n[\"sorted_reads/A.bam\", \"sorted_reads/B.bam\"]\nThis can also be used if multiple wildcards are used i.e.\nexpand(\"sorted_reads/{sample}.{replicate}.bam\", sample=SAMPLES, replicate=[0, 1])\nwould create\n[\"sorted_reads/A.0.bam\", \"sorted_reads/A.1.bam\", \"sorted_reads/B.0.bam\", \"sorted_reads/B.1.bam\"]\nLet’s start with defining the list of samples ad-hoc in plain Python at the top of the Snakefile and add a rule at the end of our Snakefile:\n\nSAMPLES = [\"A\", \"B\"]\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=SAMPLES),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=SAMPLES)\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\nWith multiple input or output files, it is sometimes handy to refer to them separately in the shell command. This can be done by specifying names for input or output files, for example with fa=.... The files can then be referred to in the shell command by name, for example with {input.fa}.\nFor long shell commands like this one, it is advisable to split the string over multiple indented lines. Python will automatically merge it into one. Further, you will notice that the input or output file lists can contain arbitrary Python statements, as long as it returns a string, or a list of strings. Here, we invoke our expand function to aggregate over the aligned reads of all samples.\n\n#create a DAG\nsnakemake --dag calls/all.vcf | dot -Tsvg &gt; dag.svg\n\n\n\nUsing custom scripts\nUsually, a workflow not only consists of invoking various tools, but also contains custom code to for example calculate summary statistics or create plots. While Snakemake also allows you to directly write Python code inside a rule, it is usually reasonable to move such logic into separate scripts. For this purpose, Snakemake offers the script directive. Add the following rule to your Snakefile:\nWith this rule, we will eventually generate a histogram of the quality scores that have been assigned to the variant calls in the file calls/all.vcf. The actual Python code to generate the plot is hidden in the script scripts/plot-quals.py.\nScript paths are always relative to the referring Snakefile. In the script, all properties of the rule like input, output, wildcards, etc. are available as attributes of a global snakemake object.\n\nrule plot_quals:\n    input:\n        \"calls/all.vcf\"\n    output:\n        \"plots/quals.svg\"\n    script:\n        \"scripts/plot-quals.py\"\n\nCreate the file scripts/plot-quals.py, with the following content:\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom pysam import VariantFile\n\nquals = [record.qual for record in VariantFile(snakemake.input[0])]\nplt.hist(quals)\n\nplt.savefig(snakemake.output[0])\n\nAlthough there are other strategies to invoke separate scripts from your workflow (for example, invoking them via shell commands), the benefit of this is obvious: the script logic is separated from the workflow logic (and can even be shared between workflows), but boilerplate code like the parsing of command line arguments is unnecessary.\nApart from Python scripts, it is also possible to use R scripts. In R scripts, an S4 object named snakemake analogous to the Python case above is available and allows access to input and output files and other parameters. Here, the syntax follows that of S4 classes with attributes that are R lists, for example we can access the first input file with snakemake@input[[1]] (note that the first file does not have index 0 here, because R starts counting from 1). Named input and output files can be accessed in the same way, by just providing the name instead of an index, for example snakemake@input[[“myfile”]].\nFor details and examples, see the External scripts section in the Documentation.\n\n\nAdding a target rule\nSo far, we always executed the workflow by specifying a target file at the command line. Apart from filenames, Snakemake also accepts rule names as targets if the requested rule does not have wildcards.\nIt is possible to write target rules collecting particular subsets of the desired results or all results. Moreover, if no target is given at the command line, Snakemake will define the first rule of the Snakefile as the target. Hence, it is best practice to have a rule all at the top of the workflow which has all typically desired target files as input files.\nTo do this add the following to the top of the workflow:\n\nrule all:\n    input:\n        \"plots/quals.svg\"\n\nAnd execute snakemake with:\n\nsnakemake -n\n\nIf we run this the execution plan for creating the file plots/quals.svg, which contains and summarizes all our results, will be shown.\nNote that, apart from Snakemake considering the first rule of the workflow as the default target, the order of rules in the Snakefile is arbitrary and does not influence the DAG of jobs.\n\n\nExecute full workflow\n\n#execution \nsnakemake --cores 1\n\n#view output\nwslview plots/quals.svg\n\n#force to re-execute samtools_sort\nsnakemake --cores 1 --forcerun samtools_sort"
  },
  {
    "objectID": "source/snakemake/tutorial.html#advanced-options",
    "href": "source/snakemake/tutorial.html#advanced-options",
    "title": "Snakemake tutorial",
    "section": "Advanced options",
    "text": "Advanced options\n\nSpecifying the number of threads\nSnakemake can be made aware of the threads a rule needs with the threads directive. Ie change the bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    threads: 2\n    shell:\n        \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\nThe number of threads can be propagated to the shell command with the familiar braces notation (i.e. {threads}). If no threads directive is given, a rule is assumed to need 1 thread.\nWhen a workflow is executed, the number of threads the jobs need is considered by the Snakemake scheduler. In particular, the scheduler ensures that the sum of the threads of all jobs running at the same time does not exceed a given number of available CPU cores. This number is given with the –cores command line argument, which is mandatory for snakemake calls that actually run the workflow.\nFor example snakemake --cores 10 would execute the workflow with 10 cores. Since the rule bwa_map needs 8 threads, only one job of the rule can run at a time, and the Snakemake scheduler will try to saturate the remaining cores with other jobs like, e.g., samtools_sort.\nThe threads directive in a rule is interpreted as a maximum: when less cores than threads are provided, the number of threads a rule uses will be reduced to the number of given cores.\nIf --cores is given without a number, all available cores are used.\n\nsnakemake --forceall --cores 2\nsnakemake --forceall --cores 4\n\n\n\nConfiguration files\nWe can use a configuration file if we want your workflow to be customizable:\n\nCheck out the full readme here\nCan be provided as JSON or YAML\nThey are used with the configfile direction\nvariable names in the rule and the key name in the configuration file do not have to be identical. Snakemake matches variables in the rule based on their position within the curly braces {}.\n\nA config file is specified by adding this to the top of our Snakemake workflow:\n\nconfigfile: \"config.yaml\"\n\nSnakemake will load the config file and store its contents into a globally available dictionary named config. In our case, it makes sense to specify the samples in config.yaml as:\nsamples:\n    A: data/samples/A.fastq\n    B: data/samples/B.fastq\nAnd remove the statement defining the SAMPLES in the Snakemake file. And change the rule of bcftools_call to:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\n\n\nInput functions\nSince we have stored the path to the FASTQ files in the config file, we can also generalize the rule bwa_map to use these paths. This case is different to the rule bcftools_call we modified above. To understand this, it is important to know that Snakemake workflows are executed in three phases.\n\nIn the initialization phase, the files defining the workflow are parsed and all rules are instantiated.\nIn the DAG phase, the directed acyclic dependency graph of all jobs is built by filling wildcards and matching input files to output files.\nIn the scheduling phase, the DAG of jobs is executed, with jobs started according to the available resources.\n\nThe expand functions in the list of input files of the rule bcftools_call are executed during the initialization phase.\nIn this phase, we don’t know about jobs, wildcard values and rule dependencies. Hence, we cannot determine the FASTQ paths for rule bwa_map from the config file in this phase, because we don’t even know which jobs will be generated from that rule.\nInstead, we need to defer the determination of input files to the DAG phase. This can be achieved by specifying an input function instead of a string as inside of the input directive. For the rule bwa_map this works as follows:\n::: {.cell}\ndef get_bwa_map_input_fastqs(wildcards):\n   return config[\"samples\"][wildcards.sample]\n\nrule bwa_map:\n   input:\n       \"data/genome.fa\",\n       get_bwa_map_input_fastqs\n   output:\n       \"mapped_reads/{sample}.bam\"\n   threads: 8\n   shell:\n       \"bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n:::\nHow wild cards work here:\n\nThe wildcards object is a special Snakemake variable that represents placeholders in rule input and output file names. In your case, you are using {sample} as a placeholder in the rule’s input section, and Snakemake automatically parses this placeholder into a wildcards object.\nWhen you define the function get_bwa_map_input_fastqs(wildcards), you are essentially telling Snakemake that this function accepts a wildcards object as an argument. This allows the function to access and use the wildcards to determine which sample’s input files to fetch.\nWhen Snakemake processes the rule bwa_map, it looks at the rule’s input section and identifies that it contains the {sample} placeholder.\nSnakemake then matches the placeholder to the wildcards object. If you have multiple samples defined in your config.yaml, Snakemake will create separate jobs for each sample, each with a different wildcards.sample value.\nFor each job, the wildcards.sample attribute will be replaced with the actual sample name being processed. For example, if Snakemake is processing the sample “A,” then wildcards.sample will be “A.”\nThe function get_bwa_map_input_fastqs uses config[“samples”][wildcards.sample] to fetch the corresponding input file path for the sample being processed. So, for sample “A,” it would retrieve “data/samples/A.fastq” as the input file path.\n\nBenefits of using python function:\n\nModularity and Reusability: By defining a separate Python function (get_bwa_map_input_fastqs) to retrieve the input files, you create a modular and reusable piece of code. This can be particularly useful if you have multiple rules that need to access the same input files, as you can reuse the function across those rules.\nAbstraction: The function abstracts the details of where the input files come from. In your first version, you had to specify the exact path to the input files in the rule itself. With the function, you only need to specify how to obtain the input files for a given sample, making your rule more abstract and easier to maintain.\nFlexibility: If you need to change the way you obtain input files in the future (for example, if you decide to store them in a different directory or use a different naming convention), you can update the function in one place, and all the rules that use it will automatically adapt.\nReduced Redundancy: Your original version of the rule had repetitive code for specifying input and output paths for each sample. With the function, you reduce redundancy and the potential for errors, as the input file retrieval logic is centralized.\nEasier Testing: You can more easily write unit tests for the input file retrieval function to ensure it works correctly, which can be challenging when input paths are hard-coded in multiple rules.\n\n\nExercise\nIn the data/samples folder, there is an additional sample C.fastq. Add that sample to the config file and see how Snakemake wants to recompute the part of the workflow belonging to the new sample, when invoking with snakemake -n --forcerun bcftools_call\nNote: Snakemake does not automatically rerun jobs when new input files are added as in the excercise below. However, you can get a list of output files that are affected by such changes with snakemake –list-input-changes. To trigger a rerun, this bit of bash magic helps:\n\nsnakemake -n --forcerun $(snakemake --list-input-changes)\n\n\n\n\nRule parameters\nSometimes shell commands are not only composed of input and output files but require additional parameters.\nWe can set additional parameters need to be set depending on the wildcard values of the job. For this, Snakemake allows to define arbitrary parameters for rules with the params directive. I.e. edit bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    threads: 8\n    shell:\n        \"bwa mem -R '{params.rg}' -t {threads} {input} | samtools view -Sb - &gt; {output}\"\n\n\nsnakemake -np --forceall\n\nNext, consider another important parameter, the prior mutation rate (1e-3 per default). It is set via the flag -P of the bcftools call command. Consider making this flag configurable via adding a new key to the config file and using the params directive in the rule bcftools_call to propagate it to the shell command.\nAdd this in the config file:\nbcftools:\n    evalue: 0.001\nChange this in the Snakemakefile:\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}\"\n\n\nsnakemake -p --cores 1 --forceall\n\n\n\nLogging\nWhen executing a large workflow, it is usually desirable to store the logging output of each job into a separate file, instead of just printing all logging output to the terminal—when multiple jobs are run in parallel, this would result in chaotic output. For this purpose, Snakemake allows to specify log files for rules. Log files are defined via the log directive and handled similarly to output files, but they are not subject of rule matching and are not cleaned up when a job fails. We modify our rule bwa_map as follows:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        \"mapped_reads/{sample}.bam\"\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThe shell command is modified to collect STDERR output of both bwa and samtools and pipe it into the file referred to by {log}. Log files must contain exactly the same wildcards as the output files to avoid file name clashes between different jobs of the same rule.\nLet’s add a log to the to the bcftools_call rule as well.\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    params:\n        eval = config[\"bcftools\"][\"evalue\"]\n    log:\n        \"logs/bcftools/all.log\"\n    shell:\n        \"(bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -P {params.eval} -mv - &gt; {output}) 2&gt; {log}\"\n\n\nsnakemake -p --cores 1 --forceall\n\nThe ability to track the provenance of each generated result is an important step towards reproducible analyses. Apart from the report functionality discussed before, Snakemake can summarize various provenance information for all output files of the workflow. The flag –summary prints a table associating each output file with the rule used to generate it, the creation date and optionally the version of the tool used for creation is provided. Further, the table informs about updated input files and changes to the source code of the rule after creation of the output file.\n\nsnakemake -p --cores 1 --forceall --summary\n\n\n\nTemporary and protected files\nIn our workflow, we create two BAM files for each sample, namely the output of the rules bwa_map and samtools_sort. When not dealing with examples, the underlying data is usually huge. Hence, the resulting BAM files need a lot of disk space and their creation takes some time. To save disk space, you can mark output files as temporary. Snakemake will delete the marked files for you, once all the consuming jobs (that need it as input) have been executed. We use this mechanism for the output file of the rule bwa_map:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nThis results in the deletion of the BAM file once the corresponding samtools_sort job has been executed. Since the creation of BAM files via read mapping and sorting is computationally expensive, it is reasonable to protect the final BAM file from accidental deletion or modification. We modify the rule samtools_sort to mark its output file as protected:\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        protected(\"sorted_reads/{sample}.bam\")\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nRun Snakemake with the target mapped_reads/A.bam. Although the file is marked as temporary, you will see that Snakemake does not delete it because it is specified as a target file.\n\nsnakemake --forceall --cores 1 mapped_reads/A.bam"
  },
  {
    "objectID": "source/snakemake/tutorial.html#additional-features",
    "href": "source/snakemake/tutorial.html#additional-features",
    "title": "Snakemake tutorial",
    "section": "Additional features",
    "text": "Additional features\nFor more, check out this documentation.\n\nModularization\nIn order to re-use building blocks or simply to structure large workflows, it is sometimes reasonable to split a workflow into modules. For this, Snakemake provides the include directive to include another Snakefile into the current one, e.g.:\ninclude: \"path/to/other.snakefile\"\nAlternatively, Snakemake allows to define sub-workflows. A sub-workflow refers to a working directory with a complete Snakemake workflow. Output files of that sub-workflow can be used in the current Snakefile. When executing, Snakemake ensures that the output files of the sub-workflow are up-to-date before executing the current workflow. This mechanism is particularly useful when you want to extend a previous analysis without modifying it. For details about sub-workflows, see the documentation.\nIn addition to the central Snakefile, rules can be stored in a modular way, using the optional subfolder workflow/rules. Such modules should end with .smk, the recommended file extension of Snakemake.\nLets put the read mapping into a separate snakefile and use include to make it available in our workflow:\nIn rules add a new file bwa_mem.smk with the following content:\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    threads: 2\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nRemove the bwa_mem rule from the workflow and add include: \"rules/bwa_mem.smk and run the workflow with :\n\nsnakemake --forceall -p --cores 1\n\n\n\nAutomatic deployment of software dependencies\nIn order to get a fully reproducible data analysis, it is not sufficient to be able to execute each step and document all used parameters. The used software tools and libraries have to be documented as well. In this tutorial, you have already seen how Conda can be used to specify an isolated software environment for a whole workflow. With Snakemake, you can go one step further and specify Conda environments per rule. This way, you can even make use of conflicting software versions (e.g. combine Python 2 with Python 3).\nIn our example, instead of using an external environment we can specify environments per rule, e.g.:\n\nrule samtools_index:\n  input:\n      \"sorted_reads/{sample}.bam\"\n  output:\n      \"sorted_reads/{sample}.bam.bai\"\n  conda:\n      \"envs/samtools.yaml\"\n  shell:\n      \"samtools index {input}\"\n\nwith envs/samtools.yaml defined as:\n\nchannels:\n  - bioconda\n  - conda-forge\ndependencies:\n  - samtools =1.9\n\nWhen executing the workflow with snakemake --use-conda --cores 1 it will automatically create required environments and activate them before a job is executed. It is best practice to specify at least the major and minor version of any packages in the environment definition.\nSpecifying environments per rule in this way has two advantages: - First, the workflow definition also documents all used software versions. - Second, a workflow can be re-executed (without admin rights) on a vanilla system, without installing any prerequisites apart from Snakemake and Miniconda.\n\n\nTool wrappers\nIn order to simplify the utilization of popular tools, Snakemake provides a repository of so-called wrappers (the Snakemake wrapper repository). A wrapper is a short script that wraps (typically) a command line application and makes it directly addressable from within Snakemake. For this, Snakemake provides the wrapper directive that can be used instead of shell, script, or run. For example, the rule bwa_map could alternatively look like this:\n\nrule bwa_mem:\n  input:\n      ref=\"data/genome.fa\",\n      sample=lambda wildcards: config[\"samples\"][wildcards.sample]\n  output:\n      temp(\"mapped_reads/{sample}.bam\")\n  log:\n      \"logs/bwa_mem/{sample}.log\"\n  params:\n      \"-R '@RG\\tID:{sample}\\tSM:{sample}'\"\n  threads: 8\n  wrapper:\n      \"0.15.3/bio/bwa/mem\"\n\n\n\nCluster execution\nBy default, Snakemake executes jobs on the local machine it is invoked on. Alternatively, it can execute jobs in distributed environments, e.g., compute clusters or batch systems. If the nodes share a common file system, Snakemake supports three alternative execution modes.\nIn cluster environments, compute jobs are usually submitted as shell scripts via commands like qsub. Snakemake provides a generic mode to execute on such clusters. By invoking Snakemake with snakemake --cluster qsub --jobs 100 each job will be compiled into a shell script that is submitted with the given command (here qsub). The –jobs flag limits the number of concurrently submitted jobs to 100.\nFind out more for SLURM submissions.\n\n\nConstraining wildcards\nSometimes it is useful to constrain the values a wildcard can have. This can be achieved by adding a regular expression that describes the set of allowed wildcard values. For example, the wildcard sample in the output file \"sorted_reads/{sample}.bam\" can be constrained to only allow alphanumeric sample names as \"sorted_reads/{sample,[A-Za-z0-9]+}.bam\".\nConstraints may be defined per rule or globally using the wildcard_constraints keyword, as demonstrated in Wildcards. This mechanism helps to solve two kinds of ambiguity.\n\nIt can help to avoid ambiguous rules, i.e. two or more rules that can be applied to generate the same output file. Other ways of handling ambiguous rules are described in the Section Handling Ambiguous Rules.\nIt can help to guide the regular expression based matching so that wildcards are assigned to the right parts of a file name. Consider the output file {sample}.{group}.txt and assume that the target file is A.1.normal.txt. It is not clear whether dataset=“A.1” and group=“normal” or dataset=“A” and group=“1.normal” is the right assignment. Here, constraining the dataset wildcard by {sample,[A-Z]+}.{group}solves the problem.\n\nWhen dealing with ambiguous rules, it is best practice to first try to solve the ambiguity by using a proper file structure, for example, by separating the output files of different steps in different directories.\n\n\nGenerate html report\nA basic report contains runtime statistics, a visualization of the workflow topology, used software and data provenance information.\n\nsnakemake --report report.html --rerun-incomplete\n\nIn addition, you can mark any output file generated in your workflow for inclusion into the report. It will be encoded directly into the report, such that it can be, e.g., emailed as a self-contained document. The reader (e.g., a collaborator of yours) can at any time download the enclosed results from the report for further use, e.g., in a manuscript you write together.\nLet’s mark the output file “results/plots/quals.svg” for inclusion by replacing it with report(“results/plots/quals.svg”, caption=“report/calling.rst”) and adding a file report/calling.rst, containing some description of the output file. This description will be presented as caption in the resulting report."
  },
  {
    "objectID": "source/metagenomics/singlem.html#singlem",
    "href": "source/metagenomics/singlem.html#singlem",
    "title": "Bioinformatics guidance page",
    "section": "SingleM",
    "text": "SingleM\n\nIntroduction\nSingleM is a tool for profiling shotgun metagenomes (Woodcroft et al. 2024). It was originally designed to determine the relative abundance of bacterial and archaeal taxa in a sample. As of version 0.19.0, it can also be used to profile dsDNA phages (see Lyrebird).\nIt shows good accuracy in estimating the relative abundances of community members, and has a particular strength in dealing with novel lineages. The method it uses also makes it suitable for some related tasks, such as assessing eukaryotic contamination, finding bias in genome recovery, and lineage-targeted MAG recovery. It can also be used as the basis for choosing metagenomes which, when coassembled, maximise the recovery of novel MAGs (see Bin Chicken).\nThe main idea of SingleM is to profile metagenomes by targeting short 20 amino acid stretches (“windows”) within single copy marker genes. It finds reads which cover an entire window, and analyses these further. By constraining analysis to these short windows, it becomes possible to know how novel each read is compared to known genomes. Then, using the fact that each analysed gene is (almost always) found exactly once in each genome, the abundance of each lineage can be accurately estimated.\nFor more information, visit the SingleM documentation.\n\n\nInstallation\nInstalled on Crunchomics: Yes,\n\nSingleM v0.19.0 is installed as part of the bioinformatics share. If you have access to Crunchomics and have not yet access to the bioinformatics share, then you can send an email with your Uva netID to Nina Dombrowski, n.dombrowski@uva.nl.\nAfterwards, you can add the bioinformatics share as follows (if you have already done this in the past, you don’t need to run this command):\n\n\nconda config --add envs_dirs /zfs/omics/projects/bioinformatics/software/miniconda3/envs/\n\nIf you want to install it yourself, you can run:\n\n\nShow the code\nmamba create --name singlem_0.19.0 -c bioconda singlem=0.19.0\n\n# Test installation \nconda activate singlem_0.19.0\nsinglem -h\nlyrebird -h\n\n# Get metadatapackage for newest gtdb version\n# Adjust directory path as needed\nsinglem data --output-directory data_dir\n\nconda deactivate\n\n\n\n\nUsage\nSingleM comes with several tools (subcommands), for full usage information, please visit the tools documentation.\n\nSingle sample\nThe main tool to use is SingleM pipe. In its most common usage, the SingleM pipe subcommand takes as input raw metagenomic reads and outputs a taxonomic profile. It can also take as input whole genomes (or contigs), and can output a table of OTUs. When using metagenomic reads, please use raw metagenomic reads, not quality trimmed reads. Quality trimming with e.g. Trimmomatic reads often makes them too short for SingleM to use.\n\nconda activate singlem_0.19.0\n\n# Add path to metadata\n# Adjust the path as needed when installing this on your own system\nexport SINGLEM_METAPACKAGE_PATH='/zfs/omics/projects/bioinformatics/databases/singlem/S5.4.0.GTDB_r226.metapackage_20250331.smpkg.zb'\n\n# Generate OTU tables with GTDB taxonomic profiles\nmkdir results \n\nsinglem pipe \\\n    -1 data/NIOZ114_R1.fastq.gz \\\n    -2 data/NIOZ114_R2.fastq.gz \\\n    -p results/output.profile.tsv \\\n    --otu-table results/output.table.tsv \\\n    --threads 10\n\nUseful options (for a full list use the tools help function or visit the documentation):\nSingleM pipe:\n\n-1: nucleotide read sequence(s) (forward or unpaired) to be searched. Can be FASTA or FASTQ format, GZIP-compressed or not.\n-2: reverse reads to be searched. Can be FASTA or FASTQ format, GZIP-compressed or not.\n--genome-fasta-files: nucleotide genome sequence(s) to be searched\n-p filename: output a ‘condensed’ taxonomic profile for each sample based on the OTU table. Taxonomic profiles output can be further converted to other formats using singlem summarise.\n--otu-table filename: outputs and output OTU table\n--assignment-method: Method of assigning taxonomy to OTUs and taxonomic profiles [default: smafa_naive_then_diamond]. Options: {smafa_naive_then_diamond, scann_naive_then_diamond, annoy_then_diamond, scann_then_diamond, diamond,diamond_example, annoy, pplacer}\n\n\n\nMultiple samples\nSingleM can be run on multiple samples. There are two ways. It is possible to specify multiple input files to the singlem pipe subcommand directly by space separating them. Alternatively singlem pipe can be run on each sample and OTU tables combined using singlem summarise. The results should be identical, though there are some performance trade-offs. For large numbers of metagenomes (&gt;100) it is probably preferable to run each sample individually or in smaller groups.\nNote that the performance of a single pipe when run on many genomes drastically improved in version 0.17.0, and it now sensible to run up to 10,000 genomes at a time.\nBelow the example of how to run SingleM individually followed by using singlem summarize. The SingleM summarise subcommand transforms taxonomic profiles and OTU tables into a variety of different formats. The summarise subcommand is useful for transforming the default output formats of pipe, visualising the results of a SingleM analysis, and for performing some downstream analyses.\n\nconda activate singlem_0.19.0\n\n# Add path to metadata\n# Adjust the path as needed when installing this on your own system\nexport SINGLEM_METAPACKAGE_PATH='/zfs/omics/projects/bioinformatics/databases/singlem/S5.4.0.GTDB_r226.metapackage_20250331.smpkg.zb'\n\n# Generate OTU tables with GTDB taxonomic profiles\n# Loop through all R1 files found in the data folder (assumes that the sample name is part of the file name)\nmkdir results \n\nfor R1 in data/*_R1.fastq.gz; do\n    # Derive corresponding R2 file name by replacing _R1 with _R2\n    R2=\"${R1/_R1/_R2}\"\n\n    # Extract the sample name from the file name\n    SAMPLE=$(basename \"$R1\" | sed 's/_R1.fastq.gz//')\n\n    # Run singlem pipe\n    singlem pipe \\\n        -1 \"$R1\" -2 \"$R2\" \\\n        -p results/output.${SAMPLE}.profile.tsv \\\n        --otu-table results/output.${SAMPLE}.table.tsv \\\n        --threads 10 \ndone\n\n# Generate one single otu table in long format\nsinglem summarise --input-otu-tables results/output.*.table.tsv \\\n    --output-otu-table results/combined.otu_table.csv\n\n# Generate Krona plots \nsinglem summarise --input-taxonomic-profile results/output.*.profile.tsv \\\n    --output-taxonomic-profile-krona results/doco_example.profile.html\n\n# Generate relative abundance and species by site tbles (for each taxonomic level)\nsinglem summarise --input-taxonomic-profile results/output.*.profile.tsv \\\n    --output-species-by-site-relative-abundance-prefix results/myprefix\n\n# Store data in long format with additional columns \nsinglem summarise --input-taxonomic-profile output.*.profile.tsv \\\n    --output-taxonomic-profile-with-extras results/doco_example.with_extras.tsv",
    "crumbs": [
      "Sequence data analyses",
      "(Meta)genomics",
      "SingleM"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#qiime2-terminology",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#qiime2-terminology",
    "title": "A short QIIME tutorial",
    "section": "QIIME2 terminology",
    "text": "QIIME2 terminology\nData produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact contains data and metadata. The metadata describes things about the data, such as its type, format, and how it was generated (i.e. provenance). A QIIME 2 artifact typically has the .qza file extension when stored in a file. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), you must create a QIIME 2 artifact by importing data.\nVisualizations are another type of data generated by QIIME 2. When written to disk, visualization files typically have the .qzv file extension. Visualizations contain similar types of metadata as QIIME 2 artifacts, including provenance information. Similar to QIIME 2 artifacts, visualizations are standalone information that can be archived or shared with collaborators. Use https://view.qiime2.org to easily view QIIME 2 artifacts and visualizations files.\nEvery artifact generated by QIIME 2 has a semantic type associated with it. Semantic types enable QIIME 2 to identify artifacts that are suitable inputs to an analysis. For example, if an analysis expects a distance matrix as input, QIIME 2 can determine which artifacts have a distance matrix semantic type and prevent incompatible artifacts from being used in the analysis (e.g. an artifact representing a phylogenetic tree).\nPlugins are software packages that can be developed by anyone. The QIIME 2 team has developed several plugins for an initial end-to-end microbiome analysis pipeline, but third-party developers are encouraged to create their own plugins to provide additional analyses. A method accepts some combination of QIIME 2 artifacts and parameters as input, and produces one or more QIIME 2 artifacts as output. A visualizer is similar to a method in that it accepts some combination of QIIME 2 artifacts and parameters as input. In contrast to a method, a visualizer produces exactly one visualization as output.",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#setup",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#setup",
    "title": "A short QIIME tutorial",
    "section": "Setup",
    "text": "Setup\n\nInstallation\nWe can install a QIIME 2 environment with with mamba:\n\nwget https://data.qiime2.org/distro/core/qiime2-2023.7-py38-linux-conda.yml\nmamba env create -n qiime2-2023.7 --file qiime2-2023.7-py38-linux-conda.yml\n\nIf you run into problem, have a look at QIIME 2 installation guide.\n\n\nDownload the example data\nNext, we download the example files for this tutorial. The folder contains:\n\nDe-multiplexed sequence fastq files in the data folder\nA manifest.csv file that lists the sampleID, sample location and read orientation in the data folder\nFiles generated during the workflow\n\n\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\ncd metabarcoding-qiime2-datapackage-v2021.06.2\n\n\n\nSet environmental variables\nNotice:\n\nChange the path stored in wdir to wherever you downloaded the data\nSetting an environmental variable to your working directory is not needed but useful to have if you resume an analysis after closing the terminal or simply to remind yourself where you analysed your data\n\n\n#set working environment\nwdir=\"&lt;path_to_your_downloaded_folder&gt;/metabarcoding-qiime2-datapackage-v2021.06.2/\"\ncd $wdir\n\n#activate QIIME environment \nconda deactivate\nmamba activate qiime2-2023.7\n\n\n\nGetting help\nThe help argument allows you to get an explanation about what each plugin in QIIME 2 does:\n\n#find out what plugins are installed\nqiime --help\n\n#get help for a plugin of interest\nqiime diversity --help\n\n#get help for an action in a plugin\nqiime diversity alpha-phylogenetic --help",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#quality-controls",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#quality-controls",
    "title": "A short QIIME tutorial",
    "section": "Quality controls",
    "text": "Quality controls\n\nFastQC\nBefore running any analysis, it is important to check the quality of the data.\nFastQC is a tool to judge the quality of sequencing data (in a visual way). It is installed on Crunchomics and if you want to install it on your own machine follow the instructions found here.\n\nmkdir -p prep/fastqc\nfastqc data/*gz -o prep/fastqc\n\nIf you want to know what the output means, you can look at this tutorial explaining each plot in detail.\nWith the data at hand we won’t encounter any problems, but if you do, you might want to consider quality filtering. This can be done with tools like FastP or Trimmomatic.",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#import-data",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#import-data",
    "title": "A short QIIME tutorial",
    "section": "Import data",
    "text": "Import data\n\nThe manifest file\nThe manifest file includes the sample ids, the path to where each sequence read file, i.e. fastq.gz file, is stored, and the read orientation. Because we have paired-end data and thus two files for each sample, we will list each sample twice, once for the forward and once for the reverse orientation. This is what the first few lines of the manifest file look like if you open the file manually or run head data/MANIFEST.csv in the terminal:\nsample-id,absolute-filepath,direction\nDUMMY10,$PWD/data/Sample-DUMMY10_R1.fastq.gz,forward\nDUMMY10,$PWD/data/Sample-DUMMY10_R2.fastq.gz,reverse\nDUMMY11,$PWD/data/Sample-DUMMY11_R1.fastq.gz,forward\nDUMMY11,$PWD/data/Sample-DUMMY11_R2.fastq.gz,reverse\nDUMMY12,$PWD/data/Sample-DUMMY12_R1.fastq.gz,forward\nDUMMY12,$PWD/data/Sample-DUMMY12_R2.fastq.gz,reverse\nNotice:\n\nThe manifest file is a csv file, i.e. a comma-separated file\nWe use $PWD, standing for print working directory, to say that the input files are in the data folder which is found in our working directory.\n\n\n\nData import\nWe can import our data into QIIME as follows:\n\n#prepare a folder in which we store our imported data\nmkdir -p prep\n\n#import the fastq files into QIIME\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path data/MANIFEST.csv \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#check artifact type\nqiime tools peek prep/demux-seqs.qza \n\n#generate summary\nqiime demux summarize \\\n    --i-data prep/demux-seqs.qza \\\n    --o-visualization prep/demux-seqs.qzv\n\nWe can visualize prep/demux-seqs.qzv with QIIME view to get an overview about our samples. Below is an example of what you might see:\n\nSome additional comments on importing data:\n\nA general tutorial on importing data can be found here\nYou can import data by:\n\nproviding the path to the sequences with --input-path\nOr you can import sequences using the maifest file, which maps the sample identifiers to a fastq.gz or fastq file. It requires a file path, which as to be an absolute filepath (but can use $PWD/) as well as the direction of the reads. The manifest file is compatible with the metadata format\n\nThere are different phred scores used in the input-format: Phred33 and Phred64. In Phred 64 files, you should not be seeing phred values &lt;64. But you can also look at upper case and lower case of quality representations (ascii letters). In general, lower case means 64.\n\nUseful Options:\n\n--type {TEXT} The semantic type of the artifact that will be created upon importing. Use –show-importable-types to see what importable semantic types are available in the current deployment. [required]\n--input-path {PATH} Path to file or directory that should be imported. [required]\n--output-path {ARTIFACT} Path where output artifact should be written. [required]\n--input-format {TEXT} The format of the data to be imported. If not provided, data must be in the format expected by the semantic type provided via –type.\n--show-importable-types Show the semantic types that can be supplied to –type to import data into an artifact.\n\n\n\nPrimer removal\nThe next step is to remove any primer sequences. We will use the cutadapt QIIME2 plugin for that. Because we have paired-end data, there is a forward and reverse primer, referenced by the parameters --p-front-f and --p-front-r.\n\nqiime cutadapt trim-paired \\\n    --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f GTGYCAGCMGCCGCGGTAA \\\n  --p-front-r CCGYCAATTYMTTTRAGTTT \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 2 \\\n  --verbose\n\n#generate summary \nqiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#export data (not necessary but to give an example how to do this)\nqiime tools extract \\\n    --output-path prep/trimmed_remove_primers \\\n    --input-path prep/trimmed-seqs.qza\n\nUseful options (for the full list, check the help function):\n\nThere are many ways an adaptor can be ligated. Check the help function for all options and if you do not know where the adaptor is contact your sequencing center.\n--p-front-f TEXT… Sequence of an adapter ligated to the 5’ end. Searched in forward read\n--p-front-r TEXT… Sequence of an adapter ligated to the 5’ end. Searched in reverse read\n--p-error-rate PROPORTION Range(0, 1, inclusive_end=True) Maximum allowed error rate. [default: 0.1]\n--p-indels / --p-no-indels Allow insertions or deletions of bases when matching adapters. [default: True]\n--p-times INTEGER Remove multiple occurrences of an adapter if it is Range(1, None) repeated, up to times times. [default: 1]\n--p-match-adapter-wildcards / --p-no-match-adapter-wildcards Interpret IUPAC wildcards (e.g., N) in adapters. [default: True]\n--p-minimum-length INTEGER Range(1, None) Discard reads shorter than specified value. Note, the cutadapt default of 0 has been overridden, because that value produces empty sequence records. [default: 1]\n--p-max-expected-errors NUMBER Range(0, None) Discard reads that exceed maximum expected erroneous nucleotides. [optional] --p-max-n NUMBER Discard reads with more than COUNT N bases. If Range(0, None) COUNT_or_FRACTION is a number between 0 and 1, it is interpreted as a fraction of the read length. [optional] --p-quality-cutoff-5end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 5 prime end. [default: 0] --p-quality-cutoff-3end INTEGER Range(0, None) Trim nucleotides with Phred score quality lower than threshold from 3 prime end. [default: 0] --p-quality-base INTEGER Range(0, None) How the Phred score is encoded (33 or 64). [default: 33]",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#generate-asvs-using-deblur-or-dada2",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#generate-asvs-using-deblur-or-dada2",
    "title": "A short QIIME tutorial",
    "section": "Generate ASVs using Deblur or DADA2",
    "text": "Generate ASVs using Deblur or DADA2\nIn order to minimize the risks of sequencer error in targeted sequencing, clustering approaches were initially developed. Clustering approaches are based upon the idea that related/similar organisms will have similar gene sequences and that rare sequencing errors will have a trivial contribution, if any, to the consensus sequence for these clusters, or operating taxonomic units (OTUs).\nIn contrast, ASV methods generate an error model tailored to an individual sequencing run and employing algorithms that use the model to distinguish between true biological sequences and those generated by error. The two ASV methods we explore in this tutorial are Deblur and DADA2.\nDADA2 implements an algorithm that models the errors introduced during amplicon sequencing, and uses that error model to infer the true sample composition. DADA2 replaces the traditional OTU-picking step in amplicon sequencing workflows and instead produces tables of amplicon sequence variants (ASVs).\nDeblur uses pre-computed error profiles to obtain putative error-free sequences from Illumina MiSeq and HiSeq sequencing platforms. Unlike DADA2, Deblur operates on each sample independently. Additionally, reads are depleted from sequencing artifacts either using a set of known sequencing artifacts (such as PhiX) (negative filtering) or using a set of known 16S sequences (positive filtering).\n\nDeblur\n\n1. Merge paired end reads\nNotice:\n\nDepending on the QIIME 2 version you use, you might need to change qiime vsearch join-pairs to vsearch merge-pairs.\nSome more details on merging reads can be found here\nIf you plan to use DADA2 to join and denoise your paired end data, do not join your reads prior to denoising with DADA2; DADA2 expects reads that have not yet been joined, and will join the reads for you during the denoising process.\nSome notes on overlap calculation for dada2\nAnother option for joining is fastq-join\n\n\nmkdir -p deblur\n\n#merge reads\nqiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 2 \\\n  --verbose\n\n#summarize data\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nUseful options:\n\n--p-minlen {INTEGER} Sequences shorter than minlen after truncation are Range(0, None) discarded. [default: 1]\n--p-maxns {INTEGER} Sequences with more than maxns N characters are Range(0, None) discarded. [optional]\n--p-maxdiffs {INTEGER} Maximum number of mismatches in the area of overlap Range(0, None) during merging. [default: 10]\n--p-minmergelen {INTEGER Range(0, None) } Minimum length of the merged read to be retained. [optional]\n--p-maxee {NUMBER} Maximum number of expected errors in the merged read Range(0.0, None) to be retained. [optional]\n--p-threads {INTEGER Range(0, 8, inclusive_end=True)} The number of threads to use for computation. Does not scale much past 4 threads. [default: 1]\n\nWhen running this, we will get a lot of information printed to the screen. It is useful to look at this to know if the merge worked well. For example, we see below that ~90% of our sequences merged well, telling us that everything went fine. Much lower values might indicate some problems with the trimmed data.\nMerging reads 100%\n     45195  Pairs\n     40730  Merged (90.1%)\n      4465  Not merged (9.9%)\n\nPairs that failed merging due to various reasons:\n        50  too few kmers found on same diagonal\n        30  multiple potential alignments\n      1878  too many differences\n      1790  alignment score too low, or score drop too high\n       717  staggered read pairs\n\nStatistics of all reads:\n    233.02  Mean read length\n\nStatistics of merged reads:\n    374.39  Mean fragment length\n     14.67  Standard deviation of fragment length\n      0.32  Mean expected error in forward sequences\n      0.78  Mean expected error in reverse sequences\n      0.51  Mean expected error in merged sequences\n      0.21  Mean observed errors in merged region of forward sequences\n      0.71  Mean observed errors in merged region of reverse sequences\n      0.92  Mean observed errors in merged region\nNotice that if we look at deblur/joined-seqs.qzv with QIIME view we see an increased quality score in the middle of the region:\n\nWhen merging reads with tools like vsearch, the quality scores often increase, not decrease, in the region of overlap. The reason being, if the forward and reverse read call the same base at the same position (even if both are low quality), then the quality estimate for the base in that position goes up. That is, you have two independent observations of that base in that position. This is often the benefit of being able to merge paired reads, as you can recover / increase your confidence of the sequence in the region of overlap.\n\n\n2. Quality filtering\nNotice: Both DADA and deblur will do some quality filtering, so we do not need to be too strict here. One could consider whether to even include this step, however, running this might be useful to reduce the dataset site.\n\nqiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\n#summarize stats\nqiime metadata tabulate \\\n   --m-input-file deblur/filt-stats.qza \\\n   --o-visualization deblur/filt-stats.qzv\n\nDefault settings:\n\n--p-min-quality4 All PHRED scores less that this value are considered to be low PHRED scores.\n--p-quality-window 3: The maximum number of low PHRED scores that can be observed in direct succession before truncating a sequence read.\n--p-min-length-fraction 0.75: The minimum length that a sequence read can be following truncation and still be retained. This length should be provided as a fraction of the input sequence length. --p-max-ambiguous 0: The maximum number of ambiguous (i.e., N) base calls.\n\n\n\n3. Create ASVs with deblur\n\n#default settings\nqiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length 370 \\\n  --o-representative-sequences deblur/deblur-reprseqs2.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 2 \\\n  --verbose\n\nHow to set the parameters:\n\nWhen you use deblur-denoise, you need to truncate your fragments such that they are all of the same length. The position at which sequences are truncated is specified by the --p-trim-length parameter. Any sequence that is shorter than this value will be lost from your analyses. Any sequence that is longer will be truncated at this position.\nTo decide on what value to choose, inspect deblur/filt-seqs.qzv with QIIME view. Once you open the interactive quality plot and scroll down we see the following:\n\n\n\n\n\n\n\nWe can set --p-trim-length of 370, because that resulted in minimal data loss. That is, only &lt;9% of the reads were discarded for being too short, and only 4 bases were trimmed off from sequences of median length.\n\nUseful options:\n\n--p-trim-length INTEGER Sequence trim length, specify -1 to disable trimming. [required]\n--p-left-trim-len {INTEGER} Range(0, None) Sequence trimming from the 5’ end. A value of 0 will disable this trim. [default: 0]\n--p-mean-error NUMBER The mean per nucleotide error, used for original sequence estimate. [default: 0.005]\n--p-indel-probNUMBER Insertion/deletion (indel) probability (same for N indels). [default: 0.01]\n--p-indel-max INTEGER Maximum number of insertion/deletions. [default: 3]\n--p-min-reads INTEGER Retain only features appearing at least min-reads times across all samples in the resulting feature table. [default: 10]\n--p-min-size INTEGER In each sample, discard all features with an abundance less than min-size. [default: 2]\n--p-jobs-to-start INTEGER Number of jobs to start (if to run in parallel). [default: 1]\n\nGenerated outputs:\n\nA feature table artifact with the frequencies per sample and feature.\nA representative sequences artifact with one single fasta sequence for each of the features in the feature table.\nA stats artifact with details of how many reads passed each filtering step of the deblur procedure.\n\nWe can create visualizations from the different outputs as follows:\n\n#look at stats\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats_noremoval.qza \\\n  --o-visualization deblur/deblur-stats_noremova.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table_noremoval.qza \\\n  --o-visualization deblur/deblur-table_noremova.qzv\n\nIf we check the visualization with QIIME 2 view (or qiime tools view deblur/deblur-table.qzv) we get some feelings about how many ASVs were generated and where we lost things. Often times the Interactive sample detail page can be particularily interesting. This shows you the frequency per sample. If there is large variation in this number between samples, it means it is difficult to directly compare samples. In that case it is often recommended to standardize your data by for instance rarefaction. Rarefaction will not be treated in detail in the tutorial, but the idea is laid out below.\n\nOut of the 1,346,802 filtered and merged reads we get 3,636 features with a total frequency of 290,671. So we only retained ~21%. If we compare this to the QIIME CMI tutorial we have 662 features with a total frequency of 1,535,691 (~84) so this is definitely something to watch out for.\nWith this dataset we loose a lot to “fraction-artifact-with-minsize” . This value is related to the --p-min-size parameter which sets the min abundance of a read to be retained (default 2). So we simply might loose things because it is a subsetted dataset and it means that most of your reads are singletons and so are discarded. For a real dataset this could be due to improper merging, not having removed primer/barcodes from your reads, or that there just is that many singletons (though unlikely imo)\nloss of sequences with merged reads in deblur compared to just the forward reads has been observed and discussed before see here\n\n\n\n\nqiime tools validate: Code for sanity checking\nThe code below is not required to be run. However, the code is useful in case something goes wrong and you need to check if the file you work with is corrupted.\n\n#check file: Result deblur/filt-seqs.qza appears to be valid at level=max.\nqiime tools validate deblur/filt-seqs.qza\n\n\n\nDada2\nThe denoise_paired action requires a few parameters that you’ll set based on the sequence quality score plots that you previously generated in the summary of the demultiplex reads. You should review those plots to:\n\nidentify where the quality begins to decrease, and use that information to set the trunc_len_* parameters. You’ll set that for both the forward and reverse reads using the trunc_len_f and trunc_len_r parameters, respectively.\nIf you notice a region of lower quality in the beginning of the forward and/or reverse reads, you can optionally trim bases from the beginning of the reads using the trim_left_f and trim_left_r parameters for the forward and reverse reads, respectively.\nIf you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads\nFigaro is a tool to automatically choose the trunc_len\nChimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to running DADA2\nYour reads must still overlap after truncation in order to merge them later!\n\n\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trim-left-f 0 \\\n  --p-trim-left-r 0 \\\n  --p-trunc-len-f 230 \\\n  --p-trunc-len-r 220 \\\n  --p-n-threads 4 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose\n\nA word of caution: The --p-trunc-len-f and --p-trunc-len-r parameters are the most important parameters to set correctly. If you set these too high, you will lose too many reads. If you set them too low, you will have too many errors in your reads. The --p-trim-left-f and --p-trim-left-r parameters are less important, but can be useful if you have low quality bases at the beginning of your reads.\nLet’s look at the impact of the --p-trunc-len-r/f settings in our example:\n\nIn our calculation at the start of the tutorial, we said that the overlap was ~90 bp.\nIn the example below, we set the truncation length to 230 and 220 for the forward and reverse reads, respectively.\nIf we now calculate the overlap with (length of forward read)+(length of reverse read)−(amplicon length) we get 230 + 220 - 411 = 39 bp.\nDada2 requires a minimum overlap of 12 bp, so we are good to go.\n\nUseful options:\n\n--p-trunc-len-f INTEGER Position at which forward read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trunc-len-r INTEGER Position at which reverse read sequences should be truncated due to decrease in quality. This truncates the 3’ end of the of the input sequences, which will be the bases that were sequenced in the last cycles. Reads that are shorter than this value will be discarded. After this parameter is applied there must still be at least a 12 nucleotide overlap between the forward and reverse reads. If 0 is provided, no truncation or length filtering will be performed [required]\n--p-trim-left-f INTEGER Position at which forward read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-trim-left-r INTEGER Position at which reverse read sequences should be trimmed due to low quality. This trims the 5’ end of the input sequences, which will be the bases that were sequenced in the first cycles. [default: 0]\n--p-max-ee-f NUMBER Forward reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-max-ee-r NUMBER Reverse reads with number of expected errors higher than this value will be discarded. [default: 2.0]\n--p-trunc-q INTEGER Reads are truncated at the first instance of a quality score less than or equal to this value. If the resulting read is then shorter than trunc-len-f or trunc-len-r (depending on the direction of the read) it is discarded. [default: 2]\n--p-min-overlap INTEGER Range(4, None) The minimum length of the overlap required for merging the forward and reverse reads. [default: 12]\n--p-pooling-method TEXT Choices(‘independent’, ‘pseudo’) The method used to pool samples for denoising. “independent”: Samples are denoised indpendently. “pseudo”: The pseudo-pooling method is used to approximate pooling of samples. In short, samples are denoised independently once, ASVs detected in at least 2 samples are recorded, and samples are denoised independently a second time, but this time with prior knowledge of the recorded ASVs and thus higher sensitivity to those ASVs. [default: ‘independent’]\n--p-chimera-method TEXT Choices(‘consensus’, ‘none’, ‘pooled’) The method used to remove chimeras. “none”: No chimera removal is performed. “pooled”: All reads are pooled prior to chimera detection. “consensus”: Chimeras are detected in samples individually, and sequences found chimeric in a sufficient fraction of samples are removed. [default: ‘consensus’]\n--p-min-fold-parent-over-abundance NUMBER The minimum abundance of potential parents of a sequence being tested as chimeric, expressed as a fold-change versus the abundance of the sequence being tested. Values should be greater than or equal to 1 (i.e. parents should be more abundant than the sequence being tested). This parameter has no effect if chimera-method is “none”. [default: 1.0]\n--p-n-threads INTEGER The number of threads to use for multithreaded processing. If 0 is provided, all available cores will be used. [default: 1]\n--p-n-reads-learn INTEGER The number of reads to use when training the error model. Smaller numbers will result in a shorter run time but a less reliable error model. [default: 1000000]\n\nLet’s now summarize the DADA2 output:\n\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nNotes:\n\nSanity check: in the stats visualizations for both DADA2 and deblur check that outside of filtering, there should be no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification\nsequences that are much longer or shorter than expected may be the result of non-specific priming. You could consider removing those sequences from the ASV table.\n\nIf we compare the two tools, we should see something like this:\n\nDeblur:\n\n3,636 features\n290,671 total frequency (out of 1,346,802 filtered and merged reads, ca 21%)\nnothing at frequencies below 2500, most samples with frequencies between 2500-7500, 1 sample with 20 000\n\nDada2:\n\n13,934 features\n648,666 total frequency (out of 1554929 trimmed reads, ~41%)\nmost samples at 10,000 frequency/sample peak, histrogram looks closer to normal distribution, 1 sample with ~37 000 frequencies\n\n\nOveral Dada2 seems beneficial in that less reads are lost but without some deeper dive into the data the qzv files are not informative enough to decide what happened to the sequences. The reason why more DADA2 retains more reads is related to the quality filtering steps. To better compare sequences, we could compare them at a sampling depth of 2000 and should see:\n\nRetained 94,000 (32.34%) features in 47 (100.00%) samples (deblur) instead of 290,671 (100.00%) features in 47 (100.00%)\nRetained 94,000 (14.49%) features in 47 (95.92%) samples (dada2) instead of 648,666 (100.00%) features in 49 (100.00%)\n\nBased on that both methods produce very similar results.",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#taxonomic-classification-naive-bayes",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#taxonomic-classification-naive-bayes",
    "title": "A short QIIME tutorial",
    "section": "Taxonomic classification (Naive Bayes)",
    "text": "Taxonomic classification (Naive Bayes)\nTo identify the organisms in a sample it is usually not enough using the closest alignment — because other sequences that are equally close matches or nearly as close may have different taxonomic annotations. So we use taxonomy classifiers to determine the closest taxonomic affiliation with some degree of confidence or consensus (which may not be a species name if one cannot be predicted with certainty!), based on alignment, k-mer frequencies, etc.\nq2-feature-classifier contains three different classification methods:\n\nclassify-consensus-blast\nclassify-consensus-vsearch\nMachine-learning-based classification methods\n\nThe first two are alignment-based methods that find a consensus assignment across N top hits. These methods take reference database FeatureData[Taxonomy] and FeatureData[Sequence] files directly, and do not need to be pre-trained\nMachine-learning-based classification methods are available through classify-sklearn, and theoretically can apply any of the classification methods available in scikit-learn. These classifiers must be trained, e.g., to learn which features best distinguish each taxonomic group, adding an additional step to the classification process. Classifier training is reference database- and marker-gene-specific and only needs to happen once per marker-gene/reference database combination; that classifier may then be re-used as many times as you like without needing to re-train! QIIME 2 developers provide several pre-trained classifiers for public use.\nIn general classify-sklearn with a Naive Bayes classifier can slightly outperform other methods based on several criteria for classification of 16S rRNA gene and fungal ITS sequences. It can be more difficult and frustrating for some users, however, since it requires that additional training step. That training step can be memory intensive, becoming a barrier for some users who are unable to use the pre-trained classifiers.\nCheck out the Qiime info page for other classifiers.\nNotes:\n\nTaxonomic classifiers perform best when they are trained based on your specific sample preparation and sequencing parameters, including the primers that were used for amplification and the length of your sequence reads. Therefore in general you should follow the instructions in Training feature classifiers with q2-feature-classifier to train your own taxonomic classifiers (for example, from the marker gene reference databases below).\nGreengenes2 has succeeded Greengenes 13_8\nThe Silva classifiers provided here include species-level taxonomy. While Silva annotations do include species, Silva does not curate the species-level taxonomy so this information may be unreliable.\nCheck out this study to learn more about QIIMEs feature classifier [@bokulich2018].\nCheck out this study discussing reproducible sequence taxonomy reference database management [@ii2021].\nCheck out this study about incorporating environment-specific taxonomic abundance information in taxonomic classifiers [@kaehler2019].\n\nFor the steps below a classifier using SILVA_138_99 is pre-computed the classifier. Steps outlining how to do this can be found here.\nRequirements:\n\nThe reference sequences\nThe reference taxonomy\n\n\nImport the database\nFirst, we import the SILVA database (consisting of fasta sequences and ta taxonomy table) into QIIME:\n\nqiime tools import \\\n  --type \"FeatureData[Sequence]\" \\\n  --input-path db/SILVA_138_99_16S-ref-seqs.fna \\\n  --output-path db/SILVA_138_99_16S-ref-seqs.qza\n\nqiime tools import \\\n  --type \"FeatureData[Taxonomy]\" \\\n  --input-format HeaderlessTSVTaxonomyFormat \\\n  --input-path db/SILVA_138_99_16S-ref-taxonomy.txt \\\n  --output-path db/SILVA_138_99_16S-ref-taxonomy.qza\n\n\n\nExtract reference reads\nWe can now use these same sequences we used to amplify our 16S sequences to extract the corresponding region of the 16S rRNA sequences from the SILVA database, like so:\n\nqiime feature-classifier extract-reads \\\n  --i-sequences db/SILVA_138_99_16S-ref-seqs.qza \\\n  --p-f-primer GTGYCAGCMGCCGCGGTAA \\\n  --p-r-primer CCGYCAATTYMTTTRAGTTT \\\n  --o-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --p-n-jobs 2\n\nOptions:\n\n--p-trim-right 0\n--p-trunc-len 0\n--p-identity {NUMBER} minimum combined primer match identity threshold. [default: 0.8] –p-min-length {INTEGER} Minimum amplicon length. Shorter amplicons are Range(0, None) discarded. Applied after trimming and truncation, so be aware that trimming may impact sequence retention. Set to zero to disable min length filtering. [default: 50]\n--p-max-length 0\n\nThis results in :\n\nMin length: 56 (we have partial sequences)\nMax length: 2316 (indicates we might have some non-16S seqs or some 16S sequences have introns)\nMean length: 394 (What we would expect based on our 16S data)\n\n\n\nTrain the classifier\nNext, we use the reference fragments you just created to train your classifier specifically on your region of interest.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have &gt;32GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the db folder.\n\n\n\nqiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/SILVA_138_99_16S-ref-frags.qza \\\n  --i-reference-taxonomy db/SILVA_138_99_16S-ref-taxonomy.qza \\\n  --o-classifier db/SILVA_138_99_16S-ref-classifier.qza\n\n\n\nTaxonomic classification\nNow that we have a trained classifier and a set of representative sequences from our DADA2-denoise analyses we can finally classify the sequences in our dataset.\n\n\n\n\n\n\nWarning\n\n\n\nYou should only run this step if you have ~50 GB of RAM available!\nIf you run this on a desktop computer, you can use the pre-computed classifier found in the taxonomy folder.\n\n\n\nqiime feature-classifier classify-sklearn \\\n  --i-classifier db/SILVA_138_99_16S-ref-classifier.qza \\\n  --p-n-jobs 16 \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza\n\nArguments:\n\n--p-confidence {VALUE Float % Range(0, 1, inclusive_end=True) | Str % Choices(‘disable’)} Confidence threshold for limiting taxonomic depth. Set to “disable” to disable confidence calculation, or 0 to calculate confidence but not apply it to limit the taxonomic depth of the assignments. [default: 0.7]\n--p-read-orientation {TEXT Choices(‘same’, ‘reverse-complement’, ‘auto’)} Direction of reads with respect to reference sequences. same will cause reads to be classified unchanged; reverse-complement will cause reads to be reversed and complemented prior to classification. “auto” will autodetect orientation based on the confidence estimates for the first 100 reads. [default: ‘auto’]\n\nNext, view the data with:\n\nqiime metadata tabulate \\\n --m-input-file taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nqiime2 tools view taxonomy/dada2-SILVA_138_99_16S-taxonomy.qzv\n\nWe see:\n\nThat this also reports a confidence score ranging between 0.7 and 1. The lower limit of 0.7 is the default value (see also the qiime feature-classifier classify-sklearn help function). You can opt for a lower value to increase the number of features with a classification, but beware that that will also increase the risk of spurious classifcations!\n\n\n\nPlot data\nBefore running the command, we will need to prepare a metadata file. This metadata file should contain information on the samples. For instance, at which depth was the sample taken, from which location does it come, was it subjected to experimental or control treatment etc. etc. This information is of course very specific to the study design but at the very least it should look like this (see also data/META.tsv):\n#SampleID\n#q2:types\nDUMMY1\nDUMMY10\n...\nbut we can add any variables, like so:\n#SampleID    BarcodeSequence Location        depth   location        treatment       grainsize       flowrate        age\n#q2:types    categorical     categorical     categorical     catechorical    categorical     numeric numeric numeric\n&lt;your data&gt;\nNext, we can generate a barplot like this:\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-SILVA_138_99_16S-taxonomy.qza \\\n  --m-metadata-file data/META.tsv \\\n  --o-visualization taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv\n\nqiime tools view taxonomy/dada2-SILVA_138_99_16S-taxplot.qzv",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#filtering",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#filtering",
    "title": "A short QIIME tutorial",
    "section": "Filtering",
    "text": "Filtering\n\nFiltering the feature table\nThese are just some general comments on filtering ASV tables and are not yet implemented in this specific tutorial. A full description of filtering methods can be found on the QIIME website\n\n#filter using the metadata info \n#filtering\nqiime feature-table filter-samples \\\n  --i-table feature-table.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-where 'autoFmtGroup IS NOT NULL' \\\n  --o-filtered-table autofmt-table.qza\n\n#filter out samples from which we have obtained fewer than 10,000 sequences\nqiime feature-table filter-samples \\\n  --i-table filtered-table-3.qza \\\n  --p-min-frequency 10000 \\\n  --o-filtered-table filtered-table-4.qza\n\n#filter features from the feature table if they don’t occur in at least two samples\nqiime feature-table filter-features \\\n  --i-table filtered-table-1.qza \\\n  --p-min-samples 2 \\\n  --o-filtered-table filtered-table-2.qza\n\n#create imaginary list of ids to keep and use this to filter\necho L1S8 &gt;&gt; samples-to-keep.tsv\n\nqiime feature-table filter-samples \\\n  --i-table table.qza \\\n  --m-metadata-file samples-to-keep.tsv \\\n  --o-filtered-table id-filtered-table.qza\n\nOptions for qiime feature-table filter-samples:\n\nAny features with a frequency of zero after sample filtering will also be removed.\n--p-min-frequency {INTEGER} The minimum total frequency that a sample must have to be retained. [default: 0]\n--p-max-frequency {INTEGER} The maximum total frequency that a sample can have to be retained. If no value is provided this will default to infinity (i.e., no maximum frequency filter will be applied). [optional]\n--p-min-features {INTEGER} The minimum number of features that a sample must have to be retained. [default: 0]\n--p-max-features {INTEGER}\n--m-metadata-file METADATA… (multiple Sample metadata used with where parameter when arguments will selecting samples to retain, or with exclude-ids when be merged) selecting samples to discard. [optional]\n--p-where {TEXT} SQLite WHERE clause specifying sample metadata criteria that must be met to be included in the filtered feature table. If not provided, all samples in metadata that are also in the feature table will be retained. [optional] --p-exclude-ids / --p-no-exclude-ids If true, the samples selected by metadata or where parameters will be excluded from the filtered table instead of being retained. [default: False] --p-filter-empty-features / --p-no-filter-empty-features If true, features which are not present in any retained samples are dropped. [default: True]\n--p-min-samples {number}: filter features from a table contingent on the number of samples they’re observed in.\n--p-min-features {number}: filter samples that contain only a few features\n\n\n\nFiltering the sequences\n\n#filter the representative sequences based on a filtered ASV table\nqiime feature-table filter-seqs \\\n  --i-data rep-seqs.qza \\\n  --i-table filtered-table-2.qza \\\n  --o-filtered-data filtered-sequences-1.qza\n\n\n\nFilter by taxonomy\nA common step in 16S analysis is to remove sequences from an analysis that aren’t assigned to a phylum. In a human microbiome study such as this, these may for example represent reads of human genome sequence that were unintentionally sequences.\nHere, we:\n\nspecify with the include paramater that an annotation must contain the text p__, which in the Greengenes taxonomy is the prefix for all phylum-level taxonomy assignments. Taxonomic labels that don’t contain p__ therefore were maximally assigned to the domain (i.e., kingdom) level.\nremove features that are annotated with p__; (which means that no named phylum was assigned to the feature), as well as annotations containing Chloroplast or Mitochondria (i.e., organelle 16S sequences).\n\n\nqiime taxa filter-table \\\n  --i-table filtered-table-2.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode contains \\\n  --p-include p__ \\\n  --p-exclude 'p__;,Chloroplast,Mitochondria' \\\n  --o-filtered-table filtered-table-3.qza\n\nOther options:\nFilter by taxonomy:\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria \\\n  --o-filtered-table table-no-mitochondria.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --o-filtered-table table-with-phyla.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-table table-with-phyla-no-mitochondria-no-chloroplast.qza\n\nqiime taxa filter-table \\\n  --i-table table.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-mode exact \\\n  --p-exclude \"k__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__mitochondria\" \\\n  --o-filtered-table table-no-mitochondria-exact.qza\n\nWe can also filter our sequences:\n\nqiime taxa filter-seqs \\\n  --i-sequences sequences.qza \\\n  --i-taxonomy taxonomy.qza \\\n  --p-include p__ \\\n  --p-exclude mitochondria,chloroplast \\\n  --o-filtered-sequences sequences-with-phyla-no-mitochondria-no-chloroplast.qza",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#phylogenetic-tree-construction",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#phylogenetic-tree-construction",
    "title": "A short QIIME tutorial",
    "section": "Phylogenetic tree construction",
    "text": "Phylogenetic tree construction\nNote: This are general notes and not yet implemented in this tutorial\n\nBasics\nOne advantage of pipelines is that they combine ordered sets of commonly used commands, into one condensed simple command. To keep these “convenience” pipelines easy to use, it is quite common to only expose a few options to the user. That is, most of the commands executed via pipelines are often configured to use default option settings. However, options that are deemed important enough for the user to consider setting, are made available. The options exposed via a given pipeline will largely depend upon what it is doing. Pipelines are also a great way for new users to get started, as it helps to lay a foundation of good practices in setting up standard operating procedures.\nRather than run one or more of the following QIIME 2 commands listed below:\nqiime alignment mafft ...\n\nqiime alignment mask ...\n\nqiime phylogeny fasttree ...\n\nqiime phylogeny midpoint-root ...\nWe can make use of the pipeline align-to-tree-mafft-fasttree to automate the above four steps in one go. Here is the description taken from the pipeline help doc:\nMore details can be found in the QIIME docs.\nIn general there are two approaches:\n\nA reference-based fragment insertion approach. Which, is likely the ideal choice. Especially, if your reference phylogeny (and associated representative sequences) encompass neighboring relatives of which your sequences can be reliably inserted. Any sequences that do not match well enough to the reference are not inserted. For example, this approach may not work well if your data contain sequences that are not well represented within your reference phylogeny (e.g. missing clades, etc.). For more information, check out these great fragment insertion examples.\nA de novo approach. Marker genes that can be globally aligned across divergent taxa, are usually amenable to sequence alignment and phylogenetic investigation through this approach. Be mindful of the length of your sequences when constructing a de novo phylogeny, short reads many not have enough phylogenetic information to capture a meaningful phylogeny.\n\n\n\nRuning the Qiime pipeline\n\nqiime phylogeny align-to-tree-mafft-fasttree \\\n  --i-sequences filtered-sequences-2.qza \\\n  --output-dir phylogeny-align-to-tree-mafft-fasttree\n\nThe final unrooted phylogenetic tree will be used for analyses that we perform next - specifically for computing phylogenetically aware diversity metrics. While output artifacts will be available for each of these steps, we’ll only use the rooted phylogenetic tree later.\nNotice:\n\nFor an easy and direct way to view your tree.qza files, upload them to iTOL. Here, you can interactively view and manipulate your phylogeny. Even better, while viewing the tree topology in “Normal mode”, you can drag and drop your associated alignment.qza (the one you used to build the phylogeny) or a relevent taxonomy.qza file onto the iTOL tree visualization. This will allow you to directly view the sequence alignment or taxonomy alongside the phylogeny\n\nMethods in qiime phylogeny:\n\nalign-to-tree-mafft-iqtree\niqtree Construct a phylogenetic tree with IQ-TREE.\niqtree-ultrafast-bootstrap Construct a phylogenetic tree with IQ-TREE with bootstrap supports.\nmidpoint-root Midpoint root an unrooted phylogenetic tree.\nrobinson-foulds Calculate Robinson-Foulds distance between phylogenetic trees.\n\n\n\nRun things step by step\nBefore running iq-tree check out:\n\nqiime alignment mafft\nqiime alignment mask\n\nExample to run the iqtree command with default settings and automatic model selection (MFP) is like so:\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --o-tree iqt-tree.qza \\\n  --verbose\n\nExample running iq-tree with single branch testing:\nSingle branch tests are commonly used as an alternative to the bootstrapping approach we’ve discussed above, as they are substantially faster and often recommended when constructing large phylogenies (e.g. &gt;10,000 taxa).\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-alrt 1000 \\\n  --p-abayes \\\n  --p-lbp 1000 \\\n  --p-substitution-model 'GTR+I+G' \\\n  --o-tree iqt-sbt-tree.qza \\\n  --verbose\n\nIQ-tree settings:\nThere are quite a few adjustable parameters available for iqtree that can be modified improve searches through “tree space” and prevent the search algorithms from getting stuck in local optima.\nOne particular best practice to aid in this regard, is to adjust the following parameters: –p-perturb-nni-strength and –p-stop-iter (each respectively maps to the -pers and -nstop flags of iqtree ).\nIn brief, the larger the value for NNI (nearest-neighbor interchange) perturbation, the larger the jumps in “tree space”. This value should be set high enough to allow the search algorithm to avoid being trapped in local optima, but not to high that the search is haphazardly jumping around “tree space”. One way of assessing this, is to do a few short trial runs using the --verbose flag. If you see that the likelihood values are jumping around to much, then lowering the value for --p-perturb-nni-strength may be warranted.\nAs for the stopping criteria, i.e. --p-stop-iter, the higher this value, the more thorough your search in “tree space”. Be aware, increasing this value may also increase the run time. That is, the search will continue until it has sampled a number of trees, say 100 (default), without finding a better scoring tree. If a better tree is found, then the counter resets, and the search continues.\nThese two parameters deserve special consideration when a given data set contains many short sequences, quite common for microbiome survey data. We can modify our original command to include these extra parameters with the recommended modifications for short sequences, i.e. a lower value for perturbation strength (shorter reads do not contain as much phylogenetic information, thus we should limit how far we jump around in “tree space”) and a larger number of stop iterations.\n\nqiime phylogeny iqtree \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-fast-tree.qza \\\n  --verbose\n\niqtree-ultrafast-bootstrap:\nwe can also use IQ-TREE to evaluate how well our splits / bipartitions are supported within our phylogeny via the ultrafast bootstrap algorithm. Below, we’ll apply the plugin’s ultrafast bootstrap command: automatic model selection (MFP), perform 1000 bootstrap replicates (minimum required), set the same generally suggested parameters for constructing a phylogeny from short sequences, and automatically determine the optimal number of CPU cores to use:\n\nqiime phylogeny iqtree-ultrafast-bootstrap \\\n  --i-alignment masked-aligned-rep-seqs.qza \\\n  --p-perturb-nni-strength 0.2 \\\n  --p-stop-iter 200 \\\n  --p-n-cores 1 \\\n  --o-tree iqt-nnisi-bootstrap-tree.qza \\\n  --verbose",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#identifying-an-even-sampling-depth-for-use-in-diversity-metrics",
    "title": "A short QIIME tutorial",
    "section": "Identifying an even sampling depth for use in diversity metrics",
    "text": "Identifying an even sampling depth for use in diversity metrics\n\nNotes on rarefication\nNotice: Notes copied taken from here\nFeature tables are composed of sparse and compositional data [@gloor2017]. Measuring microbial diversity using 16S rRNA sequencing is dependent on sequencing depth. By chance, a sample that is more deeply sequenced is more likely to exhibit greater diversity than a sample with a low sequencing depth.\nRarefaction is the process of subsampling reads without replacement to a defined sequencing depth, thereby creating a standardized library size across samples. Any sample with a total read count less than the defined sequencing depth used to rarefy will be discarded. Post-rarefaction all samples will have the same read depth. How do we determine the magic sequencing depth at which to rarefy? We typically use an alpha rarefaction curve.\nAs the sequencing depth increases, you recover more and more of the diversity observed in the data. At a certain point (read depth), diversity will stabilize, meaning the diversity in the data has been fully captured. This point of stabilization will result in the diversity measure of interest plateauing.\nNotice:\n\nYou may want to skip rarefaction if library sizes are fairly even. Rarefaction is more beneficial when there is a greater than ~10x difference in library size [@weiss2017].\nRarefying is generally applied only to diversity analyses, and many of the methods in QIIME 2 will use plugin specific normalization methods\n\nSome literature on the debate:\n\nWaste not, want not: why rarefying microbiome data is inadmissible [@mcmurdie2014]\nNormalization and microbial differential abundance strategies depend upon data characteristics [@weiss2017]\n\n\n\nAlpha rarefaction plots\nAlpha diversity is within sample diversity. When exploring alpha diversity, we are interested in the distribution of microbes within a sample or metadata category. This distribution not only includes the number of different organisms (richness) but also how evenly distributed these organisms are in terms of abundance (evenness).\nRichness: High richness equals more ASVs or more phylogenetically dissimilar ASVs in the case of Faith’s PD. Diversity increases as richness and evenness increase. Evenness: High values suggest more equal numbers between species.\nHere is a description of the different metrices we can use. And check here for a comparison of indices.\nList of indices:\n\nShannon’s diversity index (a quantitative measure of community richness)\nObserved Features (a qualitative measure of community richness)\nFaith’s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features)\nEvenness (or Pielou’s Evenness; a measure of community evenness)\n\nAn important parameter that needs to be provided to this script is --p-sampling-depth, which is the even sampling (i.e. rarefaction) depth. Because most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if you provide --p-sampling-depth 500, this step will subsample the counts in each sample without replacement so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be dropped from the diversity analysis.\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --p-metrics shannon \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/shannon-rarefaction-plot.qzv\n\nNote: The value that you provide for --p-max-depth should be determined by reviewing the “Frequency per sample” information presented in the table.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don’t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth.\nInclude phylo:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --p-metrics faith_pd \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/faith-rarefaction-plot.qzv\n\nGenerate different metrics at the same time:\n\nqiime diversity alpha-rarefaction \\\n  --i-table filtered-table-4.qza \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-max-depth 33000 \\\n  --o-visualization visualizations/alpha-rarefaction-plot.qzv \n\nWe can use this plot in combination with our feature table summary to decide at which depth we would like to rarefy. We need to choose a sequencing depth at which the diveristy in most of the samples has been captured and most of the samples have been retained.\nChoosing this value is tricky. We recommend making your choice by reviewing the information presented in the feature table summary file. Choose a value that is as high as possible (so you retain more sequences per sample) while excluding as few samples as possible.",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#computing-diversity-metrics",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#computing-diversity-metrics",
    "title": "A short QIIME tutorial",
    "section": "Computing diversity metrics",
    "text": "Computing diversity metrics\nNote: These are general notes and not yet implemented in this tutorial\ncore-metrics-phylogeneticrequires your feature table, your rooted phylogenetic tree, and your sample metadata as input. It additionally requires that you provide the sampling depth that this analysis will be performed at. Determining what value to provide for this parameter is often one of the most confusing steps of an analysis for users, and we therefore have devoted time to discussing this in the lectures and in the previous chapter. In the interest of retaining as many of the samples as possible, we’ll set our sampling depth to 10,000 for this analysis.\n\nqiime diversity core-metrics-phylogenetic \\\n  --i-phylogeny phylogeny-align-to-tree-mafft-fasttree/rooted_tree.qza \\\n  --i-table filtered-table-4.qza \\\n  --p-sampling-depth 10000 \\\n  --m-metadata-file sample-metadata.tsv \\\n  --output-dir diversity-core-metrics-phylogenetic\n\n\nAlpha diversity\nThe code below generates Alpha Diversity Boxplots as well as statistics based on the observed features. It allows us to explore the microbial composition of the samples in the context of the sample metadata.\n\nqiime diversity alpha-group-significance \\\n  --i-alpha-diversity diversity-core-metrics-phylogenetic/observed_features_vector.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --o-visualization visualizations/alpha-group-sig-obs-feats.qzv\n\nThe first thing to notice is the high variability in each individual’s richness (PatientID). The centers and spreads of the individual distributions are likely to obscure other effects, so we will want to keep this in mind. Additionally, we have repeated measures of each individual, so we are violating independence assumptions when looking at other categories. (Kruskal-Wallis is a non-parameteric test, but like most tests, still requires samples to be independent.)",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#differential-abundance-testing",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#differential-abundance-testing",
    "title": "A short QIIME tutorial",
    "section": "Differential abundance testing",
    "text": "Differential abundance testing\nANCOM can be applied to identify features that are differentially abundant (i.e. present in different abundances) across sample groups. As with any bioinformatics method, you should be aware of the assumptions and limitations of ANCOM before using it. We recommend reviewing the ANCOM paper before using this method [@mandal2015].\nDifferential abundance testing in microbiome analysis is an active area of research. There are two QIIME 2 plugins that can be used for this: q2-gneiss and q2-composition. The moving pictures tutorial uses q2-composition, but there is another tutorial which uses gneiss on a different dataset if you are interested in learning more.\nANCOM is implemented in the q2-composition plugin. ANCOM assumes that few (less than about 25%) of the features are changing between groups. If you expect that more features are changing between your groups, you should not use ANCOM as it will be more error-prone (an increase in both Type I and Type II errors is possible). If you for example assume that a lot of features change across sample types/body sites/subjects/etc then ANCOM could be good to use.",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/3_evelyn_tutorial_notes.html#sec-crunchomics",
    "href": "source/Qiime/3_evelyn_tutorial_notes.html#sec-crunchomics",
    "title": "A short QIIME tutorial",
    "section": "Analyses on crunchomics",
    "text": "Analyses on crunchomics\nIf submitting jobs via the srun command it can be useful to submit them via a screen for long running jobs. Some basics on using a screen can be found here.\n\nGet access to QIIME on crunchomics\nGeneral notice: In Evelyn’s tutorial qiime2-2021.2 was used. For this run, qiime2 was updated to a newer version and the tutorial code adjusted if needed.\nTo be able to access the amplicomics share, and the QIIME 2 installation and necessary databases, please contact Nina Dombrowski with your UvAnetID.\n\n#check what environments we already have\nconda env list\n\n#add amplicomics environment to be able to use the QIIME conda install\nconda config --add envs_dirs /zfs/omics/projects/amplicomics/miniconda3/envs/\n\n\n\nDownload the data\nIf you run this the first time, start by downloading the tutorial data.\n\n#download data data\nwget https://zenodo.org/record/5025210/files/metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\ntar -xzvf metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz?download=1\nrm metabarcoding-qiime2-datapackage-v2021.06.2.tar.gz\\?download\\=1\n\n\n\nSetting your working environment\nBelow, we set some environmental variables describing the location for key files we want to use. The only thing that you need to change yourself is the path to the working directory, ie. wdir, and the path_to_where_you_downloaded_the_data.\n\nwdir=\"/home/ndombro/tutorials/evelyn_qiime\"\ncd $wdir\n\n#make symbolic link to have the structures in the tutorial work\nln -s path_to_where_you_downloaded_the_data/metabarcoding-qiime2-datapackage-v2021.06.2/data ./\n\n#set environmental variables\nexport MANIFEST=\"data/MANIFEST.csv\"\nexport META=\"data/META.tsv\"\nexport PRIMFWD=\"GTGYCAGCMGCCGCGGTAA\"\nexport PRIMREV=\"CCGYCAATTYMTTTRAGTTT\"\nexport TRIMLENG=370\nexport TRUNKFWD=230\nexport TRUNKREV=220\n\nexport DBSEQ=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-seqs.qza\"\nexport DBTAX=\"/zfs/omics/projects/amplicomics/databases/SILVA/SILVA_138_QIIME/data/silva-138-99-tax.qza\"\nexport DBPREFIX=\"SILVA_138_99_16S\"\n\n#activate qiime environment\nmamba activate qiime2-2023.7\n\n\n\nSetup folders to store our outputs\n\nmkdir -p logs\nmkdir -p prep\nmkdir -p deblur\nmkdir -p dada2\nmkdir -p db\nmkdir -p taxonomy\n\n\n\nImport data\nWhen using Slurm we use the following settings to say that:\n\nthe job consists of 1 task (-n 1)\nthe job needs to be run on 1 cpu (–cpus-per-task 1)\n\n\nsrun -n 1 --cpus-per-task 1 qiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path $MANIFEST \\\n  --input-format PairedEndFastqManifestPhred33 \\\n  --output-path prep/demux-seqs.qza\n\n#create visualization\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/demux-seqs.qza \\\n  --o-visualization prep/demux-seqs.qzv\n\n#instructions to view qvz (tool not available, need to check if I can find it) \n#how-to-view-this-qzv prep/demux-seqs.qzv\n\nSummary of statistics:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\nPrimer removal\n\nsrun -n 1 --cpus-per-task 4 qiime cutadapt trim-paired \\\n  --i-demultiplexed-sequences prep/demux-seqs.qza \\\n  --p-front-f $PRIMFWD \\\n  --p-front-r $PRIMREV \\\n  --p-error-rate 0 \\\n  --o-trimmed-sequences prep/trimmed-seqs.qza \\\n  --p-cores 4 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-cutadapt-trim-paired.log\n\nsrun -n 1 --cpus-per-task 1 qiime demux summarize \\\n  --i-data prep/trimmed-seqs.qza \\\n  --o-visualization prep/trimmed-seqs.qzv\n\n#make a table out of this (quick, so ok to run on headnode)\nsrun -n 1 --cpus-per-task 1 qiime tools extract \\\n   --input-path prep/trimmed-seqs.qzv \\\n   --output-path visualizations/trimmed-seqs\n\nImportant:\nWhen increasing srun --cpus-per-task 1 to a higher number you need to ensure that the actual qiime command also uses the same number of cores with --p-cores. If you don’t do this the job will still run but it will be slower than it could be.\nMore information about the used command:\n\nInformation about the 2&gt;&1 is a redirection operator :\n\nIt says to “redirect stderr (file descriptor 2) to the same location as stdout (file descriptor 1).” This means that both stdout and stderr will be combined and sent to the same destination\n\nThe tee command is used to:\n\nLogging: It captures the standard output (stdout) and standard error (stderr) of the command that precedes it and saves it to a file specified as its argument. In this case, it saves the output and errors to the file logs/qiime-cutadapt-trim-paired.log.\nDisplaying Output: In addition to saving the output to a file, tee also displays the captured output and errors on the terminal in real-time. This allows you to see the progress and any error messages while the command is running.\n\nIf we would want to run the command without tee we could do 2&gt;&1 &gt; logs/qiime-cutadapt-trim-paired.log but this won’t print output to the screen\n\nSummary of run:\nforward reads   reverse reads\nMinimum     35  35\nMedian  30031.5     30031.5\nMean    31098.58    31098.58\nMaximum     100575  100575\nTotal   1554929     1554929\n\n\nFeature table construction\n\nDeblur: Join reads\nNote that for the qiime cutadapt trim-paired-command you used p-cores while here you need p-threads\n\nsrun -n 1 --cpus-per-task 4 qiime vsearch merge-pairs \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --o-merged-sequences deblur/joined-seqs.qza \\\n  --p-threads 4 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-vsearch-join-pairs.log\n\nqiime demux summarize \\\n  --i-data deblur/joined-seqs.qza \\\n  --o-visualization deblur/joined-seqs.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\nDeblur: Quality filtering\nNotice: This step cannot be sped up because the qiime quality-filter q-score command does not have an option to use more cpus or threads.\n\nsrun -n 1 --cpus-per-task 1 qiime quality-filter q-score \\\n  --i-demux deblur/joined-seqs.qza \\\n  --o-filtered-sequences deblur/filt-seqs.qza \\\n  --o-filter-stats deblur/filt-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-quality-filter-q-score.log\n\nqiime demux summarize \\\n  --i-data deblur/filt-seqs.qza \\\n  --o-visualization deblur/filt-seqs.qzv\n\nqiime metadata tabulate \\\n  --m-input-file deblur/filt-stats.qza \\\n  --o-visualization deblur/filt-stats.qzv\n\nSummary of output:\nforward reads\nMinimum     34\nMedian  26109.5\nMean    26930.02\nMaximum     52400\nTotal   1346501\n\n\nRun deblur\nBelow we:\n\nWe defined a memory allocation of 16GB. This is the total amount of RAM you expect to need for this job (+ a bit more to be on the safe side). How much you need is not always easy to predict and requires some experience/trial and error. As a rule of thumb: if you get an Out Of Memory error, double it and try again. Note that you can also define mem-per-cpu, which may be easier to work with if you often change the number of cpus between analyses.\nWe get a warning warn' method is deprecated, use 'warning' instead, but we can ignore that\n\n\n#job id: 398288\nsrun -n 1 --cpus-per-task 10 --mem=16GB qiime deblur denoise-16S \\\n  --i-demultiplexed-seqs deblur/filt-seqs.qza \\\n  --p-trim-length $TRIMLENG \\\n  --o-representative-sequences deblur/deblur-reprseqs.qza \\\n  --o-table deblur/deblur-table.qza \\\n  --p-sample-stats \\\n  --p-min-size 2 \\\n  --o-stats deblur/deblur-stats.qza \\\n  --p-jobs-to-start 10 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-deblur-denoise-16S.log\n\n#check memory requirements: 1.6GB\n#maxrss = maximum amount of memory used at any time by any process in that job\nsacct -j 398288 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\ncat deblur.log &gt;&gt; logs/qiime_deblur_denoise-16S.log && rm deblur.log\n\nqiime deblur visualize-stats \\\n  --i-deblur-stats deblur/deblur-stats.qza \\\n  --o-visualization deblur/deblur-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table deblur/deblur-table.qza \\\n  --o-visualization deblur/deblur-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data deblur/deblur-reprseqs.qza \\\n  --o-visualization deblur/deblur-reprseqs.qzv\n\n\n\nDada2\n\nDADA2 performs filtering, joining and denoising all with one single command, and therefor takes longer to run. Here, it takes ca. 30m5.334s using 8 cpus. It is tempting to just increase the number of cpus when impatient, but please note that not all commands are very efficient at running more jobs in parallel (i.e. on multiple cpus). The reason often is that the rate-limiting step in the workflow is unable to use multiple cpus, so all but one are idle.\n\n\n#job id: 398291\ntime srun -n 1 --cpus-per-task 8 --mem=32GB qiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs prep/trimmed-seqs.qza \\\n  --p-trunc-len-f $TRUNKFWD \\\n  --p-trunc-len-r $TRUNKREV \\\n  --p-n-threads 8 \\\n  --o-table dada2/dada2-table.qza \\\n  --o-representative-sequences dada2/dada2-reprseqs.qza \\\n  --o-denoising-stats dada2/dada2-stats.qza \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-dada2-denoise-paired.log\n\n#get mem usage\nsacct -j 398291 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#get summaries\nqiime metadata tabulate \\\n  --m-input-file dada2/dada2-stats.qza \\\n  --o-visualization dada2/dada2-stats.qzv\n\nqiime feature-table summarize \\\n  --i-table dada2/dada2-table.qza \\\n  --o-visualization dada2/dada2-table.qzv\n\nqiime feature-table tabulate-seqs \\\n  --i-data dada2/dada2-reprseqs.qza \\\n  --o-visualization dada2/dada2-reprseqs.qzv\n\nUsage info –&gt; 7.64878 GB needed and 50 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398291            qiime          8              00:49:55\n398291.exte+     extern          8          0   00:49:55\n398291.0          qiime          8   7648780K   00:49:55\n\n\n\nTaxonomic classification\n\nExtract reference reads\n\n#job id: 398333\ntime srun -n 1 --cpus-per-task 8 --mem=4GB qiime feature-classifier extract-reads \\\n  --i-sequences $DBSEQ \\\n  --p-f-primer $PRIMFWD \\\n  --p-r-primer $PRIMREV \\\n  --o-reads db/$DBPREFIX-ref-frags.qza \\\n  --p-n-jobs 8 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398333 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nUsage info –&gt; 1.43846 GB needed in 4 min\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398333            qiime          8              00:04:33\n398333.exte+     extern          8          0   00:04:33\n398333.0          qiime          8   1438460K   00:04:33\n\n\nTrain the classifier\nTraining a classifier takes quite some time, especially because the qiime feature-classifier fit-classifier-naive-bayes- command cannot use multiple cpus in parallel (see qiime feature-classifier fit-classifier-naive-bayes –help). Here, it took ca. 145m56.836s. You can however often re-use your classifier, provided you used the same primers and db.\n\n#job id: 398335\ntime srun -n 1 --cpus-per-task 1 --mem=32GB qiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads db/$DBPREFIX-ref-frags.qza \\\n  --i-reference-taxonomy $DBTAX \\\n  --o-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --p-verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-extract-reads.log\n\n#get mem usage\nsacct -j 398335 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\nResources needed –&gt; 33.4GB\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398335            qiime          2              02:19:24\n398335.exte+     extern          2          0   02:19:24\n398335.0          qiime          1  33469976K   02:19:24\n\n\nTaxonomic classification\nImportant\nThe classifier takes up quite some disc space. If you ‘just’ run it you are likely to get a No space left on device-error. You can avoid this by changing the directory where temporary files are written to, but make sure you have sufficient space there as well of course.\n\n#prepare tmp dir \nmkdir -p tmptmp\nexport TMPDIR=$PWD/tmptmp/\n\n#job id: 398339\ntime srun -n 1 --cpus-per-task 32 --mem=160GB qiime feature-classifier classify-sklearn \\\n  --i-classifier db/$DBPREFIX-ref-classifier.qza \\\n  --i-reads dada2/dada2-reprseqs.qza \\\n  --o-classification taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --p-n-jobs 32 \\\n  --verbose \\\n  2&gt;&1 | tee logs/qiime-feature-classifier-classify-sklearn.log\n\n#get mem usage\nsacct -j 398339 --format=\"JobID,JobName,AllocCPUs,MaxRSS,elapsed\" \n\n#cleanup\nrm -fr tmptmp\n\nqiime metadata tabulate \\\n  --m-input-file taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxonomy.qzv\n\nResource need –&gt; 158.7 GB:\n       JobID    JobName  AllocCPUS     MaxRSS    Elapsed\n------------ ---------- ---------- ---------- ----------\n398339            qiime         32              00:13:57\n398339.exte+     extern         32          0   00:13:57\n398339.0          qiime         32 158749076K   00:13:57\n\n\nPlotting\n\nqiime taxa barplot \\\n  --i-table dada2/dada2-table.qza \\\n  --i-taxonomy taxonomy/dada2-$DBPREFIX-taxonomy.qza \\\n  --m-metadata-file $META \\\n  --o-visualization taxonomy/dada2-$DBPREFIX-taxplot.qzv\n\n\n\n\nDownload qzvs\n\nscp crunchomics:'/home/ndombro/tutorials/evelyn_qiime/*/*.qzv' crunchomics_files/",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses",
      "A short QIIME tutorial"
    ]
  },
  {
    "objectID": "source/Qiime/readme.html#qiime-2",
    "href": "source/Qiime/readme.html#qiime-2",
    "title": "Bioinformatics guidance page",
    "section": "QIIME 2",
    "text": "QIIME 2\nQIIME 2 is a platform for microbial community analysis such as 16S rRNA gene amplicon analysis. The QIIME 2 website comes with a wealth of documentation and tutorials that are worthwhile to check out before running QIIME 2 on your own data:\n\nInstallation instructions\nThe QIIME 2 documentation.\nThe cancer microbiome tutorial\nq2book a very detailed, but still unfinished, tutorial on QIIME 2.\n\nAdditionally, we also provide some tutorials:\n\nA short QIIME tutorial\nAnalysing an OTU table with R",
    "crumbs": [
      "Sequence data analyses",
      "Amplicon analyses"
    ]
  }
]